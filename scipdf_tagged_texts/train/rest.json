{"text": "Maoqin @ DravidianLangTech - EACL2021 : The Application of Transformer - Based Model", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "I use the transformer - based language model with BiGRU - Attention to complete this task .", "entities": [[9, 10, "MethodName", "BiGRU"]]}
{"text": "So I choose ALBERT", "entities": [[3, 4, "MethodName", "ALBERT"]]}
{"text": "To get a more effective and higher accuracy model , BiGRU combined with attention .", "entities": [[7, 8, "MetricName", "accuracy"], [10, 11, "MethodName", "BiGRU"]]}
{"text": "There are many competitions about offensive language detection ( such as HASOC ( Chakravarthi et al , 2020c ; Mandl et al , 2020 ) and TRAC ( Kumar et al , 2018 ) ) , and many corresponding methods have been produced .", "entities": [[28, 29, "DatasetName", "Kumar"]]}
{"text": "People often tend to abstract this task into a text classification task ( Howard and Ruder , 2018 ) .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "Text classification is called extracting features from original text data and predicting the category of text data based on these features .", "entities": [[0, 2, "TaskName", "Text classification"]]}
{"text": "In the past few decades , many models for text classification have been proposed ( Qian , 2020 ) .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "From the 1960s to the 2010s , text classification models based on shallow learning dominated .", "entities": [[7, 9, "TaskName", "text classification"]]}
{"text": "Shallow learning means statistical - based models such as Naive Bayes ( NB ) , K Nearest Neighbors ( KNN ) ( Cover and Hart , 1967 ) and Support Vector Machines ( SVM ) .", "entities": [[33, 34, "MethodName", "SVM"]]}
{"text": "Compared with earlier rulebased methods , this method has obvious advantages in accuracy and stability .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "Since the 2010s , text classification has gradually changed from a shallow learning model to a deep learning model .", "entities": [[4, 6, "TaskName", "text classification"]]}
{"text": "Therefore , most of the text classification research work is based on DNN ( Yu et al , 2013 ) , which is a data - driven method with high computational complexity .", "entities": [[5, 7, "TaskName", "text classification"]]}
{"text": "The shallow learning model speeds up the text classification speed , improves the accuracy , and expands the application range of shallow learning .", "entities": [[7, 9, "TaskName", "text classification"], [13, 14, "MetricName", "accuracy"]]}
{"text": "In my job , I use the ALBERT model as my base model and take BiGRU - Attention behind it .", "entities": [[7, 8, "MethodName", "ALBERT"], [15, 16, "MethodName", "BiGRU"]]}
{"text": "The ALBERT model belongs to transformer - based language models .", "entities": [[1, 2, "MethodName", "ALBERT"]]}
{"text": "The ALBERT model is improved on the basis of Bidirectional Encoder Representations for Transformers ( BERT ) ( Devlin et al , 2018 ) model .", "entities": [[1, 2, "MethodName", "ALBERT"], [15, 16, "MethodName", "BERT"]]}
{"text": "It has designed a parameter reduction method to reduce memory consumption by changing the result of the original embedding parameter P ( the product of the vocabulary size V and the hidden layer size H ) .", "entities": [[31, 34, "HyperparameterName", "hidden layer size"]]}
{"text": "In BERT , E = H.", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "While in AL - BERT , H > >", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "E , so the number of parameters will be greatly reduced .", "entities": [[4, 7, "HyperparameterName", "number of parameters"]]}
{"text": "At the same time , the self - supervised loss is used to focus on the internal coherence in the construction of sentences .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "The ALBERT model implements three embedding layers : word embedding , position embedding , and segment embedding .", "entities": [[1, 2, "MethodName", "ALBERT"]]}
{"text": "Segment embedding helps BERT distinguish between paired input sequences .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "The BiGRU - Attention model ( Cover and Hart , 1967 ) is divided into three parts : text vector input layer , hidden layer , and output layer .", "entities": [[1, 2, "MethodName", "BiGRU"]]}
{"text": "Among them , the hidden layer consists of three layers : the BiGRU layer , the attention layer , and the Dense layer ( fully connected layer ) .", "entities": [[12, 13, "MethodName", "BiGRU"]]}
{"text": "I set the output of the ALBERT model as the input .", "entities": [[6, 7, "MethodName", "ALBERT"]]}
{"text": "After receiving the input , it uses the BiGRU neural network layer to extract features of the deep - level information of the text firstly .", "entities": [[8, 9, "MethodName", "BiGRU"]]}
{"text": "Finally , the text feature information with different weights is put into the softmax function layer for classification .", "entities": [[13, 14, "MethodName", "softmax"]]}
{"text": "The structure of the BiGRU - Attention model is shown in Figure 3 .", "entities": [[4, 5, "MethodName", "BiGRU"]]}
{"text": "In this task , I use the ALBERT model to pre - train the task .", "entities": [[7, 8, "MethodName", "ALBERT"]]}
{"text": "For the ALBERT model , the main hyperparameters I pay attention to are the training step size , batch size and learning rate .", "entities": [[2, 3, "MethodName", "ALBERT"], [15, 17, "HyperparameterName", "step size"], [18, 20, "HyperparameterName", "batch size"], [21, 23, "HyperparameterName", "learning rate"]]}
{"text": "I have obtained good performance using the ALBERT - BASE .", "entities": [[7, 8, "MethodName", "ALBERT"], [9, 10, "MethodName", "BASE"]]}
{"text": "Considering that BiGRU - Attention can capture contextual information well and extract text information features more accurately ( Radford et al , 2018 ) , I add it after AL - BERT .", "entities": [[2, 3, "MethodName", "BiGRU"], [31, 32, "MethodName", "BERT"]]}
{"text": "The standard of judgment is a weighted F1 - score , and this standard is the judgment standard used for my task .", "entities": [[7, 10, "MetricName", "F1 - score"]]}
{"text": "In this paper , I present my result on Offensive Language Identification in Dravidian Languages - EACL 2021 which includes three tasks of different languages .", "entities": [[10, 12, "TaskName", "Language Identification"]]}
{"text": "For this task , I regard it as a multiple classification task , I use the BiGRU - Attention based on the ALBERT model to complete , and my model works very well .", "entities": [[16, 17, "MethodName", "BiGRU"], [22, 23, "MethodName", "ALBERT"]]}
{"text": "At the same time , I will also consider whether I can use other transfer learning models to perform better on multi - classification tasks .", "entities": [[14, 16, "TaskName", "transfer learning"]]}
{"text": "Recent advances in GPU hardware have led to the emergence of bi - directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .", "entities": [[32, 33, "TaskName", "NER"], [43, 44, "MethodName", "CRF"]]}
{"text": "This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .", "entities": [[11, 12, "TaskName", "NER"], [35, 37, "TaskName", "structured prediction"]]}
{"text": "In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .", "entities": [[31, 37, "TaskName", "part - of - speech tagging"], [38, 41, "TaskName", "named entity recognition"], [42, 43, "TaskName", "NER"]]}
{"text": "Their computational cost grows with the number of layers , but not the input size , up to the memory and threading limitations of the hardware .", "entities": [[6, 9, "HyperparameterName", "number of layers"]]}
{"text": "This provides , for example , audio generation models that can be trained in parallel .", "entities": [[6, 8, "TaskName", "audio generation"]]}
{"text": "Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w \u2212 1 ) + 1 .", "entities": [[14, 15, "MethodName", "convolution"]]}
{"text": "The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .", "entities": [[1, 4, "HyperparameterName", "number of layers"]]}
{"text": "For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate .", "entities": [[17, 18, "MetricName", "loss"], [27, 30, "HyperparameterName", "number of parameters"]]}
{"text": "More concretely , just four stacked dilated convolutions of width 3 produces token representations with a n effective input width of 31 tokens - longer than the average sentence length ( 23 ) in the Penn TreeBank .", "entities": [[35, 37, "DatasetName", "Penn TreeBank"]]}
{"text": "When performing prediction using independent classification , the ID - CNN consistently outperforms a bidirectional LSTM ( Bi - LSTM ) , and performs on par with inference in a CRF with logits from a Bi - LSTM ( Bi - LSTM - CRF ) .", "entities": [[14, 16, "MethodName", "bidirectional LSTM"], [19, 20, "MethodName", "LSTM"], [30, 31, "MethodName", "CRF"], [37, 38, "MethodName", "LSTM"], [41, 42, "MethodName", "LSTM"], [43, 44, "MethodName", "CRF"]]}
{"text": "As an extractor of per - token logits for a CRF , our model out - performs the Bi - LSTM - CRF .", "entities": [[10, 11, "MethodName", "CRF"], [20, 21, "MethodName", "LSTM"], [22, 23, "MethodName", "CRF"]]}
{"text": "We also apply ID - CNNs to entire documents , where independent token classification is as accurate as the Bi - LSTM - CRF while decoding almost 8\u00d7 faster .", "entities": [[12, 14, "TaskName", "token classification"], [21, 22, "MethodName", "LSTM"], [23, 24, "MethodName", "CRF"]]}
{"text": "The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context - rich models .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "For example , RNN - based features require iterative passes along the length of x. We also consider a linear - chain CRF model that couples all of y together :", "entities": [[22, 23, "MethodName", "CRF"]]}
{"text": "CRF prediction explicitly reasons about interactions among neighboring output tags , whereas prediction in the first model compiles this reasoning into the feature extraction step ( Liang et al , 2008 ) .", "entities": [[0, 1, "MethodName", "CRF"]]}
{"text": "While CRF prediction requires non - trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging ( Ramshaw and Marcus , 1999 ) , will always be satisfied .", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over \u03b4 inputs at a time , where \u03b4 is the dilation width .", "entities": [[17, 18, "MethodName", "convolution"], [29, 30, "HyperparameterName", "\u03b4"], [36, 37, "HyperparameterName", "\u03b4"]]}
{"text": "We define the dilated convolution operator : c t = W c r k=0 x t\u00b1k\u03b4 .", "entities": [[3, 5, "MethodName", "dilated convolution"]]}
{"text": "( 4 ) A dilated convolution of width 1 is equivalent to a simple convolution .", "entities": [[4, 6, "MethodName", "dilated convolution"], [14, 15, "MethodName", "convolution"]]}
{"text": "By feeding the outputs of each dilated convolution as the input to the next , increasingly non - local information is incorporated into each pixel 's representation .", "entities": [[6, 8, "MethodName", "dilated convolution"]]}
{"text": "Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded .", "entities": [[5, 6, "MethodName", "convolution"]]}
{"text": "By doubling the dilation width at each layer , the size of the effective input width grows exponentially while the number of parameters grows only linearly with the number of layers , so a pixel representation quickly incorporates rich global evidence from the entire image .", "entities": [[20, 23, "HyperparameterName", "number of parameters"], [28, 31, "HyperparameterName", "number of layers"]]}
{"text": "We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate , allowing follow - on iterations to observe and resolve dependency violations .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "We denote the jth dilated convolutional layer of dilation width \u03b4 as D ( j ) \u03b4 .", "entities": [[10, 11, "HyperparameterName", "\u03b4"], [16, 17, "HyperparameterName", "\u03b4"]]}
{"text": "The first layer in the net - work is a dilation - 1 convolution D ( 0 ) 1 that transforms the input to a representation i t : i t = D ( 0 ) 1 x t ( 5 )", "entities": [[13, 14, "MethodName", "convolution"], [16, 17, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "Beginning with c t ( 0 ) = i t we define the stack of layers with the following recurrence : c t ( j ) = r D ( j\u22121 ) 2 Lc\u22121 c t ( j\u22121 ) ( 6 ) and add a final dilation - 1 layer to the stack : c t ( Lc+1 ) = r D ( Lc ) 1", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Here , maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag , with natural parameters given by Eqn . ( 9 ) : 1 T T t=1 log P ( y t | h t ( L b ) )", "entities": [[19, 21, "MethodName", "logistic regression"]]}
{"text": "( 10 ) We can also use the ID - CNN as logits for the CRF model ( Eqn . ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm .", "entities": [[15, 16, "MethodName", "CRF"]]}
{"text": "LSTMs ( Hochreiter and Schmidhuber , 1997 ) were used for NER as early as the CoNLL shared task in 2003 ( Hammerton , 2003 ; Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[11, 12, "TaskName", "NER"]]}
{"text": "More recently , a wide variety of neural network architectures for NER have been proposed .", "entities": [[11, 12, "TaskName", "NER"]]}
{"text": "al ( 2011 ) employ a one - layer CNN with pre - trained word embeddings , capitalization and lexicon features , and CRF - based prediction .", "entities": [[14, 16, "TaskName", "word embeddings"], [23, 24, "MethodName", "CRF"]]}
{"text": "Lample et al ( 2016 ) proposed two models which incorporated Bi - LSTM - composed character embeddings alongside words : a Bi - LSTM - CRF , and a greedy stack LSTM which uses a simple shift - reduce grammar to compose words into labeled entities .", "entities": [[13, 14, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"], [26, 27, "MethodName", "CRF"], [32, 33, "MethodName", "LSTM"]]}
{"text": "Their Bi - LSTM - CRF obtained the state - of - the - art on four languages without word shape or lexicon features .", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"]]}
{"text": "Ma and Hovy ( 2016 ) use CNNs rather than LSTMs to compose characters in a Bi - LSTM - CRF , achieving state - ofthe - art performance on part - of - speech tagging and CoNLL NER without lexicons .", "entities": [[18, 19, "MethodName", "LSTM"], [20, 21, "MethodName", "CRF"], [30, 36, "TaskName", "part - of - speech tagging"], [38, 39, "TaskName", "NER"]]}
{"text": "Chiu and Nichols ( 2016 ) evaluate a similar network but propose a novel method for encoding lexicon matches , presenting results on CoNLL and OntoNotes NER .", "entities": [[25, 26, "DatasetName", "OntoNotes"], [26, 27, "TaskName", "NER"]]}
{"text": "Yang et al ( 2016 ) use GRU - CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages .", "entities": [[7, 8, "MethodName", "GRU"]]}
{"text": "In general , distributed representations for text can provide useful generalization capabilities for NER systems , since they can leverage unsupervised pre - training of distributed word representations ( Turian et al , 2010 ;", "entities": [[13, 14, "TaskName", "NER"], [20, 24, "TaskName", "unsupervised pre - training"]]}
{"text": "In these NER approaches , CNNs were used for low - level feature extraction that feeds into alternative architectures .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "Overall , end - to - end CNNs have mainly been used in NLP for sentence classification , where the output representation is lower resolution than that of the input Kim ( 2014 ) ; Kalchbrenner et al ( 2014 ) ; ; Toutanova et al ( 2015 ) .", "entities": [[15, 17, "TaskName", "sentence classification"]]}
{"text": "Dilated convolutions were recently applied to the task of speech generation ( van den , and concurrent with this work , posted a pre - print describing the similar ByteNet network for machine translation that uses dilated convolutions in the encoder and decoder components .", "entities": [[32, 34, "TaskName", "machine translation"]]}
{"text": "Additionally , we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "We describe experiments on two benchmark English named entity recognition datasets .", "entities": [[7, 10, "TaskName", "named entity recognition"]]}
{"text": "On CoNLL - 2003 English NER , our ID - CNN performs on par with a Bi - LSTM not only when used to produce per - token logits for structured inference , but the ID - CNN with greedy decoding also performs on - par with the Bi - LSTM - CRF while running at more than 14 times the speed .", "entities": [[5, 6, "TaskName", "NER"], [18, 19, "MethodName", "LSTM"], [50, 51, "MethodName", "LSTM"], [52, 53, "MethodName", "CRF"]]}
{"text": "We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong Kim Sang and De Meulder , 2003 ) and OntoNotes 5.0 ( Hovy et al , 2006 ; Pradhan et al , 2006 ) .", "entities": [[23, 25, "DatasetName", "OntoNotes 5.0"]]}
{"text": "Following previous work , we use the same OntoNotes data split used for co - reference resolution in the CoNLL - 2012 shared task ( Pradhan et al , 2012 ) .", "entities": [[8, 9, "DatasetName", "OntoNotes"]]}
{"text": "As in previous work we evaluate the performance of our models using segment - level micro - averaged F1 score .", "entities": [[18, 20, "MetricName", "F1 score"]]}
{"text": "We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .", "entities": [[8, 9, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"], [24, 25, "MethodName", "CRF"], [29, 30, "MethodName", "LSTM"], [31, 32, "MethodName", "CRF"]]}
{"text": "We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "Collobert et al ( 2011 ) 86.96 Lample et al ( 2016 ) 90.33 Bi - LSTM 89.34 \u00b1 0.28 4 - layer CNN 89.97 \u00b1 0.20 5 - layer CNN 90.23 \u00b1 0.16 ID - CNN 90.32 \u00b1 0.26 Collobert et al ( 2011 ) 88.67 Passos et al ( 2014 ) 90.05 Lample et al ( 2016 )", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "90.20 Bi - LSTM - CRF ( re - impl ) 90.43 \u00b1 0.12 ID - CNN - CRF 90.54 \u00b1", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"], [18, 19, "MethodName", "CRF"]]}
{"text": "LSTM when paired with greedy decoding , suggesting that CNNs are better token encoders than Bi - LSTMs for independent logistic regression .", "entities": [[0, 1, "MethodName", "LSTM"], [20, 22, "MethodName", "logistic regression"]]}
{"text": "When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .", "entities": [[17, 18, "MethodName", "LSTM"]]}
{"text": "Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "Table 2 lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF .", "entities": [[17, 18, "MethodName", "LSTM"], [19, 20, "MethodName", "CRF"]]}
{"text": "We report decoding times using the fastest batch size for each method .", "entities": [[7, 9, "HyperparameterName", "batch size"]]}
{"text": "The ID - CNN model decodes nearly 50 % faster than the Bi - LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "With Viterbi decoding , the gap closes somewhat but the ID - CNN - CRF still comes out ahead , about 30 % faster than the Bi - LSTM - CRF .", "entities": [[14, 15, "MethodName", "CRF"], [28, 29, "MethodName", "LSTM"], [30, 31, "MethodName", "CRF"]]}
{"text": "Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as CoNLL - 2003 .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "We believe this model sees greater improvement with the addition of document - level context than the Bi - LSTM - CRF due to the ID - CNN learning a feature function better suited for representing broad context , in contrast with the Bi - LSTM which , though better than a simple RNN at encoding long memories of sequences , may reach its limit when provided with sequences more than 1 , 000 tokens long such as entire documents .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [45, 46, "MethodName", "LSTM"]]}
{"text": "In Table 6 we show that , in addition to being more accurate , our ID - CNN model is also much faster than the Bi - LSTM - CRF when incorporating context from entire documents , decoding at almost 8 times the speed .", "entities": [[27, 28, "MethodName", "LSTM"], [29, 30, "MethodName", "CRF"]]}
{"text": "On these long sequences , it also tags at more than 4.5 times the speed of the greedy Bi - LSTM , demonstrative of the benefit of our ID - CNNs context - aggregating computation that does not depend on the length of the sequence .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "We observe similar patterns on OntoNotes as we do on CoNLL . icalized greedy model of Ratinov and Roth ( 2009 ) , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of Durrett and Klein ( 2014 ) which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co - reference .", "entities": [[5, 6, "DatasetName", "OntoNotes"], [34, 35, "MethodName", "LSTM"], [58, 59, "DatasetName", "OntoNotes"], [66, 68, "TaskName", "entity linking"]]}
{"text": "Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re - implementation , which appears to be the new state - of - the - art on this dataset .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "CRF"]]}
{"text": "We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi - token named entity segment in CoNLL is about one token shorter than in OntoNotes .", "entities": [[13, 14, "DatasetName", "OntoNotes"], [43, 44, "DatasetName", "OntoNotes"]]}
{"text": "Still , our ID - CNN outperforms all other greedy methods , achieving our goal of learning a better token encoder for structured prediction .", "entities": [[22, 24, "TaskName", "structured prediction"]]}
{"text": "Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly .", "entities": [[12, 13, "DatasetName", "OntoNotes"], [18, 19, "MethodName", "LSTM"], [20, 21, "MethodName", "CRF"]]}
{"text": "For the first time , we see the score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [42, 43, "MethodName", "LSTM"], [44, 45, "MethodName", "CRF"]]}
{"text": "We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to CoNLL - 2003 : CoNLL - 2003 contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with disambiguation .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [19, 20, "DatasetName", "OntoNotes"]]}
{"text": "In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"]]}
{"text": "We are also grateful to Guillaume Lample for sharing his pretrained word embeddings .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "This work was supported in part by the Center for Intelligent Information Retrieval , in part by DARPA under agreement number FA8750 - 13 - 2 - 0020 , in part by Defense Advanced Research Agency ( DARPA ) contract number HR0011 - 15 - 2 - 0036 , in part by the National Science Foundation ( NSF ) grant number DMR - 1534431 , and in part by the National Science Foundation ( NSF ) grant number IIS - 1514053 .", "entities": [[11, 13, "TaskName", "Information Retrieval"], [17, 18, "DatasetName", "DARPA"], [37, 38, "DatasetName", "DARPA"]]}
{"text": "Validating Label Consistency in NER Data Annotation", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "Data annotation plays a crucial role in ensuring your named entity recognition ( NER ) projects are trained with the correct information to learn from .", "entities": [[9, 12, "TaskName", "named entity recognition"], [13, 14, "TaskName", "NER"]]}
{"text": "In this work , we present an empirical method to explore the relationship between label ( in - ) consistency and NER model performance .", "entities": [[21, 22, "TaskName", "NER"]]}
{"text": "It can be used to validate the label consistency ( or catch the inconsistency ) in multiple sets of NER data annotation .", "entities": [[19, 20, "TaskName", "NER"]]}
{"text": "In experiments , our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) .", "entities": [[13, 14, "DatasetName", "SCIERC"], [15, 16, "DatasetName", "CoNLL03"]]}
{"text": "Named entity recognition ( NER ) is one of the foundations of many downstream tasks such as relation extraction , event detection , and knowledge graph construction .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"], [17, 19, "TaskName", "relation extraction"], [20, 22, "TaskName", "event detection"], [25, 27, "TaskName", "graph construction"]]}
{"text": "NER models require vast amounts of labeled data to learn and identify patterns that humans can not continuously .", "entities": [[0, 1, "TaskName", "NER"]]}
{"text": "When end - to - end neural models achieve excellent performance on NER in various domains ( Lample et al , 2016 ; Liu et al , 2018 ; Luan et", "entities": [[12, 13, "TaskName", "NER"]]}
{"text": "al , 2018 ; Zeng et al , , 2021 , building useful and challenging NER benchmarks , such as CoNLL03 , WNUT16 , and SCIERC , contributes significantly to the research community .", "entities": [[15, 16, "TaskName", "NER"], [20, 21, "DatasetName", "CoNLL03"], [25, 26, "DatasetName", "SCIERC"]]}
{"text": "For example , in the CoNLL03 dataset ( Sang and De Meulder , 2003 ) , a standard NER benchmark that has been cited over 2 , 300 times , label mistakes were found in 5.38 % of the test set ( Wang et al , 2019 ) .", "entities": [[5, 6, "DatasetName", "CoNLL03"], [18, 19, "TaskName", "NER"]]}
{"text": "Another example is SCIERC ( Luan et", "entities": [[3, 4, "DatasetName", "SCIERC"]]}
{"text": "al , 2018 ) ( cited \u223c50 times ) which is a multi - task ( including NER ) benchmark in AI domain .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "When we looked at the false predictions given by SCIIE which was a multi - task model released along with the SCIERC dataset , we found that as many as 147 ( 26.7 % of the test set ) sentences were not properly annotated .", "entities": [[21, 22, "DatasetName", "SCIERC"]]}
{"text": "As shown in the experiments section , after the correction , the NER performance becomes more accurate and stable .", "entities": [[12, 13, "TaskName", "NER"]]}
{"text": "Table 1 : Three examples to compare original and corrected annotation in the test set of the SCIERC dataset .", "entities": [[17, 18, "DatasetName", "SCIERC"]]}
{"text": "We apply the SCIIE NER model on the new test set .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "Results on SCIERC show that the test set ( red ) is less predictive of training samples ( orange ) than the training set itself ( blue or green ) .", "entities": [[2, 3, "DatasetName", "SCIERC"]]}
{"text": "Experiments show that they are effective on the CoNLL03 and SCIERC datasets .", "entities": [[8, 9, "DatasetName", "CoNLL03"], [10, 11, "DatasetName", "SCIERC"]]}
{"text": "Take SCIERC as an example .", "entities": [[1, 2, "DatasetName", "SCIERC"]]}
{"text": "Then we train the SCIIE NER model ( Luan et al , 2018 ) to perform on the new test set .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "Take SCIERC as an example .", "entities": [[1, 2, "DatasetName", "SCIERC"]]}
{"text": "Here we deploy five state - of - the - art NER models to investigate their performance on the corrected SCIERC dataset .", "entities": [[11, 12, "TaskName", "NER"], [20, 21, "DatasetName", "SCIERC"]]}
{"text": "The NER models are BiLSTM - CRF ( Lample et al , 2016 ) , LM - BiLSTM - CRF ( Liu et al , 2018 ) ,", "entities": [[1, 2, "TaskName", "NER"], [4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "CRF"], [17, 18, "MethodName", "BiLSTM"], [19, 20, "MethodName", "CRF"]]}
{"text": "As shown in Table 2 , all NER models deliver better performance on the corrected SCIERC than the original dataset .", "entities": [[7, 8, "TaskName", "NER"], [15, 16, "DatasetName", "SCIERC"]]}
{"text": "NER is typically cast as a sequence labeling problem and solved by models integrate LSTMs , CRF , and language models ( Lample et al , 2016 ; Liu et al , 2018 ; Zeng et al , 2019 .", "entities": [[0, 1, "TaskName", "NER"], [16, 17, "MethodName", "CRF"]]}
{"text": "The multiple tasks include concept recognition , relation extraction , and co - reference resolution .", "entities": [[7, 9, "TaskName", "relation extraction"]]}
{"text": "The mistake re - weighting mechanism is effective in the NER task ( Wang et al , 2019 ) .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "We presented an empirical method to explore the relationship between label consistency and NER model performance .", "entities": [[13, 14, "TaskName", "NER"]]}
{"text": "It identified the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) .", "entities": [[9, 10, "DatasetName", "SCIERC"], [11, 12, "DatasetName", "CoNLL03"]]}
{"text": "It validated the label consistency in multiple sets of NER data annotation on two benchmarks , CoNLL03 and SCIERC .", "entities": [[9, 10, "TaskName", "NER"], [16, 17, "DatasetName", "CoNLL03"], [18, 19, "DatasetName", "SCIERC"]]}
{"text": "Controllable Text Simplification with Explicit Paraphrasing", "entities": [[1, 3, "TaskName", "Text Simplification"]]}
{"text": "Text Simplification improves the readability of sentences through several rewriting transformations , such as lexical paraphrasing , deletion , and splitting .", "entities": [[0, 2, "TaskName", "Text Simplification"]]}
{"text": "We introduce a new data augmentation method to improve the paraphrasing capability of our model .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "Text Simplification aims to improve the readability of texts with simpler grammar and word choices while preserving meaning ( Saggion , 2017 ) .", "entities": [[0, 2, "TaskName", "Text Simplification"]]}
{"text": "It also helps with downstream natural language processing tasks , such as parsing ( Chandrasekar et al , 1996 ) , semantic role labelling ( Vickrey and Koller , 2008 ) , information extraction ( Miwa et al , 2010 ) , and machine translation ( MT , Chen et al , 2012 ; \u0160tajner and Popovic , 2016 ) .", "entities": [[43, 45, "TaskName", "machine translation"]]}
{"text": "Since 2016 , nearly all text simplification systems have been sequence - to - sequence ( seq2seq ) Table 1 : Output statistics of 500 random sentences from the Newsela test set .", "entities": [[5, 7, "TaskName", "text simplification"], [16, 17, "MethodName", "seq2seq"], [29, 30, "DatasetName", "Newsela"]]}
{"text": "We hypothesize that the seq2seq generation model will learn lexical and structural paraphrases more efficiently from the parallel corpus , when we offload some of the burden of sentence splitting ( e.g. , split at comma ) and deletion ( e.g. , remove trailing preposition phrases ) decisions to a separate component .", "entities": [[4, 5, "MethodName", "seq2seq"]]}
{"text": "In contrast , our approach provides a more flexible and dynamic integration of linguistic rules with the neural models through ranking and data augmentation ( Figure 1 ) .", "entities": [[22, 24, "TaskName", "data augmentation"]]}
{"text": "We also demonstrate that our model can control the extent of each simplification operation by : ( 1 ) imposing a soft constraint on the percentage of words to be copied from the input in the seq2seq model , thus limiting lexical paraphrasing ; and ( 2 ) selecting candidates that underwent a desired amount of splitting and/or deletion .", "entities": [[36, 37, "MethodName", "seq2seq"]]}
{"text": "Finally , we create a new test dataset with multiple human references for Newsela ( Xu et al , 2015 ) , the widely used text simplification corpus , to specifically evaluate lexical paraphrasing .", "entities": [[13, 14, "DatasetName", "Newsela"], [25, 27, "TaskName", "text simplification"]]}
{"text": "These intermediate sentences are then used for two purposes : ( 1 ) Selected by a pairwise neural ranking model ( 2.2 ) based on the simplification quality and then rewritten by the paraphrasing component ; ( 2 ) Used for data augmentation to improve the diversity of the paraphrasing model ( 2.3 ) .", "entities": [[41, 43, "TaskName", "data augmentation"]]}
{"text": "The compression ratio is calculated as the number of words in a candidate simplification v i ( which may contain one or more sub - sentences ) divided by that of the original sentence x. To further increase the variety of generated candidates , we supplement DisSim with a Neural Deletion and Split module trained on the text simplification corpus ( 3.1 ) .", "entities": [[57, 59, "TaskName", "text simplification"]]}
{"text": "We use a Transformer seq2seq model with the same configuration as the base model for paraphrasing ( 2.3 ) .", "entities": [[3, 4, "MethodName", "Transformer"], [4, 5, "MethodName", "seq2seq"]]}
{"text": "We train the model on a standard text simplification corpus consisting of pairs of complex sentence x and manually simplified reference y. Scoring Function .", "entities": [[7, 9, "TaskName", "text simplification"]]}
{"text": "e \u2212\u03bb | | \u03c6v i \u2212\u03c6y | | \u00d7 BERT Score ( v i , y )", "entities": [[10, 11, "MethodName", "BERT"], [11, 12, "MetricName", "Score"]]}
{"text": "( 1 ) BERTScore ( Zhang et al , 2020b ) is a text similarity metric that uses BERT ( Devlin et al , 2019 ) embeddings to find soft matches between word pieces ( Wu et al , 2016 ) instead of exact string matching .", "entities": [[13, 15, "TaskName", "text similarity"], [18, 19, "MethodName", "BERT"]]}
{"text": "For the feedforward network g ( . ) , we use the following features : number of words in v i and x , compression ratio of v i with respect to x , Jaccard similarity between v i and x ,", "entities": [[2, 4, "MethodName", "feedforward network"]]}
{"text": "Our paraphrase generation model can explicitly control the extent of lexical paraphrasing by specifying the percentage of words to be copied from the input sentence as a soft constraint .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}
{"text": "We also introduce a data augmentation method to encourage our model to generate more diverse outputs .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "Our base generation model is a Transformer encoder - decoder initialized by the BERT checkpoint ( ? ) , which achieved the best reported performance on text simplification in the recent work .", "entities": [[6, 7, "MethodName", "Transformer"], [13, 14, "MethodName", "BERT"], [26, 28, "TaskName", "text simplification"]]}
{"text": ", v l ) of l words and the percentage of copying cp ( 0 , 1 ] , our goal is to paraphrase the rest of ( 1 \u2212 cp ) \u00d7 l words inv to a simpler version .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "To achieve this , we convert cp into a vector of the same dimension as BERT embeddings using Gaussian binning ( Maddela and Xu , 2018 ) and add it to the beginning of the input sequencev .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "The Transformer encoder then produces a sequence of context - aware hidden states", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "The Transformer decoder generates the output sequence fromH.", "entities": [[1, 3, "MethodName", "Transformer decoder"]]}
{"text": "We train the paraphrasing model and the copy network in a multi - task learning setup , where predicting whether a word should be copied serves as an auxiliary task .", "entities": [[11, 15, "TaskName", "multi - task learning"]]}
{"text": "When a word occurs multiple times in the input , we rely on the monolingual word alignment results from JacanaAlign ( Yao et al , 2013 ) to determine which occurrence is the one that gets copied .", "entities": [[15, 17, "TaskName", "word alignment"]]}
{"text": "We train the Transformer model and the copy network jointly by minimizing the cross - entropy loss for both decoder generation and binary word classification .", "entities": [[3, 4, "MethodName", "Transformer"], [16, 17, "MetricName", "loss"]]}
{"text": "We provide implementation and training details in Appendix A. Data Augmentation .", "entities": [[9, 11, "TaskName", "Data Augmentation"]]}
{"text": "We perform only the paraphrase generation step for paraphrase - focused simplification .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}
{"text": "We train and evaluate our models on Newsela ( Xu et al , 2015 )", "entities": [[7, 8, "DatasetName", "Newsela"]]}
{"text": "3 and Wikipedia copora ( Zhu et al , 2010 ; Woodsend and Lapata , 2011 ; Coster and Kauchak , 2011 Table 2 : Automatic evaluation results on NEWSELA - AUTO test set .", "entities": [[29, 30, "DatasetName", "NEWSELA"]]}
{"text": "We report SARI , the main automatic metric for simplification , and its three edit scores namely precision for delete ( del ) and F1 scores for add and keep operations .", "entities": [[24, 25, "MetricName", "F1"]]}
{"text": "We also report FKGL ( FK ) , average sentence length ( SLen ) , output length ( OLen ) , compression ratio ( CR ) , self - BLEU ( s - BL ) , percentage of sentence splits ( % split ) , average percentage of new words added to the output ( % new ) , and percentage of sentences identical to the input ( % eq ) .", "entities": [[29, 30, "MetricName", "BLEU"]]}
{"text": "We used the complex - simple sentence pairs automatically aligned by , called the NEWSELA - AUTO dataset .", "entities": [[14, 15, "DatasetName", "NEWSELA"]]}
{"text": "Besides Newsela , we also provide the details of experiments on Wikipedia corpus in Appendix F , which show similar trends .", "entities": [[1, 2, "DatasetName", "Newsela"]]}
{"text": "To demonstrate that our model can be controlled to generate diverse simplifications , we evaluate under the following settings : ( i ) Standard evaluation on the NEWSELA - AUTO test set similar to the methodology in the recent literature", "entities": [[27, 28, "DatasetName", "NEWSELA"]]}
{"text": "Dong et al , 2019 ; Zhang and Lapata , 2017 ) , and ( ii ) Evaluation on different subsets of the NEWSELA - AUTO test set that concentrate on a specific operation .", "entities": [[23, 24, "DatasetName", "NEWSELA"]]}
{"text": "We created a new dataset , called NEWSELA - TURK , to evaluate lexical paraphrasing .", "entities": [[7, 8, "DatasetName", "NEWSELA"]]}
{"text": "4 Similar to the WIKIPEDIA - TURK benchmark corpus ( Xu et al , 2016 ) , NEWSELA - TURK consists of human - written references focused on lexical para - phrasing .", "entities": [[17, 18, "DatasetName", "NEWSELA"]]}
{"text": "We first selected sentence pairs from the NEWSELA - AUTO test set of roughly similar length ( compression ratio between 0.8 and 1.2 ) and no sentence splits because they more likely involve paraphrasing .", "entities": [[7, 8, "DatasetName", "NEWSELA"]]}
{"text": "Then , we asked Amazon Mechanical Turk workers to simplify the complex sentence without any loss in meaning .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "We use the following simplification approaches as baselines : ( i ) BERT - Initialized Transfomer ( ? ) , where the encoder is initialized with BERT base checkpoint and the decoder is randomly initialized .", "entities": [[12, 13, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"]]}
{"text": "It is the current state - of - the - art for text simplification .", "entities": [[12, 14, "TaskName", "text simplification"]]}
{"text": "( iii ) LSTM baseline , a vanilla encoderdecoder model used in Zhang and Lapata ( 2017 ) .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "( iv ) Hybrid - NG ( Narayan and Gardent , 2014 ) , 7 one of the best existing hybrid systems that performs splitting and deletion using a probabilistic model and lexical substitution with a phrase - based machine translation system .", "entities": [[39, 41, "TaskName", "machine translation"]]}
{"text": "Table 3 : Automatic evaluation results on NEWSELA - TURK that focuses on paraphrasing ( 500 complex sentences with 4 human written paraphrases ) .", "entities": [[7, 8, "DatasetName", "NEWSELA"]]}
{"text": "The model 's deletion capability is measured by the F1 score for n - grams that are kept ( keep ) and precision for those deleted ( del ) .", "entities": [[9, 11, "MetricName", "F1 score"]]}
{"text": "9 SARI score of a reference with itself may not always be 100 as it considers 0 divided by 0 as 0 , instead of 1 , when calculating n - gram precision and recall .", "entities": [[16, 17, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [21, 22, "DatasetName", "0"]]}
{"text": "This avoids the inflation of del scores when the input is same as the output . phrasing capability and diversity , we calculate the BLEU score with respect to the input ( s - BL ) , the percentage of new words ( % new ) added , and the percentage of system outputs identical to the input ( % eq ) .", "entities": [[24, 26, "MetricName", "BLEU score"]]}
{"text": "We do not report BLEU because it often does not correlate with simplicity ( Sulem et al , 2018a , b ; Xu et", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "Table 6 : Human evaluation of 100 random simplifications from the NEWSELA - AUTO test set and the split - focused subset of the same test set .", "entities": [[11, 12, "DatasetName", "NEWSELA"]]}
{"text": "Transformer model alone is rather conservative and copies 10.2 % of the sentences directly to the output .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "We provide examples of system outputs in Table 9 and Appendix C. Tables 3 , 4 , and 5 show the results on NEWSELA - TURK , split - focused , and delete - focused subsets of NEWSELA - AUTO test set respectively .", "entities": [[23, 24, "DatasetName", "NEWSELA"], [37, 38, "DatasetName", "NEWSELA"]]}
{"text": "11 For the first one , we asked five Amazon Mechanical Turk workers to evaluate fluency , adequacy and simplicity of 100 random simplifications from the NEWSELA - AUTO test set .", "entities": [[26, 27, "DatasetName", "NEWSELA"]]}
{"text": "We supplemented the 2 - 3 readability levels in NEWSELA - AUTO , which contained more lexical overlaps and inflated the scores for EditNTS .", "entities": [[9, 10, "DatasetName", "NEWSELA"]]}
{"text": "11 We provide instructions in Appendix E. fluency and adequacy ratings with binary questions described in Zhang et al ( 2020a ) for the second evaluation over another 100 simplifications from the NEWSELA - AUTO split - focused test set .", "entities": [[32, 33, "DatasetName", "NEWSELA"]]}
{"text": "The adequacy rating is also very close to that of Transformer bert and EditNTS even though our model is performing more paraphrasing ( Table 2 ) , which verifies that the changes made by our system are meaningful .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "We evaluate our key design choices , namely candidate ranking that is based on length - penalized BERTScore and paraphrase generation that uses data augmentation and copy attention .", "entities": [[19, 21, "TaskName", "paraphrase generation"], [23, 25, "TaskName", "data augmentation"]]}
{"text": "Compared to our final model ( Our Model ) , its variants without data augmentation ( \u2212 augmentation ) and copy mechanism ( \u2212 copy attn ) suffer a drop of 1.0 and 2.6 points in SARI respectively and a decrease of at least 3.0 % of new words , which demonstrates that these components encourage the system to paraphrase .", "entities": [[13, 15, "TaskName", "data augmentation"]]}
{"text": "Our model trained on only DisSim ( \u2212 only DisSim ) and Transformer ( \u2212 only Transformer ) candidates performs close to our best model ( Our Model ) in terms of SARI .", "entities": [[12, 13, "MethodName", "Transformer"], [16, 17, "MethodName", "Transformer"]]}
{"text": "To understand the errors generated by our model , we manually classified 200 simplifications from the NEWSELA - AUTO test set into the following categories : ( a ) Good , where the model generated meaningful simplifications , ( b ) Hallucinations , where the model introduced information not in the input , ( c ) Fluency Errors , where the model generated ungrammatical output , ( d )", "entities": [[16, 17, "DatasetName", "NEWSELA"]]}
{"text": "Before the advent of neural networks , text simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules ( Carroll et al , 1999 ; Siddharthan , 2002 ; Siddharthan et al , 2004 ) or data - driven methods based on parallel corpora ( Zhu et al , 2010 ; Woodsend and Lapata , 2011 ; Narayan and Gardent , 2014 ) .", "entities": [[7, 9, "TaskName", "text simplification"]]}
{"text": "Following neural machine translation , the trend changed to performing all the operations together end - toend ( Zhang and Lapata , 2017 ; Nisioi et al , 2017 ; Zhao et al , 2018 ; Alva - Manchego et al , 2017 they discovered that the ship had been important .", "entities": [[2, 4, "TaskName", "machine translation"]]}
{"text": "LSTM experts say china 's air pollution exacts a tremendous toll on human health .", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "Transformer bert experts say china 's pollution has a tremendous effect on human health .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "al , 2019 ; at the cost of controllability and performance as shown in this paper . Controllable text simplification has been attempted before , but only with limited capability .", "entities": [[18, 20, "TaskName", "text simplification"]]}
{"text": "Kumar et al ( 2020 ) proposed a linguistic scoring function to control the edits to the input .", "entities": [[0, 1, "DatasetName", "Kumar"]]}
{"text": "Another long body of research focuses on a single simplification operation and can be broadly divided into three categories : ( 1 ) Lexical Simplification ( Specia et al , 2012 ; Horn et al , 2014 ; Glava\u0161 and \u0160tajner , 2015 ; Paetzold andSpecia , 2017 , 2015 ; Maddela and Xu , 2018 ; Qiang et al , 2020 ) , where complex words are substituted with simpler words .", "entities": [[23, 25, "TaskName", "Lexical Simplification"]]}
{"text": "( 2 ) Syntactic Simplification ( Siddharthan , 2006 ; Aharoni and Goldberg , 2018 ; Botha et al , 2018 ; Niklaus et al , 2019 ) , which deals exclusively with sentence splitting , and ( 3 ) Sentence Compression ( Filippova et al , 2015 ;", "entities": [[40, 42, "DatasetName", "Sentence Compression"]]}
{"text": "We designed a new data augmentation method to encourage the model to paraphrase .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "We created a new dataset , NEWSELA - TURK , to evaluate paraphrasing - focused simplifications .", "entities": [[6, 7, "DatasetName", "NEWSELA"]]}
{"text": "We implemented two separate Transformer models for neural deletion and split component ( 2.1 ) and paraphrase generation ( 2.3 ) using the Fairseq 12 toolkit .", "entities": [[4, 5, "MethodName", "Transformer"], [16, 18, "TaskName", "paraphrase generation"]]}
{"text": "Both the encoder and decoder follow BERT base 13 architecture , while the encoder is also initialized with BERT base checkpoint .", "entities": [[6, 7, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"]]}
{"text": "The copy attention mechanism is a feedforward network containing 3 hidden layers , 1000 nodes in each layer with tanh activation , and a single linear output node with sigmoid activation .", "entities": [[6, 8, "MethodName", "feedforward network"], [19, 21, "MethodName", "tanh activation"], [29, 31, "MethodName", "sigmoid activation"]]}
{"text": "We used BERT WordPiece tokenizer .", "entities": [[2, 3, "MethodName", "BERT"], [3, 4, "MethodName", "WordPiece"]]}
{"text": "We did not perform any hyperparameter search and directly used the hyperparameters of the BERT - initialized Transformer described in ? .", "entities": [[14, 15, "MethodName", "BERT"], [17, 18, "MethodName", "Transformer"]]}
{"text": "Our pairwise ranking model , implemented using the PyTorch framework , consists of 3 hidden layers , 100 nodes in each layer , tanh activation , and a single linear output node .", "entities": [[23, 25, "MethodName", "tanh activation"]]}
{"text": "LSTM this year , the faa has approved dozens of permits for agricultural drone businesses .", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "Transformer bert this year , the faa has approved dozens of permits for agricultural businesses .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "LSTM the room echoed with the sounds of song , the voices of young men .", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "Transformer bert the room echoed with the sound of song , the beat of drums , the voices of young men .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "Table 11 : Automatic evaluation results on a subset of Newsela test set that focuses on paraphrasing ( 8371 complexsimple sentence with compression ratio > 0.9 and no splits ) .", "entities": [[10, 11, "DatasetName", "Newsela"]]}
{"text": "36.1 2.5 67.4 38.5 11.7 20.9 22.4 1.02 6.4 63.5 13.5 0.0 Our Model 35.9 4.7 63.6 39.6 9.2 14.7 19.8 0.9 33.7 63.2 12.9 9.2 Our Model ( no split ; cp = 0.6 ) 36.5 4.9 63.2 41.4 10.8 18.6 19.9 0.89 6.7 61.9 12.4 3.9 Our Model ( no split ; cp = 0.7 ) 37 . 5 4.3 68.8 39.4 11.2 19.1 20.9 0.94 8.9 72.6 8.6 12.3 Our Model ( no split ; cp = 0.8 ) 37 . 0", "entities": [[84, 85, "DatasetName", "0"]]}
{"text": "For validation and testing purposes , we use the following two corpora : ( i ) TURK corpus ( Xu et al , 2015 ) for lexical paraphrasing and ( ii ) ASSET corpus ( Alva - Manchego et al , 2020 ) for multiple rewrite operations .", "entities": [[32, 34, "DatasetName", "ASSET corpus"]]}
{"text": "Tables 12 and 13 show the results on TURK and ASSET respectively .", "entities": [[10, 11, "DatasetName", "ASSET"]]}
{"text": "We thank Newsela for sharing the data and NVIDIA for providing GPU computing resources .", "entities": [[2, 3, "DatasetName", "Newsela"]]}
{"text": "Our system uses pairwise learning to rank methods on rich set of hand designed and representation learning features .", "entities": [[15, 17, "TaskName", "representation learning"]]}
{"text": "The system achieved second highest results on official metrics MAP and good results on other search metrics .", "entities": [[9, 10, "DatasetName", "MAP"]]}
{"text": "In online forums question answering is one of the most popular way for users to share information between each other .", "entities": [[3, 5, "TaskName", "question answering"]]}
{"text": "Along with these we also used Glove embeddings ( Pennington et al , 2014 ) which were pretrained using 6 billion tokens from Wikipedia - 2014 and Gigaword dataset .", "entities": [[6, 8, "MethodName", "Glove embeddings"]]}
{"text": "These have been studied in information retrieval literature and they power many of the industrial search engines .", "entities": [[5, 7, "TaskName", "information retrieval"]]}
{"text": "The system uses Siamese network to learn these similarity measures .", "entities": [[3, 5, "MethodName", "Siamese network"]]}
{"text": "Siamese nets were first introduced in the early 1990s by ( Bromley et al , 1993 ) to solve signature verification as an image matching problem .", "entities": [[23, 25, "TaskName", "image matching"]]}
{"text": "The weights between both the networks are shared generally , so that they project the similar texts not far in the embedding dimension .", "entities": [[21, 23, "HyperparameterName", "embedding dimension"]]}
{"text": "Figure 1 shows a siamese network , where X 1 represents the original question text and X 2 represents the candidate question text .", "entities": [[4, 6, "MethodName", "siamese network"]]}
{"text": "The euclidean distance of the vectors is used to compute the contrastive loss .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "We use following networks to generate text embedding : Long Short Term Memory LSTM ( Hochreiter and Schmidhuber , 1997 ) are popular variant of the the recurrent neural network architecture that captures the long term dependency in text and deals with vanishing gradient problem .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "For bi - directional LSTM , the hidden unit is a LSTM cell combining of various gates .", "entities": [[4, 5, "MethodName", "LSTM"], [11, 12, "MethodName", "LSTM"]]}
{"text": "Gated Recurrent Unit Gated recurrent unit ( GRU ) ( Chung et al , 2014 ) is another variant of RNN which were introduced recently as compared to LSTM .", "entities": [[0, 3, "MethodName", "Gated Recurrent Unit"], [3, 6, "MethodName", "Gated recurrent unit"], [7, 8, "MethodName", "GRU"], [28, 29, "MethodName", "LSTM"]]}
{"text": "They also have seen similar success as LSTM in various NLP tasks .", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "We use Bi - GRU as another network to generate the neural embeddings trained by siamese network similar to Bi - LSTM .", "entities": [[4, 5, "MethodName", "GRU"], [15, 17, "MethodName", "siamese network"], [21, 22, "MethodName", "LSTM"]]}
{"text": "The final hidden embedding size is 256 dimension for our Bi - GRU network also .", "entities": [[12, 13, "MethodName", "GRU"]]}
{"text": "Convolution Net We also use convolution networks as another neural network architecture to generate embeddings inside the siamese network .", "entities": [[0, 1, "MethodName", "Convolution"], [5, 6, "MethodName", "convolution"], [17, 19, "MethodName", "siamese network"]]}
{"text": "We use 1D - convolution with 128 kernels , stride of 5 followed by 1D - max pool with pool - size of 5 and finally a dense layer to create a 128 dimension vector .", "entities": [[4, 5, "MethodName", "convolution"]]}
{"text": "This gives the system the baseline accuracy of the search engine .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Apart from the neural network learned semantic features , we also employ semantic similarity between question text generated by semantic net .", "entities": [[12, 14, "TaskName", "semantic similarity"]]}
{"text": "Then the sentence similarity is obtained as a linear combination of semantic similarity and the word order similarity .", "entities": [[11, 13, "TaskName", "semantic similarity"]]}
{"text": "To generate semantic similarity , cosine between semantic vectors is obtained .", "entities": [[2, 4, "TaskName", "semantic similarity"]]}
{"text": "Word order similarity is computed in the similar way as semantic similarity but the position of word in the sentence is used to generate the word order vector .", "entities": [[10, 12, "TaskName", "semantic similarity"]]}
{"text": "The feature encodes semantic similarity and gives boost to system , shown in the results table .", "entities": [[3, 5, "TaskName", "semantic similarity"]]}
{"text": "There has been a lot of research in machine translation and summarization community to find metrics that correlate with human judgement on these tasks .", "entities": [[8, 10, "TaskName", "machine translation"], [11, 12, "TaskName", "summarization"]]}
{"text": "We compute BLEU", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "We use Latent Dirichlet al ocation ( LDA ) ( Blei et al , 2003 ) to compute topic similarity between texts .", "entities": [[7, 8, "MethodName", "LDA"]]}
{"text": "We train LDA topic model using the whole text ( body and subject ) as corpus .", "entities": [[2, 3, "MethodName", "LDA"]]}
{"text": "This formulation is more closer to ranking task than predicting relevance as regression and also has theoretical guarantees of maximizing the MAP in ranking ( Chen et al , 2009 ) .", "entities": [[21, 22, "DatasetName", "MAP"]]}
{"text": "The results generated by the system on test data were submitted as an entry to SemEval - 2017 task 3 subtask B. Our primary entry achieved second place on the MAP which was official metric for ranking .", "entities": [[30, 31, "DatasetName", "MAP"]]}
{"text": "Also it achieved highest MRR amongst all the primary submissions .", "entities": [[4, 5, "MetricName", "MRR"]]}
{"text": "Our both contrastive submissions trained on SVM achieved better test accuracy than training on Logistic Regression .", "entities": [[6, 7, "MethodName", "SVM"], [10, 11, "MetricName", "accuracy"], [14, 16, "MethodName", "Logistic Regression"]]}
{"text": "Thus the Ranking - SVM is able to generalize better .", "entities": [[4, 5, "MethodName", "SVM"]]}
{"text": "We also experimented with pointwise learning to rank method and got inferior results thus corroborating the fact that pairwise methods are helping our system in achieving better accuracy .", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "We also plan to use Triplet loss ( Hoffer and Ailon , 2015 ) which captures ranking task in better way .", "entities": [[5, 7, "MethodName", "Triplet loss"]]}
{"text": "Another direction is to use state - of - art listwise learning to rank methods that can directly optimize MAP .", "entities": [[19, 20, "DatasetName", "MAP"]]}
{"text": "Compressing BERT : Studying the Effects of Weight Pruning on Transfer Learning", "entities": [[1, 2, "MethodName", "BERT"], [10, 12, "TaskName", "Transfer Learning"]]}
{"text": "Pre - trained feature extractors , such as BERT for natural language processing and VGG for computer vision , have become effective methods for improving deep learning models without requiring more labeled data .", "entities": [[8, 9, "MethodName", "BERT"], [14, 15, "MethodName", "VGG"]]}
{"text": "We explore weight pruning for BERT and ask : how does compression during pretraining affect transfer learning ?", "entities": [[5, 6, "MethodName", "BERT"], [15, 17, "TaskName", "transfer learning"]]}
{"text": "We find that pruning affects transfer learning in three broad regimes .", "entities": [[5, 7, "TaskName", "transfer learning"]]}
{"text": "Medium levels of pruning increase the pre - training loss and prevent useful pre - training information from being transferred to downstream tasks .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "Finally , we observe that finetuning BERT on a specific task does not improve its prunability .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "We conclude that BERT can be pruned once during pre - training rather than separately for each task without affecting performance .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Pre - trained feature extractors , such as BERT ( Devlin et al , 2018 ) for natural language processing and VGG ( Simonyan and Zisserman , 2014 ) for computer vision , have become effective methods for improving the performance of deep learning models .", "entities": [[8, 9, "MethodName", "BERT"], [21, 22, "MethodName", "VGG"]]}
{"text": "In the last year , models similar to BERT have become state - of - the - art in many NLP tasks , including natural language inference ( NLI ) , named entity recognition ( NER ) , sentiment analysis , etc .", "entities": [[8, 9, "MethodName", "BERT"], [24, 27, "TaskName", "natural language inference"], [31, 34, "TaskName", "named entity recognition"], [35, 36, "TaskName", "NER"], [38, 40, "TaskName", "sentiment analysis"]]}
{"text": "Pre - trained models usually achieve higher accuracy than any model trained on downstream data alone .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Second , due to the size of the pre - training dataset , BERT models tend to be slow and require impractically large amounts of GPU memory .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "BERT - Large can only be used with access to a Google TPU , and BERT - Base requires some optimization tricks such as gradient checkpointing or gradient accumulation to be trained effectively on consumer hardware", "entities": [[0, 1, "MethodName", "BERT"], [11, 12, "DatasetName", "Google"], [15, 16, "MethodName", "BERT"], [24, 26, "MethodName", "gradient checkpointing"]]}
{"text": "Training BERT - Base from scratch costs \u223c$7k and emits \u223c1438 pounds of CO 2 ( Strubell et al , 2019 ) .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "It might also be used to trade accuracy for memory in some low - resource cases , such as deploying to smartphones for real - time prediction .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "The main questions this paper attempts to answer are : Does compressing BERT impede it 's ability to transfer to new tasks ?", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "And does fine - tuning make BERT more or less compressible ?", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "To explore these questions , we compressed English BERT using magnitude weight pruning ( Han et al , 2015 ) and observed the results on transfer learning to the General Language Understanding Evaluation ( GLUE ) benchmark , a diverse set of natural language understanding tasks including sentiment analysis , NLI , and textual similarity evaluation .", "entities": [[8, 9, "MethodName", "BERT"], [25, 27, "TaskName", "transfer learning"], [29, 30, "DatasetName", "General"], [34, 35, "DatasetName", "GLUE"], [42, 45, "TaskName", "natural language understanding"], [47, 49, "TaskName", "sentiment analysis"]]}
{"text": "We chose magnitude weight pruning , which compresses models by removing weights close to 0 , because it is one of the most fine - grained and effective compression methods and because there are many interesting ways to view pruning , which we explore in the next section .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "Medium levels of pruning increase the pre - training loss and prevent useful pre - training information from being transferred to downstream tasks .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "This information is not equally useful to each task ; tasks degrade linearly with pre - train loss , but at different rates .", "entities": [[17, 18, "MetricName", "loss"]]}
{"text": "Finally , we observe that fine - tuning BERT on a specific task does not improve its prunability or change the order of pruning by a meaningful amount .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "To our knowledge , prior work had not shown whether BERT could be compressed in a taskgeneric way , keeping the benefits of pre - training while avoiding costly experimentation associated with compressing and re - training BERT multiple times .", "entities": [[10, 11, "MethodName", "BERT"], [37, 38, "MethodName", "BERT"]]}
{"text": "Nor had it shown whether BERT could be over - pruned for a memory / accuracy trade - off for deployment to low - resource devices .", "entities": [[5, 6, "MethodName", "BERT"], [15, 16, "MetricName", "accuracy"]]}
{"text": "In this work , we conclude that BERT can be pruned prior to distribution without affecting it 's universality , and that BERT may be over - pruned during pre - training for a reasonable accuracy trade - off for certain tasks .", "entities": [[7, 8, "MethodName", "BERT"], [22, 23, "MethodName", "BERT"], [35, 36, "MetricName", "accuracy"]]}
{"text": "2 Pruning : Compression , Regularization , Architecture Search Neural network pruning involves examining a trained network and removing parts deemed to be unnecessary by some heuristic saliency criterion .", "entities": [[10, 12, "TaskName", "network pruning"]]}
{"text": "Compression Pruning a neural network decreases the number of parameters required to specify the model , which decreases the disk space required to store it .", "entities": [[7, 10, "HyperparameterName", "number of parameters"]]}
{"text": "The main difference between L0 or L1 regularization and weight pruning is that the former induce sparsity via a penalty on the loss function , which is learned during gradient descent via stochastic relaxation .", "entities": [[6, 8, "MethodName", "L1 regularization"], [22, 23, "MetricName", "loss"]]}
{"text": "Sparse Architecture Search Finally , we can view neural network pruning as a type of sparse architecture search .", "entities": [[9, 11, "TaskName", "network pruning"]]}
{"text": "Under this lens , stochastic gradient descent ( SGD ) induces network sparsity , and pruning simply makes that sparsity explicit .", "entities": [[4, 7, "MethodName", "stochastic gradient descent"], [8, 9, "MethodName", "SGD"]]}
{"text": "BERT is a large Transformer encoder ; for background , we refer readers to Vaswani et", "entities": [[0, 1, "MethodName", "BERT"], [4, 5, "MethodName", "Transformer"]]}
{"text": "BERT - Base consists of 12 encoder layers , each of which contains 6 prunable matrices : 4 for the multiheaded self - attention and 2 for the layer 's output feed - forward network .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "Recall that self - attention first projects layer inputs into key , query , and value embeddings via linear projections .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "6 We prune word embeddings in the same way we prune feed - foward networks and self - attention parameters .", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "In BERT - Base specifically , 5 The weights in almost every matrix in BERT - Base are approximately normally distributed with mean 0 and variance between 0.03 and 0.05 ( Table A ) .", "entities": [[1, 2, "MethodName", "BERT"], [14, 15, "MethodName", "BERT"], [23, 24, "DatasetName", "0"]]}
{"text": "7 Interestingly , pruning word embeddings is slightly more interpretable that pruning other matrices .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "for a heatmap of embedding magnitudes , which shows that shorter subwords tend to be pruned more than longer subwords and that certain dimensions are almost never pruned in any subword .", "entities": [[2, 3, "MethodName", "heatmap"]]}
{"text": "Our experimental code for pruning BERT , based on the public BERT repository , is available here .", "entities": [[5, 6, "MethodName", "BERT"], [11, 12, "MethodName", "BERT"]]}
{"text": "We perform weight magnitude pruning on a pretrained BERT - Base model .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "9 We select sparsities from 0 % to 90 % in increments of 10 % and gradually prune BERT to this sparsity over the first 10k steps of training .", "entities": [[5, 6, "DatasetName", "0"], [18, 19, "MethodName", "BERT"]]}
{"text": "We continue pre - training on English Wikipedia and BookCorpus for another 90k steps to regain any lost accuracy .", "entities": [[9, 10, "DatasetName", "BookCorpus"], [18, 19, "MetricName", "accuracy"]]}
{"text": "We then fine - tune these pruned models on tasks from the General Language Understanding Evaluation ( GLUE ) benchmark , which is a standard set of 9 tasks that include sentiment analysis , natural language inference , etc .", "entities": [[12, 13, "DatasetName", "General"], [17, 18, "DatasetName", "GLUE"], [31, 33, "TaskName", "sentiment analysis"], [34, 37, "TaskName", "natural language inference"]]}
{"text": "We avoid WNLI , which is known to be problematic .", "entities": [[2, 3, "DatasetName", "WNLI"]]}
{"text": "11 We also avoid tasks with less than 5k training examples because the results tend to be noisy ( RTE , MRPC , STS - B ) .", "entities": [[19, 20, "DatasetName", "RTE"], [21, 22, "DatasetName", "MRPC"], [23, 26, "DatasetName", "STS - B"]]}
{"text": "We fine - tune a separate model on each of the remaining 5 GLUE tasks for 3 epochs and try 4 learning rates : [ 2 , 3 , 4 , 5 ] \u00d7 10 \u22125 .", "entities": [[13, 14, "DatasetName", "GLUE"]]}
{"text": "Individual task results are in Table 1 . BERT can be used as a static feature - extractor or as a pre - trained model which is fine - tuned endto - end .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "In all experiments , we fine - tune weights in all layers of BERT on downstream tasks .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "Pruning involves two steps : it deletes the information stored in a weight by setting it to 0 and then regularizes the model by preventing that weight from changing during further training .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "To disentangle these two effects ( model complexity restriction and information deletion ) , we repeat the experiments from Section 3.2 with an identical pre - training setup , but instead of pruning we simply set the weights to 0 and allow them to vary during downstream training .", "entities": [[39, 40, "DatasetName", "0"]]}
{"text": "We also fine - tune on downstream tasks until training loss becomes comparable to models with no pruning .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "We might expect that BERT would be more compressible after downstream fine - tuning .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "To test this , we fine - tuned pre - trained BERT - Base on downstream data for 3 epochs .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "This makes sense from the perspective of pruning as sparse architecture search : when we initialize BERT - Base , we initialize many possible subnetworks .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "SGD selects the best one for pre - training and pushes the rest of the weights to 0 .", "entities": [[0, 1, "MethodName", "SGD"], [17, 18, "DatasetName", "0"]]}
{"text": "On one hand , pruning deletes pre - training information by setting weights to 0 , preventing the transfer of the useful inductive biases learned during pre - training .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "This leads us to believe that the amount a model can be pruned 12 We know , however , that increasing the size of BERT to BERT - Large improves performance .", "entities": [[24, 25, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"]]}
{"text": "This may be caused by dropout , or it may be a general property of our training regime ( SGD ) .", "entities": [[19, 20, "MethodName", "SGD"]]}
{"text": "We might consider finding a lottery ticket for BERT , which we would expect to fit the GLUE training data just as well as pre - trained BERT .", "entities": [[8, 9, "MethodName", "BERT"], [17, 18, "DatasetName", "GLUE"], [27, 28, "MethodName", "BERT"]]}
{"text": "Notice that information deletion fits the training data better than un - pruned models at all sparsity levels but does not fully recover evaluation accuracy .", "entities": [[24, 25, "MetricName", "accuracy"]]}
{"text": "Also , models pruned after downstream fine - tuning have the same or worse development accuracy , despite achieving lower training losses .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "Note : none of the pruned models are overfitting because un - pruned models have the lowest training loss and the highest development accuracy .", "entities": [[18, 19, "MetricName", "loss"], [23, 24, "MetricName", "accuracy"]]}
{"text": "We believe the slope of each line tells us how much a bit of BERT is worth to each task .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "( CoLA at 90 % is excluded from the line of best fit . )", "entities": [[1, 2, "DatasetName", "CoLA"]]}
{"text": "Features are extracted from activations of all 12 layers of BERT and compared layer - wise to a model that has not been pruned .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "However , these models do not recover all evaluation accuracy , despite matching un - pruned model 's training loss .", "entities": [[9, 10, "MetricName", "accuracy"], [19, 20, "MetricName", "loss"]]}
{"text": "Table 1 shows that on the MNLI and QQP tasks , which have the largest amount of training data , information deletion performs much better than pruning .", "entities": [[6, 7, "DatasetName", "MNLI"], [8, 9, "DatasetName", "QQP"]]}
{"text": "In contrast , models do not recover as well on SST - 2 and CoLA , which have less data .", "entities": [[10, 11, "DatasetName", "SST"], [14, 15, "DatasetName", "CoLA"]]}
{"text": "We 've seen that over - pruning BERT deletes information useful for downstream tasks .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "We might consider the pre - training loss as a proxy for how much pre - training information we 've deleted in total .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "For every bit of information we delete from BERT , it appears only a fraction is useful for CoLA , and an even smaller fraction useful for QQP .", "entities": [[8, 9, "MethodName", "BERT"], [18, 19, "DatasetName", "CoLA"], [27, 28, "DatasetName", "QQP"]]}
{"text": "This relationship should be taken into account when considering the memory / accuracy trade - off of overpruning .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "Pruning an extra 30 % of BERT 's weights Figure 3 : ( Top ) The measured difference in pruning masks between models pruned during pre - training and models pruned during downstream fine - tuning .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "It 's unclear , however , whether this is because the pre - training task is less relevant to QQP or whether QQP simply has a bigger dataset with more information content .", "entities": [[19, 20, "DatasetName", "QQP"], [22, 23, "DatasetName", "QQP"]]}
{"text": "Table 2 shows that the magnitude sorting order of weights is mostly preserved ; weights move on average 0 - 4 % away from their starting positions in the sort order .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "Compressing BERT for Specific Tasks Section 5 showed that downstream fine - tuning does not increase prunability .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "Tang et al ( 2019 ) used BERT as a knowledge distillation teacher to compress relevant information into smaller Bi - LSTMs , while Kuncoro et al ( 2019 ) took a similar distillation approach .", "entities": [[7, 8, "MethodName", "BERT"], [10, 12, "MethodName", "knowledge distillation"]]}
{"text": "While fine - tuning does not increase prunability , task - specific knowledge might be extracted from BERT with other methods .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "They show redundancy in BERT after fine - tuning on a single downstream task ; in contrast , our work emphasizes the interplay between compression and transfer learning to many tasks , pruning both before and after finetuning .", "entities": [[4, 5, "MethodName", "BERT"], [26, 28, "TaskName", "transfer learning"]]}
{"text": "Also , magnitude weight pruning allows us to additionally prune the feed - foward networks and sub - word embeddings in BERT ( not just selfattention ) , which account for \u223c72 % of BERT 's total memory usage .", "entities": [[18, 20, "TaskName", "word embeddings"], [21, 22, "MethodName", "BERT"], [34, 35, "MethodName", "BERT"]]}
{"text": "We suspect that attention head pruning and weight pruning remove different redundancies from BERT .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "We 've shown that encoding BERT 's inductive bias requires many more weights than are required to fit downstream data .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Future work on compressing pre - trained models should focus on maintaining that inductive bias and quantifying its relevance to various tasks during accuracy / memory trade - offs .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "For magnitude weight pruning , we 've shown that 30 - 40 % of the weights do not encode any useful inductive bias and can be discarded without affecting BERT 's universality .", "entities": [[30, 31, "MethodName", "BERT"]]}
{"text": "It 's reasonable to believe that these conclusions will generalize to other pre - trained language models such as Kermit ( Chan et al , 2019 ) , XLNet ( Yang et al , 2019 ) ,", "entities": [[28, 29, "MethodName", "XLNet"]]}
{"text": "GPT - 2 ( Radford et al , 2019 ) , RoBERTa ( Liu et al , 2019a ) or ELMO ( Peters et al , 2018 ) .", "entities": [[0, 1, "MethodName", "GPT"], [11, 12, "MethodName", "RoBERTa"], [20, 21, "MethodName", "ELMO"]]}
{"text": "All of these learn some variant of language modeling , and most use Transformer architectures .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "Figure 5 : The sum of weights pruned at each sparsity level for one shot pruning of BERT .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "Given the motivation for our saliency criterion , it seems strange that such a large magnitude of weights can be pruned without decreasing accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "LR MNLI QQP QNL SST - 2 CoLA 2e - 5 1.91 \u00b1 1.81 1.82 \u00b1 1.72 1.27 \u00b1 1.22 1.06 \u00b1 1.03 0.79 \u00b1 0.77 3e - 5 2.68 \u00b1 2.51 2.56 \u00b1 2.40 1.79 \u00b1 1.69 1.54 \u00b1 1.47 1.06 \u00b1 1.03 4e - 5 3.41 \u00b1 3.18 3.30 \u00b1 3.10 2.31 \u00b1 2.19 1.99 \u00b1 1.89 1.11 \u00b1 1.09 5e - 5 4.12 \u00b1 3.83 4.02 \u00b1 3.74 2.77 \u00b1 2.62 2.38 \u00b1 2.29 1.47 \u00b1 1.43 Table 2 : We compute the magnitude sorting order of each weight before and after downstream fine - tuning .", "entities": [[1, 2, "DatasetName", "MNLI"], [2, 3, "DatasetName", "QQP"], [4, 5, "DatasetName", "SST"], [7, 8, "DatasetName", "CoLA"]]}
{"text": "Sorting order changes mostly locally across tasks : a weight moves , on average , 0 - 4 % away from its starting position .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "This plot is nearly identical for every model and learning rate , so we only show it once .", "entities": [[9, 11, "HyperparameterName", "learning rate"]]}
{"text": "Figure 7 : A heatmap of the weight magnitudes of the 12 horizontally stacked self - attention key projection matrices for layer 1 .", "entities": [[4, 5, "MethodName", "heatmap"]]}
{"text": "( Right ) A heatmap of the weight magnitudes of BERT 's subword embeddings .", "entities": [[4, 5, "MethodName", "heatmap"], [10, 11, "MethodName", "BERT"]]}
{"text": "Interestingly , pruning BERT embeddings are more interpretable ; we can see shorter subwords ( top rows ) have smaller magnitude values and thus will be pruned earlier than other subword embeddings .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Weight Mean Weight STD embeddings word embeddings - 0.0282 0.042 layer 0 attention output FC - 0.0000 0.029 layer 0 self attn key 0.0000 0.043 layer 0 self attn query 0.0000 0.043 layer 0 self attn value - 0.0000 0 .", "entities": [[5, 7, "TaskName", "word embeddings"], [11, 12, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [39, 40, "DatasetName", "0"]]}
{"text": "Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask", "entities": [[0, 2, "TaskName", "Question Generation"], [3, 5, "TaskName", "Reading Comprehension"]]}
{"text": "However , many existing Question Generation ( QG ) systems focus on generating literal questions from the text , and have no way to control the type of the generated question .", "entities": [[4, 6, "TaskName", "Question Generation"]]}
{"text": "In this paper , we study QG for reading comprehension where inferential questions are critical and extractive techniques can not be used .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}
{"text": "We propose a new reading comprehension dataset that contains questions annotated with story - based reading comprehension skills ( SBRCS ) , allowing for a more complete reader assessment .", "entities": [[4, 6, "TaskName", "reading comprehension"], [15, 17, "TaskName", "reading comprehension"]]}
{"text": "Thus , to improve the educational process , and lighten the load on teachers , we need tools to automate Question Generation ( QG ) : the task of writing questions for a given passage .", "entities": [[20, 22, "TaskName", "Question Generation"]]}
{"text": "Previous works focused on this aspect of the questions in reading comprehension and discarded the comprehension skills ( e.g. close reading , predicting , figurative language , etc . )", "entities": [[10, 12, "TaskName", "reading comprehension"]]}
{"text": "We take inspiration from continual learning ( Parisi et al , 2019 ) , which orders a set of learning tasks to improve model performance .", "entities": [[4, 6, "TaskName", "continual learning"]]}
{"text": "This paper focuses on the generation of questions for story - based reading comprehension skills ( SBRCS ) , which are varied and cover many aspects of reading comprehension .", "entities": [[12, 14, "TaskName", "reading comprehension"], [27, 29, "TaskName", "reading comprehension"]]}
{"text": "Although our aim in creating this dataset is to enrich educational applications , this dataset can be considered as a source for general QG and question answering ( QA ) systems in NLP .", "entities": [[25, 27, "TaskName", "question answering"]]}
{"text": "The dataset contains advanced reading comprehension skills extracted from stories .", "entities": [[4, 6, "TaskName", "reading comprehension"]]}
{"text": "We demonstrate the efficiency of the proposed method after extensive experiments , and we investigate its performance in a few - shot learning setting .", "entities": [[19, 23, "TaskName", "few - shot learning"]]}
{"text": "Many different QG models have been proposed , starting for simple vanilla Sequence to Sequence Neural Networks models ( seq2seq ) Yuan et", "entities": [[12, 15, "MethodName", "Sequence to Sequence"], [19, 20, "MethodName", "seq2seq"]]}
{"text": "Despite the recent efforts for building reading comprehension QA datasets , to the best of our knowledge , none of the available datasets explored SBRCS .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}
{"text": "Additionally , we use a extensive set of reading comprehension skills that deeply evaluates the abilities of the readers ( e.g. imagination skill by Visualizing ) .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}
{"text": "This content variety leads annotators towards asking non - localized questions that test for more advanced reading comprehension skills .", "entities": [[16, 18, "TaskName", "reading comprehension"]]}
{"text": "Given the fact that including more data in a reading comprehension system is important for gen - eralization ( Chung et al , 2018 ; Talmor and Berant , 2019 ) , and given that our created dataset has the SBRCS which are missed in previous datasets , we propose a two - steps method to generate skillrelated questions from a given story : HTA followed by WTA .", "entities": [[9, 11, "TaskName", "reading comprehension"]]}
{"text": "We use two well - known datasets , SQuAD ( Rajpurkar et al , 2016 ) and Cos - mosQA ( Huang et al , 2019 ) .", "entities": [[8, 9, "DatasetName", "SQuAD"]]}
{"text": "For the generation model , we use the pre - trained Text - to - Text Transfer Transformer T5 ( Raffel et al , 2020 ) , which closely follows the encoder - decoder architecture of the transformer model ( Vaswani et al , 2017 ) .", "entities": [[17, 18, "MethodName", "Transformer"], [18, 19, "MethodName", "T5"]]}
{"text": "T5 is a SOTA model on multiple tasks , including QA .", "entities": [[0, 1, "MethodName", "T5"]]}
{"text": "Previous works showed that incorporating more data when training a reading comprehension model improves performance and generalizability ( Chung et al , 2018 ; Talmor and Berant , 2019 ) .", "entities": [[10, 12, "TaskName", "reading comprehension"]]}
{"text": "Thus , we train a T5 model on SQuAD and CosmosQA datasets to teach the model how to ask questions .", "entities": [[5, 6, "MethodName", "T5"], [8, 9, "DatasetName", "SQuAD"], [10, 11, "DatasetName", "CosmosQA"]]}
{"text": "Previous neural question generation models take the passage as input , along with the answer .", "entities": [[2, 4, "TaskName", "question generation"]]}
{"text": "However , this distribution can only be learned using an extractive dataset ( e.g. SQuAD ) ; the model can not learn to generate inferential questions .", "entities": [[14, 15, "DatasetName", "SQuAD"]]}
{"text": "QG often uses standard evaluation metrics from text summarization and machine translation ( BLEU ( Papineni et", "entities": [[7, 9, "TaskName", "text summarization"], [10, 12, "TaskName", "machine translation"], [13, 14, "MetricName", "BLEU"]]}
{"text": "al , 2002 ) , ROUGE ( Lin , 2004 ) , METEOR ( Banerjee and Lavie , 2005 ) , etc . ) .", "entities": [[12, 13, "DatasetName", "METEOR"]]}
{"text": "BLEURT is a BERT - based model that uses multi - task learning to evaluate a generated text by giving it a value mostly between 0.0 and 1.0 .", "entities": [[3, 4, "MethodName", "BERT"], [9, 13, "TaskName", "multi - task learning"]]}
{"text": "We tested the T5 - large model , but we did not notice any improvements considering BLEURT metric .", "entities": [[3, 4, "MethodName", "T5"]]}
{"text": "We use a single NVIDIA TITAN RTX with 24 G RAM .", "entities": [[5, 6, "DatasetName", "TITAN"], [10, 11, "MethodName", "RAM"]]}
{"text": "For HTA , we validate on a combined version of the validation sets from both datasets ( SQuAD and CosmosQA ) .", "entities": [[17, 18, "DatasetName", "SQuAD"], [19, 20, "DatasetName", "CosmosQA"]]}
{"text": "For all of the following baselines , we use SQuAD , CosmosQA , and the collected dataset for training and we test on the test part of the collected dataset : Vanilla Seq2seq ( Sutskever et al , 2014 ) : a basic encoder - decoder sequence learning system for machine translation .", "entities": [[9, 10, "DatasetName", "SQuAD"], [11, 12, "DatasetName", "CosmosQA"], [32, 33, "MethodName", "Seq2seq"], [50, 52, "TaskName", "machine translation"]]}
{"text": "NQG - Seq : another Seq2seq that implements an attention layer on top of a bidirectional - LSTM encoder .", "entities": [[5, 6, "MethodName", "Seq2seq"], [17, 18, "MethodName", "LSTM"]]}
{"text": "NQG - Max ( Zhao et al , 2018 ) 8 : a QG system with a maxout pointer mechanism and gated self - attention LSTM - based encoder to address the challenges of processing long text input .", "entities": [[17, 18, "MethodName", "maxout"], [25, 26, "MethodName", "LSTM"]]}
{"text": "CGC - QG ( Liu et al , 2019a ) : a Clue Guided Copy network for Question Generation , which is a sequence - to - sequence generative model with a copying mechanism that takes a passage and an answer ( as a span in the text ) and generate the question .", "entities": [[17, 19, "TaskName", "Question Generation"]]}
{"text": "The text representation in the encoder ( GRU network ) is represented using a variety of features such as GloVe vectors , POS information , answer position , clue word , etc . AnswerQuest ( Roemmele et al , 2021 ) : a pipeline model that uses as a first step a previous model to retrieve the relevant sentence that has the answer from a document .", "entities": [[7, 8, "MethodName", "GRU"], [19, 20, "MethodName", "GloVe"]]}
{"text": "One - Step : a baseline that uses T5 model trained with all data in one step instead of having separate HTA and WTA steps .", "entities": [[8, 9, "MethodName", "T5"]]}
{"text": "T5 - WTA : the WTA model trained using T5 model as a seed model .", "entities": [[0, 1, "MethodName", "T5"], [9, 10, "MethodName", "T5"]]}
{"text": "For all of the previous baselines that require the answer to be a sub - span in the passage , we use the semantic text similarity method that was proposed in ( Ghanem et al , 2019 ) to retrieve the most similar span in the passage .", "entities": [[24, 26, "TaskName", "text similarity"]]}
{"text": "In this work , we replace the ngrams features of a text with embeddings extracted from RoBERTa model ( Liu et al , 2019b ) .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}
{"text": "We can see that out of the baselines , T5 - WTA performs best in terms of BLEURT score ( 32.96 % ) , followed by NQG - Max with a value of 31.78 % .", "entities": [[9, 10, "MethodName", "T5"]]}
{"text": "Regarding the generated questions type , in Table 3 we show the performance of the T5 - based models per question type ( inferential and literal ) .", "entities": [[15, 16, "MethodName", "T5"]]}
{"text": "We see a similar scenario when comparing One - Step and T5 - WTA models , yet , the gap is smaller .", "entities": [[11, 12, "MethodName", "T5"]]}
{"text": "The T5 model was able to learn how to quote the proper segment of the passage when generating questions .", "entities": [[1, 2, "MethodName", "T5"]]}
{"text": "The One - Step model performs similarly to the baselines , although it has been trained using the T5 model and on all three datasets .", "entities": [[18, 19, "MethodName", "T5"]]}
{"text": "We use a \" master \" qualification criteria to restrict the participation of workers in our evaluation study to those who have a high historical HIT accuracy , and workers are required to be located in an English speaking country .", "entities": [[26, 27, "MetricName", "accuracy"]]}
{"text": "We can clearly see a large gap in accuracy between both models , and this becomes clear with the skills that have a low number of instances in the dataset ( e.g. Figurative Language , Predicting , etc . ) .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "The result at 10 % ( 33.21 % ) exceeds the results of most of the baselines and is higher than T5 - WTA and NQG - MAX models when trained on all the datasets ( see Table 2 ) .", "entities": [[21, 22, "MethodName", "T5"]]}
{"text": "In this paper , we presented a new reading comprehension dataset to assess reading skills using stories .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}
{"text": "Additionally , we manually examined the stories and the created questions to ensure there are no privacy or ethical concerns , e.g. , toxic language , hate speech , or any bias against underrepresented groups .", "entities": [[26, 28, "DatasetName", "hate speech"]]}
{"text": "This skill requires advanced reading comprehension ability from the reader since its answers can not be extracted directly from the story text , where inferential skills are needed .", "entities": [[4, 6, "TaskName", "reading comprehension"]]}
{"text": "The Readers and Writers Workshop curricula were highly instrumental to us in breaking reading comprehension into sub - skills .", "entities": [[13, 15, "TaskName", "reading comprehension"]]}
{"text": "In addition to the collected dataset , we use two well - known datasets , SQuAD and CosmosQA .", "entities": [[15, 16, "DatasetName", "SQuAD"], [17, 18, "DatasetName", "CosmosQA"]]}
{"text": "SQuAD", "entities": [[0, 1, "DatasetName", "SQuAD"]]}
{"text": "A reading comprehension dataset , consists of questions created by crowdworkers on a set of Wikipedia articles that cover a large set of topics ( from musical celebrities to abstract concepts ) , where the answer to every question is a span from the corresponding reading passage ( Rajpurkar et al , 2016 ) .", "entities": [[1, 3, "TaskName", "reading comprehension"]]}
{"text": "In this work , we use SQuAD 2.0 version with discarding the questions that have no answers .", "entities": [[6, 7, "DatasetName", "SQuAD"]]}
{"text": "CosmosQA", "entities": [[0, 1, "DatasetName", "CosmosQA"]]}
{"text": "It is another reading comprehension dataset consisting of 35.6 K paragraph / question pairs that require commonsense - based reading comprehension .", "entities": [[3, 5, "TaskName", "reading comprehension"], [19, 21, "TaskName", "reading comprehension"]]}
{"text": "To quantify the impact of including the skill name token , we run T5 - WTA without including the skill name token ( T5 - WTA - unskilled ) .", "entities": [[13, 14, "MethodName", "T5"], [23, 24, "MethodName", "T5"]]}
{"text": "We compare the T5 - WTAunskilled to the One - Step model ; the only difference between these models is that One - Step model includes SQuAD and CosmosQA datasets in the training data .", "entities": [[3, 4, "MethodName", "T5"], [26, 27, "DatasetName", "SQuAD"], [28, 29, "DatasetName", "CosmosQA"]]}
{"text": "T5 - WTA - unskilled BLEURT performance is lower than the BLEURT scores of the other two models .", "entities": [[0, 1, "MethodName", "T5"]]}
{"text": "Error Analysis .", "entities": [[0, 1, "MetricName", "Error"]]}
{"text": "In the following , we elaborate more on the reading comprehension skills :", "entities": [[9, 11, "TaskName", "reading comprehension"]]}
{"text": "We do not experiment with One - Step model as we need to sample SQuAD and Cos - mosQA datasets when we sample the collected data ; it is hard to set up a fair comparison here as , for instance , sampling 10 % of SQuAD dataset is larger than the whole collected dataset .", "entities": [[14, 15, "DatasetName", "SQuAD"], [46, 47, "DatasetName", "SQuAD"]]}
{"text": "We present ADVISER 1 - an open - source , multi - domain dialog system toolkit that enables the development of multi - modal ( incorporating speech , text and vision ) , sociallyengaged ( e.g. emotion recognition , engagement level prediction and backchanneling ) conversational agents .", "entities": [[36, 38, "TaskName", "emotion recognition"]]}
{"text": "Dialog systems or chatbots , both text - based and multi - modal , have received much attention in recent years , with an increasing number of dialog systems in both industrial contexts such as Amazon Alexa , Apple Siri , Microsoft Cortana , Google Duplex , XiaoIce ( Zhou", "entities": [[44, 45, "DatasetName", "Google"]]}
{"text": "The purpose we envision for dialog systems developed using our toolkit is not the same as the objective of a social chatbot such as XiaoIce ( Zhou et al , 2018 ) .", "entities": [[21, 22, "TaskName", "chatbot"]]}
{"text": "The main objective of this work is to develop a multi - domain dialog system toolkit that allows for multi - modal information processing and that provides different modules for extracting social signals such as emotional states and for integrating them into the decision making process .", "entities": [[43, 45, "TaskName", "decision making"]]}
{"text": "By assigning computationally heavy tasks such as speech recognition and speech synthesis to a more powerful computing node , it is possible to reduce differences in latency when processing different modalities , therefore achieving more natural interactions .", "entities": [[7, 9, "TaskName", "speech recognition"], [10, 12, "TaskName", "speech synthesis"]]}
{"text": "We present the three modules of ADVISER for processing social signals : ( a ) emotion recognition , ( b ) engagement level prediction , and ( c ) backchanneling .", "entities": [[15, 17, "TaskName", "emotion recognition"]]}
{"text": "Figure 1 illustrates an example of our system tracking emotion states and engagement levels .", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "Therefore , the emotion recognition module can subscribe to the particular input streams of interest ( see section 4 for details ) and apply emotion prediction either in a time - continuous fashion or discretely per turn .", "entities": [[3, 5, "TaskName", "emotion recognition"], [24, 25, "DatasetName", "emotion"]]}
{"text": "In our example implementation in the toolkit , we integrate speech emotion recognition , i.e. using the acoustic signal as features .", "entities": [[10, 13, "TaskName", "speech emotion recognition"]]}
{"text": "The models are trained on the IEMOCAP dataset ( Busso et al , 2008 ) .", "entities": [[6, 7, "DatasetName", "IEMOCAP"]]}
{"text": "The output of the emotion recognition module consists of three predictions per user turn , which can then be used by the user state tracker ( see section 3.4 ) .", "entities": [[4, 6, "TaskName", "emotion recognition"]]}
{"text": "Automatic Speech Recognition ( ASR )", "entities": [[0, 3, "TaskName", "Automatic Speech Recognition"]]}
{"text": "The speech recognition module receives a speech signal as input , which can come from an internal or external microphone , and outputs decoded text .", "entities": [[1, 3, "TaskName", "speech recognition"]]}
{"text": "We provide an end - to - end ASR model for English based on the Transformer neural network architecture .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "We use the end - to - end speech processing toolkit ESPnet ( Watanabe et al , 2018 ) and the IMS - speech English multi - dataset recipe ( Denisov and Vu , 2019 ) , updated to match the LibriSpeech Transformer - based system in ESPnet ( Karita et al , 2019 ) and to include more training data .", "entities": [[11, 12, "MethodName", "ESPnet"], [41, 42, "DatasetName", "LibriSpeech"], [42, 43, "MethodName", "Transformer"], [47, 48, "MethodName", "ESPnet"]]}
{"text": "Training data comprises the LibriSpeech , Switchboard , TED - LIUM 3 , AMI , WSJ , Common Voice 3 , SWC , VoxForge and M - AILABS datasets with a total amount of 3249 hours .", "entities": [[4, 5, "DatasetName", "LibriSpeech"], [8, 12, "DatasetName", "TED - LIUM 3"], [17, 19, "DatasetName", "Common Voice"], [23, 24, "DatasetName", "VoxForge"]]}
{"text": "Speech Synthesis For ADVISER 's voice output , we use the ESPnet - TTS toolkit , which is an extension of the ESPnet toolkit mentioned above .", "entities": [[0, 2, "TaskName", "Speech Synthesis"], [11, 12, "MethodName", "ESPnet"], [22, 23, "MethodName", "ESPnet"]]}
{"text": "We use FastSpeech as the synthesis model speeding up mel - spectrogram generation by a factor of 270 and voice generation by a factor of 38 compared to autoregressive Transformer TTS ( Ren et al , 2019 ) .", "entities": [[29, 30, "MethodName", "Transformer"]]}
{"text": "We use a Parallel Wave - GAN ( Yamamoto et al , 2020 ) to generate waveforms that is computationally efficient and achieves a high mean opinion score of 4.16 .", "entities": [[6, 7, "MethodName", "GAN"]]}
{"text": "The FastSpeech and WaveGAN models were trained with 24 hours of the LJSpeech dataset from a single speaker ( Ito , 2017 ) and are capable of generating voice output in real - time when using a GPU .", "entities": [[3, 4, "MethodName", "WaveGAN"], [12, 13, "DatasetName", "LJSpeech"]]}
{"text": "The natural language understanding ( NLU ) unit parses the textual user input ( De Mori et al , 2008 ) - or the output from the speech recognition systemand extracts the user action type , generally referred to as intent in goal - oriented dialog systems ( e.g. Inform and Request ) , as well as the corresponding slots and values .", "entities": [[1, 4, "TaskName", "natural language understanding"], [27, 29, "TaskName", "speech recognition"], [42, 46, "TaskName", "goal - oriented dialog"]]}
{"text": "In the current implementation , the user state consists of the user 's engagement level , valence , arousal , and emotion category ( details in section 3.1 ) .", "entities": [[21, 22, "DatasetName", "emotion"]]}
{"text": "For off - policy batch - training , we make use of prioritized experience replay ( Schaul et al , 2015 ) .", "entities": [[12, 15, "MethodName", "prioritized experience replay"]]}
{"text": "As this policy is domain - agnostic , predicting the next system emotion output rather than the next system action , it can be used alongside any of the previously mentioned policies .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "At runtime , the utterance is then generated from the template associated with the current system emotion and system action .", "entities": [[16, 17, "DatasetName", "emotion"]]}
{"text": "To make the dialog multi - modal , speech and vision modules are introduced next , along with modules to extract engagement and emotion .", "entities": [[23, 24, "DatasetName", "emotion"]]}
{"text": "It further offers audio and visual processing , such as speech recognition and face tracking , as well as output , such as synthesis and avatar rendering .", "entities": [[10, 12, "TaskName", "speech recognition"]]}
{"text": "And the MuMMER ( multimodal Mall Entertainment Robot ) project ( Foster et al , 2016 ) is based on the SoftBank Robotics Pepper platform , and thereby comprises processing of audio - , visual - and social signals , with the aim to develop a socially engaging robot that can be deployed in public spaces .", "entities": [[5, 6, "DatasetName", "Mall"]]}
{"text": "Currently , all the Latin treebanks except the llct are available also in the Universal Dependencies collection ( UD ) ( Nivre et al , 2016 ) .", "entities": [[14, 16, "DatasetName", "Universal Dependencies"], [18, 19, "DatasetName", "UD"]]}
{"text": "9 In the treebank area , the UD collection includes more than 100 treebanks sharing the same annotation guidelines and provides different tools for querying the treebanks on - line .", "entities": [[7, 8, "DatasetName", "UD"]]}
{"text": "The Lemma is the key node type in LiLa .", "entities": [[1, 2, "DatasetName", "Lemma"]]}
{"text": "A Lemma is an ( inflected ) Form conventionally chosen as the citation form of a lexical item .", "entities": [[1, 2, "DatasetName", "Lemma"]]}
{"text": "Interoperability can be achieved by linking the entries in lexical resources and the corpus tokens pointing to the same lemma .", "entities": [[19, 20, "DatasetName", "lemma"]]}
{"text": "17 Lemlat relies on a lexical basis resulting from the collation of three Latin dictionaries ( Georges andGeorges , 1913 1918 ; Glare , 1982 ; Gradenwitz , 1904 ) for a total of 40 , 014 lexical entries and 43 , 432 lemmas , as more than one lemma can be included in one lexical entry .", "entities": [[49, 50, "DatasetName", "lemma"]]}
{"text": "The current prototype of the LiLa RDF triplestore database connects the following resources for Latin : ( a ) the collection of lemmas provided by Lemlat , ( b ) the wfl lexicon , and ( c ) three treebanks ( four by version ) : ( c.1 ) pro\u0131el in its UD version ( release 2.3 ) , ( c.2 - 3 ) the \u0131t - tb in both its UD 2.3 and original version , and ( c.4 ) a selection of 3 , 900 sentences ( 105 , 380 tokens ) of the llct .", "entities": [[52, 53, "DatasetName", "UD"], [71, 72, "DatasetName", "UD"]]}
{"text": "The figure shows a three - word sentence from the Vulgata ( Matt . 6.10 ) , taken from the UD 2.3 version of the pro\u0131el corpus : veniat regnum tuum ( \" thy kingdom come \" ) .", "entities": [[20, 21, "DatasetName", "UD"]]}
{"text": "The UD 2.3 tree for this sentence is shown in Figure 3 . 19 Tokens and sentences are defined using the NIF vocabulary .", "entities": [[1, 2, "DatasetName", "UD"]]}
{"text": "For instance , in Figure 2 this is the case of the string \" Case = Nom | Gen - der = Neut | Number = Sing \" , which is linked to the pro\u0131el token with ID s15924_2 ( for the word regnum \" kingdom \" ) via the relation conll : FEAT , linking the morphological features taken from files in the CoNLL - U format of UD . 20 Other types of tagging ( such as syntactic dependencies , or sentence boundaries ) are expressed by links between the nodes for tokens or sentences .", "entities": [[69, 70, "DatasetName", "UD"]]}
{"text": "Finally , a third group of linguistic annotations , like the part of speech , directly relate tokens to concepts from an ontology of linguistic data ( OLiA ) .", "entities": [[22, 23, "MethodName", "ontology"]]}
{"text": "Tokens are connected to the appropriate Lemma nodes recorded in the LiLa Knowledge Base .", "entities": [[6, 7, "DatasetName", "Lemma"]]}
{"text": "In Figure 2 , for instance , the token s15924_2 ( regnum ) is linked to lemma 34146 , which has written representation regnum .", "entities": [[16, 17, "DatasetName", "lemma"]]}
{"text": "Via this connection , it becomes possible to access all the other information that is also pointing to that lemma .", "entities": [[19, 20, "DatasetName", "lemma"]]}
{"text": "In the figure , the lemma 34146 is connected to a node for a lexical base ( 1133 ) , the same to which also lemmas rex \" king \" ( 34799 ) and regno \" to rule , to be king \" ( 34145 ) are attached .", "entities": [[5, 6, "DatasetName", "lemma"]]}
{"text": "The lemma regnum is also formed with the suffix \" - n \" ( represented by the node affix:111 in Figure 2 ) , the same found in e.g. fanum \" shrine \" ( not shown here for reasons of space ) .", "entities": [[1, 2, "DatasetName", "lemma"]]}
{"text": "Consider , for instance , the case of a researcher interested in the relation between the syntactic role of subject and the semantic role of agent in Latin .", "entities": [[25, 26, "DatasetName", "agent"]]}
{"text": "One possible approach to study the question would be to start by collecting and analyzing the sentences where nouns formed with a typical morpheme for agent nouns like \" - ( t ) or \" ( common to several Indo - European languages ) are attested as subject of an active verb .", "entities": [[25, 26, "DatasetName", "agent"]]}
{"text": "Though the number of linguistic resources currently interlinked in LiLa is still small , it is already possible to design a single SPARQL query to extract this information from our RDF versions of pro\u0131el , \u0131t - tb ( UD version ) and llct .", "entities": [[39, 40, "DatasetName", "UD"]]}
{"text": "The query allows us to extract 143 passages , with 80 different verbs and 58 agent nouns .", "entities": [[15, 16, "DatasetName", "agent"]]}
{"text": "Improving Adversarial Text Generation by Modeling the Distant Future", "entities": [[1, 3, "TaskName", "Adversarial Text"]]}
{"text": "Auto - regressive text generation models usually focus on local fluency , and may cause inconsistent semantic meaning in long text generation .", "entities": [[3, 5, "TaskName", "text generation"], [20, 22, "TaskName", "text generation"]]}
{"text": "Text generation is an important area of investigation within machine learning .", "entities": [[0, 2, "TaskName", "Text generation"]]}
{"text": "Example applications include image captioning ( Ren et al , 2017 ; Rennie et al , 2016 ) , text summarization ( Li et al , 2018b ; Paulus et al , 2017 ; Rush et al , 2015 ) , and adversarial text generation ( Guo et al , 2017 ; Lin et al , 2017 ; Zhu et al , 2018 ) .", "entities": [[3, 5, "TaskName", "image captioning"], [19, 21, "TaskName", "text summarization"], [42, 44, "TaskName", "adversarial text"]]}
{"text": "The sequence - to - sequence framework ( Seq2Seq ) ( Sutskever et al , 2014 ) is a popular technique for text generation .", "entities": [[8, 9, "MethodName", "Seq2Seq"], [22, 24, "TaskName", "text generation"]]}
{"text": "By contrast , sequence - level training with RL provides an effective means of solving this challenge , by treating text generation as a sequential decision - making problem .", "entities": [[20, 22, "TaskName", "text generation"]]}
{"text": "Furthermore , the recurrent models focus more on local fluency , and may cause inconsistent semantic meanings for long text generation .", "entities": [[19, 21, "TaskName", "text generation"]]}
{"text": "For RL - based text generation , most existing works rely on a model - free framework , which has been criticized for its high variance and poor sample efficiency ( Sutton and Barto , 1998 ) .", "entities": [[4, 6, "TaskName", "text generation"]]}
{"text": "Text Generation Model Text generation models learn to generate a sentence", "entities": [[0, 2, "TaskName", "Text Generation"], [3, 5, "TaskName", "Text generation"]]}
{"text": "X. Here each y t is a token from vocabulary A. Starting from the initial state s 0 , a recurrent neural network ( RNN ) produces a sequence of states ( s 1 , . . .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "= [ softmax ( g ( s t ) )", "entities": [[2, 3, "MethodName", "softmax"]]}
{"text": "In order to generate sentence Y s from a ( trained ) model , one iteratively applies the following operations : y s t+1 \u223c Multi ( 1 , softmax ( g ( s t ) ) ) , ( 2 ) s t = h ( s t\u22121 , e ( y s t ) ) , where Multi ( 1 , ) denotes one draw from a multinomial distribution .", "entities": [[29, 30, "MethodName", "softmax"]]}
{"text": "Model - Based Imitation Learning Text generation can be considered as an RL problem with a large number of discrete actions , deterministic transitions , and deterministic terminal rewards .", "entities": [[3, 5, "TaskName", "Imitation Learning"], [5, 7, "TaskName", "Text generation"]]}
{"text": "The objective is to maximize the expected reward : J ( \u03c0 ) = t=1 E P , \u03c0 \u03b3 t\u22121 r ( s t , y t ) .", "entities": [[19, 20, "HyperparameterName", "\u03b3"]]}
{"text": "In model - based imitation learning ( Baram et al , 2017 ; Cheng et al , 2019 ) , a model is built to make predictions for future state s t+ t conditioned on the current state 1 , which can be used for action selection , e.g. , next - token generation .", "entities": [[4, 6, "TaskName", "imitation learning"]]}
{"text": "The model is illustrated in Figure 1 , with an autoeocoder ( AE ) structure for sentence feature extraction and generation .", "entities": [[12, 13, "MethodName", "AE"]]}
{"text": "Overall , text generation can be formulated as an imitationlearning problem .", "entities": [[2, 4, "TaskName", "text generation"]]}
{"text": "At each timestep t , the agent , also called a generator ( which corresponds to the LSTM decoder ) , takes the current LSTM state as input , denoted as s t .", "entities": [[6, 7, "DatasetName", "agent"], [17, 18, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"]]}
{"text": "The objective of text generation is to maximize the total reward as in ( 4 ) .", "entities": [[3, 5, "TaskName", "text generation"]]}
{"text": "The guider network , implemented as an RNN with LSTM units , is adopted to model environment dynamics to assist text generation .", "entities": [[9, 10, "MethodName", "LSTM"], [20, 22, "TaskName", "text generation"]]}
{"text": "Here f t is the input to the LSTM guider , which represents the feature of the current generated sentence extracted 1 t > 1 ; the model predicts future states based on the collected trajectories .", "entities": [[8, 9, "MethodName", "LSTM"]]}
{"text": "z z 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F o 9 z P d P 5 w 7 d t 1 p +", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A a E 6 4 x Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "q x 3 q 2 P + W j F K n c O w R 9 Y n z / E B Z X 0 < / l", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "J I k E a 0 W j u 5 n f G j N t e C o", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A O o 5 K k j A T 5 v N z p / j M K X 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "I l c Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "L s 0 C x 7 M / E / r 5 P Z + D r M u V S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v /", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "g k u 6 j d 1 / + G y 1 r g t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A K b 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "J I k E a 0 W j u 5 n f G j N t e C o", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A O o 5 K k j A T 5 v N z p / j M K X 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "I l c Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "L s 0 C x 7 M / E / r 5 P Z + D r M u V S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v /", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "g k u 6 j d 1 / + G y 1 r g t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A K b 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "J I k E a 0 W j u 5 n f G j N t e C o", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A O o 5 K k j A T 5 v N z p / j M K X 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "I l c Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "L s 0 C x 7 M / E / r 5 P Z + D r M u V S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v /", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "g k u 6 j d 1 / + G y 1 r g t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A K b 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "J I k E a 0 W j u 5 n f G j N t e C o", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A O o 5 K k j A T 5 v N z p / j M K X 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "I l c Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "L s 0 C x 7 M / E / r 5 P Z + D r M u V S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v /", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "g k u 6 j d 1 / + G y 1 r g t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A K b 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[20, 21, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "v E y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "z y a a m X G p 5 Q N q Z D 3 r V U 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[20, 21, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "v E y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "z y a a m X G p 5 Q N q Z D 3 r V U 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 0 y i Q v B N M", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "b O 8 X d 0 t 7 + w e F R +", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "i 6 x a b g R 2 E k V 0", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "z O / / Y R K 8 0 Q + m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "v O W X V 0 m r W v E u K 9 X 7 W r l e y + M o w C m c w Q V 4 c A V 1 u", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "V i 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 0 y i Q v B N M", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "b O 8 X d 0 t 7 + w e F R +", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "i 6 x a b g R 2 E k V 0", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "z O / / Y R K 8 0 Q + m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "v O W X V 0 m r W v E u K 9 X 7 W r l e y + M o w C m c w Q V 4 c A V 1 u", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "V i 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 0 y i Q v B N M", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "b O 8 X d 0 t 7 + w e F R +", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "i 6 x a b g R 2 E k V 0", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "z O / / Y R K 8 0 Q + m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "v O W X V 0 m r W v E u K 9 X 7 v 5 B + f C o Z V S m", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i Q R G v e v P f u K 0 5 a P X B s o /", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "8 U l 5 d a 5 g k 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "n I 1 9 8 N E d k 0 / t h + N B g F S H 7 V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "O 0 G 5 4", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "MLP w t < l a t e", "entities": [[0, 1, "DatasetName", "MLP"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "a 0 E x m", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "3 p z 0 K t B", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "v W h W M T n L M j k C 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u H F Z 0 b G F", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "K 4 8 2 K 4 z A U G Q q M 6 2 Y W 1 R S Y 0", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u", "entities": [[9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "b R e 1 B v R i 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "/ U f e / D c u 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "q + w c P J s k 0 4 z 5", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "C s V 9 F C h 5 O 9 W c x q H k r X B 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "z 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "w k b U k 0 Z 2 n Q q N g R v 8 e V l 4 p / V L + v e r V t r X B V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0 V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0 R 3 i G V", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "a 0 E x m", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "3 p z 0 K t B", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P", "entities": [[2, 3, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E /", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "z o c w w m c g Q 8 X c A 2 3 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P", "entities": [[2, 3, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E /", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "z o c w w m c g Q 8 X c A 2 3 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "L A y B W 3 o S N t 8 F p M 0", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "r k U t C P S K 4 k L 0 Q K 8 p", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "w d 2 0 2 k 5 M 6 B l 4 l a k C R U 6 A / v L j w T J E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h d F r 3 c 0 U z T M Z 4 S P u G", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "K i h m 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "o U 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "+ s 0 2 1 d V G", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "o Q 0 3 0", "entities": [[2, 3, "DatasetName", "0"], [4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[20, 21, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 n f 6 5 c r b t W d g 6", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "1 d 8 T E x o Z k 0 W B 7 Y w o", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "a i m D G 0 6 J R u C t / z y K", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "n V q t 5 F t X Z / W a n f V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[38, 39, "DatasetName", "0"], [62, 63, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 m / 1 i 9 X 3 K o 7 B 1 k l X k 4 q k K P R L 3 / 1 B j F L I 6 6", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "p M 1 3 M T 9 C d U o 2 C S T 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "d p 0 S j Y E b / n l V", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[20, 21, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "v E y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "z y a a m X G p 5 Q N q Z D 3 r V U 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "V i 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Y 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Y 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Y 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "A s V F l s L q h m v n 6 4 8 e 2 6 0 3 D m g K", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "N o L 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "O 2 W t I 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A a E 6 4 x Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "g P E V j M j S U o 5 g o L 5 + H n 8 E z o 4 Q w E t I c r u F c / b 2 R o 1 g V 8 c x k j P R E L X u F + J 8 3 T H V 0", "entities": [[60, 61, "DatasetName", "0"]]}
{"text": "W b D v W g 0 7 1 r 1 d q u s", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A 0 J l x j h p Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F o 9 z P 3 P z h 2 r f r T s", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "U O A 0 J l x j h p Q a u E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "D W s C i C x h S S b B m U 0 M Q l t R k", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "v W b D P W 8 0 b 1 v 1 d q u s o", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "y L 3 s 0 4 B p / V", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "G k J 3 0 G 8 q C M a m W T Q g = \" >", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F o 9 z P m v n D t W / X n Y Y", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A c 1 k j P R Y L X q F + J 8 3 S H V 0 6 W W U J 6 k m H M", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "j 0 Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "G 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "O K f c p C Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F V e 5 n T v 5 w 7 d t 1", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A a E 6 4 x Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "z z 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F o 9 z P d P 5 w 7 d t 1 p +", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A a E 6 4 x Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "q x 3 q 2 P + W j F K n c O w R 9 Y n z / E B Z X 0 < / l", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "j 0 Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "k b y 7 U Y + C Q Q L V R a b C 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "A u j h D A S 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "e I Y k w t r 0 V T c l u K t f X", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "F b 9 a T 9 W K 9 W x / L 0 Q 2 r 2 m m A P 7 A + f", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "A 0 K U k q l F L / F S 8 e F P H q H + L N / 8 Z 0 6 0 E 3 H 4 Q 8 3 v v 9 y M s L E k a V d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T 0", "entities": [[1, 2, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [71, 72, "DatasetName", "0"]]}
{"text": "j 0 Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "k b y 7 U Y + C Q Q L V R a b C 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Z 5 v f 0 1 C g", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "c I 0 Z U m r s", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "l P U k 0 4 X j 4 U p Q", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "A 0 K U k q l F L / F S 8 e F P H q H + L N / 8 Z 0 6 0 E 3 H 4 Q 8 3 v v 9 y M s L E k a V d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T 0", "entities": [[1, 2, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [71, 72, "DatasetName", "0"]]}
{"text": "j 0 Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "k b y 7 U Y + C Q Q L V R a b C 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Y 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A A A B + n i c b V C 7 T s M w F L 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "i h y H L e 1 6 j i R 7 Y C q 0 E 9 h", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "Y M p m 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "D c 0 8 S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "S W l l d W 1 / L r h Y 3 N r e 0 d c 3 e v o a J E U l a n", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "g l o y T V P 0 4 4 X C V 8 N A /", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A 9 1 t Y 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D A j 0 1 b w 3 E v / z 2 g l 0", "entities": [[3, 4, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}
{"text": "o i K E m h E q u d 8 W 0 T y S", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "The initial state of the guider network is the encoded feature of a true input sentence by the same convolutional neural network ( CNN ) , i.e. , s G 0", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "This provides an important \" guide \" to the LSTM decoder , accounting for the global properties of the generated text .", "entities": [[9, 10, "MethodName", "LSTM"]]}
{"text": "Text Generation with Planning We first explain how one uses the guider network to guide next - word generation for the generator ( the LSTM decoder in Figure 1 ) .", "entities": [[0, 2, "TaskName", "Text Generation"], [24, 25, "MethodName", "LSTM"]]}
{"text": "( s G t\u22121 , f t ) as a future feature representation , by feeding f t into the LSTM guider .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "The weight w t is applied to the output O t of the LSTM decoder by an element - wise multiplication operation .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "The result is then fed into a softmax layer to generate the next token y t .", "entities": [[7, 8, "MethodName", "softmax"]]}
{"text": "O t = g ( s t\u22121 ) , w t = \u03d5 ( G \u03c8 ( s G t\u22121 , f t ) ) , ( 5 ) y t \u223c Multi ( 1 , softmax ( O t w t ) ) , ( 6 ) s G t = h G ( s G t\u22121 , f t ) , s t = h ( s t\u22121 , e ( y t ) ) .", "entities": [[36, 37, "MethodName", "softmax"]]}
{"text": "f T ) for a training sentence , we seek to update the guider network such that it is able to predict f t+c given f t , where c > 0 is the number of steps that are looked ahead .", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "As a result , the prediction is used to construct an intermediate reward , used to update the generator ( the LSTM decoder ) , as described further below .", "entities": [[21, 22, "MethodName", "LSTM"]]}
{"text": "At the last step of text generation , i.e. , t = T , the corresponding reward measures the quality of the whole generated sentence , thus it is called a final reward .", "entities": [[5, 7, "TaskName", "text generation"]]}
{"text": "t \u03b3 i r", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "g i with \u03b3 a discount factor , as a featurematching reward .", "entities": [[3, 4, "HyperparameterName", "\u03b3"]]}
{"text": "As a result , we combine the adversarial reward r f [ 0 , 1 ] by the discriminator ( Yu et al Generate a sequence Y 1 ... T \u223c \u03c0 \u03c6 .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "( T i = t \u03b3 i r g i ) \u00d7 r f .", "entities": [[5, 6, "HyperparameterName", "\u03b3"]]}
{"text": "The generator is initialized by pre - training on sentences with an autoencoder structure , based on MLE training .", "entities": [[12, 13, "MethodName", "autoencoder"]]}
{"text": "Algorithm 1 describes the proposed model - based imitation learning framework for text generation .", "entities": [[8, 10, "TaskName", "imitation learning"], [12, 14, "TaskName", "text generation"]]}
{"text": "Model - based or Model - free Text generation seeks to generate the next word ( action ) given the current ( sub - ) sentence ( state ) .", "entities": [[7, 9, "TaskName", "Text generation"]]}
{"text": "The generator is considered as an agent that learns a policy to predict the next word given its current state .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "r / e Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U 0 B / h", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h v h p", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "n i U x 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "r / e Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U 0 B / h", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h v h p", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "n i U x 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "r / e Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U 0 B / h", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h v h p", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "n i U x 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "r / e Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "a 0 E x m", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "3 p z 0 K t B", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "v W h W M T n L M j k C 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u H F Z 0 b G F", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "Y W 1 R S Y 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u", "entities": [[9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "v W h W M T n L M j k C 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u H F Z 0 b G F", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "Y W 1 R S Y 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u", "entities": [[9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "b R e 1 B v R i 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "/ U f e / D c u 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "q + w c P J s k 0 4 z 5", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "C s V 9 F C h 5 O 9 W c x q H k r X B 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U k 0 Z 2 n Q q N g R v 8 e V l 4 p / V L + v e r V t r X B V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" Y f B p l r X h t V g c d Z q 6 r a 0 D R Y", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u N B l B c c W 2 r F k 0", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "F T u f B v T n 4 W 2 H g h 8 n J O Q e 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z a r D A C Q 5 G p z L R", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 0 V t", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "A o x y j l f S 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "T K T g 5 K 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "w k g 3 K x M D b r g g 1 0 / Z l R A s r 7 w K 4 X n 9 q h 7 c + 1", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" Y f B p l r X h t V g c d Z q 6 r a 0 D R Y", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u N B l B c c W 2 r F k 0", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "F T u f B v T n 4 W 2 H g h 8 n J O Q e 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z a r D A C Q 5 G p z L R", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 0 V t", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "A o x y j l f S 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "T K T g 5 K 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "w k g 3 K x M D b r g g 1 0 / Z l R A s r 7 w K 4 X n 9 q h 7 c + 1", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "As illustrated in Figure 2 , our framework naturally provides a way for style transfer , where the guider network plays the role of style selection , and the generator only focuses on maintaining content without considering the styles .", "entities": [[13, 15, "TaskName", "style transfer"]]}
{"text": "To make the guider network focus on the guidance of styles , we assign the label l as the initial state s G 0 of the guider network .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "g ( s t\u22121 ) , w t = \u03d5 ( G \u03c8 ( s G t\u22121 , [ f t , l ] ) ) , ( 9 ) y t \u223c Multi ( 1 , softmax ( O t w t ) ) .", "entities": [[37, 38, "MethodName", "softmax"]]}
{"text": "For the generator , we put an adversarial regularizer on the encoded latent s 0 ( X ) and penalize it if it contains the sentiment information , by maximizing the entropy , i.e. , max l p ( l | s 0 ( X ) ) log p ( l | s 0 ( X ) ) , where p is a pre - trained classifier .", "entities": [[14, 15, "DatasetName", "0"], [42, 43, "DatasetName", "0"], [53, 54, "DatasetName", "0"]]}
{"text": "We first review related works that combine RL and GAN for text generation .", "entities": [[9, 10, "MethodName", "GAN"], [11, 13, "TaskName", "text generation"]]}
{"text": "For example , RankGAN ( Lin et al , 2017 ) proposes to replace the reward from the GAN discriminator with a rankingbased reward , MaliGAN ( Che et al , 2017 ) modifies the GAN objective and proposes techniques to reduce gradient variance , MaskGAN uses a filling technique to define a Q - value reward for sentence completion , RelGAN ( Nie et al , 2019 ) uses a relational memory based generator for the long - distance dependency modeling , FM - GAN uses a feature mover distance to match features of real and generated sentences inspired by optimal transport ( Chen et al , 2019 ; , and LeakGAN ( Guo et al , 2017 ) tries to address the sparse - reward issue for long - text generation with hierarchical RL by utilizing the leaked information from a GAN discriminator .", "entities": [[18, 19, "MethodName", "GAN"], [35, 36, "MethodName", "GAN"], [58, 60, "TaskName", "sentence completion"], [85, 86, "MethodName", "GAN"], [131, 133, "TaskName", "text generation"], [143, 144, "MethodName", "GAN"]]}
{"text": "By contrast , by relying on a model - based imitation learning approach , our method learns global - structure information , which generates more - diverse sentences , and can be extended to conditional text generation .", "entities": [[10, 12, "TaskName", "imitation learning"], [34, 37, "TaskName", "conditional text generation"]]}
{"text": "RL techniques can also be used in other ways for text generation ( Bachman and Precup , 2015 ) .", "entities": [[10, 12, "TaskName", "text generation"]]}
{"text": "To reduce variance of the vanilla REINFORCE , Bahdanau et al ( 2017 ) adopted the actor - critic framework for sequence prediction .", "entities": [[6, 7, "MethodName", "REINFORCE"]]}
{"text": "Furthermore , Rennie et al ( 2016 ) trained a baseline algorithm with a greedy decoding scheme for the REINFORCE method .", "entities": [[19, 20, "MethodName", "REINFORCE"]]}
{"text": "Planning techniques in RL have also been explored to improve text generation ( Gulcehre et al , 2017 ; Serdyuk et al , 2018 ) .", "entities": [[10, 12, "TaskName", "text generation"]]}
{"text": "We use the COCO Image Captions Dataset , in which most sentences have a length of about 10 words .", "entities": [[3, 4, "DatasetName", "COCO"]]}
{"text": "Since we consider unconditional text generation , only image captions are used as the training data .", "entities": [[4, 6, "TaskName", "text generation"]]}
{"text": "We observe that GM - GAN performs significantly better than the baseline models .", "entities": [[5, 6, "MethodName", "GAN"]]}
{"text": "Specifically , besides achieving higher test - BLEU scores , the proposed method also generates samples with very good diversity in terms of self - BLEU scores .", "entities": [[7, 8, "MetricName", "BLEU"], [25, 26, "MetricName", "BLEU"]]}
{"text": "LeakGAN represents the state - of - the - art in adversarial text generation , however , its diversity measurement is relatively poor ( Zhu et al , 2018 ) .", "entities": [[11, 13, "TaskName", "adversarial text"]]}
{"text": "We suspect that the high BLEU score achieved by LeakGAN is due to its mode collapse on some good samples , resulting in high self - BLEU scores .", "entities": [[5, 7, "MetricName", "BLEU score"], [26, 27, "MetricName", "BLEU"]]}
{"text": "Other baselines achieve lower self - BLEU scores since they can not generate reasonable sentences .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "Long Text Generation : EMNLP2017 WMT", "entities": [[1, 3, "TaskName", "Text Generation"]]}
{"text": "We conduct ablation studies on long text generation to investigate the improvements brought by each part of our proposed method .", "entities": [[6, 8, "TaskName", "text generation"]]}
{"text": "( 2 ) The UK is Google ' s largest non - US market , he has added \" 20 , before the best team is amount of fewer than one or the closest home or two years ago .", "entities": [[6, 7, "DatasetName", "Google"]]}
{"text": "We also report the BLEU scores with original sentences ( BLEU ) and human references ( BLEU - ref ) ( Li et al , 2018a ) , to evaluate the content preservation of transferred sentences .", "entities": [[4, 5, "MetricName", "BLEU"], [10, 11, "MetricName", "BLEU"], [16, 17, "MetricName", "BLEU"]]}
{"text": "Our proposed model exhibits higher transfer accuracy and better content preservation , indicating the guider network provides good sentiment guidance to better preserve the content information .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Table 8 : Generated samples of guided style transfer .", "entities": [[7, 9, "TaskName", "style transfer"]]}
{"text": "We have proposed a model - based imitationlearning framework for adversarial text generation , by introducing a guider network to model the generation environment .", "entities": [[10, 12, "TaskName", "adversarial text"]]}
{"text": "Our proposed models are validated on both unconditional and conditional text generation , including adversarial text generation and non - parallel style transfer .", "entities": [[9, 12, "TaskName", "conditional text generation"], [14, 16, "TaskName", "adversarial text"], [21, 23, "TaskName", "style transfer"]]}
{"text": "More Generated Samples of Text Generation Table 13 lists more generated samples on the proposed GMGAN and its baselines .", "entities": [[4, 6, "TaskName", "Text Generation"]]}
{"text": "We conduct experiments on image captioning ( Karpathy and Fei - Fei , 2015 ) , investigating benefits brought by the Guider network .", "entities": [[4, 6, "TaskName", "image captioning"]]}
{"text": "In image captioning , instead of using a discriminator to define final rewards for generated sentence , we adopt evaluation metrics computed based on human references .", "entities": [[1, 3, "TaskName", "image captioning"]]}
{"text": "We test our proposed model on the MS COCO dataset ( Karpathy and Fei - Fei , 2015 ) , containing 123 , 287 images in total .", "entities": [[8, 9, "DatasetName", "COCO"]]}
{"text": "We consider two settings : ( i ) using a pre - trained 152layer ResNet ( He et al , 2016 ) for feature extraction , where we take the output of the 2048 - way pool5 layer from ResNet - 152 , pretrained on the ImageNet dataset ; and ( ii ) using semantic tags detected from the image as features .", "entities": [[14, 15, "MethodName", "ResNet"], [33, 34, "DatasetName", "2048"], [39, 40, "MethodName", "ResNet"], [46, 47, "DatasetName", "ImageNet"]]}
{"text": "We use an LSTM with 512 hidden units with mini - batches of size 64 .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "The results are summarized in comparing an AutoEncoder ( AE ) with a variant implemented by adding a guider network ( Guider ) , improvements are observed .", "entities": [[7, 8, "MethodName", "AutoEncoder"], [9, 10, "MethodName", "AE"]]}
{"text": "We compare the proposed GMST with SCST .", "entities": [[6, 7, "MethodName", "SCST"]]}
{"text": "Note the main difference between GMST and SCST is that the former employs our proposed feature - matching reward , while the latter only considers the final reward provided by evaluation metrics .", "entities": [[7, 8, "MethodName", "SCST"]]}
{"text": "GMST achieves higher scores compared with SCST on its optimized metrics .", "entities": [[6, 7, "MethodName", "SCST"]]}
{"text": "The gain of GMST compared with SCST comes from the immediate rewards , which can maintain the semantic consistency and sentence structure , preventing language - fluency damage caused by only focusing on evaluation metrics .", "entities": [[6, 7, "MethodName", "SCST"]]}
{"text": "The experiments aim to quantify the gain when incorporating MPC for imitation learning , i.e. , MLE and RL finetune .", "entities": [[11, 13, "TaskName", "imitation learning"]]}
{"text": "at time t + t. In the text generation setting , when t = 1 , we can exactly get the feature representation of the current generated sentence if the guider does not help the word selection .", "entities": [[7, 9, "TaskName", "text generation"]]}
{"text": "We use Adam ( Kingma and Ba , 2014 ) optimization algorithm to train the guider , generator and discriminator .", "entities": [[2, 3, "MethodName", "Adam"]]}
{"text": "For both tasks , the LSTM state of dimension for the generator is 300 , and the LSTM state of dimension for the generator is 300 .", "entities": [[5, 6, "MethodName", "LSTM"], [17, 18, "MethodName", "LSTM"]]}
{"text": "The LSTM state of dimension for the generator is 300 , and the LSTM state of dimension for the guider is 300 .", "entities": [[1, 2, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "The research was supported in part by DARPA , DOE , NIH , NSF and ONR .", "entities": [[7, 8, "DatasetName", "DARPA"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "j u o 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "H S k S C U b T S 3 U 0 P e 9 W a W 3 d", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "N q I D 3 r F U 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "N G 2 F J K Z + n M i p 7 E x 4 z i 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "F o V n 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "z 5 I r N F 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "j u o 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "H S k S C U b T S 3 U 0 P e 9 W a W 3 d", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "N q I D 3 r F U 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "N G 2 F J K Z + n M i p 7 E x 4 z i 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "F o V n 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "z 5 I r N F 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "j u o 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "H S k S C U b T S 3 U 0 P e 9 W a W 3 d", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "N q I D 3 r F U 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "N G 2 F J K Z + n M i p 7 E x 4 z i 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "F o V n 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "z 5 I r N F 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a 0 E x m", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "3 p z 0 K t B", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "+ F j u f B v T n 4 W 2 H g h 8 n J O Q e 0", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z Z r D A C Q 5 G p z L R", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 0 V t", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "x y j l f S 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "U z T m h q R Q O C l 3 C o s 5 F 0 P e x 7 Z D z V", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "O 0 0 X g 2 6 o S d", "entities": [[1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"]]}
{"text": "L t l F 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "q w b 0 P J T i G E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 6 M M L v M G 7 p 7 x X 7 2 P e 1 p q 3 q O 0", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "+ F j u f B v T n 4 W 2 H g h 8 n J O Q e 0", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z Z r D A C Q 5 G p z L R", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 0 V t", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "x y j l f S 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "U z T m h q R Q O C l 3 C o s 5 F 0 P e x 7 Z D z V", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "O 0 0 X g 2 6 o S d", "entities": [[1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"]]}
{"text": "L t l F 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "q w b 0 P J T i G E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 6 M M L v M G 7 p 7 x X 7 2 P e 1 p q 3 q O 0", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 8 q L e i F 2 9 W N L b Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a D i 1 X / k z X / j t s 1 B W x 8 M P N 6 b", "entities": [[27, 28, "DatasetName", "0"], [51, 52, "DatasetName", "0"]]}
{"text": "e m E p h 0 H W / n d L K 6 t r 6 R", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "j u o 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "W 0 P e 9 W a W 3 d", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "n I M v E K 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A T q / R J l G h b C s l M / T 2 R 0 9 i Y c R z a z p j i 0 C x 6 U / E / r 5 N h d B H k Q", "entities": [[18, 19, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}
{"text": "k z I 9 G / S F 5 o z l G N L K N P C 3 k r Y k G r K 0 K Z T s S F 4 i y 8", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" X / B b P o 3 p z 0", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "4 t t E P J p H f a 0 E", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "b l F J j S F J U t j O D f I 0 V t i K R 7 e z v", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "y j l A y 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "o Q g g C B v A C b / D u K e / V + 1 i 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "4 t t E P J p H f a 0 E", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "b l F J j S F J U t j O D f I 0 V t i K R 7 e z v", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "y j l A y 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "o Q g g C B v A C b / D u K e / V + 1 i 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "x B v R i 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "v z z 6 o e v Q S H 4 G m 0", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "N N 3 C 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 G B R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "9 o k H G k E 1 T 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "2 1 e N t 3 b s 0 b r q m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "g w j m 0 4", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" X / B b P o 3 p z 0", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P", "entities": [[2, 3, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E /", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "z o c w w m c g Q 8 X c A 2 3 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P", "entities": [[2, 3, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E /", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "z o c w w m c g Q 8 X c A 2 3 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "L A y B W 3 o S N t 8 F p M 0", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "r k U t C P S K 4 k L 0 Q K 8 p", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "w d 2 0 2 k 5 M 6 B l 4 l a k C R U 6 A / v L j w T J E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h d F r 3 c 0 U z T M Z 4 S P u G", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "K i h m 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "o U 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "9 o k H G k E 1 T 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "2 1 e N t 3 b s 0 b r q m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "g w j m 0 4", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "9 o k H G k E 1 T 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "2 1 e N t 3 b s 0 b r q m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "g w j m 0 4", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "F F f g w j m 0 4", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "F F f g w j m 0 4", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "F F f g w j m 0 4", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "F F f g w j m 0 4", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "q c f e a q 5 f 3 G N U B Z c 0 = \" >", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "I 6 q 3 o Q Y 8 V j C 2 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "2 J 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "q c f e a q 5 f 3 G N U B Z c 0 = \" >", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "I 6 q 3 o Q Y 8 V j C 2 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "2 J 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "q c f e a q 5 f 3 G N U B Z c 0 = \" >", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "I 6 q 3 o Q Y 8 V j C 2 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "2 J 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "A q 7 A g 1 t o w j 2 0", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "We test the proposed framework on unconditional and conditional text generation tasks , and analyze the results to understand the performance gained by the guider network .", "entities": [[8, 11, "TaskName", "conditional text generation"]]}
{"text": "We also perform an ablation investigation on the improvements brought by each part of our proposed method , and consider non - parallel style transfer .", "entities": [[23, 25, "TaskName", "style transfer"]]}
{"text": "We focus on adversarial text generation , and compare our approach with a number of related works ( Guo et al , 2017 ; Lin et al , 2017 ; Zhu et al , 2018 ) .", "entities": [[3, 5, "TaskName", "adversarial text"]]}
{"text": "In this setting , a discriminator in the GAN framework is added to the model in Figure 1 to guide the generator to generate high - quality sentences .", "entities": [[8, 9, "MethodName", "GAN"]]}
{"text": "All baseline experiments are implemented on the texygen platform ( Zhu et al , 2018 ) .", "entities": [[7, 9, "DatasetName", "texygen platform"]]}
{"text": "We adopt the BLEU score , referenced by the test set ( test - BLEU , higher value implies better quality ) and itself ( self - BLEU , lower value implies better diversity )", "entities": [[3, 5, "MetricName", "BLEU score"], [14, 15, "MetricName", "BLEU"], [27, 28, "MetricName", "BLEU"]]}
{"text": "A good generator should achieve both a high test - BLEU score and a low self - BLEU score .", "entities": [[10, 12, "MetricName", "BLEU score"], [17, 19, "MetricName", "BLEU score"]]}
{"text": "We call the proposed method guidermatching GAN ( GMGAN ) for unconditional text Res152 - SCST : a group of zebras standing in a eld .", "entities": [[6, 7, "MethodName", "GAN"], [15, 16, "MethodName", "SCST"]]}
{"text": "Tag - SCST : a zebra and a zebra drinking water from a eld of grass .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Res152 - SCST : a group of people walking down a skateboard .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Tag - SCST : a woman walking down a street with a skateboard .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Res152 - SCST : a baby sing next to a baby girae .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Tag - SCST : a black and white photo of a woman holding a teddy bear .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Res152 - SCST : a trac light on a street with a in the . Res152 - GMST : a trac light on the side of a street .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Tag - SCST : a trac light on a street with a green .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "The team looked into the influence of marriage on weight loss after surgery - as well as the effects of surgery on the quality of his administration and rest on the world .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "Multi - Style Transfer with Discriminative Feedback on Disjoint Corpus", "entities": [[2, 4, "TaskName", "Style Transfer"]]}
{"text": "Style transfer has been widely explored in natural language generation with non - parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus .", "entities": [[0, 2, "TaskName", "Style transfer"]]}
{"text": "While cascading single - dimensional models across multiple styles is a possibility , it suffers from content loss , especially when the style dimensions are not completely independent of each other .", "entities": [[17, 18, "MetricName", "loss"]]}
{"text": "We compare it against baselines involving cascaded state - of - the - art uni - dimensional style transfer models .", "entities": [[17, 19, "TaskName", "style transfer"]]}
{"text": "Style transfer is a popular task in natural language processing and has been studied on attributes like age or gender ( Subramanian et al , 2018 ) , styles emanating from social construct like formality ( Rao and Tetreault , 2018 ) and politeness ( Madaan et al , 2020 ) , linguistic styles based on author writing style ( Syed et al , 2020 ) , or psycho - linguistic styles based on personality types ( Mairesse and Walker , 2011 ) .", "entities": [[0, 2, "TaskName", "Style transfer"]]}
{"text": "While early style transfer frameworks were modeled as a supervised learning task on a parallel corpus , state - of - the - art models are semi - supervised / unsupervised and operate on nonparallel corpus .", "entities": [[2, 4, "TaskName", "style transfer"]]}
{"text": "These models achieve style transfer by aligning source and target distribution of sentences from non - parallel corpus ( Shen et al , 2017 ) , disentangling content space from style space in latent representation ( Hu et al , 2017 ) or employing self - reconstruction ( Dai et al , 2019 ) and back translation ( Lample et al , 2018 ) objectives to achieve pseudo - supervision with non - parallel corpus .", "entities": [[3, 5, "TaskName", "style transfer"]]}
{"text": "In this paper , we propose a multidimensional style transfer approach that can work off partially labelled data for style transfer across multiple dimensions simultaneously .", "entities": [[8, 10, "TaskName", "style transfer"], [19, 21, "TaskName", "style transfer"]]}
{"text": "The work by Subramanian et al ( 2018 ) attempts style transfer with multiple attributes such as age , gender , and sentiment simultaneously .", "entities": [[10, 12, "TaskName", "style transfer"]]}
{"text": "In contrast to this and other similar explorations in multi - style transfer , our approach does not require jointly labelled data across all the stylistic dimensions in source and/or target corpus .", "entities": [[11, 13, "TaskName", "style transfer"]]}
{"text": "We focus on the problem where independent corpus is available across different stylistic dimensions ( say sentiment and formality ) and we achieve style transfer spanning different stylistic dimensions ( say make a sentence more positive and formal ) .", "entities": [[23, 25, "TaskName", "style transfer"]]}
{"text": "Relaxing the requirement of jointly labelled data for multi - style transfer , by leveraging independently acquired disjoint corpus for different styles .", "entities": [[10, 12, "TaskName", "style transfer"]]}
{"text": "3 ) Achieving better style control with better content preservation in multi - dimensional style transfer than a cascaded setup of state - of - the - art unidimensional style transfer models .", "entities": [[14, 16, "TaskName", "style transfer"], [29, 31, "TaskName", "style transfer"]]}
{"text": "One line of work in style transfer attempts to learn disentangled latent representation for style and content , and transfer style by manipulating latent representation of style ( Shen et al , 2017 ) .", "entities": [[5, 7, "TaskName", "style transfer"]]}
{"text": "Although these approaches perform well with one style at a time , they do not trivially scale to multidimensional style transfer .", "entities": [[19, 21, "TaskName", "style transfer"]]}
{"text": "Dai et al ( 2019 ) achieve state - ofthe - art style transfer in single style dimensions by employing transformer - based model in conjunction with classifier - based discriminator .", "entities": [[12, 14, "TaskName", "style transfer"]]}
{"text": "Language modeling is integral to several natural language generation ( NLG ) tasks like text summarization , spelling correction , image captioning , etc .", "entities": [[14, 16, "TaskName", "text summarization"], [17, 19, "TaskName", "spelling correction"], [20, 22, "TaskName", "image captioning"]]}
{"text": "The introduction of Transformer - based architecture accompanied with generative pre - training ( Radford , 2018 ) capabilities have led to strong improvements in many downstream generation and GLUE ( Wang et al , 2018 ) tasks .", "entities": [[3, 4, "MethodName", "Transformer"], [29, 30, "DatasetName", "GLUE"]]}
{"text": "Generative pre - training aims to adapt a large Transformer language model to large unsupervised corpus .", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "This capability of generative pre - training is exploited in many large language models like BERT ( Devlin et al , 2019 ) , GPT - 2 ( Radford et al , 2018 ) , ERNIE 2.0 ( Sun et al , 2020 ) which have the ability to perform tasks like reading comprehension ( Xu et al , 2019 ) , summarization ( Liu and Lapata , 2019 ) , question - answering ( Rajpurkar et al , 2016 ) and translation ( Clinchant et", "entities": [[15, 16, "MethodName", "BERT"], [24, 25, "MethodName", "GPT"], [52, 54, "TaskName", "reading comprehension"], [62, 63, "TaskName", "summarization"]]}
{"text": "Recently these pre - trained generative language models have been explored in translation ( Conneau and Lample , 2019 ) and style transfer tasks ( Syed et al , 2020 ) .", "entities": [[21, 23, "TaskName", "style transfer"]]}
{"text": "Conneau and Lample ( 2019 ) develop cross - lingual models for unsupervised machine translation by initializing encoder and decoder with a pre - trained language model trained on Masked Language Modeling ( MLM ) ( Devlin et al , 2019 ) objective and fine - tuning the encoderdecoder framework with adversarial training .", "entities": [[12, 15, "TaskName", "unsupervised machine translation"], [29, 32, "TaskName", "Masked Language Modeling"], [33, 34, "DatasetName", "MLM"]]}
{"text": "Based on this , a language model trained on target style will have high perplexity on transferred text if it does not match target style and low perplexity otherwise .", "entities": [[14, 15, "MetricName", "perplexity"], [27, 28, "MetricName", "perplexity"]]}
{"text": "Consequently , an independently trained LM on one of the target styles might have high perplexity even if the transferred sentence fits in the corresponding target style , due to the content space of source sentence .", "entities": [[15, 16, "MetricName", "perplexity"]]}
{"text": "Similar to Syed et al ( 2020 ) , we first pre - train a Transformer - based language model with Masked Language Modeling ( MLM ) objective on English Wikipedia data extracted using WikiExtractor .", "entities": [[15, 16, "MethodName", "Transformer"], [21, 24, "TaskName", "Masked Language Modeling"], [25, 26, "DatasetName", "MLM"]]}
{"text": "Masked Language Modeling leverages bidirectional context of the input , thus enabling better language understanding .", "entities": [[0, 3, "TaskName", "Masked Language Modeling"]]}
{"text": "Following Masked Language Modeling objective from Devlin et al ( 2019 ) , we randomly sample 15 % of the tokens from the text stream and replace them with the [ MASK ] token 80 % of the time , by a random token 10 % of the time and keep them unchanged 10 % of the time , with the objective of predicting the original identity of the masked word based on its bidirectional context .", "entities": [[1, 4, "TaskName", "Masked Language Modeling"]]}
{"text": "To enable style transfer from a given sentence to target style , we use independently trained language models ( LMs ) to initialize the encoder and decoder and connect these with randomly initialized attention layers to arrive at a encoder - decoder setup .", "entities": [[2, 4, "TaskName", "style transfer"], [33, 35, "HyperparameterName", "attention layers"]]}
{"text": "As discussed by Syed et al ( 2020 ) , the Transformer architecture ( Vaswani et al , 2017 ) allows such independent initialization by implicitly aligning encoder - decoder layers via attention mechanism .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "Transformer - based models with encoder - only ( Devlin et al , 2019 ) or decoder - only blocks have been shown to perform well in generative pre - training task .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "Moreover , this also enables us to use the same pre - trained model to initialize both style transfer module and the discriminator models , explained in the following section .", "entities": [[17, 20, "MethodName", "style transfer module"]]}
{"text": "In Syed et al ( 2020 ) 's setup , both encoder and decoder in the style transfer module are initialized with the pre - trained language model ( trained on MLM objective ) .", "entities": [[16, 19, "MethodName", "style transfer module"], [31, 32, "DatasetName", "MLM"]]}
{"text": "To instill style - awareness to the encoder - decoder setup initialized with pre - trained Transformer models , we fine - tune it with Denoising Autoencoder ( DAE ) loss using the target - domain corpus .", "entities": [[16, 17, "MethodName", "Transformer"], [25, 27, "MethodName", "Denoising Autoencoder"], [30, 31, "MetricName", "loss"]]}
{"text": "Under the DAE objective , the encoder takes a noisy masked versionx of the text x as input and attempts to fill in the mask token as per the MLM objective that it was pre - trained on .", "entities": [[29, 30, "DatasetName", "MLM"]]}
{"text": "The overall training objective is L DAE ( \u03b8 G )", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "E x\u223cT [ \u2212 log P \u03b8 G", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "In conjunction , the encoder and decoder enable style transfer to the target style .", "entities": [[8, 10, "TaskName", "style transfer"]]}
{"text": "To extend the single - dimensional style transfer setup above to multi - dimensional setting , we use language models as discriminators to provide the feedback to the model for partially annotated nature of input data .", "entities": [[6, 8, "TaskName", "style transfer"]]}
{"text": "Inspired by Yang et al ( 2018 ) , we fine - tune a language model on the target style s i , so that the language model is equipped with language distribution of target domain data .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "The training loss for the LM for target style s i with corresponding corpus", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "For k - dimensional style transfer with target styles s = { s 1 , s 2 , . . .", "entities": [[4, 6, "TaskName", "style transfer"]]}
{"text": "For the transferred sentence x , the training objective for each target style s i is , argmin \u03b8", "entities": [[18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "G L s i = E x\u223cT , x \u223cP \u03b8 G ( x )", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "However , we can not directly find the argmin \u03b8 G using gradient descent because of discrete sampling of x \u223c P \u03b8 G ( x ) .", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [22, 23, "HyperparameterName", "\u03b8"]]}
{"text": "To account for this , we use a policy gradient reinforcement learning approach using REINFORCE algorithm ( Sutton et al , 1999 ) .", "entities": [[14, 15, "MethodName", "REINFORCE"]]}
{"text": "L s i given by , L s i = E x\u223cT , x \u223cP \u03b8 G ( x ) ( r ( x ) \u2212 r ( x ) )", "entities": [[15, 16, "HyperparameterName", "\u03b8"]]}
{"text": "[ \u2212 log P \u03b8 G ( x | x ) ]", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "The overall training loss for the joint encoder - decoder model is L = \u03bbDAEEx\u223cT [ \u2212 log P \u03b8 ( x | x ) ]", "entities": [[3, 4, "MetricName", "loss"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "First , we pre - train a transformer model with Masked language modeling objective as shown in Figure 1 ( Left ) .", "entities": [[10, 13, "TaskName", "Masked language modeling"]]}
{"text": "Finally , we initialize the encoder and decoder of the style transfer module with the pretrained and style - specific fine - tuned language models , respectively .", "entities": [[10, 13, "MethodName", "style transfer module"]]}
{"text": "For sentiment , we use a mixture of IMDB ( Maas et al , 2011 ) and Yelp dataset ( Li et al , 2018 ) with 300k examples in the positive and negative sentiment each .", "entities": [[8, 9, "DatasetName", "IMDB"]]}
{"text": "For formality , we use GYAFC corpus ( Rao and Tetreault , 2018 ) which has 104k examples in each formal and informal class .", "entities": [[5, 6, "DatasetName", "GYAFC"]]}
{"text": "For pre - training , we use 12 - layer Transformer model with 512 hidden units , 16 heads , a dropout rate of 0.1 and learned positional embedding .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "We train our models with the Adam optimizer , and", "entities": [[6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "optimizer"]]}
{"text": "For singledimensional style evaluation , we generate sentences from models fine - tuned on negative corpus and positive corpus and compare the style accuracy of generated sentences .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "For instance , the classifier for evaluating sentiment accuracy is trained on sentiment corpus tagged with positive and negative class in IMDB and Yelp data .", "entities": [[8, 9, "MetricName", "accuracy"], [21, 22, "DatasetName", "IMDB"]]}
{"text": "We , therefore , employ the perplexities of these fine - tuned language models to gauge the style of the input text to guide our style transfer model .", "entities": [[25, 27, "TaskName", "style transfer"]]}
{"text": "We calculate the perplexity of each of these models on the test corpus from the same style and from the opposite style .", "entities": [[3, 4, "MetricName", "perplexity"]]}
{"text": "This implies that a language model fine - tuned on positive corpus shows higher perplexity for negative sentences and lower for positive sentences and vice versa .", "entities": [[14, 15, "MetricName", "perplexity"]]}
{"text": "This corroborates the effectiveness of these fine - tuned language models to serve as discriminators for training the style transfer module .", "entities": [[18, 21, "MethodName", "style transfer module"]]}
{"text": "We scale these scores between 0 - 100 , where higher ( 100 ) lexical score signifies formal style and lower ( 0 ) score signifies informal style .", "entities": [[5, 6, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "Besides this , we also calculate BLEU score between the transferred sentence generated by our model and the corresponding human reference transferred sentence , available for GYAFC and Yelp corpus ( ref - BLEU ) .", "entities": [[6, 8, "MetricName", "BLEU score"], [26, 27, "DatasetName", "GYAFC"], [33, 34, "MetricName", "BLEU"]]}
{"text": "This Dai et al , 2019 ) , Cascaded Discriminative LM method and multi - style transfer using Adapted Rewriting LM ( Syed et al , 2020 ) .", "entities": [[15, 17, "TaskName", "style transfer"]]}
{"text": "The perplexity is the measure of log likelihood of the generated sentence on the language model .", "entities": [[1, 2, "MetricName", "perplexity"]]}
{"text": "A lower perplexity is indicative of a more fluent sentence .", "entities": [[2, 3, "MetricName", "perplexity"]]}
{"text": "Dai et al ( 2019 ) use transformer - based model ( Style Transformer ) for single - dimensional style transfer .", "entities": [[13, 14, "MethodName", "Transformer"], [19, 21, "TaskName", "style transfer"]]}
{"text": "We train two independent Style Transformer models for sentiment and formality transfer and then perform transfer one after another to compare results with our model .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "We term this as Cascaded Style Transformer setup .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "The Style Transformer model is shown to have state - of - the - art performance in single - dimensional style transfer ; thus it provides an estimate of the performance of sequential single style transfer .", "entities": [[2, 3, "MethodName", "Transformer"], [20, 22, "TaskName", "style transfer"], [34, 36, "TaskName", "style transfer"]]}
{"text": "These are the closest baselines to our proposed approach , since other works dealing with multi - style transfer assume presence of jointly annotated dataset , which is a stronger assumption that we aim to relax .", "entities": [[17, 19, "TaskName", "style transfer"]]}
{"text": "In addition to our proposed model with multiple style transfer , we also train our encoder - decoder architecture with single discriminative LM for one style at a time and perform two stage transfer , similar to one with Cascaded Style Transformer ( Dai et al , 2019 ) setup .", "entities": [[8, 10, "TaskName", "style transfer"], [41, 42, "MethodName", "Transformer"]]}
{"text": "The results in Table 3 show that our model achieves better style control than the Cascaded Style Transformer ( Dai et al , 2019 ) as well as the joint transfer using Syed et al ( 2020 ) for both sentiment and formality .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "As seen in Table 3 , cascaded style transfer models perform poorly on content preservation .", "entities": [[7, 9, "TaskName", "style transfer"]]}
{"text": "This is because transferring style one after other leads to huge loss in content , thus both the two - stage models score lower on content preservation metrics , both w.r.t .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "The effect can also be observed in Table 4 which demonstrates qualitative results for Cascaded Style Transformer model and our model .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "Among the cascaded models , the Discriminative LM scores marginally better on content preservation than the Style Transformer model .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "We attribute this to initialization with the same pre - trained LM resulting in shared content space in the underlying single style transfer models .", "entities": [[21, 23, "TaskName", "style transfer"]]}
{"text": "However , due to independent training of the two single style transfer models , they are not able to model interplay between these styles and hence perform worse on style control than our proposed model trained jointly on multiple styles .", "entities": [[10, 12, "TaskName", "style transfer"]]}
{"text": "( Syed et al ( 2020 ) and our proposed approach ) , we note that content preservation is marginally better for Syed et al ( 2020 ) 's model , however , our model is able to yield much better style transfer owing to feedback on style control by multiple discriminators .", "entities": [[41, 43, "TaskName", "style transfer"]]}
{"text": "Based on comparable style control in Cascaded Style Transformer and our proposed approach on automatic metrics , we compare the transfer quality across these two models by a small - scale human study .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "These results are in line with our automatic evaluations and add confidence to the efficacy of our proposed approach in achieving style transfer across multiple dimensions .", "entities": [[21, 23, "TaskName", "style transfer"]]}
{"text": "We propose an approach to extend currently existing style transfer work to multiple style setting without imposing any extra constraints on availability of dataset .", "entities": [[8, 10, "TaskName", "style transfer"]]}
{"text": "We exploit multiple discriminative language models with an encoder - decoder framework , all emerging from large transformer - based language models pretrained on Masked Language Modeling objective and fine - tuned separately for transfer and discriminative purposes .", "entities": [[24, 27, "TaskName", "Masked Language Modeling"]]}
{"text": "We show that unified single step transfer approach is able to achieve better transfer while offering much better content preservation which is paramount to any style transfer task .", "entities": [[25, 27, "TaskName", "style transfer"]]}
{"text": "Ethics Statement .", "entities": [[0, 1, "DatasetName", "Ethics"]]}
{"text": "As with any generative task , style transfer too suffers from the potential misuse for fact distortion , plagiarism and more .", "entities": [[6, 8, "TaskName", "style transfer"]]}
{"text": "Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text", "entities": [[10, 12, "TaskName", "Question Answering"]]}
{"text": "Numerical reasoning skills are essential for complex question answering ( CQA ) over text .", "entities": [[7, 9, "TaskName", "question answering"]]}
{"text": "Complex Question Answering ( CQA ) is a challenging task , requiring a model to perform compositional and numerical reasoning .", "entities": [[1, 3, "TaskName", "Question Answering"]]}
{"text": "Originally proposed for the visual question answering ( VQA ) task , Neural Module Networks ( NMNs ) ( Andreas et", "entities": [[4, 7, "DatasetName", "visual question answering"], [8, 9, "TaskName", "VQA"]]}
{"text": "NMNs achieves the best performance on a subset of the challenging DROP dataset ( Dua et al , 2019 ) and is interpertable by nature .", "entities": [[11, 12, "DatasetName", "DROP"]]}
{"text": "Figure 1 : Two examples in the DROP ( Dua et al , 2019 ) dataset that demonstrate the deficienties of NMNs .", "entities": [[7, 8, "DatasetName", "DROP"]]}
{"text": "Finally , we strengthen the auxiliary loss to increase attention values of entities in closer vicinity within a sentence .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "Complex Question Answering focuses on questions that require capabilities beyond multi - hop reasoning .", "entities": [[1, 3, "TaskName", "Question Answering"]]}
{"text": "A number of neural models were recently proposed to address the CQA task , such as BiDAF ( Seo et al , 2017 ) , QANet ( Yu et al , 2018 ) , NMNs ( Gupta et al , 2020 ) and NumNet ( Ran et al , 2019 ) , which achieved high performance on benchmark datasets such as DROP ( Dua et al , 2019 ) .", "entities": [[61, 62, "DatasetName", "DROP"]]}
{"text": "Using dependency parsing of questions , Saha et al ( 2021 ) focused on the numerical part and obtained excellent results on different kinds of numerical reasoning questions .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "i = softmax ( S n i ) , ( 2 ) T", "entities": [[2, 3, "MethodName", "softmax"]]}
{"text": "Inspired by this idea , we propose the question - toparagraph alignment modification to number - related modules .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Thus , if both the i th paragraph token and the j th number token belong to the same sentence , element U n ij , in row i and column j , is set to 1 , otherwise 0 : U n ij = 1 , ( i s t ) ( n j s t ) 0 , otherwise ( 9 )", "entities": [[39, 40, "DatasetName", "0"], [58, 59, "DatasetName", "0"]]}
{"text": "For instance , the auxiliary loss for the \" find - num \" module is as follows : H n loss", "entities": [[5, 6, "MetricName", "loss"], [20, 21, "MetricName", "loss"]]}
{"text": "However , these loss functions still allow irrelevant numbers to have spuriously high attention values .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "Therefore , we propose to strengthen the auxiliary loss to further concentrate attention mass to those tokens within the same sentence : H n loss", "entities": [[8, 9, "MetricName", "loss"], [24, 25, "MetricName", "loss"]]}
{"text": "We evaluate model performance on the same subset of the DROP dataset used by the original NMNs ( Gupta et al , 2020 ) , which contains approx .", "entities": [[10, 11, "DatasetName", "DROP"]]}
{"text": "Neural Moudule Networks ( NMNs ) represent an interpretable state - of - the - art approach to complex question answering over text .", "entities": [[19, 21, "TaskName", "question answering"]]}
{"text": "In order to solve the complex question answering problem , Gupta et al ( 2020 ) proposed a Neural Module Networks ( NMNs ) model .", "entities": [[6, 8, "TaskName", "question answering"]]}
{"text": "The computational resources for this work were provided by the Multi - modal Australian Sci - enceS Imaging and Visualisation Environment ( MAS - SIVE ) ( www.massive.org.au ) .", "entities": [[22, 23, "MethodName", "MAS"]]}
{"text": "More Identifiable yet Equally Performant Transformers for Text Classification", "entities": [[7, 9, "TaskName", "Text Classification"]]}
{"text": "Transformer 's predictions are widely explained by the attention weights , i.e. , a probability distribution generated at its self - attention unit ( head ) .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "We prove the applicability of such variations by providing empirical justifications on varied text classification tasks .", "entities": [[13, 15, "TaskName", "text classification"]]}
{"text": "Widely adopted Transformer architecture ( Vaswani et al , 2017 ) has obviated the need for sequential processing of the input that is enforced in traditional Recurrent Neural Networks ( RNN ) .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "As a result , compared to a single - layered LSTM or RNN model , a single - layered Transformer model is computationally more efficient , reflecting in a relatively shorter training time ( Vaswani et al , 2017 ) .", "entities": [[10, 11, "MethodName", "LSTM"], [19, 20, "MethodName", "Transformer"]]}
{"text": "This advantage encourages the training of deep Transformer - based language models on largescale datasets .", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "A large number of SOTA machine learning systems even beyond NLP ( Lu et al , 2019 ) are inspired by the building blocks of Transformer that is multi - head self - attention ( Radford et al , 2018 ; Devlin et al , 2018 ) .", "entities": [[25, 26, "MethodName", "Transformer"]]}
{"text": "k =", "entities": [[0, 2, "HyperparameterName", "k ="]]}
{"text": "A Transformer layer consists of multiple heads , where each head performs selfattention computations , we break the head computations in two phases : Phase 1 : Calculation of attention weights a ( k , i ) .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "The identifiability in Transformer has been recently studied by Brunner et al ( 2019 ) which provides theoretical claims that under mild conditions of input length , attention weights are not unique to the head 's output .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "In this work , we probe the identifiability of attention weights in Transformer from a perspective that was ignored in Brunner et al ( 2019 ) .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "We explore the previously overlooked first phase of selfattention for its contribution to the identifiability in Transformer .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "For the regular setting of the Transformer encoder where d v depends on the number of attention heads and token embedding dimension , we propose to reduce d k .", "entities": [[6, 7, "MethodName", "Transformer"], [20, 22, "HyperparameterName", "embedding dimension"]]}
{"text": "Embedding dimension can be tuned according to the sequence length up to which identifiability is desired .", "entities": [[0, 2, "HyperparameterName", "Embedding dimension"]]}
{"text": "We evaluate the performance of the proposed variants on varied text classification tasks comprising of ten datasets ( 5 ) .", "entities": [[10, 12, "TaskName", "text classification"]]}
{"text": "In this paper , our goal is to provide concrete theoretical analysis , experimental observations , and possible simple solutions to identifiability of attention weights in Transformer .", "entities": [[26, 27, "MethodName", "Transformer"]]}
{"text": "The idea behind identifiable variants of the Transformer is - the harder it is to obtain alternative attention weights , the likelier is they are identifiable , which is a desirable property of the architecture .", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "We provide Transformer variants that are identifiable and validate them empirically by analysing the numerical rank of the attention matrix generated in the self - attention head of the Transformer encoder .", "entities": [[2, 3, "MethodName", "Transformer"], [29, 30, "MethodName", "Transformer"]]}
{"text": "The variants have strong mathematical support and simple to adopt in the standard Transformer settings .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "We provide empirical evaluations on varied text classification tasks that show higher identifiability does not compromise with the task 's performance .", "entities": [[6, 8, "TaskName", "text classification"]]}
{"text": "We base our analysis on the building block of Transformer , i.e. , the encoder layer ( Vaswani et al , 2017 ) .", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": ", z ds } R de , where d e denotes token embedding dimension .", "entities": [[12, 14, "HyperparameterName", "embedding dimension"]]}
{"text": ", p ds } R de . Multi - head Attention .", "entities": [[7, 11, "MethodName", "Multi - head Attention"]]}
{"text": "The attention weights A R ds\u00d7ds can be computed by A = softmax", "entities": [[12, 13, "MethodName", "softmax"]]}
{"text": "A V D = A T , ( 2 ) where D R dv\u00d7de is a linear layer and", "entities": [[16, 18, "MethodName", "linear layer"]]}
{"text": "the matrix T R ds\u00d7de denotes the operation V D. The R ds\u00d7de output of multi - head attention can be expressed as a summation over H obtained for each head 3 .", "entities": [[15, 19, "MethodName", "multi - head attention"]]}
{"text": "1 . In regular Transformer setting , a token vector is t i { ( z j + p j ) } ds i=1 is d e = 512 dimensional , number of heads h=8 , size of d", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "= Norm ( t i + ReLU", "entities": [[6, 7, "MethodName", "ReLU"]]}
{"text": "Linear 1 and Linear 2 are linear layers with 2048 and 512 nodes , respectively .", "entities": [[9, 10, "DatasetName", "2048"]]}
{"text": "Norm denotes minibatch layer normalization .", "entities": [[3, 5, "MethodName", "layer normalization"]]}
{"text": "A is unidentifiable if there exist a\u00f1 A , ( \u00c3 = 0 ) , such that ( A + \u00c3 ) is obtainable from phase - 1 of head computations and satisfy (", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "A T = \u21d2\u00c3 T = 0 .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "Under this constraint , we get\u00e3 i T = 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "v T T = 0 } ( 3 ) dim LN ( T )", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "= 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "then LN ( T ) = { 0 } , it leads to the only solution of constraint - R1 that is\u00c3 = 0 .", "entities": [[7, 8, "DatasetName", "0"], [23, 24, "DatasetName", "0"]]}
{"text": "= min ds , 64 where the last inequality is obtained for a head in the regular Transformer", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "The model is trained to predict the sentiment of IMDB reviews ( 5 ) .", "entities": [[9, 10, "DatasetName", "IMDB"]]}
{"text": "d s \u2212 rank ( T ) = 0 if d s \u2264 d v , ( d s \u2212 d v ) if d s > d v .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "= max ( d s \u2212 d v , 0 ) ( 7 ) With this , we infer A is identifiable if d s \u2264 d v = 64 .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "As a first step , we focus our analysis on the attention matrix without applying softmax non - linearity , i.e. , A = Q K T \u221a dq .", "entities": [[15, 16, "MethodName", "softmax"]]}
{"text": "The analysis is crucial to identify constraints coming from the first phase of self - attention in Transformer that impact identifiability .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "Insights from this will help us analyse softmax version of A.", "entities": [[7, 8, "MethodName", "softmax"]]}
{"text": "The softmax over attention logits generates attention weights with each row of A ( i.e. , a i 's ) is constrained to be a probability distribution .", "entities": [[1, 2, "MethodName", "softmax"]]}
{"text": "Hence , we can define constraint over\u00c3 as ( A + \u00c3 ) \u2265 0 ( P1 )", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "A T = 0 ( P2 ) A 1 = 0 .", "entities": [[3, 4, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "( P3 ) P1 is non - negativity constraint on ( A + \u00c3 ) as it is supposed to be the output of softmax ; P2 de - notes\u00c3 LN ( T ) ; P3 can be derived from the fact ( A + \u00c3 )", "entities": [[1, 2, "DatasetName", "P3"], [24, 25, "MethodName", "softmax"], [35, 36, "DatasetName", "P3"]]}
{"text": "= 1 = \u21d2 ( A 1 + \u00c3 1 ) = 1 = \u21d2\u00c3 1 = 0 as ( A 1 = 1 ) .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "The constraint in P2 and P3 can be combined and reformulated as\u00c3 [ T , 1 ] = 0 .", "entities": [[5, 6, "DatasetName", "P3"], [18, 19, "DatasetName", "0"]]}
{"text": "= max d s \u2212 ( d v + 1 ) , 0 .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "Disregarding the extreme cases when a i is a one - hot distribution , Brunner et al ( 2019 ) proved the existence and construction of non - trivial\u00c3 's satisfying all the constraints P1 , P2 , and P3 . 5", "entities": [[39, 40, "DatasetName", "P3"]]}
{"text": "However , the proof by Brunner et al ( 2019 ) missed the constraint - R2 , hence the existence of a non - trivial\u00c3 satisfying only the set of constraints P1 , P2 and P3 may not be a valid proposition to claim attention weights unidentifiability .", "entities": [[35, 36, "DatasetName", "P3"]]}
{"text": "Essentially , the work largely ignored the constraints coming from the rank of the matrix that produces A after softmax 6 .", "entities": [[19, 20, "MethodName", "softmax"]]}
{"text": "Let A l denote logits Q K T \u221a dq and softmax ( A l ) =", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "( A + \u00c3 ) , where softmax is operated over each row of A l .", "entities": [[7, 8, "MethodName", "softmax"]]}
{"text": "The constraint P4 confirms if there exists a logit matrix A l that can generate ( A + \u00c3 ) , given constraints P1 , P2 , and P3 are satisfied .", "entities": [[28, 29, "DatasetName", "P3"]]}
{"text": "We can retrieve the set of matrices A l such that softmax ( A l ) =", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "For an arbitrarily picked\u00c3 satisfying constraint P1 , P2 , and P3 , the dimensions of affine span S of { \u00e2 1 , . . .", "entities": [[11, 12, "DatasetName", "P3"]]}
{"text": "Thus , the set of ( A + \u00c3 ) satisfying constraint P1 , P2 and P3 are not always obtainable from attention head for d s >", "entities": [[16, 17, "DatasetName", "P3"]]}
{"text": "We postulate Although it is easier to construct\u00c3 satisfying constraints P1 , P2 and P3 , it is hard to construct\u00c3 satisfying constraint P4 over the rank of logit matrix A l .", "entities": [[14, 15, "DatasetName", "P3"]]}
{"text": "From IMDB dataset ( 5 ) , we randomly sample a set of reviews with token sequence length", "entities": [[1, 2, "DatasetName", "IMDB"]]}
{"text": "For each review , we construct 1000\u00c3 's satisfying constraints P1 , P2 , and P3 - First , we train a Transformer encoder - based IMDB review sentiment classifier ( 6 ) .", "entities": [[15, 16, "DatasetName", "P3"], [22, 23, "MethodName", "Transformer"], [26, 27, "DatasetName", "IMDB"]]}
{"text": "Thus , the experimentally constructed A l 's do not claim unidentifiability of A as it fails to satisfy the constraint P4 , while for Brunner et al ( 2019 ) , it falls under the solution set to prove unidentifiability as it meets constraints P1 , P2 and P3 .", "entities": [[49, 50, "DatasetName", "P3"]]}
{"text": "Based on the Identifiability analysis in 3 , we propose basic solutions to make Transformer 's attention weights identifiable .", "entities": [[14, 15, "MethodName", "Transformer"]]}
{"text": "Contrary to the regular Transformer setting where d k = d v , a simple approach is to decrease the value of d k that is the size of the key and query vector .", "entities": [[4, 5, "MethodName", "Transformer"], [8, 10, "HyperparameterName", "k ="]]}
{"text": "To resolve the unidentifiability issue when sequence length exceeds the size of value vector , we propose to keep the value vector size and token embedding dimension to be more than ( or equal to ) the maximum allowed input tokens , i.e. , d v \u2265 d s - max .", "entities": [[25, 27, "HyperparameterName", "embedding dimension"]]}
{"text": "For the empirical analysis of our proposed solutions as mentioned in 4 , we conduct our experiments on the following varied text classification tasks :", "entities": [[21, 23, "TaskName", "text classification"]]}
{"text": "IMDB ( Maas et al , 2011 ) .", "entities": [[0, 1, "DatasetName", "IMDB"]]}
{"text": "The dataset for the task of sentiment classification consist of IMDB movie reviews with their sentiment as positive or negative .", "entities": [[10, 13, "DatasetName", "IMDB movie reviews"]]}
{"text": "TREC ( Voorhees and Tice , 2000 ) .", "entities": [[0, 1, "DatasetName", "TREC"]]}
{"text": "SST ( Socher et al , 2013 ) .", "entities": [[0, 1, "DatasetName", "SST"]]}
{"text": "Stanford sentiment analysis dataset consist of 11 , 855 sentences obtained from movie reviews .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}
{"text": "The provided train / test / valid split is 8 , 544/2 , 210/1 , 101 . 8 ds - max < de as in the regular Transformer setting .", "entities": [[27, 28, "MethodName", "Transformer"]]}
{"text": "SNLI ( Bowman et al , 2015 ) .", "entities": [[0, 1, "DatasetName", "SNLI"]]}
{"text": "There are 560 , 000 samples for training and 38 , 000 samples for testing , equally split into positive and negative polarities . DBPedia .", "entities": [[24, 25, "DatasetName", "DBPedia"]]}
{"text": "The Ontology dataset for topic classification consist of 14 non - overlapping classes each with 40 , 000 samples for training and 5 , 000 samples for testing .", "entities": [[1, 2, "MethodName", "Ontology"], [4, 6, "TaskName", "topic classification"]]}
{"text": "AG News .", "entities": [[0, 2, "DatasetName", "AG News"]]}
{"text": "The balanced dataset for 10class topic classification contain 1 , 400 , 000 samples for training and 50 , 000 samples for testing .", "entities": [[5, 7, "TaskName", "topic classification"]]}
{"text": "Except for the SST and SNLI , where the validation split is already provided , we flag 30 % of the train set as part of the validation set and the rest 70 % were used for model parameter learning .", "entities": [[3, 4, "DatasetName", "SST"], [5, 6, "DatasetName", "SNLI"]]}
{"text": "We normalize the text by lower casing , removing special characters , etc . 9 For each task , we construct separate 1 - Gram vocabulary ( U ) and initialize a trainable randomly sampled token embedding ( U \u00d7 d e ) from N ( 0 , 1 ) .", "entities": [[46, 47, "DatasetName", "0"]]}
{"text": "For the regular Transformer setting , we fix the number of heads h to 8 and the size of value vector d v = d e /h that is 64 .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "For the identifiable variant of the Transformer encoder , d v = d e = 512 , this is equal to d s - max to keep it identifiable up to the maximum permissible number of tokens .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "In the case of SNLI , we use the shared encoder for both premise and hypothesis ; the output of their first tokens is then concatenated just before the final classification layer .", "entities": [[4, 5, "DatasetName", "SNLI"]]}
{"text": "We report the test accuracy obtained at the epoch with the best validation accuracy .", "entities": [[4, 5, "MetricName", "accuracy"], [13, 14, "MetricName", "accuracy"]]}
{"text": "To generate the numerical rank plot on IMDB dataset as shown in fig .", "entities": [[7, 8, "DatasetName", "IMDB"]]}
{"text": "2 , we train a separate Transformer encoder - based classifier .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "The zero dimensional ( left ) null space of T con - firms there exist no nontrivial solution to the constraint constraint - R2 , i.e. , \u00c3 = { 0 } .", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "Figure 6 : Scatter plots in red and blue show rank ( T ) and dim LN ( T ) , respectively , for matrices T obtained from the second phase of attention by feeding IMDB samples to the encoder .", "entities": [[35, 36, "DatasetName", "IMDB"]]}
{"text": "= 0 and thus attention weights are identifiable .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "To investigate this issue , we compare the performance of the identifiable Transformer encoder against its regular settings ( 6 ) on varied text classification tasks .", "entities": [[12, 13, "MethodName", "Transformer"], [23, 25, "TaskName", "text classification"]]}
{"text": "For the regular setting , as discussed in 4 as one of the solutions , the Transformer can be made identifiable by decreasing the size of the key vector d k .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "The rows of the Table 1 corresponding to Con denotes regular Transformer setting with varying size of key vector .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "We observe the classification accuracy at the lower d k is comparable or higher than large d k values , thus , the enhanced identifiability does not compromise with the model 's classification accuracy .", "entities": [[4, 5, "MetricName", "accuracy"], [33, 34, "MetricName", "accuracy"]]}
{"text": "This could be due to the larger size of value vector leading to the more number of parameters in Add that compensate for the significant reduction in the model 's accuracy .", "entities": [[15, 18, "HyperparameterName", "number of parameters"], [30, 31, "MetricName", "accuracy"]]}
{"text": "This work probed Transformer for identifiability of self - attention , i.e. , the attention weights can be uniquely identified from the head 's output .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "The identifiable variants do not show any performance drop when experiments are done on varied text classification tasks .", "entities": [[15, 17, "TaskName", "text classification"]]}
{"text": "Future works may analyse the critical impact of identifiability on the explainability and interpretability of the Transformer .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "1\u00d7mp | v T P = 0 } ( 10 )", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "The only solution to the system of equations v P = 0 is trivial , i.e. , v=0 .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "In 3 , we utilize our knowledge of appendix A.2 and appendix A.3 to analyse identifiability in a Transformer .", "entities": [[18, 19, "MethodName", "Transformer"]]}
{"text": "NLQuAD : A Non - Factoid Long Question Answering Data Set", "entities": [[7, 9, "TaskName", "Question Answering"]]}
{"text": "We introduce NLQuAD , the first data set with baseline methods for non - factoid long question answering , a task requiring documentlevel language understanding .", "entities": [[16, 18, "TaskName", "question answering"]]}
{"text": "In contrast to existing span detection question answering data sets , NLQuAD has non - factoid questions that are not answerable by a short span of text and demanding multiple - sentence descriptive answers and opinions .", "entities": [[6, 8, "TaskName", "question answering"]]}
{"text": "We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union ( IoU ) , which measures position - sensitive overlap between the predicted and the target answer spans .", "entities": [[6, 8, "MetricName", "F1 score"], [19, 20, "MetricName", "IoU"]]}
{"text": "To establish baseline performances , we compare BERT , RoBERTa , and Longformer models .", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "MethodName", "RoBERTa"], [12, 13, "MethodName", "Longformer"]]}
{"text": "Experimental results and human evaluations show that Longformer outperforms the other architectures , but results are still far behind a human upper bound , leaving substantial room for improvements .", "entities": [[7, 8, "MethodName", "Longformer"]]}
{"text": "NLQuAD 's samples exceed the input limitation of most pretrained Transformer - based models , encouraging future research on long sequence language models .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "Over the last few years , there have been remarkable improvements in the area of Machine Reading Comprehension ( MRC ) and open - domain Question Answering ( QA ) due to the availability of large scale data sets such as SQuAD ( Rajpurkar et al , 2016 ) and pre - trained language models such as BERT ( Devlin et al , 2018 ) .", "entities": [[15, 18, "TaskName", "Machine Reading Comprehension"], [22, 27, "TaskName", "open - domain Question Answering"], [41, 42, "DatasetName", "SQuAD"], [57, 58, "MethodName", "BERT"]]}
{"text": "To support research towards non - factoid and long QA tasks and to address the existing shortcomings as identified above , we have built NLQuAD , a non - factoid long question answering data set .", "entities": [[31, 33, "TaskName", "question answering"]]}
{"text": "This is in contrast to existing long - context but factoid QA data sets such as NewsQA ( Trischler et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) , NarrativeQA ( Ko\u010disk\u00fd et al , 2018 ) , DuoRC ( Saha et al , 2018 ) , HotpotQA ( Yang et al , 2018 ) , and Natural Questions ( Kwiatkowski et al , 2019 ) .", "entities": [[16, 17, "DatasetName", "NewsQA"], [25, 26, "DatasetName", "TriviaQA"], [34, 35, "DatasetName", "NarrativeQA"], [43, 44, "DatasetName", "DuoRC"], [52, 53, "DatasetName", "HotpotQA"], [62, 64, "DatasetName", "Natural Questions"]]}
{"text": "In particular , Natural Questions covers two types of short and long answers .", "entities": [[3, 5, "DatasetName", "Natural Questions"]]}
{"text": "Furthermore , although a small portion ( 13 % ) of Natural Questions samples have only long answers , they are still spans of simple facts .", "entities": [[11, 13, "DatasetName", "Natural Questions"]]}
{"text": "In most existing QA data sets such as SQuAD , crowd - workers generate questions based on provided short passages and extract answers from the passages ( Rajpurkar et al , 2016 ) .", "entities": [[8, 9, "DatasetName", "SQuAD"]]}
{"text": "This method of question generation can make QA samples trivial because models can simply detect the most related span to the question by guessing based on shallow pattern matching ( Ko\u010disk\u00fd et al , 2018 ) .", "entities": [[3, 5, "TaskName", "question generation"]]}
{"text": "NLQuAD , unlike MS MARCO ( Bajaj et al , 2016 ) and ELI5 ( Fan et al , 2019 ) , does not use information retrieval ( IR ) methods to collect supporting documents .", "entities": [[3, 5, "DatasetName", "MS MARCO"], [13, 14, "DatasetName", "ELI5"], [25, 27, "TaskName", "information retrieval"]]}
{"text": "With an average document length and answer length of 877 and 175 words , respectively , it exceeds the maximum input length of the state of the art QA models such as BERT ( Devlin et al , 2018 ) and RoBERTa ( Liu et al , 2019 ) due to their memory and computational requirements .", "entities": [[32, 33, "MethodName", "BERT"], [41, 42, "MethodName", "RoBERTa"]]}
{"text": "We also show the shortcomings of the F1 score and ROUGE - N scores in evaluating long sequences .", "entities": [[7, 9, "MetricName", "F1 score"]]}
{"text": "There is a higher chance of overlap between the word N - grams in two long sequences causing F1 and ROUGE - N to over - estimate the performance .", "entities": [[18, 19, "MetricName", "F1"]]}
{"text": "Therefore , we propose to use Intersection over Union ( IoU ) measuring position - sensitive overlap between two spans .", "entities": [[10, 11, "MetricName", "IoU"]]}
{"text": "To handle the input length limitations of BERT and RoBERTa , we pro - pose to train these models in a sliding - window approach ; ( 4 ) We finally show that the state - of - the - art models have limited performance in the non - factoid long QA task .", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "MethodName", "RoBERTa"]]}
{"text": "SQuAD ( Rajpurkar et al , 2016 ) is a factoid span detection data set with short answers .", "entities": [[0, 1, "DatasetName", "SQuAD"]]}
{"text": "DROP ( Dua et al , 2019 ) makes the problem more challenging by adversarially - created questions requiring discrete reasoning over the text .", "entities": [[0, 1, "DatasetName", "DROP"]]}
{"text": "SQuAD and DROP use Wikipedia pages as context passages whereas SearchQA ( Dunn et al , 2017 ) uses IR approaches to collect context passages .", "entities": [[0, 1, "DatasetName", "SQuAD"], [2, 3, "DatasetName", "DROP"], [10, 11, "DatasetName", "SearchQA"]]}
{"text": "Answer generation based on a set of passages is another approach to address this task .", "entities": [[0, 2, "TaskName", "Answer generation"]]}
{"text": "MS MARCO ( Bajaj et al , 2016 ) consists of real - world search queries and retrieved documents corresponding to the queries .", "entities": [[0, 2, "DatasetName", "MS MARCO"]]}
{"text": "There is also a range of multiple - choice QA tasks such as RACE ( Lai et al , 2017 ) , ARC ( Clark et al , 2018 ) , SWAQ ( Zellers et al , 2018 ) , and COS - MOS QA ( Huang et al , 2019 ) that are clustered together with the short - context QA data sets .", "entities": [[13, 14, "DatasetName", "RACE"], [22, 23, "DatasetName", "ARC"]]}
{"text": "NewsQA ( Trischler et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) , NarrativeQA ( Ko\u010disk\u00fd et al , 2018 ) , and DuoRC ( Saha et al , 2018 ) fall into this category and their documents are extracted from news articles , stories , and movie plots , respectively .", "entities": [[0, 1, "DatasetName", "NewsQA"], [9, 10, "DatasetName", "TriviaQA"], [18, 19, "DatasetName", "NarrativeQA"], [28, 29, "DatasetName", "DuoRC"]]}
{"text": "DuReader ( He et al , 2018 ) consists of real - word Chinese queries and corresponding retrieved documents .", "entities": [[0, 1, "DatasetName", "DuReader"]]}
{"text": "HotpotQA ( Yang et al , 2018 ) is a multi - hop data set , but the answer length of its factoid questions is as limited as that of short - context QA data sets .", "entities": [[0, 1, "DatasetName", "HotpotQA"]]}
{"text": "Natural Questions ( Kwiatkowski et al , 2019 ) is a factoid QA task with much longer documents and two types of answer lengths .", "entities": [[0, 2, "DatasetName", "Natural Questions"]]}
{"text": "ELI5 ( Fan et al , 2019 ) consists of real - world questions with answers provided by the Reddit community .", "entities": [[0, 1, "DatasetName", "ELI5"], [19, 20, "DatasetName", "Reddit"]]}
{"text": "Table 1 compares existing long - context question answering data sets along with SQuAD and MS MARCO .", "entities": [[7, 9, "TaskName", "question answering"], [13, 14, "DatasetName", "SQuAD"], [15, 17, "DatasetName", "MS MARCO"]]}
{"text": "To investigate the difficulty level of NLQuAD for state - of - the - art QA systems and to establish baseline results , we evaluate the performance of BERT ( Devlin et al , 2018 ) , RoBERTa ( Liu et al , 2019 ) , and Longformer ( Beltagy et al , 2020 ) .", "entities": [[28, 29, "MethodName", "BERT"], [37, 38, "MethodName", "RoBERTa"], [47, 48, "MethodName", "Longformer"]]}
{"text": "Longformer is a scalable model for processing long documents and has been used for long sequences such as document classification ( Beltagy et al , 2020 ) and document re - ranking ( Sekuli\u0107 et al , 2020 ) .", "entities": [[0, 1, "MethodName", "Longformer"], [18, 20, "TaskName", "document classification"]]}
{"text": "The BERT QA model concatenates question and document pairs into a single sequence and predicts the answer span by a dot product between the final hidden vectors , a start vector and an end vector ( Devlin et al , 2018 ) .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "Due to the memory and computational requirements , BERT can encode sequences with a maximum length of 512 tokens that is less than the average sample length in NLQuAD .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "We train BERT on the segments independently .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "RoBERTa has the same model architecture and input length limitation as BERT but with a robustly optimized pre - training scheme allowing it to generalize better to downstream tasks such as QA ( Liu et al , 2019 ) .", "entities": [[0, 1, "MethodName", "RoBERTa"], [11, 12, "MethodName", "BERT"]]}
{"text": "We apply the same sliding window approach for RoBERTa .", "entities": [[8, 9, "MethodName", "RoBERTa"]]}
{"text": "In order to process the question and entire documents at the same time , we use the Longformer model .", "entities": [[17, 18, "MethodName", "Longformer"]]}
{"text": "It employs an attention mechanism scaling linearly with the sequence length which enables Longformer to process up to 4 , 096 tokens .", "entities": [[13, 14, "MethodName", "Longformer"]]}
{"text": "Exact Match determines if the prediction exactly matches the target which can be a too strict criterion for long answers .", "entities": [[0, 2, "MetricName", "Exact Match"]]}
{"text": "The F1 score measures the overlap between the words in the prediction and the target .", "entities": [[1, 3, "MetricName", "F1 score"]]}
{"text": "The same holds for ROUGE - L with the Longest Common Sub - sequence ( LCS ) because of a high chance of longer LCSs between two long sequences .", "entities": [[4, 7, "MetricName", "ROUGE - L"]]}
{"text": "To better take sequence similarities into account , we propose to evaluate models with the Intersection over Union ( IoU ) score , also known as Jaccard Index .", "entities": [[19, 20, "MetricName", "IoU"]]}
{"text": "IoU is defined as follows : IoU =", "entities": [[0, 1, "MetricName", "IoU"], [6, 7, "MetricName", "IoU"]]}
{"text": "Prediction : The group was set up more than 50 years ago in the era of Spanish dictator General Franco , who repressed the Basques politically and culturally .", "entities": [[18, 19, "DatasetName", "General"]]}
{"text": "The F1 and ROUGE - N scores are always higher than IoU , but the metrics perform similarly in their higher values .", "entities": [[1, 2, "MetricName", "F1"], [11, 12, "MetricName", "IoU"]]}
{"text": "We manually inspected the spans with F1>0 and IoU=0 and saw no significant semantic similarity between the predicted answer span and the target span .", "entities": [[13, 15, "TaskName", "semantic similarity"]]}
{"text": "We analyze the performance of BERT and RoBERTa with different hyper - parameters on the development set in Table 5 .", "entities": [[5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "RoBERTa"]]}
{"text": "RoBERTa constantly outperforms BERT , which is to be expected as RoBERTa is optimized robustly during the pretraining .", "entities": [[0, 1, "MethodName", "RoBERTa"], [3, 4, "MethodName", "BERT"], [11, 12, "MethodName", "RoBERTa"]]}
{"text": "We use the official AllenAI Longformer code 5 to train Longformer on NLQuAD .", "entities": [[5, 6, "MethodName", "Longformer"], [10, 11, "MethodName", "Longformer"]]}
{"text": "While Longformer significantly outperforms BERT and RoBERTa , its performance , particularly in terms of IoU and EM , is far from perfect .", "entities": [[1, 2, "MethodName", "Longformer"], [4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "RoBERTa"], [15, 16, "MetricName", "IoU"], [17, 18, "MetricName", "EM"]]}
{"text": "ing the best human answer in terms of our primary evaluation metric ( IoU ) for each sample .", "entities": [[13, 14, "MetricName", "IoU"]]}
{"text": "Figure 6 shows that the target answers are preferred in 37 % and 64 % of cases over the Longformer and RoBERTa predictions , respectively .", "entities": [[19, 20, "MethodName", "Longformer"], [21, 22, "MethodName", "RoBERTa"]]}
{"text": "Figure 7 compares the performance of BERT , RoBERTa , and Longformer for instances with different document and answer lengths .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "RoBERTa"], [11, 12, "MethodName", "Longformer"]]}
{"text": "Surprisingly , BERT and RoBERTa outperform Longformer for longer answers .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "RoBERTa"], [6, 7, "MethodName", "Longformer"]]}
{"text": "The same pattern occurs for F1 and EM ( not shown in the figure ) .", "entities": [[5, 6, "MetricName", "F1"], [7, 8, "MetricName", "EM"]]}
{"text": "Figure 7 ( right ) shows that RoBERTa and BERT behave completely differently compared to Longformer for longer answer lengths .", "entities": [[7, 8, "MethodName", "RoBERTa"], [9, 10, "MethodName", "BERT"], [15, 16, "MethodName", "Longformer"]]}
{"text": "The former models have a bias to predict longer spans while Longformer under - estimates the length of the answer span .", "entities": [[11, 12, "MethodName", "Longformer"]]}
{"text": "This different behaviour might be due to the sliding window approach and the prediction aggregation in the RoBERTa and BERT models and the attention dilation strategy in Longformer .", "entities": [[17, 18, "MethodName", "RoBERTa"], [19, 20, "MethodName", "BERT"], [27, 28, "MethodName", "Longformer"]]}
{"text": "We introduce NLQuAD , a non - factoid long question answering data set from BBC news articles .", "entities": [[9, 11, "TaskName", "question answering"]]}
{"text": "We propose to use Intersection over Union ( IoU ) as an evaluation metric for long question answering .", "entities": [[8, 9, "MetricName", "IoU"], [16, 18, "TaskName", "question answering"]]}
{"text": "To establish a baseline performance , we experimented with the BERT , RoBERTa , and Longformer question answering models .", "entities": [[10, 11, "MethodName", "BERT"], [12, 13, "MethodName", "RoBERTa"], [15, 16, "MethodName", "Longformer"], [16, 18, "TaskName", "question answering"]]}
{"text": "We hope NLQuAD will inspire more research in the area of document - level language understanding and question answering .", "entities": [[17, 19, "TaskName", "question answering"]]}
{"text": "To do so , we extend the POLAR - framework that transforms word embeddings to interpretable counterparts and apply it to word - emoji embeddings trained on four years of messaging data from the Jodel social network .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "We devise a crowdsourced human judgement experiment to study six usecases , evaluating against words only , what role emoji can play in adding interpretability to word embeddings .", "entities": [[26, 28, "TaskName", "word embeddings"]]}
{"text": "Word embeddings create a vector - space representation in which words with a similar meaning are in close proximity .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "Yet , emoji are widely used in casual communication , e.g. , Online Social Networks ( OSN ) , and are known to extend textual expressiveness , demonstrated to benefit , e.g. , sentiment analysis ( Novak et al , 2015 ;", "entities": [[33, 35, "TaskName", "sentiment analysis"]]}
{"text": "We raise the question if we can leverage the expressiveness of emoji to make word embeddings - and thus also emoji - interpretable .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "Motivated and based upon POLAR ( Mathew et al , 2020 ) , we deploy a revised variant POLAR \u03c1 that transforms arbitrary word embeddings into interpretable counterparts .", "entities": [[23, 25, "TaskName", "word embeddings"]]}
{"text": "We design a crowdsourced human judgement experiment ( 4 ) to study if adding emoji to word embeddings and POLAR \u03c1 in particular increases the interpretability - while also answering how to describe emoji best .", "entities": [[16, 18, "TaskName", "word embeddings"]]}
{"text": "Our human judgement experiment involves six campaigns explaining Words ( W/ * ) or Emoji ( E/ * ) with Words , Figure 1 : The POLAR - framework ( Mathew et al , 2020 ) makes word embeddings interpretable leveraging polar opposites .", "entities": [[37, 39, "TaskName", "word embeddings"]]}
{"text": "It provides a new interpretable embedding subspace with systematic polar opposite scales : Along six use - cases , we evaluate which role emoji expressiveness plays in adding interpretability to word embeddings .", "entities": [[30, 32, "TaskName", "word embeddings"]]}
{"text": "That is , starting with a corpus and its vocabulary V , a word embedding created by an algorithm a ( e.g. , Word2Vec or GloVe ) assigns vectors \u2212 W a v R d on d dimensions to all words v V according to an optimization function ( usually word co - occurrence ) .", "entities": [[25, 26, "MethodName", "GloVe"]]}
{"text": "Extremal Word Score ( EWSO ) .", "entities": [[2, 3, "MetricName", "Score"]]}
{"text": "We next propose an approach to improve the interpretability of word embeddings by adding emoji .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "It uses our extended version POLAR \u03c1 and adds emoji to the POLAR space by creating word embeddings that include emoji .", "entities": [[16, 18, "TaskName", "word embeddings"]]}
{"text": "Ethics .", "entities": [[0, 1, "DatasetName", "Ethics"]]}
{"text": "Prior work showed that the interpretation of emoji varies ( Miller et al , 2016 ; Kimura - Thollander and Kumar , 2019 ) , also between cultures ( Guntuku et al , 2019 ; Gupta et al , 2021 ) .", "entities": [[20, 21, "DatasetName", "Kumar"]]}
{"text": "Interpretable word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "Word embeddings are a common approach to capture meaning ; they are a learned vector space representation of text that carries semantic relationships as distances between the embedded words .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "A rich body of work aims at making word embeddings interpretable , e.g. , via contextual ( Subramanian et al , 2018 ) , sparse embeddings ( Panigrahi et al , 2019 ) , or learned ( Senel et al , 2018 transformations ( Mathew et al , 2020 ) - all focus on text only .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "The POLAR approach is similar to SEMCAT ( Senel et al , 2018 ) , but is based on the concept of semantic differentials ( Osgood et al , 1957 ) for creating a polar subspace .", "entities": [[6, 7, "DatasetName", "SEMCAT"]]}
{"text": "Few works focused on using word embeddings for creating emoji representations , e.g. , ( Eisner et al , 2016 ) or ( Reelfs et al , 2020 ) .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "Yet , the general question if the interpretability of word embeddings can be improved by adding emoji and if different meaning of emoji can be captured remains still open .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "In this work , we adapt the POLAR interpretability approach to emoji and study in a human subject experiment if word embeddings can be made interpretable by adding emoji and how emoji can be interpretated by emoji .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "We raise the question whether we can leverage the expressiveness of emoji to make word embeddings interpretable .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "Thus , we use the POLAR framework ( Mathew et al , 2020 ) that creates interpretable word embeddings through semantic differentials , polar opposites .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "We employ a revised POLAR \u03c1 method that transforms arbitrary word embeddings to interpretable counterparts to which we added emoji .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "We thank Felix Dommes , who was instrumental for this work by developing and implementing the POLAR \u03c1 projection approach and the Extremal Word Score in his Master Thesis .", "entities": [[24, 25, "MetricName", "Score"]]}
{"text": "Dual Transfer for Very Low Resource Supervised Machine Translation", "entities": [[7, 9, "TaskName", "Machine Translation"]]}
{"text": "This paper describes the NoahNMT system submitted to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .", "entities": [[18, 20, "TaskName", "Machine Translation"]]}
{"text": "The system is a standard Transformer model equipped with our recent technique of dual transfer .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "It also employs widely used techniques that are known to be helpful for neural machine translation , including iterative backtranslation , selected finetuning , and ensemble .", "entities": [[14, 16, "TaskName", "machine translation"]]}
{"text": "The final submission achieves the top BLEU for three translation directions .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "The shared task features both unsupervised machine translation and very low resource supervised machine translation .", "entities": [[5, 8, "TaskName", "unsupervised machine translation"], [13, 15, "TaskName", "machine translation"]]}
{"text": "As our core technique is mainly suitable for low resource supervised machine translation , we participated in four translation directions between Chuvash - Russian ( chv - ru ) and Upper Sorbian - German ( hsb - de ) .", "entities": [[11, 13, "TaskName", "machine translation"]]}
{"text": "Our core technique is called dual transfer ( Zhang et al , 2021 ) , which belongs to the family of transfer learning .", "entities": [[21, 23, "TaskName", "transfer learning"]]}
{"text": "It transfers from both high resource neural machine translation model and pretrained language model to improve the quality of low resource machine translation .", "entities": [[7, 9, "TaskName", "machine translation"], [21, 23, "TaskName", "machine translation"]]}
{"text": "During the preparation for the shared task , we conducted additional experiments that supplement the original paper , including the choice of parent language , the validation of Transformer big model , and the usage of dual transfer along with iterative back - translation .", "entities": [[28, 29, "MethodName", "Transformer"]]}
{"text": "Our final submission achieves the top BLEU on the blind test sets for three translation directions : chv ru , ru chv , and hsb de .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "In this shared task , when training the high resource translation model , we always initialize the shared language side with the pretrained language model BERT ( Devlin et al , 2019 ) .", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "Each language was encoded with byte pair encoding ( BPE ) ( Sennrich et al , 2016b ) .", "entities": [[5, 8, "MethodName", "byte pair encoding"], [9, 10, "MethodName", "BPE"]]}
{"text": "The BPE codes and vocabularies were learned on each language 's monolingual data , and then used to segment parallel data .", "entities": [[1, 2, "MethodName", "BPE"]]}
{"text": "After BPE segmentation , we discarded sentences with more than 128 subwords , and cleaned parallel data with length ratio 1.5 .", "entities": [[1, 2, "MethodName", "BPE"]]}
{"text": "We use Transformer ( Vaswani et al , 2017 ) as our translation model , but with slight modifications that follow the implementation of BERT 5 .", "entities": [[2, 3, "MethodName", "Transformer"], [24, 25, "MethodName", "BERT"]]}
{"text": "The absolute position embeddings are also learned as in BERT .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "We use LazyAdam as the optimizer .", "entities": [[5, 6, "HyperparameterName", "optimizer"]]}
{"text": "Hyperparameters for BERT are the same as in the original paper ( Zhang et al , 2021 ) .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "The checkpoint with the highest validation BLEU is kept .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "We conducted an experiment with Transformer base .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "Considering that we plan to use Transformer big for which data amount is likely to play a more important role , we decided to use English as the parent language for Chuvash .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "The original paper ( Zhang et al , 2021 ) evaluated dual transfer only with Transformer base .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "In this shared task , we scale up to Transformer big .", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "Results in Table 3 show that Transformer big brings consistent improvements .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "We also report the runtime of each step in dual transfer for NMT chv ru with Transformer big in Table 4 for reference , but the numbers can vary depending on implementation and data size .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "In the following experiments and our final submission , we use Transformer big models .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "The best BLEU scores are attained with two or three iterations .", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "We finetune five times with different random seeds for model ensemble .", "entities": [[7, 8, "DatasetName", "seeds"]]}
{"text": "In this paper , we describe a series of experiments that contribute to our submission to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .", "entities": [[26, 28, "TaskName", "Machine Translation"]]}
{"text": "Towards Generative Aspect - Based Sentiment Analysis *", "entities": [[2, 7, "TaskName", "Aspect - Based Sentiment Analysis"]]}
{"text": "Aspect - based sentiment analysis ( ABSA ) has received increasing attention recently .", "entities": [[0, 5, "TaskName", "Aspect - based sentiment analysis"]]}
{"text": "Two types of paradigms , namely annotation - style and extraction - style modeling , are designed to enable the training process by formulating each ABSA task as a text generation problem .", "entities": [[29, 31, "TaskName", "text generation"]]}
{"text": "Aspect - based sentiment analysis ( ABSA ) , aiming at mining fine - grained opinion information towards specific aspects , has attracted increasing attention in recent years ( Liu , 2012 ) .", "entities": [[0, 5, "TaskName", "Aspect - based sentiment analysis"]]}
{"text": "Motivated by recent success in formulating sev - eral language understanding problems such as named entity recognition , question answering , and text classification as generation tasks ( Raffel et al , 2020 ; Athiwaratkun et al , 2020 ) , we propose to tackle various ABSA problems in a unified generative approach in this paper .", "entities": [[14, 17, "TaskName", "named entity recognition"], [18, 20, "TaskName", "question answering"], [22, 24, "TaskName", "text classification"]]}
{"text": "In order to enable the Generative Aspect - based Sentiment analysis ( GAS ) , we tailor - make two paradigms , namely annotation - style and extractionstyle modeling to transform the original task as a generation problem .", "entities": [[6, 11, "TaskName", "Aspect - based Sentiment analysis"]]}
{"text": "We investigate four ABSA tasks including Aspect Opinion Pair Extraction ( AOPE ) , Unified ABSA ( UABSA ) , Aspect Sentiment Triplet Extraction ( ASTE ) , and Target Aspect Sentiment Detection ( TASD ) with the proposed unified GAS framework to verify its effectiveness and generality .", "entities": [[20, 24, "TaskName", "Aspect Sentiment Triplet Extraction"]]}
{"text": "Similarly , we replace each aspect term as [ aspect | sentiment polarity ] under the annotation - style formulation and treat the desired pairs as the target output in the extraction - style paradigm to reformulate the UABSA task as a text generation problem .", "entities": [[42, 44, "TaskName", "text generation"]]}
{"text": "Given the input sentence x , we generate a target sequence y , which is either based on the annotationstyle or extraction - style paradigm as described in the last section , with a text generation model f ( ) .", "entities": [[34, 36, "TaskName", "text generation"]]}
{"text": "We adopt the pre - trained T5 model ( Raffel et al , 2020 ) as the generation model f ( ) , which closely follows the encoder - decoder architecture of the original Transformer ( Vaswani et al , 2017 ) .", "entities": [[6, 7, "MethodName", "T5"], [34, 35, "MethodName", "Transformer"]]}
{"text": "Therefore , by formulating these ABSA tasks as a text generation problem , we can tackle them in a unified sequence - to - sequence framework without taskspecific model design .", "entities": [[9, 11, "TaskName", "text generation"]]}
{"text": "We adopt F1 scores as the main evaluation metrics for all tasks .", "entities": [[2, 3, "MetricName", "F1"]]}
{"text": "We adopt the T5 base model from huggingface Transformer library 2 for 2 https://github.com/huggingface/ transformers", "entities": [[3, 4, "MethodName", "T5"], [8, 9, "MethodName", "Transformer"]]}
{"text": "Baseline ( Brun and Nikoulina , 2018 ) - 38.10 TAS - LPM - CRF ( Wan et al , 2020 ) 54.76 64.66 TAS - SW - CRF ( Wan et al , 2020 ) 57.51 65.89 TAS - SW - TO ( Wan et al , 2020 ) 58.09 65.44 all experiments .", "entities": [[12, 13, "MethodName", "LPM"], [14, 15, "MethodName", "CRF"], [28, 29, "MethodName", "CRF"]]}
{"text": "T5 closely follows the original encoder - decoder architecture of the Transformer model , with some slight differences such as different position embedding schemes .", "entities": [[0, 1, "MethodName", "T5"], [11, 12, "MethodName", "Transformer"]]}
{"text": "Therefore , the encoder and decoder of it have similar parameter size as the BERT - BASE model .", "entities": [[14, 15, "MethodName", "BERT"], [16, 17, "MethodName", "BASE"]]}
{"text": "For example , the method fixes \" Bbq rib \" to \" BBQ rib \" ( # 1 ) and \" repeat \" to \" repeats \" ( # 2 ) .", "entities": [[7, 8, "DatasetName", "Bbq"], [12, 13, "DatasetName", "BBQ"]]}
{"text": "Our work is an initial attempt on transforming ABSA tasks , which are typically treated as classification problems , into text generation problems .", "entities": [[20, 22, "TaskName", "text generation"]]}
{"text": "State - of - the - art dialogue models still often stumble with regards to factual accuracy and self - contradiction .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "In particular , they are likely to take on the role of their interlocutor ; for example , if an agent 's partner says they are a software engineer , the agent is likely to say it is a software engineer too ( Roller et al , 2021 ) , or worse , appropriate their partners just told tale of a trip to NAACL as their own .", "entities": [[20, 21, "DatasetName", "agent"], [31, 32, "DatasetName", "agent"]]}
{"text": "These include multi - objective training , unlikelihood training , classifier - assisted re - ranking based generation , and several forms modifying the attention mechanisms of the decoder in a sequence to sequence model .", "entities": [[31, 34, "MethodName", "sequence to sequence"]]}
{"text": "A common paradigm in the state of the art of open - domain dialogue involves concatenating all relevant contextual information as input to a sequence to sequence neural model ( e.g. , transformers ( Vaswani et al , 2017 ) ) to obtain a conditioned response .", "entities": [[24, 27, "MethodName", "sequence to sequence"]]}
{"text": "To quantify the character identity problem , we take a state - of - the - art dialogue agent ( specifically , BlenderBot ( Roller et al , 2021 ) )", "entities": [[18, 19, "DatasetName", "agent"]]}
{"text": "fine - tuned on the LIGHT dialogue dataset and ask human annotators if the agent mistakes its identity based on its utterances in context .", "entities": [[14, 15, "DatasetName", "agent"]]}
{"text": "The agent conditions its response on the LIGHT context and prior utterances in the dialogue history .", "entities": [[1, 2, "DatasetName", "agent"]]}
{"text": "BlenderBot uses a Byte - Level BPE tokenizer ( Radford et al , 2019 ) ; an artifact from the Blender - Bot pre - training is that it only considers 128 such tokens in the past , and thus has no mechanism for recovering truncated information about the LIGHT context in later conversational turns .", "entities": [[6, 7, "MethodName", "BPE"], [20, 21, "MethodName", "Blender"]]}
{"text": "Our second baseline lengthens the input context to 1024 BPE tokens , which allows the entire context for every example to fit into the truncation length of the model ; we follow methods employed in to extend the positional embeddings of the model .", "entities": [[9, 10, "MethodName", "BPE"]]}
{"text": "We first define a metric , role - playing accuracy ( RPA ) , to denote how often a model 's responses are \" in - character \" ; by this , we mean how often the model 's response could feasibly be said by their character , given their assigned character identity .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "In this section we describe several strategies for improving the role - playing accuracy of dialogue agents , specifically ways to improve our transformer baselines .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "We can employ an RPA classifier in response generation by using it to rank candidate model outputs .", "entities": [[7, 9, "TaskName", "response generation"]]}
{"text": "We can control these hyperparameters to explore the speed vs. accuracy trade - off .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "We explore utilizing an unlikelihood ( UL ) loss While training on the LIGHT dataset with standard NLL loss , with some fixed probability we consider a candidate model generation for UL loss .", "entities": [[8, 9, "MetricName", "loss"], [17, 18, "MetricName", "NLL"], [18, 19, "MetricName", "loss"], [32, 33, "MetricName", "loss"]]}
{"text": "We apply UL loss to tokens that yield the wrong character classification .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "However , the RPA classifier models are trained explicitly for this task , whereas the seq2seq models are trained only to generate a plausible continuation of a dialogue history .", "entities": [[15, 16, "MethodName", "seq2seq"]]}
{"text": "M O additional transformer layers , where we vary n M O { 0 , 2 } .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "After initializing the model weights with those trained on the LIGHT response generation task , we then train only the extra layers with only the character classification objective ; once the classifier achieves suitable performance on the task , we can begin to back - propagate the character classification objective multi - tasking with the dialogue task itself to the generation model directly , in the hope that the model learns to update its internal representations of the context and/or the decoded response .", "entities": [[11, 13, "TaskName", "response generation"]]}
{"text": "Profile Grounding Inspired by models demonstrating good performance in knowledge - grounded dialogue ( Zheng and Zhou , 2019 ; Ye et al , 2020 ; Prabhumoye et al , 2021 ; Wang et al , 2019 ) , we propose a simple extension to the transformer seq2seq architecture , specifically the decoder , to ensure the model knows to condition on the pro - file .", "entities": [[2, 3, "DatasetName", "Inspired"], [47, 48, "MethodName", "seq2seq"]]}
{"text": "The standard transformer decoder first uses self - attention over the decoded response , and then cross - attention over the encoder outputs .", "entities": [[2, 4, "MethodName", "transformer decoder"]]}
{"text": "Specifically , we feed the context through a linear projection layer followed by a softmax to select the top - k tokens .", "entities": [[14, 15, "MethodName", "softmax"]]}
{"text": "We experiment with either 0 , 4 , or All prior context utterances .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "We next train baseline models for the dialogue generation task itself .", "entities": [[7, 9, "TaskName", "dialogue generation"]]}
{"text": "Automated results show a slight bump in F1 on the LIGHT valid set , and indeed a bump in RPA .", "entities": [[7, 8, "MetricName", "F1"]]}
{"text": "The RPA UL methods suffer compared to the baselines in terms of PPL and F1 , yet they retain similar RPA metrics .", "entities": [[14, 15, "MetricName", "F1"]]}
{"text": "We hypothesize that while the UL loss can adjust the model to refrain from generating outof - character responses , there are still far too many other tokens that may yield similar outcomes that are not penalized .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "hits@1 for the best model ) , this does not translate to substantial RPA improvements over the baseline .", "entities": [[0, 1, "MetricName", "hits@1"]]}
{"text": "Each model was pre - trained on 1.5B training examples from pushshift.io Reddit ( Baumgartner et al , 2020 ) , with BlenderBot additionally fine - tuned on the BST tasks ( see Roller et al ( 2021 ) for more details ) , before training on LIGHT .", "entities": [[12, 13, "DatasetName", "Reddit"]]}
{"text": "For the multiobjective models , we used the same loss ( and negative - sampling ) setup as the RPA classifiers for the character accuracy objective .", "entities": [[9, 10, "MetricName", "loss"], [24, 25, "MetricName", "accuracy"]]}
{"text": "We experiment with either 0 , 4 , or N \u2212 2 prior utterances ( dubbed \" All \" in relevant tables ) , where N is the total number of utterances ( N \u2212 2 allows the last turn for each speaker to be a candidate utterance ) .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "For any value 0 <", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "d i with N utterances { u 0 , u 1 , ... , u N } .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "i , ... , u i+n } \u2200 0 \u2264 i \u2264", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "For the baseline and re - ranker models , beam search yields the highest F1 scores ; RPA can be improved with the other inference methods when combined with a re - ranker .", "entities": [[14, 15, "MetricName", "F1"]]}
{"text": "Perplexity perplexity appears to be positively correlated with mistaken identity , and negatively correlated with engagingness .", "entities": [[0, 1, "MetricName", "Perplexity"], [1, 2, "MetricName", "perplexity"]]}
{"text": "So , perplexity is a good indicator of how fluent and engaging the model is in conversation , and can indirectly point to a better understanding of the role - playing task .", "entities": [[2, 3, "MetricName", "perplexity"]]}
{"text": "F1 F1 word overlap is positively correlated with engagingness as well , so F1 may be a good proxy of model performance .", "entities": [[0, 1, "MetricName", "F1"], [1, 2, "MetricName", "F1"], [13, 14, "MetricName", "F1"]]}
{"text": "Correlation with mistaken identity is negative here , implying that better F1 corresponds with better role - playing ability .", "entities": [[11, 12, "MetricName", "F1"]]}
{"text": "Human The human outputs are most often correct on the first turn , with gradual decay of accuracy throughout the conversation ( according to RPA ) .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "The vanilla baseline suffers a pretty dramatic drop off after the first couple of turns ; the long - context model achieves slightly higher character accuracy overall", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "Dynamic Sentence Boundary Detection for Simultaneous Translation", "entities": [[2, 4, "TaskName", "Boundary Detection"], [6, 7, "TaskName", "Translation"]]}
{"text": "Simultaneous Translation is a great challenge in which translation starts before the source sentence finished .", "entities": [[1, 2, "TaskName", "Translation"]]}
{"text": "In this paper , we propose a novel method for sentence boundary detection that takes it as a multi - class classification task under the endto - end pre - training framework .", "entities": [[11, 13, "TaskName", "boundary detection"], [18, 22, "TaskName", "multi - class classification"]]}
{"text": "Simultaneous Translation aims to translate the speech of a source language into a target language as quickly as possible without interrupting the speaker .", "entities": [[1, 2, "TaskName", "Translation"]]}
{"text": "Typically , a simultaneous translation system is comprised of an auto - speech - recognition ( ASR ) model and a machine translation ( MT ) model .", "entities": [[12, 15, "TaskName", "speech - recognition"], [21, 23, "TaskName", "machine translation"]]}
{"text": "Therefore , sentence boundary detection ( or sentence segmentation ) 1 plays an important role to narrow the gap between the ASR and transcription .", "entities": [[3, 5, "TaskName", "boundary detection"], [7, 9, "TaskName", "sentence segmentation"]]}
{"text": "Studies of sentence segmentation falls into one of the following two bins : The strategy performs segmentation from a speech perspective .", "entities": [[2, 4, "TaskName", "sentence segmentation"]]}
{"text": "F\u00fcgen et al ( 2007 ) and Bangalore et al ( 2012 ) used prosodic pauses in speech recognition as segmentation boundaries .", "entities": [[17, 19, "TaskName", "speech recognition"]]}
{"text": "According to Venuti ( 2012 ) , silence - based chunking accounts for only 6.6 % , 10 % , and 17.1 % in English , French , and German , respectively .", "entities": [[10, 11, "TaskName", "chunking"]]}
{"text": "The studies considered the problem as classification or sequence labeling , based on SVM , conditional random filed ( CRFs ) ( Lu and Ng , 2010 ; Wang et al , 2012 ; Ueffing et al , 2013 ) .", "entities": [[13, 14, "MethodName", "SVM"]]}
{"text": "In this paper , we use classification to solve the problem of sentence segmentation from the perspective of text .", "entities": [[12, 14, "TaskName", "sentence segmentation"]]}
{"text": "Inspired by the recent pre - training techniques ( Devlin et al , 2019 ; Sun et al , 2019 ) that successfully used in many NLP tasks , we used a pre - trained model for initialization and fine - tune the model on the source side of the sentence .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Overall , the contributions are as follows : We propose a novel sentence segmentation method based on pre - trained language representations , which have been successfully used in various NLP tasks .", "entities": [[12, 14, "TaskName", "sentence segmentation"]]}
{"text": "Our method dynamically predicts the boundary at multiple locations , rather than a specific location , achieving high accuracy with low latency .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "Devlin et al ( 2019 ) proposed a generalized framework BERT , to learn language representations based on a deep Transformer ( Vaswani et al , 2017 ) encoder .", "entities": [[10, 11, "MethodName", "BERT"], [20, 21, "MethodName", "Transformer"]]}
{"text": "Rather than traditionally train a language model from - left - to - right or from - rightto - left , they proposed a masked language model ( MLM ) that randomly replace some tokens in a sequence by a placeholder ( mask ) and trained the model to predict the original tokens .", "entities": [[28, 29, "DatasetName", "MLM"]]}
{"text": "Given a streaming input x = { x 1 , ... , x t , ... , x T } , the task of sentence segmentation is to determine whether x t x is the end of a sentence .", "entities": [[24, 26, "TaskName", "sentence segmentation"]]}
{"text": "Thus we should limit the context size and make a decision dynamically .", "entities": [[5, 7, "HyperparameterName", "context size"]]}
{"text": "As the input is a word streaming , the sentence boundary detection problem can be transformed as , whether there exists a sentence boundary until the current word x t .", "entities": [[10, 12, "TaskName", "boundary detection"]]}
{"text": "We propose a multi - class classification model to predict the probability of a few words before x t as sentence boundaries ( Section 3.1 ) .", "entities": [[3, 7, "TaskName", "multi - class classification"]]}
{"text": "We use the ERNIE framework to first pre - train a language representation and then fine - tune it to sentence boundary detection ( Section 3.2 ) .", "entities": [[21, 23, "TaskName", "boundary detection"]]}
{"text": "The classes are as follows : 0 1 2 \u2026 \u22122 \u22121 \u2026 \u210e \u210e 0 1 2 \u2026 \u22122 \u22121 \u2026 \u2026 \u2026 \u03d5 0 \u2212 1 \u2212 2 Classes Masked Language Model \u2026 \u210e \u210e \u2026 \u210e \u210e \" . \"", "entities": [[6, 7, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [25, 26, "DatasetName", "0"]]}
{"text": "y = \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u03c6 , no sentence boundary detected 0 , x t is the end of a sentence \u22121 ,", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "If the output class is 0 , indicating that the current word x t is the end of a sentence and we put a period after the word .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Then we define the loss function as follows : J ( \u03b8 )", "entities": [[4, 5, "MetricName", "loss"], [11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "= ( x , r ) D log ( r\u22121 t=1 p ( y t = \u03c6 | x \u2264t ; \u03b8 ) + r+M t = r p ( y t = \u2212 ( t \u2212 r ) )", "entities": [[21, 22, "HyperparameterName", "\u03b8"]]}
{"text": "| x \u2264t ; \u03b8 ) )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "predicts whether a word x t labeled as the end of a sentence or not by a binary classification : p ( y t = 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "where y t = 0 means x t is not the end of a sentence and", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "The performance of these methods is limited by incomplete semantics , without considering global boundary detection .", "entities": [[14, 16, "TaskName", "boundary detection"]]}
{"text": "= 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "If more than one sentence boundary probabilities for x t\u2212M , ... , x t exceeds the threshold \u03b8 T h at the same time , we choose the front - most position as a sentence boundary .", "entities": [[18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "From another point of view , the strategy can also compensate for some incorrect suppression of adjacent boundaries , thereby improving online prediction accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "We use two parallel corpus from machine translation task : WMT 14 3 and IWSLT 14 4 .", "entities": [[6, 8, "TaskName", "machine translation"]]}
{"text": "And our sentence boundary detection model is trained on the source transcription of IWSLT 14 unless otherwise specified ( Section 4.3 ) .", "entities": [[3, 5, "TaskName", "boundary detection"]]}
{"text": "We evaluate our model and two existing methods listed below : dynamic - base is our proposed method that detect sentence boundaries dynamically using a multi - class classification .", "entities": [[25, 29, "TaskName", "multi - class classification"]]}
{"text": "T - LSTM uses a RNN - based classification model with two classes .", "entities": [[2, 3, "MethodName", "LSTM"]]}
{"text": "In the fine - tuning stage , we use a learning rate of 2e \u22125 .", "entities": [[10, 12, "HyperparameterName", "learning rate"]]}
{"text": "Table 2 reports the results of source sentence segmentation on En - De translation , where the latency is measured by Consecutive Wait ( CW ) ( Gu et al , 2017 ) , the number of words between two translate actions .", "entities": [[7, 9, "TaskName", "sentence segmentation"]]}
{"text": "Better performance expect high F - score , BLEU , and low latency ( CW ) .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "The translation effect obtained by using the groundtruth period as the sentence segmentation is shown in the first line of Oracle .", "entities": [[11, 13, "TaskName", "sentence segmentation"]]}
{"text": "The N - gram method calculate the probability of add ( p add ) and not add ( p not ) period at each position , and decide whether to chunk by comparing whether p add /p not exceeds \u03b8 T h .", "entities": [[39, 40, "HyperparameterName", "\u03b8"]]}
{"text": "Accurate sentence segmentation brings better performance in translation , bringing an improvement of 1.55 over T - LSTM .", "entities": [[1, 3, "TaskName", "sentence segmentation"], [17, 18, "MethodName", "LSTM"]]}
{"text": "It is interesting to note that , dynamic - force performs better than dynamic - base , in terms of latency and BLEU .", "entities": [[22, 23, "MetricName", "BLEU"]]}
{"text": "This suggests the effectiveness of the force segmentation strategy , that is , select the chunking location with a sentence length limitation will not affect the accuracy of segmentation , and would enhance the translation effect .", "entities": [[15, 16, "TaskName", "chunking"], [26, 27, "MetricName", "accuracy"]]}
{"text": "Surprisingly , the Sort approach is prominent in both segmentation accuracy and translation performance .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "This may be due to the following reasons : 1 ) Sentence classification is not a difficult task , especially when M = 1 for 3 - class classification ( y [ \u03c6 , 0 , \u22121 ] ) , making the task easy to over - fit . 2 )", "entities": [[11, 13, "TaskName", "Sentence classification"], [34, 35, "DatasetName", "0"]]}
{"text": "Intuitively , larger dataset provides more diverse samples , but due to domain changes , it does not necessarily lead to improvements in accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "The performance of various models trained on WMT14 is shown in Table 3 .", "entities": [[7, 8, "DatasetName", "WMT14"]]}
{"text": "On the contrary , N - gram and T - LSTM is hardly affected .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "For T - LSTM , it even improves a little compared with its in - domain performance .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "0.19 M sentences of IWSLT2014 is insufficient to fit the parameters of T - LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "Next , we discuss the effect of changing \u03b8 .", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "Smaller \u03b8 l brings shorter latency , as well as worse performance .", "entities": [[1, 2, "HyperparameterName", "\u03b8"]]}
{"text": "Accordingly , we only use the samples shorter than a fixed value : \u03b8 l in training phrase .", "entities": [[13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "At inference time , we use both dynamic - force with the same sentence length constraint \u03b8 l and dynamic - base to predict sentence boundaries .", "entities": [[16, 17, "HyperparameterName", "\u03b8"]]}
{"text": "This demonstrates the main reason for the poor performance with small \u03b8 l is not the training - testing discrepancy but lies in the first reason that the force constraint is too harsh .", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "We experiment with M from 0 to 5 .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Sentence boundary detection has been explored for years , but the majority of these work focuses on offline punctuation restoration , instead of applied in simultaneous translation .", "entities": [[1, 3, "TaskName", "boundary detection"]]}
{"text": "The main deficiency of this method is that the dependencies outside the input window are lost , resulting in low accuracy .", "entities": [[20, 21, "MetricName", "accuracy"]]}
{"text": "Peitz et al ( 2011 ) and Cho et al ( 2012 ) treats this problem as a machine translation task , training to translate non - punctuated transcription into punctuated text .", "entities": [[18, 20, "TaskName", "machine translation"]]}
{"text": "In this paper , we propose an online sentence boundary detection approach .", "entities": [[9, 11, "TaskName", "boundary detection"]]}
{"text": "By adding this adjacent position constraint and using dynamic prediction , our method achieves higher accuracy with lower latency .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "Our experiments demonstrate robust scores through pretrained BERT ( Bidirectional Encoder Representations from Transformers ) embeddings for word representation , and more so the importance of manual features - once again highlighting the syntactic and semantic characteristics of the ellipsis phenomenon .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "For the classification decision , we notice that a simple Multilayar Perceptron ( MLP ) works well for the detection of ellipsis ; however , Recurrent Neural Networks ( RNN ) are a better choice for the much harder resolution step .", "entities": [[13, 14, "DatasetName", "MLP"]]}
{"text": "Computational approaches to the ellipsis phenomenon majorly focus on the Verb Phrase Ellipsis ( VPE ) along with a few related phenomenon such as gapping , sluicing and do - so anaphora , for instance , the detection of VPE in the Penn Treebank using pattern match ( Hardt , 1992 ) , a transformation learning - based approach to generated patterns for VPE resolution ( Hardt , 1998 ) , the domain independent VPE detection and resolution using machine learning ( Nielsen , 2003 ) , automatically parsed text ( Nielsen , 2004b ) , sentence trimming methods ( McShane et al , 2015 ) , linguistic principles ( McShane and Babkin , 2016 ) , improved parsing techniques that encode elided material dependencies for reconstruction of sentences containing gapping ( Schuster et al , 2018 ) , discriminative and margin infused algorithms", "entities": [[42, 44, "DatasetName", "Penn Treebank"]]}
{"text": "( Dean et al , 2016 ) , Multilayer Perceptrons ( MLP ) and Transformers ( Zhang et al , 2019 ) .", "entities": [[11, 12, "DatasetName", "MLP"]]}
{"text": "For the resolution process , we previously proposed a rule based system ( Khullar et al , 2019 ) that detects noun ellipsis using syntactic constraints on licensors of ellipsis and resolves them by matching Part - of - Speech ( POS ) tag similarity between the licensor of ellipsis and the modifier of the antecedent .", "entities": [[35, 38, "DatasetName", "Part - of"]]}
{"text": "We use the NoEl corpus ( Khullar et al , 2020 ) that marks noun ellipsis instances as a separate layer ( using the stand - off annotation scheme ) on the Cornell Movie Dialogs corpus ( Danescu - Niculescu - Mizil and Lee , 2011 ) .", "entities": [[32, 33, "DatasetName", "Cornell"]]}
{"text": "( l i , s ) \u2212 { 0 , 1 } where 1 denotes that l", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "i is a licensor in s , and 0 otherwise .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "We experiment with both static and contextualised word embeddings for word and context representation .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "For the former , we choose pretrained fastText ( FT ) word embeddings ( Bojanowski et al , 2016 ) as they provide representations for rare and unknown words that might be frequent in the movie dialogues .", "entities": [[7, 8, "MethodName", "fastText"], [11, 13, "TaskName", "word embeddings"]]}
{"text": "For the latter , we use pretrained BERT embeddings from the BERT base uncased wordpiece model for English ( Devlin et al , 2019 ) , as these currently offer the most powerful embeddings taking into account a large left and right context .", "entities": [[7, 8, "MethodName", "BERT"], [11, 12, "MethodName", "BERT"], [14, 15, "MethodName", "wordpiece"]]}
{"text": "fastText We take pretrained FT word embeddings for the noun modifier and sentence in which it is present and sum pool to obtain a single vector that we use to train our classifiers .", "entities": [[0, 1, "MethodName", "fastText"], [5, 7, "TaskName", "word embeddings"]]}
{"text": "For the statistical models , we choose Naive Bayes and Linear Support Vector Machine ( SVM ) , and use scikit learn ( Pedregosa et al , 2011 ) with 5 - fold cross validation for training and testing .", "entities": [[11, 14, "MethodName", "Support Vector Machine"], [15, 16, "MethodName", "SVM"]]}
{"text": "We choose a BERT We separate the sentence and the licensor with a [ SEP ] token and keep the sequence length to 300 as this is the maximum sentence length in the training data .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "After creating the concatenated set of tokens , if the number of tokens are greater than 300 , we clip it to 300 , otherwise we add [ PAD ] tokens which correspond to the embedding of 768 dimensional zero - vector .", "entities": [[28, 29, "DatasetName", "PAD"]]}
{"text": "The [ CLS ] output of the BERT model ( Devlin et al , 2019 ) is then fed into Naive Bayes , Linear SVM , MLP and bi - LSTM networks as above .", "entities": [[7, 8, "MethodName", "BERT"], [24, 25, "MethodName", "SVM"], [26, 27, "DatasetName", "MLP"], [30, 31, "MethodName", "LSTM"]]}
{"text": "the licensor l i from the detection step , and the antecedent candidate a j ; the noun ellipsis resolution task can be defined as follows : f ( a j , l i , s ) \u2212 { 0 , 1 } where 1 denotes that the antecedent candidate a j is the actual resolution of the ellipsis licensed by l i , and 0 otherwise .", "entities": [[39, 40, "DatasetName", "0"], [65, 66, "DatasetName", "0"]]}
{"text": "Embeddings Similar to the detection step , we take pretrained fastText word embeddings for the licensor , antecedent candidate and context , and sum pool to obtain a single vector .", "entities": [[10, 11, "MethodName", "fastText"], [11, 13, "TaskName", "word embeddings"]]}
{"text": "In case of BERT , we separate the sentence , the licensor and the antecedent candidate with a [ SEP ] token and follow the same steps as in the detection step .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "For MLP , we take a simple , two - layer feedforward network ( FFNN ) or two layers of multiple computational units interconnected in a feed - forward way without loops .", "entities": [[1, 2, "DatasetName", "MLP"], [11, 13, "MethodName", "feedforward network"]]}
{"text": {"text": "Maoqin @ DravidianLangTech - EACL2021 : The Application of Transformer - Based Model", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "I use the transformer - based language model with BiGRU - Attention to complete this task .", "entities": [[9, 10, "MethodName", "BiGRU"]]}
{"text": "So I choose ALBERT", "entities": [[3, 4, "MethodName", "ALBERT"]]}
{"text": "To get a more effective and higher accuracy model , BiGRU combined with attention .", "entities": [[7, 8, "MetricName", "accuracy"], [10, 11, "MethodName", "BiGRU"]]}
{"text": "There are many competitions about offensive language detection ( such as HASOC ( Chakravarthi et al , 2020c ; Mandl et al , 2020 ) and TRAC ( Kumar et al , 2018 ) ) , and many corresponding methods have been produced .", "entities": [[28, 29, "DatasetName", "Kumar"]]}
{"text": "People often tend to abstract this task into a text classification task ( Howard and Ruder , 2018 ) .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "Text classification is called extracting features from original text data and predicting the category of text data based on these features .", "entities": [[0, 2, "TaskName", "Text classification"]]}
{"text": "In the past few decades , many models for text classification have been proposed ( Qian , 2020 ) .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "From the 1960s to the 2010s , text classification models based on shallow learning dominated .", "entities": [[7, 9, "TaskName", "text classification"]]}
{"text": "Shallow learning means statistical - based models such as Naive Bayes ( NB ) , K Nearest Neighbors ( KNN ) ( Cover and Hart , 1967 ) and Support Vector Machines ( SVM ) .", "entities": [[33, 34, "MethodName", "SVM"]]}
{"text": "Compared with earlier rulebased methods , this method has obvious advantages in accuracy and stability .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "Since the 2010s , text classification has gradually changed from a shallow learning model to a deep learning model .", "entities": [[4, 6, "TaskName", "text classification"]]}
{"text": "Therefore , most of the text classification research work is based on DNN ( Yu et al , 2013 ) , which is a data - driven method with high computational complexity .", "entities": [[5, 7, "TaskName", "text classification"]]}
{"text": "The shallow learning model speeds up the text classification speed , improves the accuracy , and expands the application range of shallow learning .", "entities": [[7, 9, "TaskName", "text classification"], [13, 14, "MetricName", "accuracy"]]}
{"text": "In my job , I use the ALBERT model as my base model and take BiGRU - Attention behind it .", "entities": [[7, 8, "MethodName", "ALBERT"], [15, 16, "MethodName", "BiGRU"]]}
{"text": "The ALBERT model belongs to transformer - based language models .", "entities": [[1, 2, "MethodName", "ALBERT"]]}
{"text": "The ALBERT model is improved on the basis of Bidirectional Encoder Representations for Transformers ( BERT ) ( Devlin et al , 2018 ) model .", "entities": [[1, 2, "MethodName", "ALBERT"], [15, 16, "MethodName", "BERT"]]}
{"text": "It has designed a parameter reduction method to reduce memory consumption by changing the result of the original embedding parameter P ( the product of the vocabulary size V and the hidden layer size H ) .", "entities": [[31, 34, "HyperparameterName", "hidden layer size"]]}
{"text": "In BERT , E = H.", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "While in AL - BERT , H > >", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "E , so the number of parameters will be greatly reduced .", "entities": [[4, 7, "HyperparameterName", "number of parameters"]]}
{"text": "At the same time , the self - supervised loss is used to focus on the internal coherence in the construction of sentences .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "The ALBERT model implements three embedding layers : word embedding , position embedding , and segment embedding .", "entities": [[1, 2, "MethodName", "ALBERT"]]}
{"text": "Segment embedding helps BERT distinguish between paired input sequences .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "The BiGRU - Attention model ( Cover and Hart , 1967 ) is divided into three parts : text vector input layer , hidden layer , and output layer .", "entities": [[1, 2, "MethodName", "BiGRU"]]}
{"text": "Among them , the hidden layer consists of three layers : the BiGRU layer , the attention layer , and the Dense layer ( fully connected layer ) .", "entities": [[12, 13, "MethodName", "BiGRU"]]}
{"text": "I set the output of the ALBERT model as the input .", "entities": [[6, 7, "MethodName", "ALBERT"]]}
{"text": "After receiving the input , it uses the BiGRU neural network layer to extract features of the deep - level information of the text firstly .", "entities": [[8, 9, "MethodName", "BiGRU"]]}
{"text": "Finally , the text feature information with different weights is put into the softmax function layer for classification .", "entities": [[13, 14, "MethodName", "softmax"]]}
{"text": "The structure of the BiGRU - Attention model is shown in Figure 3 .", "entities": [[4, 5, "MethodName", "BiGRU"]]}
{"text": "In this task , I use the ALBERT model to pre - train the task .", "entities": [[7, 8, "MethodName", "ALBERT"]]}
{"text": "For the ALBERT model , the main hyperparameters I pay attention to are the training step size , batch size and learning rate .", "entities": [[2, 3, "MethodName", "ALBERT"], [15, 17, "HyperparameterName", "step size"], [18, 20, "HyperparameterName", "batch size"], [21, 23, "HyperparameterName", "learning rate"]]}
{"text": "I have obtained good performance using the ALBERT - BASE .", "entities": [[7, 8, "MethodName", "ALBERT"], [9, 10, "MethodName", "BASE"]]}
{"text": "Considering that BiGRU - Attention can capture contextual information well and extract text information features more accurately ( Radford et al , 2018 ) , I add it after AL - BERT .", "entities": [[2, 3, "MethodName", "BiGRU"], [31, 32, "MethodName", "BERT"]]}
{"text": "The standard of judgment is a weighted F1 - score , and this standard is the judgment standard used for my task .", "entities": [[7, 10, "MetricName", "F1 - score"]]}
{"text": "In this paper , I present my result on Offensive Language Identification in Dravidian Languages - EACL 2021 which includes three tasks of different languages .", "entities": [[10, 12, "TaskName", "Language Identification"]]}
{"text": "For this task , I regard it as a multiple classification task , I use the BiGRU - Attention based on the ALBERT model to complete , and my model works very well .", "entities": [[16, 17, "MethodName", "BiGRU"], [22, 23, "MethodName", "ALBERT"]]}
{"text": "At the same time , I will also consider whether I can use other transfer learning models to perform better on multi - classification tasks .", "entities": [[14, 16, "TaskName", "transfer learning"]]}
{"text": "Recent advances in GPU hardware have led to the emergence of bi - directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .", "entities": [[32, 33, "TaskName", "NER"], [43, 44, "MethodName", "CRF"]]}
{"text": "This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .", "entities": [[11, 12, "TaskName", "NER"], [35, 37, "TaskName", "structured prediction"]]}
{"text": "In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .", "entities": [[31, 37, "TaskName", "part - of - speech tagging"], [38, 41, "TaskName", "named entity recognition"], [42, 43, "TaskName", "NER"]]}
{"text": "Their computational cost grows with the number of layers , but not the input size , up to the memory and threading limitations of the hardware .", "entities": [[6, 9, "HyperparameterName", "number of layers"]]}
{"text": "This provides , for example , audio generation models that can be trained in parallel .", "entities": [[6, 8, "TaskName", "audio generation"]]}
{"text": "Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w \u2212 1 ) + 1 .", "entities": [[14, 15, "MethodName", "convolution"]]}
{"text": "The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .", "entities": [[1, 4, "HyperparameterName", "number of layers"]]}
{"text": "For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate .", "entities": [[17, 18, "MetricName", "loss"], [27, 30, "HyperparameterName", "number of parameters"]]}
{"text": "More concretely , just four stacked dilated convolutions of width 3 produces token representations with a n effective input width of 31 tokens - longer than the average sentence length ( 23 ) in the Penn TreeBank .", "entities": [[35, 37, "DatasetName", "Penn TreeBank"]]}
{"text": "When performing prediction using independent classification , the ID - CNN consistently outperforms a bidirectional LSTM ( Bi - LSTM ) , and performs on par with inference in a CRF with logits from a Bi - LSTM ( Bi - LSTM - CRF ) .", "entities": [[14, 16, "MethodName", "bidirectional LSTM"], [19, 20, "MethodName", "LSTM"], [30, 31, "MethodName", "CRF"], [37, 38, "MethodName", "LSTM"], [41, 42, "MethodName", "LSTM"], [43, 44, "MethodName", "CRF"]]}
{"text": "As an extractor of per - token logits for a CRF , our model out - performs the Bi - LSTM - CRF .", "entities": [[10, 11, "MethodName", "CRF"], [20, 21, "MethodName", "LSTM"], [22, 23, "MethodName", "CRF"]]}
{"text": "We also apply ID - CNNs to entire documents , where independent token classification is as accurate as the Bi - LSTM - CRF while decoding almost 8\u00d7 faster .", "entities": [[12, 14, "TaskName", "token classification"], [21, 22, "MethodName", "LSTM"], [23, 24, "MethodName", "CRF"]]}
{"text": "The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context - rich models .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "For example , RNN - based features require iterative passes along the length of x. We also consider a linear - chain CRF model that couples all of y together :", "entities": [[22, 23, "MethodName", "CRF"]]}
{"text": "CRF prediction explicitly reasons about interactions among neighboring output tags , whereas prediction in the first model compiles this reasoning into the feature extraction step ( Liang et al , 2008 ) .", "entities": [[0, 1, "MethodName", "CRF"]]}
{"text": "While CRF prediction requires non - trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging ( Ramshaw and Marcus , 1999 ) , will always be satisfied .", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over \u03b4 inputs at a time , where \u03b4 is the dilation width .", "entities": [[17, 18, "MethodName", "convolution"], [29, 30, "HyperparameterName", "\u03b4"], [36, 37, "HyperparameterName", "\u03b4"]]}
{"text": "We define the dilated convolution operator : c t = W c r k=0 x t\u00b1k\u03b4 .", "entities": [[3, 5, "MethodName", "dilated convolution"]]}
{"text": "( 4 ) A dilated convolution of width 1 is equivalent to a simple convolution .", "entities": [[4, 6, "MethodName", "dilated convolution"], [14, 15, "MethodName", "convolution"]]}
{"text": "By feeding the outputs of each dilated convolution as the input to the next , increasingly non - local information is incorporated into each pixel 's representation .", "entities": [[6, 8, "MethodName", "dilated convolution"]]}
{"text": "Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded .", "entities": [[5, 6, "MethodName", "convolution"]]}
{"text": "By doubling the dilation width at each layer , the size of the effective input width grows exponentially while the number of parameters grows only linearly with the number of layers , so a pixel representation quickly incorporates rich global evidence from the entire image .", "entities": [[20, 23, "HyperparameterName", "number of parameters"], [28, 31, "HyperparameterName", "number of layers"]]}
{"text": "We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate , allowing follow - on iterations to observe and resolve dependency violations .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "We denote the jth dilated convolutional layer of dilation width \u03b4 as D ( j ) \u03b4 .", "entities": [[10, 11, "HyperparameterName", "\u03b4"], [16, 17, "HyperparameterName", "\u03b4"]]}
{"text": "The first layer in the net - work is a dilation - 1 convolution D ( 0 ) 1 that transforms the input to a representation i t : i t = D ( 0 ) 1 x t ( 5 )", "entities": [[13, 14, "MethodName", "convolution"], [16, 17, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "Beginning with c t ( 0 ) = i t we define the stack of layers with the following recurrence : c t ( j ) = r D ( j\u22121 ) 2 Lc\u22121 c t ( j\u22121 ) ( 6 ) and add a final dilation - 1 layer to the stack : c t ( Lc+1 ) = r D ( Lc ) 1", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Here , maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag , with natural parameters given by Eqn . ( 9 ) : 1 T T t=1 log P ( y t | h t ( L b ) )", "entities": [[19, 21, "MethodName", "logistic regression"]]}
{"text": "( 10 ) We can also use the ID - CNN as logits for the CRF model ( Eqn . ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm .", "entities": [[15, 16, "MethodName", "CRF"]]}
{"text": "LSTMs ( Hochreiter and Schmidhuber , 1997 ) were used for NER as early as the CoNLL shared task in 2003 ( Hammerton , 2003 ; Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[11, 12, "TaskName", "NER"]]}
{"text": "More recently , a wide variety of neural network architectures for NER have been proposed .", "entities": [[11, 12, "TaskName", "NER"]]}
{"text": "al ( 2011 ) employ a one - layer CNN with pre - trained word embeddings , capitalization and lexicon features , and CRF - based prediction .", "entities": [[14, 16, "TaskName", "word embeddings"], [23, 24, "MethodName", "CRF"]]}
{"text": "Lample et al ( 2016 ) proposed two models which incorporated Bi - LSTM - composed character embeddings alongside words : a Bi - LSTM - CRF , and a greedy stack LSTM which uses a simple shift - reduce grammar to compose words into labeled entities .", "entities": [[13, 14, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"], [26, 27, "MethodName", "CRF"], [32, 33, "MethodName", "LSTM"]]}
{"text": "Their Bi - LSTM - CRF obtained the state - of - the - art on four languages without word shape or lexicon features .", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"]]}
{"text": "Ma and Hovy ( 2016 ) use CNNs rather than LSTMs to compose characters in a Bi - LSTM - CRF , achieving state - ofthe - art performance on part - of - speech tagging and CoNLL NER without lexicons .", "entities": [[18, 19, "MethodName", "LSTM"], [20, 21, "MethodName", "CRF"], [30, 36, "TaskName", "part - of - speech tagging"], [38, 39, "TaskName", "NER"]]}
{"text": "Chiu and Nichols ( 2016 ) evaluate a similar network but propose a novel method for encoding lexicon matches , presenting results on CoNLL and OntoNotes NER .", "entities": [[25, 26, "DatasetName", "OntoNotes"], [26, 27, "TaskName", "NER"]]}
{"text": "Yang et al ( 2016 ) use GRU - CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages .", "entities": [[7, 8, "MethodName", "GRU"]]}
{"text": "In general , distributed representations for text can provide useful generalization capabilities for NER systems , since they can leverage unsupervised pre - training of distributed word representations ( Turian et al , 2010 ;", "entities": [[13, 14, "TaskName", "NER"], [20, 24, "TaskName", "unsupervised pre - training"]]}
{"text": "In these NER approaches , CNNs were used for low - level feature extraction that feeds into alternative architectures .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "Overall , end - to - end CNNs have mainly been used in NLP for sentence classification , where the output representation is lower resolution than that of the input Kim ( 2014 ) ; Kalchbrenner et al ( 2014 ) ; ; Toutanova et al ( 2015 ) .", "entities": [[15, 17, "TaskName", "sentence classification"]]}
{"text": "Dilated convolutions were recently applied to the task of speech generation ( van den , and concurrent with this work , posted a pre - print describing the similar ByteNet network for machine translation that uses dilated convolutions in the encoder and decoder components .", "entities": [[32, 34, "TaskName", "machine translation"]]}
{"text": "Additionally , we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "We describe experiments on two benchmark English named entity recognition datasets .", "entities": [[7, 10, "TaskName", "named entity recognition"]]}
{"text": "On CoNLL - 2003 English NER , our ID - CNN performs on par with a Bi - LSTM not only when used to produce per - token logits for structured inference , but the ID - CNN with greedy decoding also performs on - par with the Bi - LSTM - CRF while running at more than 14 times the speed .", "entities": [[5, 6, "TaskName", "NER"], [18, 19, "MethodName", "LSTM"], [50, 51, "MethodName", "LSTM"], [52, 53, "MethodName", "CRF"]]}
{"text": "We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong Kim Sang and De Meulder , 2003 ) and OntoNotes 5.0 ( Hovy et al , 2006 ; Pradhan et al , 2006 ) .", "entities": [[23, 25, "DatasetName", "OntoNotes 5.0"]]}
{"text": "Following previous work , we use the same OntoNotes data split used for co - reference resolution in the CoNLL - 2012 shared task ( Pradhan et al , 2012 ) .", "entities": [[8, 9, "DatasetName", "OntoNotes"]]}
{"text": "As in previous work we evaluate the performance of our models using segment - level micro - averaged F1 score .", "entities": [[18, 20, "MetricName", "F1 score"]]}
{"text": "We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .", "entities": [[8, 9, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"], [24, 25, "MethodName", "CRF"], [29, 30, "MethodName", "LSTM"], [31, 32, "MethodName", "CRF"]]}
{"text": "We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "Collobert et al ( 2011 ) 86.96 Lample et al ( 2016 ) 90.33 Bi - LSTM 89.34 \u00b1 0.28 4 - layer CNN 89.97 \u00b1 0.20 5 - layer CNN 90.23 \u00b1 0.16 ID - CNN 90.32 \u00b1 0.26 Collobert et al ( 2011 ) 88.67 Passos et al ( 2014 ) 90.05 Lample et al ( 2016 )", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "90.20 Bi - LSTM - CRF ( re - impl ) 90.43 \u00b1 0.12 ID - CNN - CRF 90.54 \u00b1", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"], [18, 19, "MethodName", "CRF"]]}
{"text": "LSTM when paired with greedy decoding , suggesting that CNNs are better token encoders than Bi - LSTMs for independent logistic regression .", "entities": [[0, 1, "MethodName", "LSTM"], [20, 22, "MethodName", "logistic regression"]]}
{"text": "When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .", "entities": [[17, 18, "MethodName", "LSTM"]]}
{"text": "Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "Table 2 lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF .", "entities": [[17, 18, "MethodName", "LSTM"], [19, 20, "MethodName", "CRF"]]}
{"text": "We report decoding times using the fastest batch size for each method .", "entities": [[7, 9, "HyperparameterName", "batch size"]]}
{"text": "The ID - CNN model decodes nearly 50 % faster than the Bi - LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "With Viterbi decoding , the gap closes somewhat but the ID - CNN - CRF still comes out ahead , about 30 % faster than the Bi - LSTM - CRF .", "entities": [[14, 15, "MethodName", "CRF"], [28, 29, "MethodName", "LSTM"], [30, 31, "MethodName", "CRF"]]}
{"text": "Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as CoNLL - 2003 .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "We believe this model sees greater improvement with the addition of document - level context than the Bi - LSTM - CRF due to the ID - CNN learning a feature function better suited for representing broad context , in contrast with the Bi - LSTM which , though better than a simple RNN at encoding long memories of sequences , may reach its limit when provided with sequences more than 1 , 000 tokens long such as entire documents .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [45, 46, "MethodName", "LSTM"]]}
{"text": "In Table 6 we show that , in addition to being more accurate , our ID - CNN model is also much faster than the Bi - LSTM - CRF when incorporating context from entire documents , decoding at almost 8 times the speed .", "entities": [[27, 28, "MethodName", "LSTM"], [29, 30, "MethodName", "CRF"]]}
{"text": "On these long sequences , it also tags at more than 4.5 times the speed of the greedy Bi - LSTM , demonstrative of the benefit of our ID - CNNs context - aggregating computation that does not depend on the length of the sequence .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "We observe similar patterns on OntoNotes as we do on CoNLL . icalized greedy model of Ratinov and Roth ( 2009 ) , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of Durrett and Klein ( 2014 ) which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co - reference .", "entities": [[5, 6, "DatasetName", "OntoNotes"], [34, 35, "MethodName", "LSTM"], [58, 59, "DatasetName", "OntoNotes"], [66, 68, "TaskName", "entity linking"]]}
{"text": "Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re - implementation , which appears to be the new state - of - the - art on this dataset .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "CRF"]]}
{"text": "We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi - token named entity segment in CoNLL is about one token shorter than in OntoNotes .", "entities": [[13, 14, "DatasetName", "OntoNotes"], [43, 44, "DatasetName", "OntoNotes"]]}
{"text": "Still , our ID - CNN outperforms all other greedy methods , achieving our goal of learning a better token encoder for structured prediction .", "entities": [[22, 24, "TaskName", "structured prediction"]]}
{"text": "Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly .", "entities": [[12, 13, "DatasetName", "OntoNotes"], [18, 19, "MethodName", "LSTM"], [20, 21, "MethodName", "CRF"]]}
{"text": "For the first time , we see the score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [42, 43, "MethodName", "LSTM"], [44, 45, "MethodName", "CRF"]]}
{"text": "We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to CoNLL - 2003 : CoNLL - 2003 contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with disambiguation .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [19, 20, "DatasetName", "OntoNotes"]]}
{"text": "In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"]]}
{"text": "We are also grateful to Guillaume Lample for sharing his pretrained word embeddings .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "This work was supported in part by the Center for Intelligent Information Retrieval , in part by DARPA under agreement number FA8750 - 13 - 2 - 0020 , in part by Defense Advanced Research Agency ( DARPA ) contract number HR0011 - 15 - 2 - 0036 , in part by the National Science Foundation ( NSF ) grant number DMR - 1534431 , and in part by the National Science Foundation ( NSF ) grant number IIS - 1514053 .", "entities": [[11, 13, "TaskName", "Information Retrieval"], [17, 18, "DatasetName", "DARPA"], [37, 38, "DatasetName", "DARPA"]]}
{"text": "Validating Label Consistency in NER Data Annotation", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "Data annotation plays a crucial role in ensuring your named entity recognition ( NER ) projects are trained with the correct information to learn from .", "entities": [[9, 12, "TaskName", "named entity recognition"], [13, 14, "TaskName", "NER"]]}
{"text": "In this work , we present an empirical method to explore the relationship between label ( in - ) consistency and NER model performance .", "entities": [[21, 22, "TaskName", "NER"]]}
{"text": "It can be used to validate the label consistency ( or catch the inconsistency ) in multiple sets of NER data annotation .", "entities": [[19, 20, "TaskName", "NER"]]}
{"text": "In experiments , our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) .", "entities": [[13, 14, "DatasetName", "SCIERC"], [15, 16, "DatasetName", "CoNLL03"]]}
{"text": "Named entity recognition ( NER ) is one of the foundations of many downstream tasks such as relation extraction , event detection , and knowledge graph construction .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"], [17, 19, "TaskName", "relation extraction"], [20, 22, "TaskName", "event detection"], [25, 27, "TaskName", "graph construction"]]}
{"text": "NER models require vast amounts of labeled data to learn and identify patterns that humans can not continuously .", "entities": [[0, 1, "TaskName", "NER"]]}
{"text": "When end - to - end neural models achieve excellent performance on NER in various domains ( Lample et al , 2016 ; Liu et al , 2018 ; Luan et", "entities": [[12, 13, "TaskName", "NER"]]}
{"text": "al , 2018 ; Zeng et al , , 2021 , building useful and challenging NER benchmarks , such as CoNLL03 , WNUT16 , and SCIERC , contributes significantly to the research community .", "entities": [[15, 16, "TaskName", "NER"], [20, 21, "DatasetName", "CoNLL03"], [25, 26, "DatasetName", "SCIERC"]]}
{"text": "For example , in the CoNLL03 dataset ( Sang and De Meulder , 2003 ) , a standard NER benchmark that has been cited over 2 , 300 times , label mistakes were found in 5.38 % of the test set ( Wang et al , 2019 ) .", "entities": [[5, 6, "DatasetName", "CoNLL03"], [18, 19, "TaskName", "NER"]]}
{"text": "Another example is SCIERC ( Luan et", "entities": [[3, 4, "DatasetName", "SCIERC"]]}
{"text": "al , 2018 ) ( cited \u223c50 times ) which is a multi - task ( including NER ) benchmark in AI domain .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "When we looked at the false predictions given by SCIIE which was a multi - task model released along with the SCIERC dataset , we found that as many as 147 ( 26.7 % of the test set ) sentences were not properly annotated .", "entities": [[21, 22, "DatasetName", "SCIERC"]]}
{"text": "As shown in the experiments section , after the correction , the NER performance becomes more accurate and stable .", "entities": [[12, 13, "TaskName", "NER"]]}
{"text": "Table 1 : Three examples to compare original and corrected annotation in the test set of the SCIERC dataset .", "entities": [[17, 18, "DatasetName", "SCIERC"]]}
{"text": "We apply the SCIIE NER model on the new test set .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "Results on SCIERC show that the test set ( red ) is less predictive of training samples ( orange ) than the training set itself ( blue or green ) .", "entities": [[2, 3, "DatasetName", "SCIERC"]]}
{"text": "Experiments show that they are effective on the CoNLL03 and SCIERC datasets .", "entities": [[8, 9, "DatasetName", "CoNLL03"], [10, 11, "DatasetName", "SCIERC"]]}
{"text": "Take SCIERC as an example .", "entities": [[1, 2, "DatasetName", "SCIERC"]]}
{"text": "Then we train the SCIIE NER model ( Luan et al , 2018 ) to perform on the new test set .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "Take SCIERC as an example .", "entities": [[1, 2, "DatasetName", "SCIERC"]]}
{"text": "Here we deploy five state - of - the - art NER models to investigate their performance on the corrected SCIERC dataset .", "entities": [[11, 12, "TaskName", "NER"], [20, 21, "DatasetName", "SCIERC"]]}
{"text": "The NER models are BiLSTM - CRF ( Lample et al , 2016 ) , LM - BiLSTM - CRF ( Liu et al , 2018 ) ,", "entities": [[1, 2, "TaskName", "NER"], [4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "CRF"], [17, 18, "MethodName", "BiLSTM"], [19, 20, "MethodName", "CRF"]]}
{"text": "As shown in Table 2 , all NER models deliver better performance on the corrected SCIERC than the original dataset .", "entities": [[7, 8, "TaskName", "NER"], [15, 16, "DatasetName", "SCIERC"]]}
{"text": "NER is typically cast as a sequence labeling problem and solved by models integrate LSTMs , CRF , and language models ( Lample et al , 2016 ; Liu et al , 2018 ; Zeng et al , 2019 .", "entities": [[0, 1, "TaskName", "NER"], [16, 17, "MethodName", "CRF"]]}
{"text": "The multiple tasks include concept recognition , relation extraction , and co - reference resolution .", "entities": [[7, 9, "TaskName", "relation extraction"]]}
{"text": "The mistake re - weighting mechanism is effective in the NER task ( Wang et al , 2019 ) .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "We presented an empirical method to explore the relationship between label consistency and NER model performance .", "entities": [[13, 14, "TaskName", "NER"]]}
{"text": "It identified the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) .", "entities": [[9, 10, "DatasetName", "SCIERC"], [11, 12, "DatasetName", "CoNLL03"]]}
{"text": "It validated the label consistency in multiple sets of NER data annotation on two benchmarks , CoNLL03 and SCIERC .", "entities": [[9, 10, "TaskName", "NER"], [16, 17, "DatasetName", "CoNLL03"], [18, 19, "DatasetName", "SCIERC"]]}
{"text": "Controllable Text Simplification with Explicit Paraphrasing", "entities": [[1, 3, "TaskName", "Text Simplification"]]}
{"text": {"text": "Maoqin @ DravidianLangTech - EACL2021 : The Application of Transformer - Based Model", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "I use the transformer - based language model with BiGRU - Attention to complete this task .", "entities": [[9, 10, "MethodName", "BiGRU"]]}
{"text": "So I choose ALBERT", "entities": [[3, 4, "MethodName", "ALBERT"]]}
{"text": "To get a more effective and higher accuracy model , BiGRU combined with attention .", "entities": [[7, 8, "MetricName", "accuracy"], [10, 11, "MethodName", "BiGRU"]]}
{"text": "There are many competitions about offensive language detection ( such as HASOC ( Chakravarthi et al , 2020c ; Mandl et al , 2020 ) and TRAC ( Kumar et al , 2018 ) ) , and many corresponding methods have been produced .", "entities": [[28, 29, "DatasetName", "Kumar"]]}
{"text": "People often tend to abstract this task into a text classification task ( Howard and Ruder , 2018 ) .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "Text classification is called extracting features from original text data and predicting the category of text data based on these features .", "entities": [[0, 2, "TaskName", "Text classification"]]}
{"text": "In the past few decades , many models for text classification have been proposed ( Qian , 2020 ) .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "From the 1960s to the 2010s , text classification models based on shallow learning dominated .", "entities": [[7, 9, "TaskName", "text classification"]]}
{"text": "Shallow learning means statistical - based models such as Naive Bayes ( NB ) , K Nearest Neighbors ( KNN ) ( Cover and Hart , 1967 ) and Support Vector Machines ( SVM ) .", "entities": [[33, 34, "MethodName", "SVM"]]}
{"text": "Compared with earlier rulebased methods , this method has obvious advantages in accuracy and stability .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "Since the 2010s , text classification has gradually changed from a shallow learning model to a deep learning model .", "entities": [[4, 6, "TaskName", "text classification"]]}
{"text": "Therefore , most of the text classification research work is based on DNN ( Yu et al , 2013 ) , which is a data - driven method with high computational complexity .", "entities": [[5, 7, "TaskName", "text classification"]]}
{"text": "The shallow learning model speeds up the text classification speed , improves the accuracy , and expands the application range of shallow learning .", "entities": [[7, 9, "TaskName", "text classification"], [13, 14, "MetricName", "accuracy"]]}
{"text": "In my job , I use the ALBERT model as my base model and take BiGRU - Attention behind it .", "entities": [[7, 8, "MethodName", "ALBERT"], [15, 16, "MethodName", "BiGRU"]]}
{"text": "The ALBERT model belongs to transformer - based language models .", "entities": [[1, 2, "MethodName", "ALBERT"]]}
{"text": "The ALBERT model is improved on the basis of Bidirectional Encoder Representations for Transformers ( BERT ) ( Devlin et al , 2018 ) model .", "entities": [[1, 2, "MethodName", "ALBERT"], [15, 16, "MethodName", "BERT"]]}
{"text": "It has designed a parameter reduction method to reduce memory consumption by changing the result of the original embedding parameter P ( the product of the vocabulary size V and the hidden layer size H ) .", "entities": [[31, 34, "HyperparameterName", "hidden layer size"]]}
{"text": "In BERT , E = H.", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "While in AL - BERT , H > >", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "E , so the number of parameters will be greatly reduced .", "entities": [[4, 7, "HyperparameterName", "number of parameters"]]}
{"text": "At the same time , the self - supervised loss is used to focus on the internal coherence in the construction of sentences .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "The ALBERT model implements three embedding layers : word embedding , position embedding , and segment embedding .", "entities": [[1, 2, "MethodName", "ALBERT"]]}
{"text": "Segment embedding helps BERT distinguish between paired input sequences .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "The BiGRU - Attention model ( Cover and Hart , 1967 ) is divided into three parts : text vector input layer , hidden layer , and output layer .", "entities": [[1, 2, "MethodName", "BiGRU"]]}
{"text": "Among them , the hidden layer consists of three layers : the BiGRU layer , the attention layer , and the Dense layer ( fully connected layer ) .", "entities": [[12, 13, "MethodName", "BiGRU"]]}
{"text": "I set the output of the ALBERT model as the input .", "entities": [[6, 7, "MethodName", "ALBERT"]]}
{"text": "After receiving the input , it uses the BiGRU neural network layer to extract features of the deep - level information of the text firstly .", "entities": [[8, 9, "MethodName", "BiGRU"]]}
{"text": "Finally , the text feature information with different weights is put into the softmax function layer for classification .", "entities": [[13, 14, "MethodName", "softmax"]]}
{"text": "The structure of the BiGRU - Attention model is shown in Figure 3 .", "entities": [[4, 5, "MethodName", "BiGRU"]]}
{"text": "In this task , I use the ALBERT model to pre - train the task .", "entities": [[7, 8, "MethodName", "ALBERT"]]}
{"text": "For the ALBERT model , the main hyperparameters I pay attention to are the training step size , batch size and learning rate .", "entities": [[2, 3, "MethodName", "ALBERT"], [15, 17, "HyperparameterName", "step size"], [18, 20, "HyperparameterName", "batch size"], [21, 23, "HyperparameterName", "learning rate"]]}
{"text": "I have obtained good performance using the ALBERT - BASE .", "entities": [[7, 8, "MethodName", "ALBERT"], [9, 10, "MethodName", "BASE"]]}
{"text": "Considering that BiGRU - Attention can capture contextual information well and extract text information features more accurately ( Radford et al , 2018 ) , I add it after AL - BERT .", "entities": [[2, 3, "MethodName", "BiGRU"], [31, 32, "MethodName", "BERT"]]}
{"text": "The standard of judgment is a weighted F1 - score , and this standard is the judgment standard used for my task .", "entities": [[7, 10, "MetricName", "F1 - score"]]}
{"text": "In this paper , I present my result on Offensive Language Identification in Dravidian Languages - EACL 2021 which includes three tasks of different languages .", "entities": [[10, 12, "TaskName", "Language Identification"]]}
{"text": "For this task , I regard it as a multiple classification task , I use the BiGRU - Attention based on the ALBERT model to complete , and my model works very well .", "entities": [[16, 17, "MethodName", "BiGRU"], [22, 23, "MethodName", "ALBERT"]]}
{"text": "At the same time , I will also consider whether I can use other transfer learning models to perform better on multi - classification tasks .", "entities": [[14, 16, "TaskName", "transfer learning"]]}
{"text": "Recent advances in GPU hardware have led to the emergence of bi - directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .", "entities": [[32, 33, "TaskName", "NER"], [43, 44, "MethodName", "CRF"]]}
{"text": "This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .", "entities": [[11, 12, "TaskName", "NER"], [35, 37, "TaskName", "structured prediction"]]}
{"text": "In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .", "entities": [[31, 37, "TaskName", "part - of - speech tagging"], [38, 41, "TaskName", "named entity recognition"], [42, 43, "TaskName", "NER"]]}
{"text": "Their computational cost grows with the number of layers , but not the input size , up to the memory and threading limitations of the hardware .", "entities": [[6, 9, "HyperparameterName", "number of layers"]]}
{"text": "This provides , for example , audio generation models that can be trained in parallel .", "entities": [[6, 8, "TaskName", "audio generation"]]}
{"text": "Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w \u2212 1 ) + 1 .", "entities": [[14, 15, "MethodName", "convolution"]]}
{"text": "The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .", "entities": [[1, 4, "HyperparameterName", "number of layers"]]}
{"text": "For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate .", "entities": [[17, 18, "MetricName", "loss"], [27, 30, "HyperparameterName", "number of parameters"]]}
{"text": "More concretely , just four stacked dilated convolutions of width 3 produces token representations with a n effective input width of 31 tokens - longer than the average sentence length ( 23 ) in the Penn TreeBank .", "entities": [[35, 37, "DatasetName", "Penn TreeBank"]]}
{"text": "When performing prediction using independent classification , the ID - CNN consistently outperforms a bidirectional LSTM ( Bi - LSTM ) , and performs on par with inference in a CRF with logits from a Bi - LSTM ( Bi - LSTM - CRF ) .", "entities": [[14, 16, "MethodName", "bidirectional LSTM"], [19, 20, "MethodName", "LSTM"], [30, 31, "MethodName", "CRF"], [37, 38, "MethodName", "LSTM"], [41, 42, "MethodName", "LSTM"], [43, 44, "MethodName", "CRF"]]}
{"text": "As an extractor of per - token logits for a CRF , our model out - performs the Bi - LSTM - CRF .", "entities": [[10, 11, "MethodName", "CRF"], [20, 21, "MethodName", "LSTM"], [22, 23, "MethodName", "CRF"]]}
{"text": "We also apply ID - CNNs to entire documents , where independent token classification is as accurate as the Bi - LSTM - CRF while decoding almost 8\u00d7 faster .", "entities": [[12, 14, "TaskName", "token classification"], [21, 22, "MethodName", "LSTM"], [23, 24, "MethodName", "CRF"]]}
{"text": "The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context - rich models .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "For example , RNN - based features require iterative passes along the length of x. We also consider a linear - chain CRF model that couples all of y together :", "entities": [[22, 23, "MethodName", "CRF"]]}
{"text": "CRF prediction explicitly reasons about interactions among neighboring output tags , whereas prediction in the first model compiles this reasoning into the feature extraction step ( Liang et al , 2008 ) .", "entities": [[0, 1, "MethodName", "CRF"]]}
{"text": "While CRF prediction requires non - trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging ( Ramshaw and Marcus , 1999 ) , will always be satisfied .", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over \u03b4 inputs at a time , where \u03b4 is the dilation width .", "entities": [[17, 18, "MethodName", "convolution"], [29, 30, "HyperparameterName", "\u03b4"], [36, 37, "HyperparameterName", "\u03b4"]]}
{"text": "We define the dilated convolution operator : c t = W c r k=0 x t\u00b1k\u03b4 .", "entities": [[3, 5, "MethodName", "dilated convolution"]]}
{"text": "( 4 ) A dilated convolution of width 1 is equivalent to a simple convolution .", "entities": [[4, 6, "MethodName", "dilated convolution"], [14, 15, "MethodName", "convolution"]]}
{"text": "By feeding the outputs of each dilated convolution as the input to the next , increasingly non - local information is incorporated into each pixel 's representation .", "entities": [[6, 8, "MethodName", "dilated convolution"]]}
{"text": "Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded .", "entities": [[5, 6, "MethodName", "convolution"]]}
{"text": "By doubling the dilation width at each layer , the size of the effective input width grows exponentially while the number of parameters grows only linearly with the number of layers , so a pixel representation quickly incorporates rich global evidence from the entire image .", "entities": [[20, 23, "HyperparameterName", "number of parameters"], [28, 31, "HyperparameterName", "number of layers"]]}
{"text": "We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate , allowing follow - on iterations to observe and resolve dependency violations .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "We denote the jth dilated convolutional layer of dilation width \u03b4 as D ( j ) \u03b4 .", "entities": [[10, 11, "HyperparameterName", "\u03b4"], [16, 17, "HyperparameterName", "\u03b4"]]}
{"text": "The first layer in the net - work is a dilation - 1 convolution D ( 0 ) 1 that transforms the input to a representation i t : i t = D ( 0 ) 1 x t ( 5 )", "entities": [[13, 14, "MethodName", "convolution"], [16, 17, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "Beginning with c t ( 0 ) = i t we define the stack of layers with the following recurrence : c t ( j ) = r D ( j\u22121 ) 2 Lc\u22121 c t ( j\u22121 ) ( 6 ) and add a final dilation - 1 layer to the stack : c t ( Lc+1 ) = r D ( Lc ) 1", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Here , maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag , with natural parameters given by Eqn . ( 9 ) : 1 T T t=1 log P ( y t | h t ( L b ) )", "entities": [[19, 21, "MethodName", "logistic regression"]]}
{"text": "( 10 ) We can also use the ID - CNN as logits for the CRF model ( Eqn . ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm .", "entities": [[15, 16, "MethodName", "CRF"]]}
{"text": "LSTMs ( Hochreiter and Schmidhuber , 1997 ) were used for NER as early as the CoNLL shared task in 2003 ( Hammerton , 2003 ; Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[11, 12, "TaskName", "NER"]]}
{"text": "More recently , a wide variety of neural network architectures for NER have been proposed .", "entities": [[11, 12, "TaskName", "NER"]]}
{"text": "al ( 2011 ) employ a one - layer CNN with pre - trained word embeddings , capitalization and lexicon features , and CRF - based prediction .", "entities": [[14, 16, "TaskName", "word embeddings"], [23, 24, "MethodName", "CRF"]]}
{"text": "Lample et al ( 2016 ) proposed two models which incorporated Bi - LSTM - composed character embeddings alongside words : a Bi - LSTM - CRF , and a greedy stack LSTM which uses a simple shift - reduce grammar to compose words into labeled entities .", "entities": [[13, 14, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"], [26, 27, "MethodName", "CRF"], [32, 33, "MethodName", "LSTM"]]}
{"text": "Their Bi - LSTM - CRF obtained the state - of - the - art on four languages without word shape or lexicon features .", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"]]}
{"text": "Ma and Hovy ( 2016 ) use CNNs rather than LSTMs to compose characters in a Bi - LSTM - CRF , achieving state - ofthe - art performance on part - of - speech tagging and CoNLL NER without lexicons .", "entities": [[18, 19, "MethodName", "LSTM"], [20, 21, "MethodName", "CRF"], [30, 36, "TaskName", "part - of - speech tagging"], [38, 39, "TaskName", "NER"]]}
{"text": "Chiu and Nichols ( 2016 ) evaluate a similar network but propose a novel method for encoding lexicon matches , presenting results on CoNLL and OntoNotes NER .", "entities": [[25, 26, "DatasetName", "OntoNotes"], [26, 27, "TaskName", "NER"]]}
{"text": "Yang et al ( 2016 ) use GRU - CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages .", "entities": [[7, 8, "MethodName", "GRU"]]}
{"text": "In general , distributed representations for text can provide useful generalization capabilities for NER systems , since they can leverage unsupervised pre - training of distributed word representations ( Turian et al , 2010 ;", "entities": [[13, 14, "TaskName", "NER"], [20, 24, "TaskName", "unsupervised pre - training"]]}
{"text": "In these NER approaches , CNNs were used for low - level feature extraction that feeds into alternative architectures .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "Overall , end - to - end CNNs have mainly been used in NLP for sentence classification , where the output representation is lower resolution than that of the input Kim ( 2014 ) ; Kalchbrenner et al ( 2014 ) ; ; Toutanova et al ( 2015 ) .", "entities": [[15, 17, "TaskName", "sentence classification"]]}
{"text": "Dilated convolutions were recently applied to the task of speech generation ( van den , and concurrent with this work , posted a pre - print describing the similar ByteNet network for machine translation that uses dilated convolutions in the encoder and decoder components .", "entities": [[32, 34, "TaskName", "machine translation"]]}
{"text": "Additionally , we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "We describe experiments on two benchmark English named entity recognition datasets .", "entities": [[7, 10, "TaskName", "named entity recognition"]]}
{"text": "On CoNLL - 2003 English NER , our ID - CNN performs on par with a Bi - LSTM not only when used to produce per - token logits for structured inference , but the ID - CNN with greedy decoding also performs on - par with the Bi - LSTM - CRF while running at more than 14 times the speed .", "entities": [[5, 6, "TaskName", "NER"], [18, 19, "MethodName", "LSTM"], [50, 51, "MethodName", "LSTM"], [52, 53, "MethodName", "CRF"]]}
{"text": "We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong Kim Sang and De Meulder , 2003 ) and OntoNotes 5.0 ( Hovy et al , 2006 ; Pradhan et al , 2006 ) .", "entities": [[23, 25, "DatasetName", "OntoNotes 5.0"]]}
{"text": "Following previous work , we use the same OntoNotes data split used for co - reference resolution in the CoNLL - 2012 shared task ( Pradhan et al , 2012 ) .", "entities": [[8, 9, "DatasetName", "OntoNotes"]]}
{"text": "As in previous work we evaluate the performance of our models using segment - level micro - averaged F1 score .", "entities": [[18, 20, "MetricName", "F1 score"]]}
{"text": "We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .", "entities": [[8, 9, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"], [24, 25, "MethodName", "CRF"], [29, 30, "MethodName", "LSTM"], [31, 32, "MethodName", "CRF"]]}
{"text": "We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "Collobert et al ( 2011 ) 86.96 Lample et al ( 2016 ) 90.33 Bi - LSTM 89.34 \u00b1 0.28 4 - layer CNN 89.97 \u00b1 0.20 5 - layer CNN 90.23 \u00b1 0.16 ID - CNN 90.32 \u00b1 0.26 Collobert et al ( 2011 ) 88.67 Passos et al ( 2014 ) 90.05 Lample et al ( 2016 )", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "90.20 Bi - LSTM - CRF ( re - impl ) 90.43 \u00b1 0.12 ID - CNN - CRF 90.54 \u00b1", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"], [18, 19, "MethodName", "CRF"]]}
{"text": "LSTM when paired with greedy decoding , suggesting that CNNs are better token encoders than Bi - LSTMs for independent logistic regression .", "entities": [[0, 1, "MethodName", "LSTM"], [20, 22, "MethodName", "logistic regression"]]}
{"text": "When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .", "entities": [[17, 18, "MethodName", "LSTM"]]}
{"text": "Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "Table 2 lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF .", "entities": [[17, 18, "MethodName", "LSTM"], [19, 20, "MethodName", "CRF"]]}
{"text": "We report decoding times using the fastest batch size for each method .", "entities": [[7, 9, "HyperparameterName", "batch size"]]}
{"text": "The ID - CNN model decodes nearly 50 % faster than the Bi - LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "With Viterbi decoding , the gap closes somewhat but the ID - CNN - CRF still comes out ahead , about 30 % faster than the Bi - LSTM - CRF .", "entities": [[14, 15, "MethodName", "CRF"], [28, 29, "MethodName", "LSTM"], [30, 31, "MethodName", "CRF"]]}
{"text": "Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as CoNLL - 2003 .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "We believe this model sees greater improvement with the addition of document - level context than the Bi - LSTM - CRF due to the ID - CNN learning a feature function better suited for representing broad context , in contrast with the Bi - LSTM which , though better than a simple RNN at encoding long memories of sequences , may reach its limit when provided with sequences more than 1 , 000 tokens long such as entire documents .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [45, 46, "MethodName", "LSTM"]]}
{"text": "In Table 6 we show that , in addition to being more accurate , our ID - CNN model is also much faster than the Bi - LSTM - CRF when incorporating context from entire documents , decoding at almost 8 times the speed .", "entities": [[27, 28, "MethodName", "LSTM"], [29, 30, "MethodName", "CRF"]]}
{"text": "On these long sequences , it also tags at more than 4.5 times the speed of the greedy Bi - LSTM , demonstrative of the benefit of our ID - CNNs context - aggregating computation that does not depend on the length of the sequence .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "We observe similar patterns on OntoNotes as we do on CoNLL . icalized greedy model of Ratinov and Roth ( 2009 ) , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of Durrett and Klein ( 2014 ) which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co - reference .", "entities": [[5, 6, "DatasetName", "OntoNotes"], [34, 35, "MethodName", "LSTM"], [58, 59, "DatasetName", "OntoNotes"], [66, 68, "TaskName", "entity linking"]]}
{"text": "Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re - implementation , which appears to be the new state - of - the - art on this dataset .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "CRF"]]}
{"text": "We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi - token named entity segment in CoNLL is about one token shorter than in OntoNotes .", "entities": [[13, 14, "DatasetName", "OntoNotes"], [43, 44, "DatasetName", "OntoNotes"]]}
{"text": "Still , our ID - CNN outperforms all other greedy methods , achieving our goal of learning a better token encoder for structured prediction .", "entities": [[22, 24, "TaskName", "structured prediction"]]}
{"text": "Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly .", "entities": [[12, 13, "DatasetName", "OntoNotes"], [18, 19, "MethodName", "LSTM"], [20, 21, "MethodName", "CRF"]]}
{"text": "For the first time , we see the score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [42, 43, "MethodName", "LSTM"], [44, 45, "MethodName", "CRF"]]}
{"text": "We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to CoNLL - 2003 : CoNLL - 2003 contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with disambiguation .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [19, 20, "DatasetName", "OntoNotes"]]}
{"text": "In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .", "entities": [[19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"]]}
{"text": "We are also grateful to Guillaume Lample for sharing his pretrained word embeddings .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "This work was supported in part by the Center for Intelligent Information Retrieval , in part by DARPA under agreement number FA8750 - 13 - 2 - 0020 , in part by Defense Advanced Research Agency ( DARPA ) contract number HR0011 - 15 - 2 - 0036 , in part by the National Science Foundation ( NSF ) grant number DMR - 1534431 , and in part by the National Science Foundation ( NSF ) grant number IIS - 1514053 .", "entities": [[11, 13, "TaskName", "Information Retrieval"], [17, 18, "DatasetName", "DARPA"], [37, 38, "DatasetName", "DARPA"]]}
{"text": "Validating Label Consistency in NER Data Annotation", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "Data annotation plays a crucial role in ensuring your named entity recognition ( NER ) projects are trained with the correct information to learn from .", "entities": [[9, 12, "TaskName", "named entity recognition"], [13, 14, "TaskName", "NER"]]}
{"text": "In this work , we present an empirical method to explore the relationship between label ( in - ) consistency and NER model performance .", "entities": [[21, 22, "TaskName", "NER"]]}
{"text": "It can be used to validate the label consistency ( or catch the inconsistency ) in multiple sets of NER data annotation .", "entities": [[19, 20, "TaskName", "NER"]]}
{"text": "In experiments , our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) .", "entities": [[13, 14, "DatasetName", "SCIERC"], [15, 16, "DatasetName", "CoNLL03"]]}
{"text": "Named entity recognition ( NER ) is one of the foundations of many downstream tasks such as relation extraction , event detection , and knowledge graph construction .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"], [17, 19, "TaskName", "relation extraction"], [20, 22, "TaskName", "event detection"], [25, 27, "TaskName", "graph construction"]]}
{"text": "NER models require vast amounts of labeled data to learn and identify patterns that humans can not continuously .", "entities": [[0, 1, "TaskName", "NER"]]}
{"text": "When end - to - end neural models achieve excellent performance on NER in various domains ( Lample et al , 2016 ; Liu et al , 2018 ; Luan et", "entities": [[12, 13, "TaskName", "NER"]]}
{"text": "al , 2018 ; Zeng et al , , 2021 , building useful and challenging NER benchmarks , such as CoNLL03 , WNUT16 , and SCIERC , contributes significantly to the research community .", "entities": [[15, 16, "TaskName", "NER"], [20, 21, "DatasetName", "CoNLL03"], [25, 26, "DatasetName", "SCIERC"]]}
{"text": "For example , in the CoNLL03 dataset ( Sang and De Meulder , 2003 ) , a standard NER benchmark that has been cited over 2 , 300 times , label mistakes were found in 5.38 % of the test set ( Wang et al , 2019 ) .", "entities": [[5, 6, "DatasetName", "CoNLL03"], [18, 19, "TaskName", "NER"]]}
{"text": "Another example is SCIERC ( Luan et", "entities": [[3, 4, "DatasetName", "SCIERC"]]}
{"text": "al , 2018 ) ( cited \u223c50 times ) which is a multi - task ( including NER ) benchmark in AI domain .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "When we looked at the false predictions given by SCIIE which was a multi - task model released along with the SCIERC dataset , we found that as many as 147 ( 26.7 % of the test set ) sentences were not properly annotated .", "entities": [[21, 22, "DatasetName", "SCIERC"]]}
{"text": "As shown in the experiments section , after the correction , the NER performance becomes more accurate and stable .", "entities": [[12, 13, "TaskName", "NER"]]}
{"text": "Table 1 : Three examples to compare original and corrected annotation in the test set of the SCIERC dataset .", "entities": [[17, 18, "DatasetName", "SCIERC"]]}
{"text": "We apply the SCIIE NER model on the new test set .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "Results on SCIERC show that the test set ( red ) is less predictive of training samples ( orange ) than the training set itself ( blue or green ) .", "entities": [[2, 3, "DatasetName", "SCIERC"]]}
{"text": "Experiments show that they are effective on the CoNLL03 and SCIERC datasets .", "entities": [[8, 9, "DatasetName", "CoNLL03"], [10, 11, "DatasetName", "SCIERC"]]}
{"text": "Take SCIERC as an example .", "entities": [[1, 2, "DatasetName", "SCIERC"]]}
{"text": "Then we train the SCIIE NER model ( Luan et al , 2018 ) to perform on the new test set .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "Take SCIERC as an example .", "entities": [[1, 2, "DatasetName", "SCIERC"]]}
{"text": "Here we deploy five state - of - the - art NER models to investigate their performance on the corrected SCIERC dataset .", "entities": [[11, 12, "TaskName", "NER"], [20, 21, "DatasetName", "SCIERC"]]}
{"text": "The NER models are BiLSTM - CRF ( Lample et al , 2016 ) , LM - BiLSTM - CRF ( Liu et al , 2018 ) ,", "entities": [[1, 2, "TaskName", "NER"], [4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "CRF"], [17, 18, "MethodName", "BiLSTM"], [19, 20, "MethodName", "CRF"]]}
{"text": "As shown in Table 2 , all NER models deliver better performance on the corrected SCIERC than the original dataset .", "entities": [[7, 8, "TaskName", "NER"], [15, 16, "DatasetName", "SCIERC"]]}
{"text": "NER is typically cast as a sequence labeling problem and solved by models integrate LSTMs , CRF , and language models ( Lample et al , 2016 ; Liu et al , 2018 ; Zeng et al , 2019 .", "entities": [[0, 1, "TaskName", "NER"], [16, 17, "MethodName", "CRF"]]}
{"text": "The multiple tasks include concept recognition , relation extraction , and co - reference resolution .", "entities": [[7, 9, "TaskName", "relation extraction"]]}
{"text": "The mistake re - weighting mechanism is effective in the NER task ( Wang et al , 2019 ) .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "We presented an empirical method to explore the relationship between label consistency and NER model performance .", "entities": [[13, 14, "TaskName", "NER"]]}
{"text": "It identified the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) .", "entities": [[9, 10, "DatasetName", "SCIERC"], [11, 12, "DatasetName", "CoNLL03"]]}
{"text": "It validated the label consistency in multiple sets of NER data annotation on two benchmarks , CoNLL03 and SCIERC .", "entities": [[9, 10, "TaskName", "NER"], [16, 17, "DatasetName", "CoNLL03"], [18, 19, "DatasetName", "SCIERC"]]}
{"text": "Controllable Text Simplification with Explicit Paraphrasing", "entities": [[1, 3, "TaskName", "Text Simplification"]]}
{"text": "Text Simplification improves the readability of sentences through several rewriting transformations , such as lexical paraphrasing , deletion , and splitting .", "entities": [[0, 2, "TaskName", "Text Simplification"]]}
{"text": "We introduce a new data augmentation method to improve the paraphrasing capability of our model .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "Text Simplification aims to improve the readability of texts with simpler grammar and word choices while preserving meaning ( Saggion , 2017 ) .", "entities": [[0, 2, "TaskName", "Text Simplification"]]}
{"text": "It also helps with downstream natural language processing tasks , such as parsing ( Chandrasekar et al , 1996 ) , semantic role labelling ( Vickrey and Koller , 2008 ) , information extraction ( Miwa et al , 2010 ) , and machine translation ( MT , Chen et al , 2012 ; \u0160tajner and Popovic , 2016 ) .", "entities": [[43, 45, "TaskName", "machine translation"]]}
{"text": "Since 2016 , nearly all text simplification systems have been sequence - to - sequence ( seq2seq ) Table 1 : Output statistics of 500 random sentences from the Newsela test set .", "entities": [[5, 7, "TaskName", "text simplification"], [16, 17, "MethodName", "seq2seq"], [29, 30, "DatasetName", "Newsela"]]}
{"text": "We hypothesize that the seq2seq generation model will learn lexical and structural paraphrases more efficiently from the parallel corpus , when we offload some of the burden of sentence splitting ( e.g. , split at comma ) and deletion ( e.g. , remove trailing preposition phrases ) decisions to a separate component .", "entities": [[4, 5, "MethodName", "seq2seq"]]}
{"text": "In contrast , our approach provides a more flexible and dynamic integration of linguistic rules with the neural models through ranking and data augmentation ( Figure 1 ) .", "entities": [[22, 24, "TaskName", "data augmentation"]]}
{"text": "We also demonstrate that our model can control the extent of each simplification operation by : ( 1 ) imposing a soft constraint on the percentage of words to be copied from the input in the seq2seq model , thus limiting lexical paraphrasing ; and ( 2 ) selecting candidates that underwent a desired amount of splitting and/or deletion .", "entities": [[36, 37, "MethodName", "seq2seq"]]}
{"text": "Finally , we create a new test dataset with multiple human references for Newsela ( Xu et al , 2015 ) , the widely used text simplification corpus , to specifically evaluate lexical paraphrasing .", "entities": [[13, 14, "DatasetName", "Newsela"], [25, 27, "TaskName", "text simplification"]]}
{"text": "These intermediate sentences are then used for two purposes : ( 1 ) Selected by a pairwise neural ranking model ( 2.2 ) based on the simplification quality and then rewritten by the paraphrasing component ; ( 2 ) Used for data augmentation to improve the diversity of the paraphrasing model ( 2.3 ) .", "entities": [[41, 43, "TaskName", "data augmentation"]]}
{"text": "The compression ratio is calculated as the number of words in a candidate simplification v i ( which may contain one or more sub - sentences ) divided by that of the original sentence x. To further increase the variety of generated candidates , we supplement DisSim with a Neural Deletion and Split module trained on the text simplification corpus ( 3.1 ) .", "entities": [[57, 59, "TaskName", "text simplification"]]}
{"text": "We use a Transformer seq2seq model with the same configuration as the base model for paraphrasing ( 2.3 ) .", "entities": [[3, 4, "MethodName", "Transformer"], [4, 5, "MethodName", "seq2seq"]]}
{"text": "We train the model on a standard text simplification corpus consisting of pairs of complex sentence x and manually simplified reference y. Scoring Function .", "entities": [[7, 9, "TaskName", "text simplification"]]}
{"text": "e \u2212\u03bb | | \u03c6v i \u2212\u03c6y | | \u00d7 BERT Score ( v i , y )", "entities": [[10, 11, "MethodName", "BERT"], [11, 12, "MetricName", "Score"]]}
{"text": "( 1 ) BERTScore ( Zhang et al , 2020b ) is a text similarity metric that uses BERT ( Devlin et al , 2019 ) embeddings to find soft matches between word pieces ( Wu et al , 2016 ) instead of exact string matching .", "entities": [[13, 15, "TaskName", "text similarity"], [18, 19, "MethodName", "BERT"]]}
{"text": "For the feedforward network g ( . ) , we use the following features : number of words in v i and x , compression ratio of v i with respect to x , Jaccard similarity between v i and x ,", "entities": [[2, 4, "MethodName", "feedforward network"]]}
{"text": "Our paraphrase generation model can explicitly control the extent of lexical paraphrasing by specifying the percentage of words to be copied from the input sentence as a soft constraint .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}
{"text": "We also introduce a data augmentation method to encourage our model to generate more diverse outputs .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "Our base generation model is a Transformer encoder - decoder initialized by the BERT checkpoint ( ? ) , which achieved the best reported performance on text simplification in the recent work .", "entities": [[6, 7, "MethodName", "Transformer"], [13, 14, "MethodName", "BERT"], [26, 28, "TaskName", "text simplification"]]}
{"text": ", v l ) of l words and the percentage of copying cp ( 0 , 1 ] , our goal is to paraphrase the rest of ( 1 \u2212 cp ) \u00d7 l words inv to a simpler version .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "To achieve this , we convert cp into a vector of the same dimension as BERT embeddings using Gaussian binning ( Maddela and Xu , 2018 ) and add it to the beginning of the input sequencev .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "The Transformer encoder then produces a sequence of context - aware hidden states", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "The Transformer decoder generates the output sequence fromH.", "entities": [[1, 3, "MethodName", "Transformer decoder"]]}
{"text": "We train the paraphrasing model and the copy network in a multi - task learning setup , where predicting whether a word should be copied serves as an auxiliary task .", "entities": [[11, 15, "TaskName", "multi - task learning"]]}
{"text": "When a word occurs multiple times in the input , we rely on the monolingual word alignment results from JacanaAlign ( Yao et al , 2013 ) to determine which occurrence is the one that gets copied .", "entities": [[15, 17, "TaskName", "word alignment"]]}
{"text": "We train the Transformer model and the copy network jointly by minimizing the cross - entropy loss for both decoder generation and binary word classification .", "entities": [[3, 4, "MethodName", "Transformer"], [16, 17, "MetricName", "loss"]]}
{"text": "We provide implementation and training details in Appendix A. Data Augmentation .", "entities": [[9, 11, "TaskName", "Data Augmentation"]]}
{"text": "We perform only the paraphrase generation step for paraphrase - focused simplification .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}
{"text": "We train and evaluate our models on Newsela ( Xu et al , 2015 )", "entities": [[7, 8, "DatasetName", "Newsela"]]}
{"text": "3 and Wikipedia copora ( Zhu et al , 2010 ; Woodsend and Lapata , 2011 ; Coster and Kauchak , 2011 Table 2 : Automatic evaluation results on NEWSELA - AUTO test set .", "entities": [[29, 30, "DatasetName", "NEWSELA"]]}
{"text": "We report SARI , the main automatic metric for simplification , and its three edit scores namely precision for delete ( del ) and F1 scores for add and keep operations .", "entities": [[24, 25, "MetricName", "F1"]]}
{"text": "We also report FKGL ( FK ) , average sentence length ( SLen ) , output length ( OLen ) , compression ratio ( CR ) , self - BLEU ( s - BL ) , percentage of sentence splits ( % split ) , average percentage of new words added to the output ( % new ) , and percentage of sentences identical to the input ( % eq ) .", "entities": [[29, 30, "MetricName", "BLEU"]]}
{"text": "We used the complex - simple sentence pairs automatically aligned by , called the NEWSELA - AUTO dataset .", "entities": [[14, 15, "DatasetName", "NEWSELA"]]}
{"text": "Besides Newsela , we also provide the details of experiments on Wikipedia corpus in Appendix F , which show similar trends .", "entities": [[1, 2, "DatasetName", "Newsela"]]}
{"text": "To demonstrate that our model can be controlled to generate diverse simplifications , we evaluate under the following settings : ( i ) Standard evaluation on the NEWSELA - AUTO test set similar to the methodology in the recent literature", "entities": [[27, 28, "DatasetName", "NEWSELA"]]}
{"text": "Dong et al , 2019 ; Zhang and Lapata , 2017 ) , and ( ii ) Evaluation on different subsets of the NEWSELA - AUTO test set that concentrate on a specific operation .", "entities": [[23, 24, "DatasetName", "NEWSELA"]]}
{"text": "We created a new dataset , called NEWSELA - TURK , to evaluate lexical paraphrasing .", "entities": [[7, 8, "DatasetName", "NEWSELA"]]}
{"text": "4 Similar to the WIKIPEDIA - TURK benchmark corpus ( Xu et al , 2016 ) , NEWSELA - TURK consists of human - written references focused on lexical para - phrasing .", "entities": [[17, 18, "DatasetName", "NEWSELA"]]}
{"text": "We first selected sentence pairs from the NEWSELA - AUTO test set of roughly similar length ( compression ratio between 0.8 and 1.2 ) and no sentence splits because they more likely involve paraphrasing .", "entities": [[7, 8, "DatasetName", "NEWSELA"]]}
{"text": "Then , we asked Amazon Mechanical Turk workers to simplify the complex sentence without any loss in meaning .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "We use the following simplification approaches as baselines : ( i ) BERT - Initialized Transfomer ( ? ) , where the encoder is initialized with BERT base checkpoint and the decoder is randomly initialized .", "entities": [[12, 13, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"]]}
{"text": "It is the current state - of - the - art for text simplification .", "entities": [[12, 14, "TaskName", "text simplification"]]}
{"text": "( iii ) LSTM baseline , a vanilla encoderdecoder model used in Zhang and Lapata ( 2017 ) .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "( iv ) Hybrid - NG ( Narayan and Gardent , 2014 ) , 7 one of the best existing hybrid systems that performs splitting and deletion using a probabilistic model and lexical substitution with a phrase - based machine translation system .", "entities": [[39, 41, "TaskName", "machine translation"]]}
{"text": "Table 3 : Automatic evaluation results on NEWSELA - TURK that focuses on paraphrasing ( 500 complex sentences with 4 human written paraphrases ) .", "entities": [[7, 8, "DatasetName", "NEWSELA"]]}
{"text": "The model 's deletion capability is measured by the F1 score for n - grams that are kept ( keep ) and precision for those deleted ( del ) .", "entities": [[9, 11, "MetricName", "F1 score"]]}
{"text": "9 SARI score of a reference with itself may not always be 100 as it considers 0 divided by 0 as 0 , instead of 1 , when calculating n - gram precision and recall .", "entities": [[16, 17, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [21, 22, "DatasetName", "0"]]}
{"text": "This avoids the inflation of del scores when the input is same as the output . phrasing capability and diversity , we calculate the BLEU score with respect to the input ( s - BL ) , the percentage of new words ( % new ) added , and the percentage of system outputs identical to the input ( % eq ) .", "entities": [[24, 26, "MetricName", "BLEU score"]]}
{"text": "We do not report BLEU because it often does not correlate with simplicity ( Sulem et al , 2018a , b ; Xu et", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "Table 6 : Human evaluation of 100 random simplifications from the NEWSELA - AUTO test set and the split - focused subset of the same test set .", "entities": [[11, 12, "DatasetName", "NEWSELA"]]}
{"text": "Transformer model alone is rather conservative and copies 10.2 % of the sentences directly to the output .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "We provide examples of system outputs in Table 9 and Appendix C. Tables 3 , 4 , and 5 show the results on NEWSELA - TURK , split - focused , and delete - focused subsets of NEWSELA - AUTO test set respectively .", "entities": [[23, 24, "DatasetName", "NEWSELA"], [37, 38, "DatasetName", "NEWSELA"]]}
{"text": "11 For the first one , we asked five Amazon Mechanical Turk workers to evaluate fluency , adequacy and simplicity of 100 random simplifications from the NEWSELA - AUTO test set .", "entities": [[26, 27, "DatasetName", "NEWSELA"]]}
{"text": "We supplemented the 2 - 3 readability levels in NEWSELA - AUTO , which contained more lexical overlaps and inflated the scores for EditNTS .", "entities": [[9, 10, "DatasetName", "NEWSELA"]]}
{"text": "11 We provide instructions in Appendix E. fluency and adequacy ratings with binary questions described in Zhang et al ( 2020a ) for the second evaluation over another 100 simplifications from the NEWSELA - AUTO split - focused test set .", "entities": [[32, 33, "DatasetName", "NEWSELA"]]}
{"text": "The adequacy rating is also very close to that of Transformer bert and EditNTS even though our model is performing more paraphrasing ( Table 2 ) , which verifies that the changes made by our system are meaningful .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "We evaluate our key design choices , namely candidate ranking that is based on length - penalized BERTScore and paraphrase generation that uses data augmentation and copy attention .", "entities": [[19, 21, "TaskName", "paraphrase generation"], [23, 25, "TaskName", "data augmentation"]]}
{"text": "Compared to our final model ( Our Model ) , its variants without data augmentation ( \u2212 augmentation ) and copy mechanism ( \u2212 copy attn ) suffer a drop of 1.0 and 2.6 points in SARI respectively and a decrease of at least 3.0 % of new words , which demonstrates that these components encourage the system to paraphrase .", "entities": [[13, 15, "TaskName", "data augmentation"]]}
{"text": "Our model trained on only DisSim ( \u2212 only DisSim ) and Transformer ( \u2212 only Transformer ) candidates performs close to our best model ( Our Model ) in terms of SARI .", "entities": [[12, 13, "MethodName", "Transformer"], [16, 17, "MethodName", "Transformer"]]}
{"text": "To understand the errors generated by our model , we manually classified 200 simplifications from the NEWSELA - AUTO test set into the following categories : ( a ) Good , where the model generated meaningful simplifications , ( b ) Hallucinations , where the model introduced information not in the input , ( c ) Fluency Errors , where the model generated ungrammatical output , ( d )", "entities": [[16, 17, "DatasetName", "NEWSELA"]]}
{"text": "Before the advent of neural networks , text simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules ( Carroll et al , 1999 ; Siddharthan , 2002 ; Siddharthan et al , 2004 ) or data - driven methods based on parallel corpora ( Zhu et al , 2010 ; Woodsend and Lapata , 2011 ; Narayan and Gardent , 2014 ) .", "entities": [[7, 9, "TaskName", "text simplification"]]}
{"text": "Following neural machine translation , the trend changed to performing all the operations together end - toend ( Zhang and Lapata , 2017 ; Nisioi et al , 2017 ; Zhao et al , 2018 ; Alva - Manchego et al , 2017 they discovered that the ship had been important .", "entities": [[2, 4, "TaskName", "machine translation"]]}
{"text": "LSTM experts say china 's air pollution exacts a tremendous toll on human health .", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "Transformer bert experts say china 's pollution has a tremendous effect on human health .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "al , 2019 ; at the cost of controllability and performance as shown in this paper . Controllable text simplification has been attempted before , but only with limited capability .", "entities": [[18, 20, "TaskName", "text simplification"]]}
{"text": "Kumar et al ( 2020 ) proposed a linguistic scoring function to control the edits to the input .", "entities": [[0, 1, "DatasetName", "Kumar"]]}
{"text": "Another long body of research focuses on a single simplification operation and can be broadly divided into three categories : ( 1 ) Lexical Simplification ( Specia et al , 2012 ; Horn et al , 2014 ; Glava\u0161 and \u0160tajner , 2015 ; Paetzold andSpecia , 2017 , 2015 ; Maddela and Xu , 2018 ; Qiang et al , 2020 ) , where complex words are substituted with simpler words .", "entities": [[23, 25, "TaskName", "Lexical Simplification"]]}
{"text": "( 2 ) Syntactic Simplification ( Siddharthan , 2006 ; Aharoni and Goldberg , 2018 ; Botha et al , 2018 ; Niklaus et al , 2019 ) , which deals exclusively with sentence splitting , and ( 3 ) Sentence Compression ( Filippova et al , 2015 ;", "entities": [[40, 42, "DatasetName", "Sentence Compression"]]}
{"text": "We designed a new data augmentation method to encourage the model to paraphrase .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "We created a new dataset , NEWSELA - TURK , to evaluate paraphrasing - focused simplifications .", "entities": [[6, 7, "DatasetName", "NEWSELA"]]}
{"text": "We implemented two separate Transformer models for neural deletion and split component ( 2.1 ) and paraphrase generation ( 2.3 ) using the Fairseq 12 toolkit .", "entities": [[4, 5, "MethodName", "Transformer"], [16, 18, "TaskName", "paraphrase generation"]]}
{"text": "Both the encoder and decoder follow BERT base 13 architecture , while the encoder is also initialized with BERT base checkpoint .", "entities": [[6, 7, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"]]}
{"text": "The copy attention mechanism is a feedforward network containing 3 hidden layers , 1000 nodes in each layer with tanh activation , and a single linear output node with sigmoid activation .", "entities": [[6, 8, "MethodName", "feedforward network"], [19, 21, "MethodName", "tanh activation"], [29, 31, "MethodName", "sigmoid activation"]]}
{"text": "We used BERT WordPiece tokenizer .", "entities": [[2, 3, "MethodName", "BERT"], [3, 4, "MethodName", "WordPiece"]]}
{"text": "We did not perform any hyperparameter search and directly used the hyperparameters of the BERT - initialized Transformer described in ? .", "entities": [[14, 15, "MethodName", "BERT"], [17, 18, "MethodName", "Transformer"]]}
{"text": "Our pairwise ranking model , implemented using the PyTorch framework , consists of 3 hidden layers , 100 nodes in each layer , tanh activation , and a single linear output node .", "entities": [[23, 25, "MethodName", "tanh activation"]]}
{"text": "LSTM this year , the faa has approved dozens of permits for agricultural drone businesses .", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "Transformer bert this year , the faa has approved dozens of permits for agricultural businesses .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "LSTM the room echoed with the sounds of song , the voices of young men .", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "Transformer bert the room echoed with the sound of song , the beat of drums , the voices of young men .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "Table 11 : Automatic evaluation results on a subset of Newsela test set that focuses on paraphrasing ( 8371 complexsimple sentence with compression ratio > 0.9 and no splits ) .", "entities": [[10, 11, "DatasetName", "Newsela"]]}
{"text": "36.1 2.5 67.4 38.5 11.7 20.9 22.4 1.02 6.4 63.5 13.5 0.0 Our Model 35.9 4.7 63.6 39.6 9.2 14.7 19.8 0.9 33.7 63.2 12.9 9.2 Our Model ( no split ; cp = 0.6 ) 36.5 4.9 63.2 41.4 10.8 18.6 19.9 0.89 6.7 61.9 12.4 3.9 Our Model ( no split ; cp = 0.7 ) 37 . 5 4.3 68.8 39.4 11.2 19.1 20.9 0.94 8.9 72.6 8.6 12.3 Our Model ( no split ; cp = 0.8 ) 37 . 0", "entities": [[84, 85, "DatasetName", "0"]]}
{"text": "For validation and testing purposes , we use the following two corpora : ( i ) TURK corpus ( Xu et al , 2015 ) for lexical paraphrasing and ( ii ) ASSET corpus ( Alva - Manchego et al , 2020 ) for multiple rewrite operations .", "entities": [[32, 34, "DatasetName", "ASSET corpus"]]}
{"text": "Tables 12 and 13 show the results on TURK and ASSET respectively .", "entities": [[10, 11, "DatasetName", "ASSET"]]}
{"text": "We thank Newsela for sharing the data and NVIDIA for providing GPU computing resources .", "entities": [[2, 3, "DatasetName", "Newsela"]]}
{"text": "Our system uses pairwise learning to rank methods on rich set of hand designed and representation learning features .", "entities": [[15, 17, "TaskName", "representation learning"]]}
{"text": "The system achieved second highest results on official metrics MAP and good results on other search metrics .", "entities": [[9, 10, "DatasetName", "MAP"]]}
{"text": "In online forums question answering is one of the most popular way for users to share information between each other .", "entities": [[3, 5, "TaskName", "question answering"]]}
{"text": "Along with these we also used Glove embeddings ( Pennington et al , 2014 ) which were pretrained using 6 billion tokens from Wikipedia - 2014 and Gigaword dataset .", "entities": [[6, 8, "MethodName", "Glove embeddings"]]}
{"text": "These have been studied in information retrieval literature and they power many of the industrial search engines .", "entities": [[5, 7, "TaskName", "information retrieval"]]}
{"text": "The system uses Siamese network to learn these similarity measures .", "entities": [[3, 5, "MethodName", "Siamese network"]]}
{"text": "Siamese nets were first introduced in the early 1990s by ( Bromley et al , 1993 ) to solve signature verification as an image matching problem .", "entities": [[23, 25, "TaskName", "image matching"]]}
{"text": "The weights between both the networks are shared generally , so that they project the similar texts not far in the embedding dimension .", "entities": [[21, 23, "HyperparameterName", "embedding dimension"]]}
{"text": "Figure 1 shows a siamese network , where X 1 represents the original question text and X 2 represents the candidate question text .", "entities": [[4, 6, "MethodName", "siamese network"]]}
{"text": "The euclidean distance of the vectors is used to compute the contrastive loss .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "We use following networks to generate text embedding : Long Short Term Memory LSTM ( Hochreiter and Schmidhuber , 1997 ) are popular variant of the the recurrent neural network architecture that captures the long term dependency in text and deals with vanishing gradient problem .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "For bi - directional LSTM , the hidden unit is a LSTM cell combining of various gates .", "entities": [[4, 5, "MethodName", "LSTM"], [11, 12, "MethodName", "LSTM"]]}
{"text": "Gated Recurrent Unit Gated recurrent unit ( GRU ) ( Chung et al , 2014 ) is another variant of RNN which were introduced recently as compared to LSTM .", "entities": [[0, 3, "MethodName", "Gated Recurrent Unit"], [3, 6, "MethodName", "Gated recurrent unit"], [7, 8, "MethodName", "GRU"], [28, 29, "MethodName", "LSTM"]]}
{"text": "They also have seen similar success as LSTM in various NLP tasks .", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "We use Bi - GRU as another network to generate the neural embeddings trained by siamese network similar to Bi - LSTM .", "entities": [[4, 5, "MethodName", "GRU"], [15, 17, "MethodName", "siamese network"], [21, 22, "MethodName", "LSTM"]]}
{"text": "The final hidden embedding size is 256 dimension for our Bi - GRU network also .", "entities": [[12, 13, "MethodName", "GRU"]]}
{"text": "Convolution Net We also use convolution networks as another neural network architecture to generate embeddings inside the siamese network .", "entities": [[0, 1, "MethodName", "Convolution"], [5, 6, "MethodName", "convolution"], [17, 19, "MethodName", "siamese network"]]}
{"text": "We use 1D - convolution with 128 kernels , stride of 5 followed by 1D - max pool with pool - size of 5 and finally a dense layer to create a 128 dimension vector .", "entities": [[4, 5, "MethodName", "convolution"]]}
{"text": "This gives the system the baseline accuracy of the search engine .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Apart from the neural network learned semantic features , we also employ semantic similarity between question text generated by semantic net .", "entities": [[12, 14, "TaskName", "semantic similarity"]]}
{"text": "Then the sentence similarity is obtained as a linear combination of semantic similarity and the word order similarity .", "entities": [[11, 13, "TaskName", "semantic similarity"]]}
{"text": "To generate semantic similarity , cosine between semantic vectors is obtained .", "entities": [[2, 4, "TaskName", "semantic similarity"]]}
{"text": "Word order similarity is computed in the similar way as semantic similarity but the position of word in the sentence is used to generate the word order vector .", "entities": [[10, 12, "TaskName", "semantic similarity"]]}
{"text": "The feature encodes semantic similarity and gives boost to system , shown in the results table .", "entities": [[3, 5, "TaskName", "semantic similarity"]]}
{"text": "There has been a lot of research in machine translation and summarization community to find metrics that correlate with human judgement on these tasks .", "entities": [[8, 10, "TaskName", "machine translation"], [11, 12, "TaskName", "summarization"]]}
{"text": "We compute BLEU", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "We use Latent Dirichlet al ocation ( LDA ) ( Blei et al , 2003 ) to compute topic similarity between texts .", "entities": [[7, 8, "MethodName", "LDA"]]}
{"text": "We train LDA topic model using the whole text ( body and subject ) as corpus .", "entities": [[2, 3, "MethodName", "LDA"]]}
{"text": "This formulation is more closer to ranking task than predicting relevance as regression and also has theoretical guarantees of maximizing the MAP in ranking ( Chen et al , 2009 ) .", "entities": [[21, 22, "DatasetName", "MAP"]]}
{"text": "The results generated by the system on test data were submitted as an entry to SemEval - 2017 task 3 subtask B. Our primary entry achieved second place on the MAP which was official metric for ranking .", "entities": [[30, 31, "DatasetName", "MAP"]]}
{"text": "Also it achieved highest MRR amongst all the primary submissions .", "entities": [[4, 5, "MetricName", "MRR"]]}
{"text": "Our both contrastive submissions trained on SVM achieved better test accuracy than training on Logistic Regression .", "entities": [[6, 7, "MethodName", "SVM"], [10, 11, "MetricName", "accuracy"], [14, 16, "MethodName", "Logistic Regression"]]}
{"text": "Thus the Ranking - SVM is able to generalize better .", "entities": [[4, 5, "MethodName", "SVM"]]}
{"text": "We also experimented with pointwise learning to rank method and got inferior results thus corroborating the fact that pairwise methods are helping our system in achieving better accuracy .", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "We also plan to use Triplet loss ( Hoffer and Ailon , 2015 ) which captures ranking task in better way .", "entities": [[5, 7, "MethodName", "Triplet loss"]]}
{"text": "Another direction is to use state - of - art listwise learning to rank methods that can directly optimize MAP .", "entities": [[19, 20, "DatasetName", "MAP"]]}
{"text": "Compressing BERT : Studying the Effects of Weight Pruning on Transfer Learning", "entities": [[1, 2, "MethodName", "BERT"], [10, 12, "TaskName", "Transfer Learning"]]}
{"text": "Pre - trained feature extractors , such as BERT for natural language processing and VGG for computer vision , have become effective methods for improving deep learning models without requiring more labeled data .", "entities": [[8, 9, "MethodName", "BERT"], [14, 15, "MethodName", "VGG"]]}
{"text": "We explore weight pruning for BERT and ask : how does compression during pretraining affect transfer learning ?", "entities": [[5, 6, "MethodName", "BERT"], [15, 17, "TaskName", "transfer learning"]]}
{"text": "We find that pruning affects transfer learning in three broad regimes .", "entities": [[5, 7, "TaskName", "transfer learning"]]}
{"text": "Medium levels of pruning increase the pre - training loss and prevent useful pre - training information from being transferred to downstream tasks .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "Finally , we observe that finetuning BERT on a specific task does not improve its prunability .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "We conclude that BERT can be pruned once during pre - training rather than separately for each task without affecting performance .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Pre - trained feature extractors , such as BERT ( Devlin et al , 2018 ) for natural language processing and VGG ( Simonyan and Zisserman , 2014 ) for computer vision , have become effective methods for improving the performance of deep learning models .", "entities": [[8, 9, "MethodName", "BERT"], [21, 22, "MethodName", "VGG"]]}
{"text": "In the last year , models similar to BERT have become state - of - the - art in many NLP tasks , including natural language inference ( NLI ) , named entity recognition ( NER ) , sentiment analysis , etc .", "entities": [[8, 9, "MethodName", "BERT"], [24, 27, "TaskName", "natural language inference"], [31, 34, "TaskName", "named entity recognition"], [35, 36, "TaskName", "NER"], [38, 40, "TaskName", "sentiment analysis"]]}
{"text": "Pre - trained models usually achieve higher accuracy than any model trained on downstream data alone .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Second , due to the size of the pre - training dataset , BERT models tend to be slow and require impractically large amounts of GPU memory .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "BERT - Large can only be used with access to a Google TPU , and BERT - Base requires some optimization tricks such as gradient checkpointing or gradient accumulation to be trained effectively on consumer hardware", "entities": [[0, 1, "MethodName", "BERT"], [11, 12, "DatasetName", "Google"], [15, 16, "MethodName", "BERT"], [24, 26, "MethodName", "gradient checkpointing"]]}
{"text": "Training BERT - Base from scratch costs \u223c$7k and emits \u223c1438 pounds of CO 2 ( Strubell et al , 2019 ) .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "It might also be used to trade accuracy for memory in some low - resource cases , such as deploying to smartphones for real - time prediction .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "The main questions this paper attempts to answer are : Does compressing BERT impede it 's ability to transfer to new tasks ?", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "And does fine - tuning make BERT more or less compressible ?", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "To explore these questions , we compressed English BERT using magnitude weight pruning ( Han et al , 2015 ) and observed the results on transfer learning to the General Language Understanding Evaluation ( GLUE ) benchmark , a diverse set of natural language understanding tasks including sentiment analysis , NLI , and textual similarity evaluation .", "entities": [[8, 9, "MethodName", "BERT"], [25, 27, "TaskName", "transfer learning"], [29, 30, "DatasetName", "General"], [34, 35, "DatasetName", "GLUE"], [42, 45, "TaskName", "natural language understanding"], [47, 49, "TaskName", "sentiment analysis"]]}
{"text": "We chose magnitude weight pruning , which compresses models by removing weights close to 0 , because it is one of the most fine - grained and effective compression methods and because there are many interesting ways to view pruning , which we explore in the next section .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "Medium levels of pruning increase the pre - training loss and prevent useful pre - training information from being transferred to downstream tasks .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "This information is not equally useful to each task ; tasks degrade linearly with pre - train loss , but at different rates .", "entities": [[17, 18, "MetricName", "loss"]]}
{"text": "Finally , we observe that fine - tuning BERT on a specific task does not improve its prunability or change the order of pruning by a meaningful amount .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "To our knowledge , prior work had not shown whether BERT could be compressed in a taskgeneric way , keeping the benefits of pre - training while avoiding costly experimentation associated with compressing and re - training BERT multiple times .", "entities": [[10, 11, "MethodName", "BERT"], [37, 38, "MethodName", "BERT"]]}
{"text": "Nor had it shown whether BERT could be over - pruned for a memory / accuracy trade - off for deployment to low - resource devices .", "entities": [[5, 6, "MethodName", "BERT"], [15, 16, "MetricName", "accuracy"]]}
{"text": "In this work , we conclude that BERT can be pruned prior to distribution without affecting it 's universality , and that BERT may be over - pruned during pre - training for a reasonable accuracy trade - off for certain tasks .", "entities": [[7, 8, "MethodName", "BERT"], [22, 23, "MethodName", "BERT"], [35, 36, "MetricName", "accuracy"]]}
{"text": "2 Pruning : Compression , Regularization , Architecture Search Neural network pruning involves examining a trained network and removing parts deemed to be unnecessary by some heuristic saliency criterion .", "entities": [[10, 12, "TaskName", "network pruning"]]}
{"text": "Compression Pruning a neural network decreases the number of parameters required to specify the model , which decreases the disk space required to store it .", "entities": [[7, 10, "HyperparameterName", "number of parameters"]]}
{"text": "The main difference between L0 or L1 regularization and weight pruning is that the former induce sparsity via a penalty on the loss function , which is learned during gradient descent via stochastic relaxation .", "entities": [[6, 8, "MethodName", "L1 regularization"], [22, 23, "MetricName", "loss"]]}
{"text": "Sparse Architecture Search Finally , we can view neural network pruning as a type of sparse architecture search .", "entities": [[9, 11, "TaskName", "network pruning"]]}
{"text": "Under this lens , stochastic gradient descent ( SGD ) induces network sparsity , and pruning simply makes that sparsity explicit .", "entities": [[4, 7, "MethodName", "stochastic gradient descent"], [8, 9, "MethodName", "SGD"]]}
{"text": "BERT is a large Transformer encoder ; for background , we refer readers to Vaswani et", "entities": [[0, 1, "MethodName", "BERT"], [4, 5, "MethodName", "Transformer"]]}
{"text": "BERT - Base consists of 12 encoder layers , each of which contains 6 prunable matrices : 4 for the multiheaded self - attention and 2 for the layer 's output feed - forward network .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "Recall that self - attention first projects layer inputs into key , query , and value embeddings via linear projections .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "6 We prune word embeddings in the same way we prune feed - foward networks and self - attention parameters .", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "In BERT - Base specifically , 5 The weights in almost every matrix in BERT - Base are approximately normally distributed with mean 0 and variance between 0.03 and 0.05 ( Table A ) .", "entities": [[1, 2, "MethodName", "BERT"], [14, 15, "MethodName", "BERT"], [23, 24, "DatasetName", "0"]]}
{"text": "7 Interestingly , pruning word embeddings is slightly more interpretable that pruning other matrices .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "for a heatmap of embedding magnitudes , which shows that shorter subwords tend to be pruned more than longer subwords and that certain dimensions are almost never pruned in any subword .", "entities": [[2, 3, "MethodName", "heatmap"]]}
{"text": "Our experimental code for pruning BERT , based on the public BERT repository , is available here .", "entities": [[5, 6, "MethodName", "BERT"], [11, 12, "MethodName", "BERT"]]}
{"text": "We perform weight magnitude pruning on a pretrained BERT - Base model .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "9 We select sparsities from 0 % to 90 % in increments of 10 % and gradually prune BERT to this sparsity over the first 10k steps of training .", "entities": [[5, 6, "DatasetName", "0"], [18, 19, "MethodName", "BERT"]]}
{"text": "We continue pre - training on English Wikipedia and BookCorpus for another 90k steps to regain any lost accuracy .", "entities": [[9, 10, "DatasetName", "BookCorpus"], [18, 19, "MetricName", "accuracy"]]}
{"text": "We then fine - tune these pruned models on tasks from the General Language Understanding Evaluation ( GLUE ) benchmark , which is a standard set of 9 tasks that include sentiment analysis , natural language inference , etc .", "entities": [[12, 13, "DatasetName", "General"], [17, 18, "DatasetName", "GLUE"], [31, 33, "TaskName", "sentiment analysis"], [34, 37, "TaskName", "natural language inference"]]}
{"text": "We avoid WNLI , which is known to be problematic .", "entities": [[2, 3, "DatasetName", "WNLI"]]}
{"text": "11 We also avoid tasks with less than 5k training examples because the results tend to be noisy ( RTE , MRPC , STS - B ) .", "entities": [[19, 20, "DatasetName", "RTE"], [21, 22, "DatasetName", "MRPC"], [23, 26, "DatasetName", "STS - B"]]}
{"text": "We fine - tune a separate model on each of the remaining 5 GLUE tasks for 3 epochs and try 4 learning rates : [ 2 , 3 , 4 , 5 ] \u00d7 10 \u22125 .", "entities": [[13, 14, "DatasetName", "GLUE"]]}
{"text": "Individual task results are in Table 1 . BERT can be used as a static feature - extractor or as a pre - trained model which is fine - tuned endto - end .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "In all experiments , we fine - tune weights in all layers of BERT on downstream tasks .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "Pruning involves two steps : it deletes the information stored in a weight by setting it to 0 and then regularizes the model by preventing that weight from changing during further training .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "To disentangle these two effects ( model complexity restriction and information deletion ) , we repeat the experiments from Section 3.2 with an identical pre - training setup , but instead of pruning we simply set the weights to 0 and allow them to vary during downstream training .", "entities": [[39, 40, "DatasetName", "0"]]}
{"text": "We also fine - tune on downstream tasks until training loss becomes comparable to models with no pruning .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "We might expect that BERT would be more compressible after downstream fine - tuning .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "To test this , we fine - tuned pre - trained BERT - Base on downstream data for 3 epochs .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "This makes sense from the perspective of pruning as sparse architecture search : when we initialize BERT - Base , we initialize many possible subnetworks .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "SGD selects the best one for pre - training and pushes the rest of the weights to 0 .", "entities": [[0, 1, "MethodName", "SGD"], [17, 18, "DatasetName", "0"]]}
{"text": "On one hand , pruning deletes pre - training information by setting weights to 0 , preventing the transfer of the useful inductive biases learned during pre - training .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "This leads us to believe that the amount a model can be pruned 12 We know , however , that increasing the size of BERT to BERT - Large improves performance .", "entities": [[24, 25, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"]]}
{"text": "This may be caused by dropout , or it may be a general property of our training regime ( SGD ) .", "entities": [[19, 20, "MethodName", "SGD"]]}
{"text": "We might consider finding a lottery ticket for BERT , which we would expect to fit the GLUE training data just as well as pre - trained BERT .", "entities": [[8, 9, "MethodName", "BERT"], [17, 18, "DatasetName", "GLUE"], [27, 28, "MethodName", "BERT"]]}
{"text": "Notice that information deletion fits the training data better than un - pruned models at all sparsity levels but does not fully recover evaluation accuracy .", "entities": [[24, 25, "MetricName", "accuracy"]]}
{"text": "Also , models pruned after downstream fine - tuning have the same or worse development accuracy , despite achieving lower training losses .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "Note : none of the pruned models are overfitting because un - pruned models have the lowest training loss and the highest development accuracy .", "entities": [[18, 19, "MetricName", "loss"], [23, 24, "MetricName", "accuracy"]]}
{"text": "We believe the slope of each line tells us how much a bit of BERT is worth to each task .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "( CoLA at 90 % is excluded from the line of best fit . )", "entities": [[1, 2, "DatasetName", "CoLA"]]}
{"text": "Features are extracted from activations of all 12 layers of BERT and compared layer - wise to a model that has not been pruned .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "However , these models do not recover all evaluation accuracy , despite matching un - pruned model 's training loss .", "entities": [[9, 10, "MetricName", "accuracy"], [19, 20, "MetricName", "loss"]]}
{"text": "Table 1 shows that on the MNLI and QQP tasks , which have the largest amount of training data , information deletion performs much better than pruning .", "entities": [[6, 7, "DatasetName", "MNLI"], [8, 9, "DatasetName", "QQP"]]}
{"text": "In contrast , models do not recover as well on SST - 2 and CoLA , which have less data .", "entities": [[10, 11, "DatasetName", "SST"], [14, 15, "DatasetName", "CoLA"]]}
{"text": "We 've seen that over - pruning BERT deletes information useful for downstream tasks .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "We might consider the pre - training loss as a proxy for how much pre - training information we 've deleted in total .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "For every bit of information we delete from BERT , it appears only a fraction is useful for CoLA , and an even smaller fraction useful for QQP .", "entities": [[8, 9, "MethodName", "BERT"], [18, 19, "DatasetName", "CoLA"], [27, 28, "DatasetName", "QQP"]]}
{"text": "This relationship should be taken into account when considering the memory / accuracy trade - off of overpruning .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "Pruning an extra 30 % of BERT 's weights Figure 3 : ( Top ) The measured difference in pruning masks between models pruned during pre - training and models pruned during downstream fine - tuning .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "It 's unclear , however , whether this is because the pre - training task is less relevant to QQP or whether QQP simply has a bigger dataset with more information content .", "entities": [[19, 20, "DatasetName", "QQP"], [22, 23, "DatasetName", "QQP"]]}
{"text": "Table 2 shows that the magnitude sorting order of weights is mostly preserved ; weights move on average 0 - 4 % away from their starting positions in the sort order .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "Compressing BERT for Specific Tasks Section 5 showed that downstream fine - tuning does not increase prunability .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "Tang et al ( 2019 ) used BERT as a knowledge distillation teacher to compress relevant information into smaller Bi - LSTMs , while Kuncoro et al ( 2019 ) took a similar distillation approach .", "entities": [[7, 8, "MethodName", "BERT"], [10, 12, "MethodName", "knowledge distillation"]]}
{"text": "While fine - tuning does not increase prunability , task - specific knowledge might be extracted from BERT with other methods .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "They show redundancy in BERT after fine - tuning on a single downstream task ; in contrast , our work emphasizes the interplay between compression and transfer learning to many tasks , pruning both before and after finetuning .", "entities": [[4, 5, "MethodName", "BERT"], [26, 28, "TaskName", "transfer learning"]]}
{"text": "Also , magnitude weight pruning allows us to additionally prune the feed - foward networks and sub - word embeddings in BERT ( not just selfattention ) , which account for \u223c72 % of BERT 's total memory usage .", "entities": [[18, 20, "TaskName", "word embeddings"], [21, 22, "MethodName", "BERT"], [34, 35, "MethodName", "BERT"]]}
{"text": "We suspect that attention head pruning and weight pruning remove different redundancies from BERT .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "We 've shown that encoding BERT 's inductive bias requires many more weights than are required to fit downstream data .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Future work on compressing pre - trained models should focus on maintaining that inductive bias and quantifying its relevance to various tasks during accuracy / memory trade - offs .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "For magnitude weight pruning , we 've shown that 30 - 40 % of the weights do not encode any useful inductive bias and can be discarded without affecting BERT 's universality .", "entities": [[30, 31, "MethodName", "BERT"]]}
{"text": "It 's reasonable to believe that these conclusions will generalize to other pre - trained language models such as Kermit ( Chan et al , 2019 ) , XLNet ( Yang et al , 2019 ) ,", "entities": [[28, 29, "MethodName", "XLNet"]]}
{"text": "GPT - 2 ( Radford et al , 2019 ) , RoBERTa ( Liu et al , 2019a ) or ELMO ( Peters et al , 2018 ) .", "entities": [[0, 1, "MethodName", "GPT"], [11, 12, "MethodName", "RoBERTa"], [20, 21, "MethodName", "ELMO"]]}
{"text": "All of these learn some variant of language modeling , and most use Transformer architectures .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "Figure 5 : The sum of weights pruned at each sparsity level for one shot pruning of BERT .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "Given the motivation for our saliency criterion , it seems strange that such a large magnitude of weights can be pruned without decreasing accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "LR MNLI QQP QNL SST - 2 CoLA 2e - 5 1.91 \u00b1 1.81 1.82 \u00b1 1.72 1.27 \u00b1 1.22 1.06 \u00b1 1.03 0.79 \u00b1 0.77 3e - 5 2.68 \u00b1 2.51 2.56 \u00b1 2.40 1.79 \u00b1 1.69 1.54 \u00b1 1.47 1.06 \u00b1 1.03 4e - 5 3.41 \u00b1 3.18 3.30 \u00b1 3.10 2.31 \u00b1 2.19 1.99 \u00b1 1.89 1.11 \u00b1 1.09 5e - 5 4.12 \u00b1 3.83 4.02 \u00b1 3.74 2.77 \u00b1 2.62 2.38 \u00b1 2.29 1.47 \u00b1 1.43 Table 2 : We compute the magnitude sorting order of each weight before and after downstream fine - tuning .", "entities": [[1, 2, "DatasetName", "MNLI"], [2, 3, "DatasetName", "QQP"], [4, 5, "DatasetName", "SST"], [7, 8, "DatasetName", "CoLA"]]}
{"text": "Sorting order changes mostly locally across tasks : a weight moves , on average , 0 - 4 % away from its starting position .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "This plot is nearly identical for every model and learning rate , so we only show it once .", "entities": [[9, 11, "HyperparameterName", "learning rate"]]}
{"text": "Figure 7 : A heatmap of the weight magnitudes of the 12 horizontally stacked self - attention key projection matrices for layer 1 .", "entities": [[4, 5, "MethodName", "heatmap"]]}
{"text": "( Right ) A heatmap of the weight magnitudes of BERT 's subword embeddings .", "entities": [[4, 5, "MethodName", "heatmap"], [10, 11, "MethodName", "BERT"]]}
{"text": "Interestingly , pruning BERT embeddings are more interpretable ; we can see shorter subwords ( top rows ) have smaller magnitude values and thus will be pruned earlier than other subword embeddings .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Weight Mean Weight STD embeddings word embeddings - 0.0282 0.042 layer 0 attention output FC - 0.0000 0.029 layer 0 self attn key 0.0000 0.043 layer 0 self attn query 0.0000 0.043 layer 0 self attn value - 0.0000 0 .", "entities": [[5, 7, "TaskName", "word embeddings"], [11, 12, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [39, 40, "DatasetName", "0"]]}
{"text": "Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask", "entities": [[0, 2, "TaskName", "Question Generation"], [3, 5, "TaskName", "Reading Comprehension"]]}
{"text": "However , many existing Question Generation ( QG ) systems focus on generating literal questions from the text , and have no way to control the type of the generated question .", "entities": [[4, 6, "TaskName", "Question Generation"]]}
{"text": "In this paper , we study QG for reading comprehension where inferential questions are critical and extractive techniques can not be used .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}
{"text": "We propose a new reading comprehension dataset that contains questions annotated with story - based reading comprehension skills ( SBRCS ) , allowing for a more complete reader assessment .", "entities": [[4, 6, "TaskName", "reading comprehension"], [15, 17, "TaskName", "reading comprehension"]]}
{"text": "Thus , to improve the educational process , and lighten the load on teachers , we need tools to automate Question Generation ( QG ) : the task of writing questions for a given passage .", "entities": [[20, 22, "TaskName", "Question Generation"]]}
{"text": "Previous works focused on this aspect of the questions in reading comprehension and discarded the comprehension skills ( e.g. close reading , predicting , figurative language , etc . )", "entities": [[10, 12, "TaskName", "reading comprehension"]]}
{"text": "We take inspiration from continual learning ( Parisi et al , 2019 ) , which orders a set of learning tasks to improve model performance .", "entities": [[4, 6, "TaskName", "continual learning"]]}
{"text": "This paper focuses on the generation of questions for story - based reading comprehension skills ( SBRCS ) , which are varied and cover many aspects of reading comprehension .", "entities": [[12, 14, "TaskName", "reading comprehension"], [27, 29, "TaskName", "reading comprehension"]]}
{"text": "Although our aim in creating this dataset is to enrich educational applications , this dataset can be considered as a source for general QG and question answering ( QA ) systems in NLP .", "entities": [[25, 27, "TaskName", "question answering"]]}
{"text": "The dataset contains advanced reading comprehension skills extracted from stories .", "entities": [[4, 6, "TaskName", "reading comprehension"]]}
{"text": "We demonstrate the efficiency of the proposed method after extensive experiments , and we investigate its performance in a few - shot learning setting .", "entities": [[19, 23, "TaskName", "few - shot learning"]]}
{"text": "Many different QG models have been proposed , starting for simple vanilla Sequence to Sequence Neural Networks models ( seq2seq ) Yuan et", "entities": [[12, 15, "MethodName", "Sequence to Sequence"], [19, 20, "MethodName", "seq2seq"]]}
{"text": "Despite the recent efforts for building reading comprehension QA datasets , to the best of our knowledge , none of the available datasets explored SBRCS .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}
{"text": "Additionally , we use a extensive set of reading comprehension skills that deeply evaluates the abilities of the readers ( e.g. imagination skill by Visualizing ) .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}
{"text": "This content variety leads annotators towards asking non - localized questions that test for more advanced reading comprehension skills .", "entities": [[16, 18, "TaskName", "reading comprehension"]]}
{"text": "Given the fact that including more data in a reading comprehension system is important for gen - eralization ( Chung et al , 2018 ; Talmor and Berant , 2019 ) , and given that our created dataset has the SBRCS which are missed in previous datasets , we propose a two - steps method to generate skillrelated questions from a given story : HTA followed by WTA .", "entities": [[9, 11, "TaskName", "reading comprehension"]]}
{"text": "We use two well - known datasets , SQuAD ( Rajpurkar et al , 2016 ) and Cos - mosQA ( Huang et al , 2019 ) .", "entities": [[8, 9, "DatasetName", "SQuAD"]]}
{"text": "For the generation model , we use the pre - trained Text - to - Text Transfer Transformer T5 ( Raffel et al , 2020 ) , which closely follows the encoder - decoder architecture of the transformer model ( Vaswani et al , 2017 ) .", "entities": [[17, 18, "MethodName", "Transformer"], [18, 19, "MethodName", "T5"]]}
{"text": "T5 is a SOTA model on multiple tasks , including QA .", "entities": [[0, 1, "MethodName", "T5"]]}
{"text": "Previous works showed that incorporating more data when training a reading comprehension model improves performance and generalizability ( Chung et al , 2018 ; Talmor and Berant , 2019 ) .", "entities": [[10, 12, "TaskName", "reading comprehension"]]}
{"text": "Thus , we train a T5 model on SQuAD and CosmosQA datasets to teach the model how to ask questions .", "entities": [[5, 6, "MethodName", "T5"], [8, 9, "DatasetName", "SQuAD"], [10, 11, "DatasetName", "CosmosQA"]]}
{"text": "Previous neural question generation models take the passage as input , along with the answer .", "entities": [[2, 4, "TaskName", "question generation"]]}
{"text": "However , this distribution can only be learned using an extractive dataset ( e.g. SQuAD ) ; the model can not learn to generate inferential questions .", "entities": [[14, 15, "DatasetName", "SQuAD"]]}
{"text": "QG often uses standard evaluation metrics from text summarization and machine translation ( BLEU ( Papineni et", "entities": [[7, 9, "TaskName", "text summarization"], [10, 12, "TaskName", "machine translation"], [13, 14, "MetricName", "BLEU"]]}
{"text": "al , 2002 ) , ROUGE ( Lin , 2004 ) , METEOR ( Banerjee and Lavie , 2005 ) , etc . ) .", "entities": [[12, 13, "DatasetName", "METEOR"]]}
{"text": "BLEURT is a BERT - based model that uses multi - task learning to evaluate a generated text by giving it a value mostly between 0.0 and 1.0 .", "entities": [[3, 4, "MethodName", "BERT"], [9, 13, "TaskName", "multi - task learning"]]}
{"text": "We tested the T5 - large model , but we did not notice any improvements considering BLEURT metric .", "entities": [[3, 4, "MethodName", "T5"]]}
{"text": "We use a single NVIDIA TITAN RTX with 24 G RAM .", "entities": [[5, 6, "DatasetName", "TITAN"], [10, 11, "MethodName", "RAM"]]}
{"text": "For HTA , we validate on a combined version of the validation sets from both datasets ( SQuAD and CosmosQA ) .", "entities": [[17, 18, "DatasetName", "SQuAD"], [19, 20, "DatasetName", "CosmosQA"]]}
{"text": "For all of the following baselines , we use SQuAD , CosmosQA , and the collected dataset for training and we test on the test part of the collected dataset : Vanilla Seq2seq ( Sutskever et al , 2014 ) : a basic encoder - decoder sequence learning system for machine translation .", "entities": [[9, 10, "DatasetName", "SQuAD"], [11, 12, "DatasetName", "CosmosQA"], [32, 33, "MethodName", "Seq2seq"], [50, 52, "TaskName", "machine translation"]]}
{"text": "NQG - Seq : another Seq2seq that implements an attention layer on top of a bidirectional - LSTM encoder .", "entities": [[5, 6, "MethodName", "Seq2seq"], [17, 18, "MethodName", "LSTM"]]}
{"text": "NQG - Max ( Zhao et al , 2018 ) 8 : a QG system with a maxout pointer mechanism and gated self - attention LSTM - based encoder to address the challenges of processing long text input .", "entities": [[17, 18, "MethodName", "maxout"], [25, 26, "MethodName", "LSTM"]]}
{"text": "CGC - QG ( Liu et al , 2019a ) : a Clue Guided Copy network for Question Generation , which is a sequence - to - sequence generative model with a copying mechanism that takes a passage and an answer ( as a span in the text ) and generate the question .", "entities": [[17, 19, "TaskName", "Question Generation"]]}
{"text": "The text representation in the encoder ( GRU network ) is represented using a variety of features such as GloVe vectors , POS information , answer position , clue word , etc . AnswerQuest ( Roemmele et al , 2021 ) : a pipeline model that uses as a first step a previous model to retrieve the relevant sentence that has the answer from a document .", "entities": [[7, 8, "MethodName", "GRU"], [19, 20, "MethodName", "GloVe"]]}
{"text": "One - Step : a baseline that uses T5 model trained with all data in one step instead of having separate HTA and WTA steps .", "entities": [[8, 9, "MethodName", "T5"]]}
{"text": "T5 - WTA : the WTA model trained using T5 model as a seed model .", "entities": [[0, 1, "MethodName", "T5"], [9, 10, "MethodName", "T5"]]}
{"text": "For all of the previous baselines that require the answer to be a sub - span in the passage , we use the semantic text similarity method that was proposed in ( Ghanem et al , 2019 ) to retrieve the most similar span in the passage .", "entities": [[24, 26, "TaskName", "text similarity"]]}
{"text": "In this work , we replace the ngrams features of a text with embeddings extracted from RoBERTa model ( Liu et al , 2019b ) .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}
{"text": "We can see that out of the baselines , T5 - WTA performs best in terms of BLEURT score ( 32.96 % ) , followed by NQG - Max with a value of 31.78 % .", "entities": [[9, 10, "MethodName", "T5"]]}
{"text": "Regarding the generated questions type , in Table 3 we show the performance of the T5 - based models per question type ( inferential and literal ) .", "entities": [[15, 16, "MethodName", "T5"]]}
{"text": "We see a similar scenario when comparing One - Step and T5 - WTA models , yet , the gap is smaller .", "entities": [[11, 12, "MethodName", "T5"]]}
{"text": "The T5 model was able to learn how to quote the proper segment of the passage when generating questions .", "entities": [[1, 2, "MethodName", "T5"]]}
{"text": "The One - Step model performs similarly to the baselines , although it has been trained using the T5 model and on all three datasets .", "entities": [[18, 19, "MethodName", "T5"]]}
{"text": "We use a \" master \" qualification criteria to restrict the participation of workers in our evaluation study to those who have a high historical HIT accuracy , and workers are required to be located in an English speaking country .", "entities": [[26, 27, "MetricName", "accuracy"]]}
{"text": "We can clearly see a large gap in accuracy between both models , and this becomes clear with the skills that have a low number of instances in the dataset ( e.g. Figurative Language , Predicting , etc . ) .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "The result at 10 % ( 33.21 % ) exceeds the results of most of the baselines and is higher than T5 - WTA and NQG - MAX models when trained on all the datasets ( see Table 2 ) .", "entities": [[21, 22, "MethodName", "T5"]]}
{"text": "In this paper , we presented a new reading comprehension dataset to assess reading skills using stories .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}
{"text": "Additionally , we manually examined the stories and the created questions to ensure there are no privacy or ethical concerns , e.g. , toxic language , hate speech , or any bias against underrepresented groups .", "entities": [[26, 28, "DatasetName", "hate speech"]]}
{"text": "This skill requires advanced reading comprehension ability from the reader since its answers can not be extracted directly from the story text , where inferential skills are needed .", "entities": [[4, 6, "TaskName", "reading comprehension"]]}
{"text": "The Readers and Writers Workshop curricula were highly instrumental to us in breaking reading comprehension into sub - skills .", "entities": [[13, 15, "TaskName", "reading comprehension"]]}
{"text": "In addition to the collected dataset , we use two well - known datasets , SQuAD and CosmosQA .", "entities": [[15, 16, "DatasetName", "SQuAD"], [17, 18, "DatasetName", "CosmosQA"]]}
{"text": "SQuAD", "entities": [[0, 1, "DatasetName", "SQuAD"]]}
{"text": "A reading comprehension dataset , consists of questions created by crowdworkers on a set of Wikipedia articles that cover a large set of topics ( from musical celebrities to abstract concepts ) , where the answer to every question is a span from the corresponding reading passage ( Rajpurkar et al , 2016 ) .", "entities": [[1, 3, "TaskName", "reading comprehension"]]}
{"text": "In this work , we use SQuAD 2.0 version with discarding the questions that have no answers .", "entities": [[6, 7, "DatasetName", "SQuAD"]]}
{"text": "CosmosQA", "entities": [[0, 1, "DatasetName", "CosmosQA"]]}
{"text": "It is another reading comprehension dataset consisting of 35.6 K paragraph / question pairs that require commonsense - based reading comprehension .", "entities": [[3, 5, "TaskName", "reading comprehension"], [19, 21, "TaskName", "reading comprehension"]]}
{"text": "To quantify the impact of including the skill name token , we run T5 - WTA without including the skill name token ( T5 - WTA - unskilled ) .", "entities": [[13, 14, "MethodName", "T5"], [23, 24, "MethodName", "T5"]]}
{"text": "We compare the T5 - WTAunskilled to the One - Step model ; the only difference between these models is that One - Step model includes SQuAD and CosmosQA datasets in the training data .", "entities": [[3, 4, "MethodName", "T5"], [26, 27, "DatasetName", "SQuAD"], [28, 29, "DatasetName", "CosmosQA"]]}
{"text": "T5 - WTA - unskilled BLEURT performance is lower than the BLEURT scores of the other two models .", "entities": [[0, 1, "MethodName", "T5"]]}
{"text": "Error Analysis .", "entities": [[0, 1, "MetricName", "Error"]]}
{"text": "In the following , we elaborate more on the reading comprehension skills :", "entities": [[9, 11, "TaskName", "reading comprehension"]]}
{"text": "We do not experiment with One - Step model as we need to sample SQuAD and Cos - mosQA datasets when we sample the collected data ; it is hard to set up a fair comparison here as , for instance , sampling 10 % of SQuAD dataset is larger than the whole collected dataset .", "entities": [[14, 15, "DatasetName", "SQuAD"], [46, 47, "DatasetName", "SQuAD"]]}
{"text": "We present ADVISER 1 - an open - source , multi - domain dialog system toolkit that enables the development of multi - modal ( incorporating speech , text and vision ) , sociallyengaged ( e.g. emotion recognition , engagement level prediction and backchanneling ) conversational agents .", "entities": [[36, 38, "TaskName", "emotion recognition"]]}
{"text": "Dialog systems or chatbots , both text - based and multi - modal , have received much attention in recent years , with an increasing number of dialog systems in both industrial contexts such as Amazon Alexa , Apple Siri , Microsoft Cortana , Google Duplex , XiaoIce ( Zhou", "entities": [[44, 45, "DatasetName", "Google"]]}
{"text": "The purpose we envision for dialog systems developed using our toolkit is not the same as the objective of a social chatbot such as XiaoIce ( Zhou et al , 2018 ) .", "entities": [[21, 22, "TaskName", "chatbot"]]}
{"text": "The main objective of this work is to develop a multi - domain dialog system toolkit that allows for multi - modal information processing and that provides different modules for extracting social signals such as emotional states and for integrating them into the decision making process .", "entities": [[43, 45, "TaskName", "decision making"]]}
{"text": "By assigning computationally heavy tasks such as speech recognition and speech synthesis to a more powerful computing node , it is possible to reduce differences in latency when processing different modalities , therefore achieving more natural interactions .", "entities": [[7, 9, "TaskName", "speech recognition"], [10, 12, "TaskName", "speech synthesis"]]}
{"text": "We present the three modules of ADVISER for processing social signals : ( a ) emotion recognition , ( b ) engagement level prediction , and ( c ) backchanneling .", "entities": [[15, 17, "TaskName", "emotion recognition"]]}
{"text": "Figure 1 illustrates an example of our system tracking emotion states and engagement levels .", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "Therefore , the emotion recognition module can subscribe to the particular input streams of interest ( see section 4 for details ) and apply emotion prediction either in a time - continuous fashion or discretely per turn .", "entities": [[3, 5, "TaskName", "emotion recognition"], [24, 25, "DatasetName", "emotion"]]}
{"text": "In our example implementation in the toolkit , we integrate speech emotion recognition , i.e. using the acoustic signal as features .", "entities": [[10, 13, "TaskName", "speech emotion recognition"]]}
{"text": "The models are trained on the IEMOCAP dataset ( Busso et al , 2008 ) .", "entities": [[6, 7, "DatasetName", "IEMOCAP"]]}
{"text": "The output of the emotion recognition module consists of three predictions per user turn , which can then be used by the user state tracker ( see section 3.4 ) .", "entities": [[4, 6, "TaskName", "emotion recognition"]]}
{"text": "Automatic Speech Recognition ( ASR )", "entities": [[0, 3, "TaskName", "Automatic Speech Recognition"]]}
{"text": "The speech recognition module receives a speech signal as input , which can come from an internal or external microphone , and outputs decoded text .", "entities": [[1, 3, "TaskName", "speech recognition"]]}
{"text": "We provide an end - to - end ASR model for English based on the Transformer neural network architecture .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "We use the end - to - end speech processing toolkit ESPnet ( Watanabe et al , 2018 ) and the IMS - speech English multi - dataset recipe ( Denisov and Vu , 2019 ) , updated to match the LibriSpeech Transformer - based system in ESPnet ( Karita et al , 2019 ) and to include more training data .", "entities": [[11, 12, "MethodName", "ESPnet"], [41, 42, "DatasetName", "LibriSpeech"], [42, 43, "MethodName", "Transformer"], [47, 48, "MethodName", "ESPnet"]]}
{"text": "Training data comprises the LibriSpeech , Switchboard , TED - LIUM 3 , AMI , WSJ , Common Voice 3 , SWC , VoxForge and M - AILABS datasets with a total amount of 3249 hours .", "entities": [[4, 5, "DatasetName", "LibriSpeech"], [8, 12, "DatasetName", "TED - LIUM 3"], [17, 19, "DatasetName", "Common Voice"], [23, 24, "DatasetName", "VoxForge"]]}
{"text": "Speech Synthesis For ADVISER 's voice output , we use the ESPnet - TTS toolkit , which is an extension of the ESPnet toolkit mentioned above .", "entities": [[0, 2, "TaskName", "Speech Synthesis"], [11, 12, "MethodName", "ESPnet"], [22, 23, "MethodName", "ESPnet"]]}
{"text": "We use FastSpeech as the synthesis model speeding up mel - spectrogram generation by a factor of 270 and voice generation by a factor of 38 compared to autoregressive Transformer TTS ( Ren et al , 2019 ) .", "entities": [[29, 30, "MethodName", "Transformer"]]}
{"text": "We use a Parallel Wave - GAN ( Yamamoto et al , 2020 ) to generate waveforms that is computationally efficient and achieves a high mean opinion score of 4.16 .", "entities": [[6, 7, "MethodName", "GAN"]]}
{"text": "The FastSpeech and WaveGAN models were trained with 24 hours of the LJSpeech dataset from a single speaker ( Ito , 2017 ) and are capable of generating voice output in real - time when using a GPU .", "entities": [[3, 4, "MethodName", "WaveGAN"], [12, 13, "DatasetName", "LJSpeech"]]}
{"text": "The natural language understanding ( NLU ) unit parses the textual user input ( De Mori et al , 2008 ) - or the output from the speech recognition systemand extracts the user action type , generally referred to as intent in goal - oriented dialog systems ( e.g. Inform and Request ) , as well as the corresponding slots and values .", "entities": [[1, 4, "TaskName", "natural language understanding"], [27, 29, "TaskName", "speech recognition"], [42, 46, "TaskName", "goal - oriented dialog"]]}
{"text": "In the current implementation , the user state consists of the user 's engagement level , valence , arousal , and emotion category ( details in section 3.1 ) .", "entities": [[21, 22, "DatasetName", "emotion"]]}
{"text": "For off - policy batch - training , we make use of prioritized experience replay ( Schaul et al , 2015 ) .", "entities": [[12, 15, "MethodName", "prioritized experience replay"]]}
{"text": "As this policy is domain - agnostic , predicting the next system emotion output rather than the next system action , it can be used alongside any of the previously mentioned policies .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "At runtime , the utterance is then generated from the template associated with the current system emotion and system action .", "entities": [[16, 17, "DatasetName", "emotion"]]}
{"text": "To make the dialog multi - modal , speech and vision modules are introduced next , along with modules to extract engagement and emotion .", "entities": [[23, 24, "DatasetName", "emotion"]]}
{"text": "It further offers audio and visual processing , such as speech recognition and face tracking , as well as output , such as synthesis and avatar rendering .", "entities": [[10, 12, "TaskName", "speech recognition"]]}
{"text": "And the MuMMER ( multimodal Mall Entertainment Robot ) project ( Foster et al , 2016 ) is based on the SoftBank Robotics Pepper platform , and thereby comprises processing of audio - , visual - and social signals , with the aim to develop a socially engaging robot that can be deployed in public spaces .", "entities": [[5, 6, "DatasetName", "Mall"]]}
{"text": "Currently , all the Latin treebanks except the llct are available also in the Universal Dependencies collection ( UD ) ( Nivre et al , 2016 ) .", "entities": [[14, 16, "DatasetName", "Universal Dependencies"], [18, 19, "DatasetName", "UD"]]}
{"text": "9 In the treebank area , the UD collection includes more than 100 treebanks sharing the same annotation guidelines and provides different tools for querying the treebanks on - line .", "entities": [[7, 8, "DatasetName", "UD"]]}
{"text": "The Lemma is the key node type in LiLa .", "entities": [[1, 2, "DatasetName", "Lemma"]]}
{"text": "A Lemma is an ( inflected ) Form conventionally chosen as the citation form of a lexical item .", "entities": [[1, 2, "DatasetName", "Lemma"]]}
{"text": "Interoperability can be achieved by linking the entries in lexical resources and the corpus tokens pointing to the same lemma .", "entities": [[19, 20, "DatasetName", "lemma"]]}
{"text": "17 Lemlat relies on a lexical basis resulting from the collation of three Latin dictionaries ( Georges andGeorges , 1913 1918 ; Glare , 1982 ; Gradenwitz , 1904 ) for a total of 40 , 014 lexical entries and 43 , 432 lemmas , as more than one lemma can be included in one lexical entry .", "entities": [[49, 50, "DatasetName", "lemma"]]}
{"text": "The current prototype of the LiLa RDF triplestore database connects the following resources for Latin : ( a ) the collection of lemmas provided by Lemlat , ( b ) the wfl lexicon , and ( c ) three treebanks ( four by version ) : ( c.1 ) pro\u0131el in its UD version ( release 2.3 ) , ( c.2 - 3 ) the \u0131t - tb in both its UD 2.3 and original version , and ( c.4 ) a selection of 3 , 900 sentences ( 105 , 380 tokens ) of the llct .", "entities": [[52, 53, "DatasetName", "UD"], [71, 72, "DatasetName", "UD"]]}
{"text": "The figure shows a three - word sentence from the Vulgata ( Matt . 6.10 ) , taken from the UD 2.3 version of the pro\u0131el corpus : veniat regnum tuum ( \" thy kingdom come \" ) .", "entities": [[20, 21, "DatasetName", "UD"]]}
{"text": "The UD 2.3 tree for this sentence is shown in Figure 3 . 19 Tokens and sentences are defined using the NIF vocabulary .", "entities": [[1, 2, "DatasetName", "UD"]]}
{"text": "For instance , in Figure 2 this is the case of the string \" Case = Nom | Gen - der = Neut | Number = Sing \" , which is linked to the pro\u0131el token with ID s15924_2 ( for the word regnum \" kingdom \" ) via the relation conll : FEAT , linking the morphological features taken from files in the CoNLL - U format of UD . 20 Other types of tagging ( such as syntactic dependencies , or sentence boundaries ) are expressed by links between the nodes for tokens or sentences .", "entities": [[69, 70, "DatasetName", "UD"]]}
{"text": "Finally , a third group of linguistic annotations , like the part of speech , directly relate tokens to concepts from an ontology of linguistic data ( OLiA ) .", "entities": [[22, 23, "MethodName", "ontology"]]}
{"text": "Tokens are connected to the appropriate Lemma nodes recorded in the LiLa Knowledge Base .", "entities": [[6, 7, "DatasetName", "Lemma"]]}
{"text": "In Figure 2 , for instance , the token s15924_2 ( regnum ) is linked to lemma 34146 , which has written representation regnum .", "entities": [[16, 17, "DatasetName", "lemma"]]}
{"text": "Via this connection , it becomes possible to access all the other information that is also pointing to that lemma .", "entities": [[19, 20, "DatasetName", "lemma"]]}
{"text": "In the figure , the lemma 34146 is connected to a node for a lexical base ( 1133 ) , the same to which also lemmas rex \" king \" ( 34799 ) and regno \" to rule , to be king \" ( 34145 ) are attached .", "entities": [[5, 6, "DatasetName", "lemma"]]}
{"text": "The lemma regnum is also formed with the suffix \" - n \" ( represented by the node affix:111 in Figure 2 ) , the same found in e.g. fanum \" shrine \" ( not shown here for reasons of space ) .", "entities": [[1, 2, "DatasetName", "lemma"]]}
{"text": "Consider , for instance , the case of a researcher interested in the relation between the syntactic role of subject and the semantic role of agent in Latin .", "entities": [[25, 26, "DatasetName", "agent"]]}
{"text": "One possible approach to study the question would be to start by collecting and analyzing the sentences where nouns formed with a typical morpheme for agent nouns like \" - ( t ) or \" ( common to several Indo - European languages ) are attested as subject of an active verb .", "entities": [[25, 26, "DatasetName", "agent"]]}
{"text": "Though the number of linguistic resources currently interlinked in LiLa is still small , it is already possible to design a single SPARQL query to extract this information from our RDF versions of pro\u0131el , \u0131t - tb ( UD version ) and llct .", "entities": [[39, 40, "DatasetName", "UD"]]}
{"text": "The query allows us to extract 143 passages , with 80 different verbs and 58 agent nouns .", "entities": [[15, 16, "DatasetName", "agent"]]}
{"text": "Improving Adversarial Text Generation by Modeling the Distant Future", "entities": [[1, 3, "TaskName", "Adversarial Text"]]}
{"text": "Auto - regressive text generation models usually focus on local fluency , and may cause inconsistent semantic meaning in long text generation .", "entities": [[3, 5, "TaskName", "text generation"], [20, 22, "TaskName", "text generation"]]}
{"text": "Text generation is an important area of investigation within machine learning .", "entities": [[0, 2, "TaskName", "Text generation"]]}
{"text": "Example applications include image captioning ( Ren et al , 2017 ; Rennie et al , 2016 ) , text summarization ( Li et al , 2018b ; Paulus et al , 2017 ; Rush et al , 2015 ) , and adversarial text generation ( Guo et al , 2017 ; Lin et al , 2017 ; Zhu et al , 2018 ) .", "entities": [[3, 5, "TaskName", "image captioning"], [19, 21, "TaskName", "text summarization"], [42, 44, "TaskName", "adversarial text"]]}
{"text": "The sequence - to - sequence framework ( Seq2Seq ) ( Sutskever et al , 2014 ) is a popular technique for text generation .", "entities": [[8, 9, "MethodName", "Seq2Seq"], [22, 24, "TaskName", "text generation"]]}
{"text": "By contrast , sequence - level training with RL provides an effective means of solving this challenge , by treating text generation as a sequential decision - making problem .", "entities": [[20, 22, "TaskName", "text generation"]]}
{"text": "Furthermore , the recurrent models focus more on local fluency , and may cause inconsistent semantic meanings for long text generation .", "entities": [[19, 21, "TaskName", "text generation"]]}
{"text": "For RL - based text generation , most existing works rely on a model - free framework , which has been criticized for its high variance and poor sample efficiency ( Sutton and Barto , 1998 ) .", "entities": [[4, 6, "TaskName", "text generation"]]}
{"text": "Text Generation Model Text generation models learn to generate a sentence", "entities": [[0, 2, "TaskName", "Text Generation"], [3, 5, "TaskName", "Text generation"]]}
{"text": "X. Here each y t is a token from vocabulary A. Starting from the initial state s 0 , a recurrent neural network ( RNN ) produces a sequence of states ( s 1 , . . .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "= [ softmax ( g ( s t ) )", "entities": [[2, 3, "MethodName", "softmax"]]}
{"text": "In order to generate sentence Y s from a ( trained ) model , one iteratively applies the following operations : y s t+1 \u223c Multi ( 1 , softmax ( g ( s t ) ) ) , ( 2 ) s t = h ( s t\u22121 , e ( y s t ) ) , where Multi ( 1 , ) denotes one draw from a multinomial distribution .", "entities": [[29, 30, "MethodName", "softmax"]]}
{"text": "Model - Based Imitation Learning Text generation can be considered as an RL problem with a large number of discrete actions , deterministic transitions , and deterministic terminal rewards .", "entities": [[3, 5, "TaskName", "Imitation Learning"], [5, 7, "TaskName", "Text generation"]]}
{"text": "The objective is to maximize the expected reward : J ( \u03c0 ) = t=1 E P , \u03c0 \u03b3 t\u22121 r ( s t , y t ) .", "entities": [[19, 20, "HyperparameterName", "\u03b3"]]}
{"text": "In model - based imitation learning ( Baram et al , 2017 ; Cheng et al , 2019 ) , a model is built to make predictions for future state s t+ t conditioned on the current state 1 , which can be used for action selection , e.g. , next - token generation .", "entities": [[4, 6, "TaskName", "imitation learning"]]}
{"text": "The model is illustrated in Figure 1 , with an autoeocoder ( AE ) structure for sentence feature extraction and generation .", "entities": [[12, 13, "MethodName", "AE"]]}
{"text": "Overall , text generation can be formulated as an imitationlearning problem .", "entities": [[2, 4, "TaskName", "text generation"]]}
{"text": "At each timestep t , the agent , also called a generator ( which corresponds to the LSTM decoder ) , takes the current LSTM state as input , denoted as s t .", "entities": [[6, 7, "DatasetName", "agent"], [17, 18, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"]]}
{"text": "The objective of text generation is to maximize the total reward as in ( 4 ) .", "entities": [[3, 5, "TaskName", "text generation"]]}
{"text": "The guider network , implemented as an RNN with LSTM units , is adopted to model environment dynamics to assist text generation .", "entities": [[9, 10, "MethodName", "LSTM"], [20, 22, "TaskName", "text generation"]]}
{"text": "Here f t is the input to the LSTM guider , which represents the feature of the current generated sentence extracted 1 t > 1 ; the model predicts future states based on the collected trajectories .", "entities": [[8, 9, "MethodName", "LSTM"]]}
{"text": "z z 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F o 9 z P d P 5 w 7 d t 1 p +", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A a E 6 4 x Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "q x 3 q 2 P + W j F K n c O w R 9 Y n z / E B Z X 0 < / l", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "J I k E a 0 W j u 5 n f G j N t e C o", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A O o 5 K k j A T 5 v N z p / j M K X 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "I l c Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "L s 0 C x 7 M / E / r 5 P Z + D r M u V S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v /", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "g k u 6 j d 1 / + G y 1 r g t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A K b 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "J I k E a 0 W j u 5 n f G j N t e C o", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A O o 5 K k j A T 5 v N z p / j M K X 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "I l c Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "L s 0 C x 7 M / E / r 5 P Z + D r M u V S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v /", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "g k u 6 j d 1 / + G y 1 r g t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A K b 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "J I k E a 0 W j u 5 n f G j N t e C o", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A O o 5 K k j A T 5 v N z p / j M K X 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "I l c Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "L s 0 C x 7 M / E / r 5 P Z + D r M u V S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v /", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "g k u 6 j d 1 / + G y 1 r g t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A K b 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 4 9 m T T T l A U 0 F a l u R 8 Q w w S U L L L e C t Z V", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "J I k E a 0 W j u 5 n f G j N t e C o", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A O o 5 K k j A T 5 v N z p / j M K X 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "I l c Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "L s 0 C x 7 M / E / r 5 P Z + D r M u V S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I u F s W Z w D b F s 9 9 x n 2 t G r Z g 4 Q q j m 7 l Z M h 0 Q T a l 1 C F R e C v /", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "g k u 6 j d 1 / + G y 1 r g t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A K b 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[20, 21, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "v E y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "z y a a m X G p 5 Q N q Z D 3 r V U 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[20, 21, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "v E y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "z y a a m X G p 5 Q N q Z D 3 r V U 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 0 y i Q v B N M", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "b O 8 X d 0 t 7 + w e F R +", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "i 6 x a b g R 2 E k V 0", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "z O / / Y R K 8 0 Q + m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "v O W X V 0 m r W v E u K 9 X 7 W r l e y + M o w C m c w Q V 4 c A V 1 u", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "V i 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 0 y i Q v B N M", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "b O 8 X d 0 t 7 + w e F R +", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "i 6 x a b g R 2 E k V 0", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "z O / / Y R K 8 0 Q + m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "v O W X V 0 m r W v E u K 9 X 7 W r l e y + M o w C m c w Q V 4 c A V 1 u", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "V i 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s t u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 0 y i Q v B N M", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s p u 3 S z S", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "b O 8 X d 0 t 7 + w e F R +", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "N A p s Z 0 T N W K 9 6 c / E / r 5 e a 4 Y 2 f", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "q l m 0 3 Y 3 Q i l 9 C d 4 8 a C I V 3", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 N q 7 7 7 a y t b", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "i 6 x a b g R 2 E k V 0", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "z O / / Y R K 8 0 Q + m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "v O W X V 0 m r W v E u K 9 X 7 v 5 B + f C o Z V S m", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i Q R G v e v P f u K 0 5 a P X B s o /", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "8 U l 5 d a 5 g k 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "n I 1 9 8 N E d k 0 / t h + N B g F S H 7 V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "O 0 G 5 4", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "MLP w t < l a t e", "entities": [[0, 1, "DatasetName", "MLP"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "a 0 E x m", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "3 p z 0 K t B", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "v W h W M T n L M j k C 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u H F Z 0 b G F", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "K 4 8 2 K 4 z A U G Q q M 6 2 Y W 1 R S Y 0", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u", "entities": [[9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "b R e 1 B v R i 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "/ U f e / D c u 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "q + w c P J s k 0 4 z 5", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "C s V 9 F C h 5 O 9 W c x q H k r X B 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "z 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "w k b U k 0 Z 2 n Q q N g R v 8 e V l 4 p / V L + v e r V t r X B V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0 V j b k J 8 u m p Y 3 J s l R 6 J E m 1 L", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0 R 3 i G V", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "a 0 E x m", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "3 p z 0 K t B", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P", "entities": [[2, 3, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E /", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "z o c w w m c g Q 8 X c A 2 3 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P", "entities": [[2, 3, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E /", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "z o c w w m c g Q 8 X c A 2 3 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "L A y B W 3 o S N t 8 F p M 0", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "r k U t C P S K 4 k L 0 Q K 8 p", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "w d 2 0 2 k 5 M 6 B l 4 l a k C R U 6 A / v L j w T J E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h d F r 3 c 0 U z T M Z 4 S P u G", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "K i h m 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "o U 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "+ s 0 2 1 d V G", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "o Q 0 3 0", "entities": [[2, 3, "DatasetName", "0"], [4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[20, 21, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 n f 6 5 c r b t W d g 6", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "1 d 8 T E x o Z k 0 W B 7 Y w o", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "a i m D G 0 6 J R u C t / z y K", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "n V q t 5 F t X Z / W a n f V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[38, 39, "DatasetName", "0"], [62, 63, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1 m / 1 i 9 X 3 K o 7 B 1 k l X k 4 q k K P R L 3 / 1 B j F L I 6 6", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "p M 1 3 M T 9 C d U o 2 C S T 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "d p 0 S j Y E b / n l V", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o N 4 K X j x W t B / Q h r L Z b t q l m 0 3", "entities": [[20, 21, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q E b O E + x E d K h E K R t F K D 1", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "v E y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "z y a a m X G p 5 Q N q Z D 3 r V U 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "V i 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Y 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Y 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Y 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "A s V F l s L q h m v n 6 4 8 e 2 6 0 3 D m g K", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "N o L 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "O 2 W t I 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "A a E 6 4 x Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "g P E V j M j S U o 5 g o L 5 + H n 8 E z o 4 Q w E t I c r u F c / b 2 R o 1 g V 8 c x k j P R E L X u F + J 8 3 T H V 0", "entities": [[60, 61, "DatasetName", "0"]]}
{"text": "W b D v W g 0 7 1 r 1 d q u s", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A 0 J l x j h p Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F o 9 z P 3 P z h 2 r f r T s", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "U O A 0 J l x j h p Q a u E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "D W s C i C x h S S b B m U 0 M Q l t R k", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "v W b D P W 8 0 b 1 v 1 d q u s o", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "y L 3 s 0 4 B p / V", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "G k J 3 0 G 8 q C M a m W T Q g = \" >", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F o 9 z P m v n D t W / X n Y Y", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A c 1 k j P R Y L X q F + J 8 3 S H V 0 6 W W U J 6 k m H M", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "j 0 Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "G 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "O K f c p C Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F V e 5 n T v 5 w 7 d t 1", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A a E 6 4 x Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "z z 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "O t l r S N N 3 C 0 q Q k q T B K w a / i x", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "i X v 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "c 3 v w 2 p l s P u v k g 5 P H e 7 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "l A 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "D w U I 1 j c 0 F o 9 z P d P 5 w 7 d t 1 p +", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A a E 6 4 x Q 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "q x 3 q 2 P + W j F K n c O w R 9 Y n z / E B Z X 0 < / l", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "j 0 Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "k b y 7 U Y + C Q Q L V R a b C 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "A u j h D A S 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "e I Y k w t r 0 V T c l u K t f X", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "F b 9 a T 9 W K 9 W x / L 0 Q 2 r 2 m m A P 7 A + f", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "A 0 K U k q l F L / F S 8 e F P H q H + L N / 8 Z 0 6 0 E 3 H 4 Q 8 3 v v 9 y M s L E k a V d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T 0", "entities": [[1, 2, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [71, 72, "DatasetName", "0"]]}
{"text": "j 0 Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "k b y 7 U Y + C Q Q L V R a b C 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Z 5 v f 0 1 C g", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "c I 0 Z U m r s", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "l P U k 0 4 X j 4 U p Q", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "A 0 K U k q l F L / F S 8 e F P H q H + L N / 8 Z 0 6 0 E 3 H 4 Q 8 3 v v 9 y M s L E k a V d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T 0", "entities": [[1, 2, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [71, 72, "DatasetName", "0"]]}
{"text": "j 0 Q", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "k b y 7 U Y + C Q Q L V R a b C 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Y 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a z W 7 Y n Q i l 9 D 9 4 8 a C I V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 Q c / 7 d g p r 6 x u b W 8 X t 0", "entities": [[31, 32, "DatasetName", "0"], [61, 62, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "+ f d + V i 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A A A B + n i c b V C 7 T s M w F L 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "i h y H L e 1 6 j i R 7 Y C q 0 E 9 h", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "z I T g 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "y 9 5 M / M / r Z h h f B x M u 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "I 1 d z e 6 t I h 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "Y M p m 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "D c 0 8 S", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "S W l l d W 1 / L r h Y 3 N r e 0 d c 3 e v o a J E U l a n", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "g l o y T V P 0 4 4 X C V 8 N A /", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A 9 1 t Y 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D A j 0 1 b w 3 E v / z 2 g l 0", "entities": [[3, 4, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}
{"text": "o i K E m h E q u d 8 W 0 T y S", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "The initial state of the guider network is the encoded feature of a true input sentence by the same convolutional neural network ( CNN ) , i.e. , s G 0", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "This provides an important \" guide \" to the LSTM decoder , accounting for the global properties of the generated text .", "entities": [[9, 10, "MethodName", "LSTM"]]}
{"text": "Text Generation with Planning We first explain how one uses the guider network to guide next - word generation for the generator ( the LSTM decoder in Figure 1 ) .", "entities": [[0, 2, "TaskName", "Text Generation"], [24, 25, "MethodName", "LSTM"]]}
{"text": "( s G t\u22121 , f t ) as a future feature representation , by feeding f t into the LSTM guider .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "The weight w t is applied to the output O t of the LSTM decoder by an element - wise multiplication operation .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "The result is then fed into a softmax layer to generate the next token y t .", "entities": [[7, 8, "MethodName", "softmax"]]}
{"text": "O t = g ( s t\u22121 ) , w t = \u03d5 ( G \u03c8 ( s G t\u22121 , f t ) ) , ( 5 ) y t \u223c Multi ( 1 , softmax ( O t w t ) ) , ( 6 ) s G t = h G ( s G t\u22121 , f t ) , s t = h ( s t\u22121 , e ( y t ) ) .", "entities": [[36, 37, "MethodName", "softmax"]]}
{"text": "f T ) for a training sentence , we seek to update the guider network such that it is able to predict f t+c given f t , where c > 0 is the number of steps that are looked ahead .", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "As a result , the prediction is used to construct an intermediate reward , used to update the generator ( the LSTM decoder ) , as described further below .", "entities": [[21, 22, "MethodName", "LSTM"]]}
{"text": "At the last step of text generation , i.e. , t = T , the corresponding reward measures the quality of the whole generated sentence , thus it is called a final reward .", "entities": [[5, 7, "TaskName", "text generation"]]}
{"text": "t \u03b3 i r", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "g i with \u03b3 a discount factor , as a featurematching reward .", "entities": [[3, 4, "HyperparameterName", "\u03b3"]]}
{"text": "As a result , we combine the adversarial reward r f [ 0 , 1 ] by the discriminator ( Yu et al Generate a sequence Y 1 ... T \u223c \u03c0 \u03c6 .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "( T i = t \u03b3 i r g i ) \u00d7 r f .", "entities": [[5, 6, "HyperparameterName", "\u03b3"]]}
{"text": "The generator is initialized by pre - training on sentences with an autoencoder structure , based on MLE training .", "entities": [[12, 13, "MethodName", "autoencoder"]]}
{"text": "Algorithm 1 describes the proposed model - based imitation learning framework for text generation .", "entities": [[8, 10, "TaskName", "imitation learning"], [12, 14, "TaskName", "text generation"]]}
{"text": "Model - based or Model - free Text generation seeks to generate the next word ( action ) given the current ( sub - ) sentence ( state ) .", "entities": [[7, 9, "TaskName", "Text generation"]]}
{"text": "The generator is considered as an agent that learns a policy to predict the next word given its current state .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "r / e Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U 0 B / h", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h v h p", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "n i U x 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "r / e Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U 0 B / h", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h v h p", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "n i U x 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "r / e Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U 0 B / h", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "N 2 z Y H b X 0 w 8 H h v h p", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "n i U x 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "O 6 S G S 6 G 4 j w I l b 6 e a 0 z i U v B W O", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "O O U B z E d K B E J R t F K r X E v x z N v 0 q v W 3 L o 7", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "r / e Z 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "a 0 E x m", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "3 p z 0 K t B", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "v W h W M T n L M j k C 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u H F Z 0 b G F", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "Y W 1 R S Y 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u", "entities": [[9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "v W h W M T n L M j k C 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u H F Z 0 b G F", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "i i Z 9 E 4 b m s k M y R 2 l l D 6 C G x c q P p Y 7 3 8 b 0 Z 6 G t B", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "Y W 1 R S Y 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "z y N F T b j 4 c 0 0 b z 6 h s T L T D z T K M U p 5 X 8 t E C k 7 O u n / u", "entities": [[9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "b R e 1 B v R i 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "/ U f e / D c u 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "q + w c P J s k 0 4 z 5", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "C s V 9 F C h 5 O 9 W c x q H k r X B 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U k 0 Z 2 n Q q N g R v 8 e V l 4 p / V L + v e r V t r X B V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "D d u 2 x y 0 9", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "c H A 4 7 0 Z Z u a F q R Q G X f f b W V", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "P J s k 0 4 z 5 L Z K J b I T V c C s V 9 F C h 5 K 9 W", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "U I m q T F t z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" Y f B p l r X h t V g c d Z q 6 r a 0 D R Y", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u N B l B c c W 2 r F k 0", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "F T u f B v T n 4 W 2 H g h 8 n J O Q e 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z a r D A C Q 5 G p z L R", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 0 V t", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "A o x y j l f S 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "T K T g 5 K 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "w k g 3 K x M D b r g g 1 0 / Z l R A s r 7 w K 4 X n 9 q h 7 c + 1", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" Y f B p l r X h t V g c d Z q 6 r a 0 D R Y", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "A I r s q M G 3 U n u N B l B c c W 2 r F k 0", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "F T u f B v T n 4 W 2 H g h 8 n J O Q e 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z a r D A C Q 5 G p z L R", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 0 V t", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "A o x y j l f S 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "T K T g 5 K 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "w k g 3 K x M D b r g g 1 0 / Z l R A s r 7 w K 4 X n 9 q h 7 c + 1", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "w k b U E 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "As illustrated in Figure 2 , our framework naturally provides a way for style transfer , where the guider network plays the role of style selection , and the generator only focuses on maintaining content without considering the styles .", "entities": [[13, 15, "TaskName", "style transfer"]]}
{"text": "To make the guider network focus on the guidance of styles , we assign the label l as the initial state s G 0 of the guider network .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "g ( s t\u22121 ) , w t = \u03d5 ( G \u03c8 ( s G t\u22121 , [ f t , l ] ) ) , ( 9 ) y t \u223c Multi ( 1 , softmax ( O t w t ) ) .", "entities": [[37, 38, "MethodName", "softmax"]]}
{"text": "For the generator , we put an adversarial regularizer on the encoded latent s 0 ( X ) and penalize it if it contains the sentiment information , by maximizing the entropy , i.e. , max l p ( l | s 0 ( X ) ) log p ( l | s 0 ( X ) ) , where p is a pre - trained classifier .", "entities": [[14, 15, "DatasetName", "0"], [42, 43, "DatasetName", "0"], [53, 54, "DatasetName", "0"]]}
{"text": "We first review related works that combine RL and GAN for text generation .", "entities": [[9, 10, "MethodName", "GAN"], [11, 13, "TaskName", "text generation"]]}
{"text": "For example , RankGAN ( Lin et al , 2017 ) proposes to replace the reward from the GAN discriminator with a rankingbased reward , MaliGAN ( Che et al , 2017 ) modifies the GAN objective and proposes techniques to reduce gradient variance , MaskGAN uses a filling technique to define a Q - value reward for sentence completion , RelGAN ( Nie et al , 2019 ) uses a relational memory based generator for the long - distance dependency modeling , FM - GAN uses a feature mover distance to match features of real and generated sentences inspired by optimal transport ( Chen et al , 2019 ; , and LeakGAN ( Guo et al , 2017 ) tries to address the sparse - reward issue for long - text generation with hierarchical RL by utilizing the leaked information from a GAN discriminator .", "entities": [[18, 19, "MethodName", "GAN"], [35, 36, "MethodName", "GAN"], [58, 60, "TaskName", "sentence completion"], [85, 86, "MethodName", "GAN"], [131, 133, "TaskName", "text generation"], [143, 144, "MethodName", "GAN"]]}
{"text": "By contrast , by relying on a model - based imitation learning approach , our method learns global - structure information , which generates more - diverse sentences , and can be extended to conditional text generation .", "entities": [[10, 12, "TaskName", "imitation learning"], [34, 37, "TaskName", "conditional text generation"]]}
{"text": "RL techniques can also be used in other ways for text generation ( Bachman and Precup , 2015 ) .", "entities": [[10, 12, "TaskName", "text generation"]]}
{"text": "To reduce variance of the vanilla REINFORCE , Bahdanau et al ( 2017 ) adopted the actor - critic framework for sequence prediction .", "entities": [[6, 7, "MethodName", "REINFORCE"]]}
{"text": "Furthermore , Rennie et al ( 2016 ) trained a baseline algorithm with a greedy decoding scheme for the REINFORCE method .", "entities": [[19, 20, "MethodName", "REINFORCE"]]}
{"text": "Planning techniques in RL have also been explored to improve text generation ( Gulcehre et al , 2017 ; Serdyuk et al , 2018 ) .", "entities": [[10, 12, "TaskName", "text generation"]]}
{"text": "We use the COCO Image Captions Dataset , in which most sentences have a length of about 10 words .", "entities": [[3, 4, "DatasetName", "COCO"]]}
{"text": "Since we consider unconditional text generation , only image captions are used as the training data .", "entities": [[4, 6, "TaskName", "text generation"]]}
{"text": "We observe that GM - GAN performs significantly better than the baseline models .", "entities": [[5, 6, "MethodName", "GAN"]]}
{"text": "Specifically , besides achieving higher test - BLEU scores , the proposed method also generates samples with very good diversity in terms of self - BLEU scores .", "entities": [[7, 8, "MetricName", "BLEU"], [25, 26, "MetricName", "BLEU"]]}
{"text": "LeakGAN represents the state - of - the - art in adversarial text generation , however , its diversity measurement is relatively poor ( Zhu et al , 2018 ) .", "entities": [[11, 13, "TaskName", "adversarial text"]]}
{"text": "We suspect that the high BLEU score achieved by LeakGAN is due to its mode collapse on some good samples , resulting in high self - BLEU scores .", "entities": [[5, 7, "MetricName", "BLEU score"], [26, 27, "MetricName", "BLEU"]]}
{"text": "Other baselines achieve lower self - BLEU scores since they can not generate reasonable sentences .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "Long Text Generation : EMNLP2017 WMT", "entities": [[1, 3, "TaskName", "Text Generation"]]}
{"text": "We conduct ablation studies on long text generation to investigate the improvements brought by each part of our proposed method .", "entities": [[6, 8, "TaskName", "text generation"]]}
{"text": "( 2 ) The UK is Google ' s largest non - US market , he has added \" 20 , before the best team is amount of fewer than one or the closest home or two years ago .", "entities": [[6, 7, "DatasetName", "Google"]]}
{"text": "We also report the BLEU scores with original sentences ( BLEU ) and human references ( BLEU - ref ) ( Li et al , 2018a ) , to evaluate the content preservation of transferred sentences .", "entities": [[4, 5, "MetricName", "BLEU"], [10, 11, "MetricName", "BLEU"], [16, 17, "MetricName", "BLEU"]]}
{"text": "Our proposed model exhibits higher transfer accuracy and better content preservation , indicating the guider network provides good sentiment guidance to better preserve the content information .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Table 8 : Generated samples of guided style transfer .", "entities": [[7, 9, "TaskName", "style transfer"]]}
{"text": "We have proposed a model - based imitationlearning framework for adversarial text generation , by introducing a guider network to model the generation environment .", "entities": [[10, 12, "TaskName", "adversarial text"]]}
{"text": "Our proposed models are validated on both unconditional and conditional text generation , including adversarial text generation and non - parallel style transfer .", "entities": [[9, 12, "TaskName", "conditional text generation"], [14, 16, "TaskName", "adversarial text"], [21, 23, "TaskName", "style transfer"]]}
{"text": "More Generated Samples of Text Generation Table 13 lists more generated samples on the proposed GMGAN and its baselines .", "entities": [[4, 6, "TaskName", "Text Generation"]]}
{"text": "We conduct experiments on image captioning ( Karpathy and Fei - Fei , 2015 ) , investigating benefits brought by the Guider network .", "entities": [[4, 6, "TaskName", "image captioning"]]}
{"text": "In image captioning , instead of using a discriminator to define final rewards for generated sentence , we adopt evaluation metrics computed based on human references .", "entities": [[1, 3, "TaskName", "image captioning"]]}
{"text": "We test our proposed model on the MS COCO dataset ( Karpathy and Fei - Fei , 2015 ) , containing 123 , 287 images in total .", "entities": [[8, 9, "DatasetName", "COCO"]]}
{"text": "We consider two settings : ( i ) using a pre - trained 152layer ResNet ( He et al , 2016 ) for feature extraction , where we take the output of the 2048 - way pool5 layer from ResNet - 152 , pretrained on the ImageNet dataset ; and ( ii ) using semantic tags detected from the image as features .", "entities": [[14, 15, "MethodName", "ResNet"], [33, 34, "DatasetName", "2048"], [39, 40, "MethodName", "ResNet"], [46, 47, "DatasetName", "ImageNet"]]}
{"text": "We use an LSTM with 512 hidden units with mini - batches of size 64 .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "The results are summarized in comparing an AutoEncoder ( AE ) with a variant implemented by adding a guider network ( Guider ) , improvements are observed .", "entities": [[7, 8, "MethodName", "AutoEncoder"], [9, 10, "MethodName", "AE"]]}
{"text": "We compare the proposed GMST with SCST .", "entities": [[6, 7, "MethodName", "SCST"]]}
{"text": "Note the main difference between GMST and SCST is that the former employs our proposed feature - matching reward , while the latter only considers the final reward provided by evaluation metrics .", "entities": [[7, 8, "MethodName", "SCST"]]}
{"text": "GMST achieves higher scores compared with SCST on its optimized metrics .", "entities": [[6, 7, "MethodName", "SCST"]]}
{"text": "The gain of GMST compared with SCST comes from the immediate rewards , which can maintain the semantic consistency and sentence structure , preventing language - fluency damage caused by only focusing on evaluation metrics .", "entities": [[6, 7, "MethodName", "SCST"]]}
{"text": "The experiments aim to quantify the gain when incorporating MPC for imitation learning , i.e. , MLE and RL finetune .", "entities": [[11, 13, "TaskName", "imitation learning"]]}
{"text": "at time t + t. In the text generation setting , when t = 1 , we can exactly get the feature representation of the current generated sentence if the guider does not help the word selection .", "entities": [[7, 9, "TaskName", "text generation"]]}
{"text": "We use Adam ( Kingma and Ba , 2014 ) optimization algorithm to train the guider , generator and discriminator .", "entities": [[2, 3, "MethodName", "Adam"]]}
{"text": "For both tasks , the LSTM state of dimension for the generator is 300 , and the LSTM state of dimension for the generator is 300 .", "entities": [[5, 6, "MethodName", "LSTM"], [17, 18, "MethodName", "LSTM"]]}
{"text": "The LSTM state of dimension for the generator is 300 , and the LSTM state of dimension for the guider is 300 .", "entities": [[1, 2, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "The research was supported in part by DARPA , DOE , NIH , NSF and ONR .", "entities": [[7, 8, "DatasetName", "DARPA"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "j u o 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "H S k S C U b T S 3 U 0 P e 9 W a W 3 d", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "N q I D 3 r F U 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "N G 2 F J K Z + n M i p 7 E x 4 z i 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "F o V n 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "z 5 I r N F 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "j u o 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "H S k S C U b T S 3 U 0 P e 9 W a W 3 d", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "N q I D 3 r F U 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "N G 2 F J K Z + n M i p 7 E x 4 z i 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "F o V n 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "z 5 I r N F 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "j u o 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "H S k S C U b T S 3 U 0 P e 9 W a W 3 d", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "N q I D 3 r F U 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "N G 2 F J K Z + n M i p 7 E x 4 z i 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "F o V n 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "z 5 I r N F 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "a 0 E x m", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "3 p z 0 K t B", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "+ F j u f B v T n 4 W 2 H g h 8 n J O Q e 0", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z Z r D A C Q 5 G p z L R", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 0 V t", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "x y j l f S 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "U z T m h q R Q O C l 3 C o s 5 F 0 P e x 7 Z D z V", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "O 0 0 X g 2 6 o S d", "entities": [[1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"]]}
{"text": "L t l F 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "q w b 0 P J T i G E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 6 M M L v M G 7 p 7 x X 7 2 P e 1 p q 3 q O 0", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "+ F j u f B v T n 4 W 2 H g h 8 n J O Q e 0", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z Z r D A C Q 5 G p z L R", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 0 V t", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "x y j l f S 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "U z T m h q R Q O C l 3 C o s 5 F 0 P e x 7 Z D z V", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "O 0 0 X g 2 6 o S d", "entities": [[1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"]]}
{"text": "L t l F 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "q w b 0 P J T i G E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "I 6 M M L v M G 7 p 7 x X 7 2 P e 1 p q 3 q O 0", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 8 q L e i F 2 9 W N L b Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a D i 1 X / k z X / j t s 1 B W x 8 M P N 6 b", "entities": [[27, 28, "DatasetName", "0"], [51, 52, "DatasetName", "0"]]}
{"text": "e m E p h 0 H W / n d L K 6 t r 6 R", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "j u o 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "W 0 P e 9 W a W 3 d", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "n I M v E K 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A T q / R J l G h b C s l M / T 2 R 0 9 i Y c R z a z p j i 0 C x 6 U / E / r 5 N h d B H k Q", "entities": [[18, 19, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}
{"text": "k z I 9 G / S F 5 o z l G N L K N P C 3 k r Y k G r K 0 K Z T s S F 4 i y 8", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" X / B b P o 3 p z 0", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "4 t t E P J p H f a 0 E", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "b l F J j S F J U t j O D f I 0 V t i K R 7 e z v", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "y j l A y 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "o Q g g C B v A C b / D u K e / V + 1 i 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "4 t t E P J p H f a 0 E", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "b l F J j S F J U t j O D f I 0 V t i K R 7 e z v", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "y j l A y 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "o Q g g C B v A C b / D u K e / V + 1 i 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "x B v R i 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "U I m q T E d z 0 0", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "v z z 6 o e v Q S H 4 G m 0", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "N N 3 C 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0 G B R 3 i G V", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "9 o k H G k E 1 T 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "2 1 e N t 3 b s 0 b r q m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "g w j m 0 4", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" X / B b P o 3 p z 0", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "+ v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "g i M R K E K 0 0 u 4 R S U 1 R", "entities": [[7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "a 8 0 y P N E Y T e Z 3 C 3 y 7", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p +", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "j / b r w J 0 W X 7 p h 0", "entities": [[6, 7, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P", "entities": [[2, 3, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E /", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "z o c w w m c g Q 8 X c A 2 3 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P", "entities": [[2, 3, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E /", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "z o c w w m c g Q 8 X c A 2 3 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "L A y B W 3 o S N t 8 F p M 0", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "r k U t C P S K 4 k L 0 Q K 8 p", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "w d 2 0 2 k 5 M 6 B l 4 l a k C R U 6 A / v L j w T J E", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h d F r 3 c 0 U z T M Z 4 S P u G", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "K i h m 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "o U 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "9 o k H G k E 1 T 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "2 1 e N t 3 b s 0 b r q m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "g w j m 0 4", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "9 o k H G k E 1 T 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "2 1 e N t 3 b s 0 b r q m", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "g w j m 0 4", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "F F f g w j m 0 4", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "F F f g w j m 0 4", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "F F f g w j m 0 4", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" U 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "N A y 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "G S p w T F W Q z 6 J P 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "F F f g w j m 0 4", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "q c f e a q 5 f 3 G N U B Z c 0 = \" >", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "I 6 q 3 o Q Y 8 V j C 2 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "2 J 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "q c f e a q 5 f 3 G N U B Z c 0 = \" >", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "I 6 q 3 o Q Y 8 V j C 2 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "2 J 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "q c f e a q 5 f 3 G N U B Z c 0 = \" >", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "I 6 q 3 o Q Y 8 V j C 2 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "2 J 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "A q 7 A g 1 t o w j 2 0", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "We test the proposed framework on unconditional and conditional text generation tasks , and analyze the results to understand the performance gained by the guider network .", "entities": [[8, 11, "TaskName", "conditional text generation"]]}
{"text": "We also perform an ablation investigation on the improvements brought by each part of our proposed method , and consider non - parallel style transfer .", "entities": [[23, 25, "TaskName", "style transfer"]]}
{"text": "We focus on adversarial text generation , and compare our approach with a number of related works ( Guo et al , 2017 ; Lin et al , 2017 ; Zhu et al , 2018 ) .", "entities": [[3, 5, "TaskName", "adversarial text"]]}
{"text": "In this setting , a discriminator in the GAN framework is added to the model in Figure 1 to guide the generator to generate high - quality sentences .", "entities": [[8, 9, "MethodName", "GAN"]]}
{"text": "All baseline experiments are implemented on the texygen platform ( Zhu et al , 2018 ) .", "entities": [[7, 9, "DatasetName", "texygen platform"]]}
{"text": "We adopt the BLEU score , referenced by the test set ( test - BLEU , higher value implies better quality ) and itself ( self - BLEU , lower value implies better diversity )", "entities": [[3, 5, "MetricName", "BLEU score"], [14, 15, "MetricName", "BLEU"], [27, 28, "MetricName", "BLEU"]]}
{"text": "A good generator should achieve both a high test - BLEU score and a low self - BLEU score .", "entities": [[10, 12, "MetricName", "BLEU score"], [17, 19, "MetricName", "BLEU score"]]}
{"text": "We call the proposed method guidermatching GAN ( GMGAN ) for unconditional text Res152 - SCST : a group of zebras standing in a eld .", "entities": [[6, 7, "MethodName", "GAN"], [15, 16, "MethodName", "SCST"]]}
{"text": "Tag - SCST : a zebra and a zebra drinking water from a eld of grass .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Res152 - SCST : a group of people walking down a skateboard .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Tag - SCST : a woman walking down a street with a skateboard .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Res152 - SCST : a baby sing next to a baby girae .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Tag - SCST : a black and white photo of a woman holding a teddy bear .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Res152 - SCST : a trac light on a street with a in the . Res152 - GMST : a trac light on the side of a street .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "Tag - SCST : a trac light on a street with a green .", "entities": [[2, 3, "MethodName", "SCST"]]}
{"text": "The team looked into the influence of marriage on weight loss after surgery - as well as the effects of surgery on the quality of his administration and rest on the world .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "Multi - Style Transfer with Discriminative Feedback on Disjoint Corpus", "entities": [[2, 4, "TaskName", "Style Transfer"]]}
{"text": "Style transfer has been widely explored in natural language generation with non - parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus .", "entities": [[0, 2, "TaskName", "Style transfer"]]}
{"text": "While cascading single - dimensional models across multiple styles is a possibility , it suffers from content loss , especially when the style dimensions are not completely independent of each other .", "entities": [[17, 18, "MetricName", "loss"]]}
{"text": "We compare it against baselines involving cascaded state - of - the - art uni - dimensional style transfer models .", "entities": [[17, 19, "TaskName", "style transfer"]]}
{"text": "Style transfer is a popular task in natural language processing and has been studied on attributes like age or gender ( Subramanian et al , 2018 ) , styles emanating from social construct like formality ( Rao and Tetreault , 2018 ) and politeness ( Madaan et al , 2020 ) , linguistic styles based on author writing style ( Syed et al , 2020 ) , or psycho - linguistic styles based on personality types ( Mairesse and Walker , 2011 ) .", "entities": [[0, 2, "TaskName", "Style transfer"]]}
{"text": "While early style transfer frameworks were modeled as a supervised learning task on a parallel corpus , state - of - the - art models are semi - supervised / unsupervised and operate on nonparallel corpus .", "entities": [[2, 4, "TaskName", "style transfer"]]}
{"text": "These models achieve style transfer by aligning source and target distribution of sentences from non - parallel corpus ( Shen et al , 2017 ) , disentangling content space from style space in latent representation ( Hu et al , 2017 ) or employing self - reconstruction ( Dai et al , 2019 ) and back translation ( Lample et al , 2018 ) objectives to achieve pseudo - supervision with non - parallel corpus .", "entities": [[3, 5, "TaskName", "style transfer"]]}
{"text": "In this paper , we propose a multidimensional style transfer approach that can work off partially labelled data for style transfer across multiple dimensions simultaneously .", "entities": [[8, 10, "TaskName", "style transfer"], [19, 21, "TaskName", "style transfer"]]}
{"text": "The work by Subramanian et al ( 2018 ) attempts style transfer with multiple attributes such as age , gender , and sentiment simultaneously .", "entities": [[10, 12, "TaskName", "style transfer"]]}
{"text": "In contrast to this and other similar explorations in multi - style transfer , our approach does not require jointly labelled data across all the stylistic dimensions in source and/or target corpus .", "entities": [[11, 13, "TaskName", "style transfer"]]}
{"text": "We focus on the problem where independent corpus is available across different stylistic dimensions ( say sentiment and formality ) and we achieve style transfer spanning different stylistic dimensions ( say make a sentence more positive and formal ) .", "entities": [[23, 25, "TaskName", "style transfer"]]}
{"text": "Relaxing the requirement of jointly labelled data for multi - style transfer , by leveraging independently acquired disjoint corpus for different styles .", "entities": [[10, 12, "TaskName", "style transfer"]]}
{"text": "3 ) Achieving better style control with better content preservation in multi - dimensional style transfer than a cascaded setup of state - of - the - art unidimensional style transfer models .", "entities": [[14, 16, "TaskName", "style transfer"], [29, 31, "TaskName", "style transfer"]]}
{"text": "One line of work in style transfer attempts to learn disentangled latent representation for style and content , and transfer style by manipulating latent representation of style ( Shen et al , 2017 ) .", "entities": [[5, 7, "TaskName", "style transfer"]]}
{"text": "Although these approaches perform well with one style at a time , they do not trivially scale to multidimensional style transfer .", "entities": [[19, 21, "TaskName", "style transfer"]]}
{"text": "Dai et al ( 2019 ) achieve state - ofthe - art style transfer in single style dimensions by employing transformer - based model in conjunction with classifier - based discriminator .", "entities": [[12, 14, "TaskName", "style transfer"]]}
{"text": "Language modeling is integral to several natural language generation ( NLG ) tasks like text summarization , spelling correction , image captioning , etc .", "entities": [[14, 16, "TaskName", "text summarization"], [17, 19, "TaskName", "spelling correction"], [20, 22, "TaskName", "image captioning"]]}
{"text": "The introduction of Transformer - based architecture accompanied with generative pre - training ( Radford , 2018 ) capabilities have led to strong improvements in many downstream generation and GLUE ( Wang et al , 2018 ) tasks .", "entities": [[3, 4, "MethodName", "Transformer"], [29, 30, "DatasetName", "GLUE"]]}
{"text": "Generative pre - training aims to adapt a large Transformer language model to large unsupervised corpus .", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "This capability of generative pre - training is exploited in many large language models like BERT ( Devlin et al , 2019 ) , GPT - 2 ( Radford et al , 2018 ) , ERNIE 2.0 ( Sun et al , 2020 ) which have the ability to perform tasks like reading comprehension ( Xu et al , 2019 ) , summarization ( Liu and Lapata , 2019 ) , question - answering ( Rajpurkar et al , 2016 ) and translation ( Clinchant et", "entities": [[15, 16, "MethodName", "BERT"], [24, 25, "MethodName", "GPT"], [52, 54, "TaskName", "reading comprehension"], [62, 63, "TaskName", "summarization"]]}
{"text": "Recently these pre - trained generative language models have been explored in translation ( Conneau and Lample , 2019 ) and style transfer tasks ( Syed et al , 2020 ) .", "entities": [[21, 23, "TaskName", "style transfer"]]}
{"text": "Conneau and Lample ( 2019 ) develop cross - lingual models for unsupervised machine translation by initializing encoder and decoder with a pre - trained language model trained on Masked Language Modeling ( MLM ) ( Devlin et al , 2019 ) objective and fine - tuning the encoderdecoder framework with adversarial training .", "entities": [[12, 15, "TaskName", "unsupervised machine translation"], [29, 32, "TaskName", "Masked Language Modeling"], [33, 34, "DatasetName", "MLM"]]}
{"text": "Based on this , a language model trained on target style will have high perplexity on transferred text if it does not match target style and low perplexity otherwise .", "entities": [[14, 15, "MetricName", "perplexity"], [27, 28, "MetricName", "perplexity"]]}
{"text": "Consequently , an independently trained LM on one of the target styles might have high perplexity even if the transferred sentence fits in the corresponding target style , due to the content space of source sentence .", "entities": [[15, 16, "MetricName", "perplexity"]]}
{"text": "Similar to Syed et al ( 2020 ) , we first pre - train a Transformer - based language model with Masked Language Modeling ( MLM ) objective on English Wikipedia data extracted using WikiExtractor .", "entities": [[15, 16, "MethodName", "Transformer"], [21, 24, "TaskName", "Masked Language Modeling"], [25, 26, "DatasetName", "MLM"]]}
{"text": "Masked Language Modeling leverages bidirectional context of the input , thus enabling better language understanding .", "entities": [[0, 3, "TaskName", "Masked Language Modeling"]]}
{"text": "Following Masked Language Modeling objective from Devlin et al ( 2019 ) , we randomly sample 15 % of the tokens from the text stream and replace them with the [ MASK ] token 80 % of the time , by a random token 10 % of the time and keep them unchanged 10 % of the time , with the objective of predicting the original identity of the masked word based on its bidirectional context .", "entities": [[1, 4, "TaskName", "Masked Language Modeling"]]}
{"text": "To enable style transfer from a given sentence to target style , we use independently trained language models ( LMs ) to initialize the encoder and decoder and connect these with randomly initialized attention layers to arrive at a encoder - decoder setup .", "entities": [[2, 4, "TaskName", "style transfer"], [33, 35, "HyperparameterName", "attention layers"]]}
{"text": "As discussed by Syed et al ( 2020 ) , the Transformer architecture ( Vaswani et al , 2017 ) allows such independent initialization by implicitly aligning encoder - decoder layers via attention mechanism .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "Transformer - based models with encoder - only ( Devlin et al , 2019 ) or decoder - only blocks have been shown to perform well in generative pre - training task .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "Moreover , this also enables us to use the same pre - trained model to initialize both style transfer module and the discriminator models , explained in the following section .", "entities": [[17, 20, "MethodName", "style transfer module"]]}
{"text": "In Syed et al ( 2020 ) 's setup , both encoder and decoder in the style transfer module are initialized with the pre - trained language model ( trained on MLM objective ) .", "entities": [[16, 19, "MethodName", "style transfer module"], [31, 32, "DatasetName", "MLM"]]}
{"text": "To instill style - awareness to the encoder - decoder setup initialized with pre - trained Transformer models , we fine - tune it with Denoising Autoencoder ( DAE ) loss using the target - domain corpus .", "entities": [[16, 17, "MethodName", "Transformer"], [25, 27, "MethodName", "Denoising Autoencoder"], [30, 31, "MetricName", "loss"]]}
{"text": "Under the DAE objective , the encoder takes a noisy masked versionx of the text x as input and attempts to fill in the mask token as per the MLM objective that it was pre - trained on .", "entities": [[29, 30, "DatasetName", "MLM"]]}
{"text": "The overall training objective is L DAE ( \u03b8 G )", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "E x\u223cT [ \u2212 log P \u03b8 G", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "In conjunction , the encoder and decoder enable style transfer to the target style .", "entities": [[8, 10, "TaskName", "style transfer"]]}
{"text": "To extend the single - dimensional style transfer setup above to multi - dimensional setting , we use language models as discriminators to provide the feedback to the model for partially annotated nature of input data .", "entities": [[6, 8, "TaskName", "style transfer"]]}
{"text": "Inspired by Yang et al ( 2018 ) , we fine - tune a language model on the target style s i , so that the language model is equipped with language distribution of target domain data .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "The training loss for the LM for target style s i with corresponding corpus", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "For k - dimensional style transfer with target styles s = { s 1 , s 2 , . . .", "entities": [[4, 6, "TaskName", "style transfer"]]}
{"text": "For the transferred sentence x , the training objective for each target style s i is , argmin \u03b8", "entities": [[18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "G L s i = E x\u223cT , x \u223cP \u03b8 G ( x )", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "However , we can not directly find the argmin \u03b8 G using gradient descent because of discrete sampling of x \u223c P \u03b8 G ( x ) .", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [22, 23, "HyperparameterName", "\u03b8"]]}
{"text": "To account for this , we use a policy gradient reinforcement learning approach using REINFORCE algorithm ( Sutton et al , 1999 ) .", "entities": [[14, 15, "MethodName", "REINFORCE"]]}
{"text": "L s i given by , L s i = E x\u223cT , x \u223cP \u03b8 G ( x ) ( r ( x ) \u2212 r ( x ) )", "entities": [[15, 16, "HyperparameterName", "\u03b8"]]}
{"text": "[ \u2212 log P \u03b8 G ( x | x ) ]", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "The overall training loss for the joint encoder - decoder model is L = \u03bbDAEEx\u223cT [ \u2212 log P \u03b8 ( x | x ) ]", "entities": [[3, 4, "MetricName", "loss"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "First , we pre - train a transformer model with Masked language modeling objective as shown in Figure 1 ( Left ) .", "entities": [[10, 13, "TaskName", "Masked language modeling"]]}
{"text": "Finally , we initialize the encoder and decoder of the style transfer module with the pretrained and style - specific fine - tuned language models , respectively .", "entities": [[10, 13, "MethodName", "style transfer module"]]}
{"text": "For sentiment , we use a mixture of IMDB ( Maas et al , 2011 ) and Yelp dataset ( Li et al , 2018 ) with 300k examples in the positive and negative sentiment each .", "entities": [[8, 9, "DatasetName", "IMDB"]]}
{"text": "For formality , we use GYAFC corpus ( Rao and Tetreault , 2018 ) which has 104k examples in each formal and informal class .", "entities": [[5, 6, "DatasetName", "GYAFC"]]}
{"text": "For pre - training , we use 12 - layer Transformer model with 512 hidden units , 16 heads , a dropout rate of 0.1 and learned positional embedding .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "We train our models with the Adam optimizer , and", "entities": [[6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "optimizer"]]}
{"text": "For singledimensional style evaluation , we generate sentences from models fine - tuned on negative corpus and positive corpus and compare the style accuracy of generated sentences .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "For instance , the classifier for evaluating sentiment accuracy is trained on sentiment corpus tagged with positive and negative class in IMDB and Yelp data .", "entities": [[8, 9, "MetricName", "accuracy"], [21, 22, "DatasetName", "IMDB"]]}
{"text": "We , therefore , employ the perplexities of these fine - tuned language models to gauge the style of the input text to guide our style transfer model .", "entities": [[25, 27, "TaskName", "style transfer"]]}
{"text": "We calculate the perplexity of each of these models on the test corpus from the same style and from the opposite style .", "entities": [[3, 4, "MetricName", "perplexity"]]}
{"text": "This implies that a language model fine - tuned on positive corpus shows higher perplexity for negative sentences and lower for positive sentences and vice versa .", "entities": [[14, 15, "MetricName", "perplexity"]]}
{"text": "This corroborates the effectiveness of these fine - tuned language models to serve as discriminators for training the style transfer module .", "entities": [[18, 21, "MethodName", "style transfer module"]]}
{"text": "We scale these scores between 0 - 100 , where higher ( 100 ) lexical score signifies formal style and lower ( 0 ) score signifies informal style .", "entities": [[5, 6, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "Besides this , we also calculate BLEU score between the transferred sentence generated by our model and the corresponding human reference transferred sentence , available for GYAFC and Yelp corpus ( ref - BLEU ) .", "entities": [[6, 8, "MetricName", "BLEU score"], [26, 27, "DatasetName", "GYAFC"], [33, 34, "MetricName", "BLEU"]]}
{"text": "This Dai et al , 2019 ) , Cascaded Discriminative LM method and multi - style transfer using Adapted Rewriting LM ( Syed et al , 2020 ) .", "entities": [[15, 17, "TaskName", "style transfer"]]}
{"text": "The perplexity is the measure of log likelihood of the generated sentence on the language model .", "entities": [[1, 2, "MetricName", "perplexity"]]}
{"text": "A lower perplexity is indicative of a more fluent sentence .", "entities": [[2, 3, "MetricName", "perplexity"]]}
{"text": "Dai et al ( 2019 ) use transformer - based model ( Style Transformer ) for single - dimensional style transfer .", "entities": [[13, 14, "MethodName", "Transformer"], [19, 21, "TaskName", "style transfer"]]}
{"text": "We train two independent Style Transformer models for sentiment and formality transfer and then perform transfer one after another to compare results with our model .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "We term this as Cascaded Style Transformer setup .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "The Style Transformer model is shown to have state - of - the - art performance in single - dimensional style transfer ; thus it provides an estimate of the performance of sequential single style transfer .", "entities": [[2, 3, "MethodName", "Transformer"], [20, 22, "TaskName", "style transfer"], [34, 36, "TaskName", "style transfer"]]}
{"text": "These are the closest baselines to our proposed approach , since other works dealing with multi - style transfer assume presence of jointly annotated dataset , which is a stronger assumption that we aim to relax .", "entities": [[17, 19, "TaskName", "style transfer"]]}
{"text": "In addition to our proposed model with multiple style transfer , we also train our encoder - decoder architecture with single discriminative LM for one style at a time and perform two stage transfer , similar to one with Cascaded Style Transformer ( Dai et al , 2019 ) setup .", "entities": [[8, 10, "TaskName", "style transfer"], [41, 42, "MethodName", "Transformer"]]}
{"text": "The results in Table 3 show that our model achieves better style control than the Cascaded Style Transformer ( Dai et al , 2019 ) as well as the joint transfer using Syed et al ( 2020 ) for both sentiment and formality .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "As seen in Table 3 , cascaded style transfer models perform poorly on content preservation .", "entities": [[7, 9, "TaskName", "style transfer"]]}
{"text": "This is because transferring style one after other leads to huge loss in content , thus both the two - stage models score lower on content preservation metrics , both w.r.t .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "The effect can also be observed in Table 4 which demonstrates qualitative results for Cascaded Style Transformer model and our model .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "Among the cascaded models , the Discriminative LM scores marginally better on content preservation than the Style Transformer model .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "We attribute this to initialization with the same pre - trained LM resulting in shared content space in the underlying single style transfer models .", "entities": [[21, 23, "TaskName", "style transfer"]]}
{"text": "However , due to independent training of the two single style transfer models , they are not able to model interplay between these styles and hence perform worse on style control than our proposed model trained jointly on multiple styles .", "entities": [[10, 12, "TaskName", "style transfer"]]}
{"text": "( Syed et al ( 2020 ) and our proposed approach ) , we note that content preservation is marginally better for Syed et al ( 2020 ) 's model , however , our model is able to yield much better style transfer owing to feedback on style control by multiple discriminators .", "entities": [[41, 43, "TaskName", "style transfer"]]}
{"text": "Based on comparable style control in Cascaded Style Transformer and our proposed approach on automatic metrics , we compare the transfer quality across these two models by a small - scale human study .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "These results are in line with our automatic evaluations and add confidence to the efficacy of our proposed approach in achieving style transfer across multiple dimensions .", "entities": [[21, 23, "TaskName", "style transfer"]]}
{"text": "We propose an approach to extend currently existing style transfer work to multiple style setting without imposing any extra constraints on availability of dataset .", "entities": [[8, 10, "TaskName", "style transfer"]]}
{"text": "We exploit multiple discriminative language models with an encoder - decoder framework , all emerging from large transformer - based language models pretrained on Masked Language Modeling objective and fine - tuned separately for transfer and discriminative purposes .", "entities": [[24, 27, "TaskName", "Masked Language Modeling"]]}
{"text": "We show that unified single step transfer approach is able to achieve better transfer while offering much better content preservation which is paramount to any style transfer task .", "entities": [[25, 27, "TaskName", "style transfer"]]}
{"text": "Ethics Statement .", "entities": [[0, 1, "DatasetName", "Ethics"]]}
{"text": "As with any generative task , style transfer too suffers from the potential misuse for fact distortion , plagiarism and more .", "entities": [[6, 8, "TaskName", "style transfer"]]}
{"text": "Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text", "entities": [[10, 12, "TaskName", "Question Answering"]]}
{"text": "Numerical reasoning skills are essential for complex question answering ( CQA ) over text .", "entities": [[7, 9, "TaskName", "question answering"]]}
{"text": "Complex Question Answering ( CQA ) is a challenging task , requiring a model to perform compositional and numerical reasoning .", "entities": [[1, 3, "TaskName", "Question Answering"]]}
{"text": "Originally proposed for the visual question answering ( VQA ) task , Neural Module Networks ( NMNs ) ( Andreas et", "entities": [[4, 7, "DatasetName", "visual question answering"], [8, 9, "TaskName", "VQA"]]}
{"text": "NMNs achieves the best performance on a subset of the challenging DROP dataset ( Dua et al , 2019 ) and is interpertable by nature .", "entities": [[11, 12, "DatasetName", "DROP"]]}
{"text": "Figure 1 : Two examples in the DROP ( Dua et al , 2019 ) dataset that demonstrate the deficienties of NMNs .", "entities": [[7, 8, "DatasetName", "DROP"]]}
{"text": "Finally , we strengthen the auxiliary loss to increase attention values of entities in closer vicinity within a sentence .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "Complex Question Answering focuses on questions that require capabilities beyond multi - hop reasoning .", "entities": [[1, 3, "TaskName", "Question Answering"]]}
{"text": "A number of neural models were recently proposed to address the CQA task , such as BiDAF ( Seo et al , 2017 ) , QANet ( Yu et al , 2018 ) , NMNs ( Gupta et al , 2020 ) and NumNet ( Ran et al , 2019 ) , which achieved high performance on benchmark datasets such as DROP ( Dua et al , 2019 ) .", "entities": [[61, 62, "DatasetName", "DROP"]]}
{"text": "Using dependency parsing of questions , Saha et al ( 2021 ) focused on the numerical part and obtained excellent results on different kinds of numerical reasoning questions .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "i = softmax ( S n i ) , ( 2 ) T", "entities": [[2, 3, "MethodName", "softmax"]]}
{"text": "Inspired by this idea , we propose the question - toparagraph alignment modification to number - related modules .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Thus , if both the i th paragraph token and the j th number token belong to the same sentence , element U n ij , in row i and column j , is set to 1 , otherwise 0 : U n ij = 1 , ( i s t ) ( n j s t ) 0 , otherwise ( 9 )", "entities": [[39, 40, "DatasetName", "0"], [58, 59, "DatasetName", "0"]]}
{"text": "For instance , the auxiliary loss for the \" find - num \" module is as follows : H n loss", "entities": [[5, 6, "MetricName", "loss"], [20, 21, "MetricName", "loss"]]}
{"text": "However , these loss functions still allow irrelevant numbers to have spuriously high attention values .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "Therefore , we propose to strengthen the auxiliary loss to further concentrate attention mass to those tokens within the same sentence : H n loss", "entities": [[8, 9, "MetricName", "loss"], [24, 25, "MetricName", "loss"]]}
{"text": "We evaluate model performance on the same subset of the DROP dataset used by the original NMNs ( Gupta et al , 2020 ) , which contains approx .", "entities": [[10, 11, "DatasetName", "DROP"]]}
{"text": "Neural Moudule Networks ( NMNs ) represent an interpretable state - of - the - art approach to complex question answering over text .", "entities": [[19, 21, "TaskName", "question answering"]]}
{"text": "In order to solve the complex question answering problem , Gupta et al ( 2020 ) proposed a Neural Module Networks ( NMNs ) model .", "entities": [[6, 8, "TaskName", "question answering"]]}
{"text": "The computational resources for this work were provided by the Multi - modal Australian Sci - enceS Imaging and Visualisation Environment ( MAS - SIVE ) ( www.massive.org.au ) .", "entities": [[22, 23, "MethodName", "MAS"]]}
{"text": "More Identifiable yet Equally Performant Transformers for Text Classification", "entities": [[7, 9, "TaskName", "Text Classification"]]}
{"text": "Transformer 's predictions are widely explained by the attention weights , i.e. , a probability distribution generated at its self - attention unit ( head ) .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "We prove the applicability of such variations by providing empirical justifications on varied text classification tasks .", "entities": [[13, 15, "TaskName", "text classification"]]}
{"text": "Widely adopted Transformer architecture ( Vaswani et al , 2017 ) has obviated the need for sequential processing of the input that is enforced in traditional Recurrent Neural Networks ( RNN ) .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "As a result , compared to a single - layered LSTM or RNN model , a single - layered Transformer model is computationally more efficient , reflecting in a relatively shorter training time ( Vaswani et al , 2017 ) .", "entities": [[10, 11, "MethodName", "LSTM"], [19, 20, "MethodName", "Transformer"]]}
{"text": "This advantage encourages the training of deep Transformer - based language models on largescale datasets .", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "A large number of SOTA machine learning systems even beyond NLP ( Lu et al , 2019 ) are inspired by the building blocks of Transformer that is multi - head self - attention ( Radford et al , 2018 ; Devlin et al , 2018 ) .", "entities": [[25, 26, "MethodName", "Transformer"]]}
{"text": "k =", "entities": [[0, 2, "HyperparameterName", "k ="]]}
{"text": "A Transformer layer consists of multiple heads , where each head performs selfattention computations , we break the head computations in two phases : Phase 1 : Calculation of attention weights a ( k , i ) .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "The identifiability in Transformer has been recently studied by Brunner et al ( 2019 ) which provides theoretical claims that under mild conditions of input length , attention weights are not unique to the head 's output .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "In this work , we probe the identifiability of attention weights in Transformer from a perspective that was ignored in Brunner et al ( 2019 ) .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "We explore the previously overlooked first phase of selfattention for its contribution to the identifiability in Transformer .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "For the regular setting of the Transformer encoder where d v depends on the number of attention heads and token embedding dimension , we propose to reduce d k .", "entities": [[6, 7, "MethodName", "Transformer"], [20, 22, "HyperparameterName", "embedding dimension"]]}
{"text": "Embedding dimension can be tuned according to the sequence length up to which identifiability is desired .", "entities": [[0, 2, "HyperparameterName", "Embedding dimension"]]}
{"text": "We evaluate the performance of the proposed variants on varied text classification tasks comprising of ten datasets ( 5 ) .", "entities": [[10, 12, "TaskName", "text classification"]]}
{"text": "In this paper , our goal is to provide concrete theoretical analysis , experimental observations , and possible simple solutions to identifiability of attention weights in Transformer .", "entities": [[26, 27, "MethodName", "Transformer"]]}
{"text": "The idea behind identifiable variants of the Transformer is - the harder it is to obtain alternative attention weights , the likelier is they are identifiable , which is a desirable property of the architecture .", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "We provide Transformer variants that are identifiable and validate them empirically by analysing the numerical rank of the attention matrix generated in the self - attention head of the Transformer encoder .", "entities": [[2, 3, "MethodName", "Transformer"], [29, 30, "MethodName", "Transformer"]]}
{"text": "The variants have strong mathematical support and simple to adopt in the standard Transformer settings .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "We provide empirical evaluations on varied text classification tasks that show higher identifiability does not compromise with the task 's performance .", "entities": [[6, 8, "TaskName", "text classification"]]}
{"text": "We base our analysis on the building block of Transformer , i.e. , the encoder layer ( Vaswani et al , 2017 ) .", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": ", z ds } R de , where d e denotes token embedding dimension .", "entities": [[12, 14, "HyperparameterName", "embedding dimension"]]}
{"text": ", p ds } R de . Multi - head Attention .", "entities": [[7, 11, "MethodName", "Multi - head Attention"]]}
{"text": "The attention weights A R ds\u00d7ds can be computed by A = softmax", "entities": [[12, 13, "MethodName", "softmax"]]}
{"text": "A V D = A T , ( 2 ) where D R dv\u00d7de is a linear layer and", "entities": [[16, 18, "MethodName", "linear layer"]]}
{"text": "the matrix T R ds\u00d7de denotes the operation V D. The R ds\u00d7de output of multi - head attention can be expressed as a summation over H obtained for each head 3 .", "entities": [[15, 19, "MethodName", "multi - head attention"]]}
{"text": "1 . In regular Transformer setting , a token vector is t i { ( z j + p j ) } ds i=1 is d e = 512 dimensional , number of heads h=8 , size of d", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "= Norm ( t i + ReLU", "entities": [[6, 7, "MethodName", "ReLU"]]}
{"text": "Linear 1 and Linear 2 are linear layers with 2048 and 512 nodes , respectively .", "entities": [[9, 10, "DatasetName", "2048"]]}
{"text": "Norm denotes minibatch layer normalization .", "entities": [[3, 5, "MethodName", "layer normalization"]]}
{"text": "A is unidentifiable if there exist a\u00f1 A , ( \u00c3 = 0 ) , such that ( A + \u00c3 ) is obtainable from phase - 1 of head computations and satisfy (", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "A T = \u21d2\u00c3 T = 0 .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "Under this constraint , we get\u00e3 i T = 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "v T T = 0 } ( 3 ) dim LN ( T )", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "= 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "then LN ( T ) = { 0 } , it leads to the only solution of constraint - R1 that is\u00c3 = 0 .", "entities": [[7, 8, "DatasetName", "0"], [23, 24, "DatasetName", "0"]]}
{"text": "= min ds , 64 where the last inequality is obtained for a head in the regular Transformer", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "The model is trained to predict the sentiment of IMDB reviews ( 5 ) .", "entities": [[9, 10, "DatasetName", "IMDB"]]}
{"text": "d s \u2212 rank ( T ) = 0 if d s \u2264 d v , ( d s \u2212 d v ) if d s > d v .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "= max ( d s \u2212 d v , 0 ) ( 7 ) With this , we infer A is identifiable if d s \u2264 d v = 64 .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "As a first step , we focus our analysis on the attention matrix without applying softmax non - linearity , i.e. , A = Q K T \u221a dq .", "entities": [[15, 16, "MethodName", "softmax"]]}
{"text": "The analysis is crucial to identify constraints coming from the first phase of self - attention in Transformer that impact identifiability .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "Insights from this will help us analyse softmax version of A.", "entities": [[7, 8, "MethodName", "softmax"]]}
{"text": "The softmax over attention logits generates attention weights with each row of A ( i.e. , a i 's ) is constrained to be a probability distribution .", "entities": [[1, 2, "MethodName", "softmax"]]}
{"text": "Hence , we can define constraint over\u00c3 as ( A + \u00c3 ) \u2265 0 ( P1 )", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "A T = 0 ( P2 ) A 1 = 0 .", "entities": [[3, 4, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "( P3 ) P1 is non - negativity constraint on ( A + \u00c3 ) as it is supposed to be the output of softmax ; P2 de - notes\u00c3 LN ( T ) ; P3 can be derived from the fact ( A + \u00c3 )", "entities": [[1, 2, "DatasetName", "P3"], [24, 25, "MethodName", "softmax"], [35, 36, "DatasetName", "P3"]]}
{"text": "= 1 = \u21d2 ( A 1 + \u00c3 1 ) = 1 = \u21d2\u00c3 1 = 0 as ( A 1 = 1 ) .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "The constraint in P2 and P3 can be combined and reformulated as\u00c3 [ T , 1 ] = 0 .", "entities": [[5, 6, "DatasetName", "P3"], [18, 19, "DatasetName", "0"]]}
{"text": "= max d s \u2212 ( d v + 1 ) , 0 .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "Disregarding the extreme cases when a i is a one - hot distribution , Brunner et al ( 2019 ) proved the existence and construction of non - trivial\u00c3 's satisfying all the constraints P1 , P2 , and P3 . 5", "entities": [[39, 40, "DatasetName", "P3"]]}
{"text": "However , the proof by Brunner et al ( 2019 ) missed the constraint - R2 , hence the existence of a non - trivial\u00c3 satisfying only the set of constraints P1 , P2 and P3 may not be a valid proposition to claim attention weights unidentifiability .", "entities": [[35, 36, "DatasetName", "P3"]]}
{"text": "Essentially , the work largely ignored the constraints coming from the rank of the matrix that produces A after softmax 6 .", "entities": [[19, 20, "MethodName", "softmax"]]}
{"text": "Let A l denote logits Q K T \u221a dq and softmax ( A l ) =", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "( A + \u00c3 ) , where softmax is operated over each row of A l .", "entities": [[7, 8, "MethodName", "softmax"]]}
{"text": "The constraint P4 confirms if there exists a logit matrix A l that can generate ( A + \u00c3 ) , given constraints P1 , P2 , and P3 are satisfied .", "entities": [[28, 29, "DatasetName", "P3"]]}
{"text": "We can retrieve the set of matrices A l such that softmax ( A l ) =", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "For an arbitrarily picked\u00c3 satisfying constraint P1 , P2 , and P3 , the dimensions of affine span S of { \u00e2 1 , . . .", "entities": [[11, 12, "DatasetName", "P3"]]}
{"text": "Thus , the set of ( A + \u00c3 ) satisfying constraint P1 , P2 and P3 are not always obtainable from attention head for d s >", "entities": [[16, 17, "DatasetName", "P3"]]}
{"text": "We postulate Although it is easier to construct\u00c3 satisfying constraints P1 , P2 and P3 , it is hard to construct\u00c3 satisfying constraint P4 over the rank of logit matrix A l .", "entities": [[14, 15, "DatasetName", "P3"]]}
{"text": "From IMDB dataset ( 5 ) , we randomly sample a set of reviews with token sequence length", "entities": [[1, 2, "DatasetName", "IMDB"]]}
{"text": "For each review , we construct 1000\u00c3 's satisfying constraints P1 , P2 , and P3 - First , we train a Transformer encoder - based IMDB review sentiment classifier ( 6 ) .", "entities": [[15, 16, "DatasetName", "P3"], [22, 23, "MethodName", "Transformer"], [26, 27, "DatasetName", "IMDB"]]}
{"text": "Thus , the experimentally constructed A l 's do not claim unidentifiability of A as it fails to satisfy the constraint P4 , while for Brunner et al ( 2019 ) , it falls under the solution set to prove unidentifiability as it meets constraints P1 , P2 and P3 .", "entities": [[49, 50, "DatasetName", "P3"]]}
{"text": "Based on the Identifiability analysis in 3 , we propose basic solutions to make Transformer 's attention weights identifiable .", "entities": [[14, 15, "MethodName", "Transformer"]]}
{"text": "Contrary to the regular Transformer setting where d k = d v , a simple approach is to decrease the value of d k that is the size of the key and query vector .", "entities": [[4, 5, "MethodName", "Transformer"], [8, 10, "HyperparameterName", "k ="]]}
{"text": "To resolve the unidentifiability issue when sequence length exceeds the size of value vector , we propose to keep the value vector size and token embedding dimension to be more than ( or equal to ) the maximum allowed input tokens , i.e. , d v \u2265 d s - max .", "entities": [[25, 27, "HyperparameterName", "embedding dimension"]]}
{"text": "For the empirical analysis of our proposed solutions as mentioned in 4 , we conduct our experiments on the following varied text classification tasks :", "entities": [[21, 23, "TaskName", "text classification"]]}
{"text": "IMDB ( Maas et al , 2011 ) .", "entities": [[0, 1, "DatasetName", "IMDB"]]}
{"text": "The dataset for the task of sentiment classification consist of IMDB movie reviews with their sentiment as positive or negative .", "entities": [[10, 13, "DatasetName", "IMDB movie reviews"]]}
{"text": "TREC ( Voorhees and Tice , 2000 ) .", "entities": [[0, 1, "DatasetName", "TREC"]]}
{"text": "SST ( Socher et al , 2013 ) .", "entities": [[0, 1, "DatasetName", "SST"]]}
{"text": "Stanford sentiment analysis dataset consist of 11 , 855 sentences obtained from movie reviews .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}
{"text": "The provided train / test / valid split is 8 , 544/2 , 210/1 , 101 . 8 ds - max < de as in the regular Transformer setting .", "entities": [[27, 28, "MethodName", "Transformer"]]}
{"text": "SNLI ( Bowman et al , 2015 ) .", "entities": [[0, 1, "DatasetName", "SNLI"]]}
{"text": "There are 560 , 000 samples for training and 38 , 000 samples for testing , equally split into positive and negative polarities . DBPedia .", "entities": [[24, 25, "DatasetName", "DBPedia"]]}
{"text": "The Ontology dataset for topic classification consist of 14 non - overlapping classes each with 40 , 000 samples for training and 5 , 000 samples for testing .", "entities": [[1, 2, "MethodName", "Ontology"], [4, 6, "TaskName", "topic classification"]]}
{"text": "AG News .", "entities": [[0, 2, "DatasetName", "AG News"]]}
{"text": "The balanced dataset for 10class topic classification contain 1 , 400 , 000 samples for training and 50 , 000 samples for testing .", "entities": [[5, 7, "TaskName", "topic classification"]]}
{"text": "Except for the SST and SNLI , where the validation split is already provided , we flag 30 % of the train set as part of the validation set and the rest 70 % were used for model parameter learning .", "entities": [[3, 4, "DatasetName", "SST"], [5, 6, "DatasetName", "SNLI"]]}
{"text": "We normalize the text by lower casing , removing special characters , etc . 9 For each task , we construct separate 1 - Gram vocabulary ( U ) and initialize a trainable randomly sampled token embedding ( U \u00d7 d e ) from N ( 0 , 1 ) .", "entities": [[46, 47, "DatasetName", "0"]]}
{"text": "For the regular Transformer setting , we fix the number of heads h to 8 and the size of value vector d v = d e /h that is 64 .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "For the identifiable variant of the Transformer encoder , d v = d e = 512 , this is equal to d s - max to keep it identifiable up to the maximum permissible number of tokens .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "In the case of SNLI , we use the shared encoder for both premise and hypothesis ; the output of their first tokens is then concatenated just before the final classification layer .", "entities": [[4, 5, "DatasetName", "SNLI"]]}
{"text": "We report the test accuracy obtained at the epoch with the best validation accuracy .", "entities": [[4, 5, "MetricName", "accuracy"], [13, 14, "MetricName", "accuracy"]]}
{"text": "To generate the numerical rank plot on IMDB dataset as shown in fig .", "entities": [[7, 8, "DatasetName", "IMDB"]]}
{"text": "2 , we train a separate Transformer encoder - based classifier .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "The zero dimensional ( left ) null space of T con - firms there exist no nontrivial solution to the constraint constraint - R2 , i.e. , \u00c3 = { 0 } .", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "Figure 6 : Scatter plots in red and blue show rank ( T ) and dim LN ( T ) , respectively , for matrices T obtained from the second phase of attention by feeding IMDB samples to the encoder .", "entities": [[35, 36, "DatasetName", "IMDB"]]}
{"text": "= 0 and thus attention weights are identifiable .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "To investigate this issue , we compare the performance of the identifiable Transformer encoder against its regular settings ( 6 ) on varied text classification tasks .", "entities": [[12, 13, "MethodName", "Transformer"], [23, 25, "TaskName", "text classification"]]}
{"text": "For the regular setting , as discussed in 4 as one of the solutions , the Transformer can be made identifiable by decreasing the size of the key vector d k .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "The rows of the Table 1 corresponding to Con denotes regular Transformer setting with varying size of key vector .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "We observe the classification accuracy at the lower d k is comparable or higher than large d k values , thus , the enhanced identifiability does not compromise with the model 's classification accuracy .", "entities": [[4, 5, "MetricName", "accuracy"], [33, 34, "MetricName", "accuracy"]]}
{"text": "This could be due to the larger size of value vector leading to the more number of parameters in Add that compensate for the significant reduction in the model 's accuracy .", "entities": [[15, 18, "HyperparameterName", "number of parameters"], [30, 31, "MetricName", "accuracy"]]}
{"text": "This work probed Transformer for identifiability of self - attention , i.e. , the attention weights can be uniquely identified from the head 's output .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "The identifiable variants do not show any performance drop when experiments are done on varied text classification tasks .", "entities": [[15, 17, "TaskName", "text classification"]]}
{"text": "Future works may analyse the critical impact of identifiability on the explainability and interpretability of the Transformer .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "1\u00d7mp | v T P = 0 } ( 10 )", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "The only solution to the system of equations v P = 0 is trivial , i.e. , v=0 .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "In 3 , we utilize our knowledge of appendix A.2 and appendix A.3 to analyse identifiability in a Transformer .", "entities": [[18, 19, "MethodName", "Transformer"]]}
{"text": "NLQuAD : A Non - Factoid Long Question Answering Data Set", "entities": [[7, 9, "TaskName", "Question Answering"]]}
{"text": "We introduce NLQuAD , the first data set with baseline methods for non - factoid long question answering , a task requiring documentlevel language understanding .", "entities": [[16, 18, "TaskName", "question answering"]]}
{"text": "In contrast to existing span detection question answering data sets , NLQuAD has non - factoid questions that are not answerable by a short span of text and demanding multiple - sentence descriptive answers and opinions .", "entities": [[6, 8, "TaskName", "question answering"]]}
{"text": "We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union ( IoU ) , which measures position - sensitive overlap between the predicted and the target answer spans .", "entities": [[6, 8, "MetricName", "F1 score"], [19, 20, "MetricName", "IoU"]]}
{"text": "To establish baseline performances , we compare BERT , RoBERTa , and Longformer models .", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "MethodName", "RoBERTa"], [12, 13, "MethodName", "Longformer"]]}
{"text": "Experimental results and human evaluations show that Longformer outperforms the other architectures , but results are still far behind a human upper bound , leaving substantial room for improvements .", "entities": [[7, 8, "MethodName", "Longformer"]]}
{"text": "NLQuAD 's samples exceed the input limitation of most pretrained Transformer - based models , encouraging future research on long sequence language models .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "Over the last few years , there have been remarkable improvements in the area of Machine Reading Comprehension ( MRC ) and open - domain Question Answering ( QA ) due to the availability of large scale data sets such as SQuAD ( Rajpurkar et al , 2016 ) and pre - trained language models such as BERT ( Devlin et al , 2018 ) .", "entities": [[15, 18, "TaskName", "Machine Reading Comprehension"], [22, 27, "TaskName", "open - domain Question Answering"], [41, 42, "DatasetName", "SQuAD"], [57, 58, "MethodName", "BERT"]]}
{"text": "To support research towards non - factoid and long QA tasks and to address the existing shortcomings as identified above , we have built NLQuAD , a non - factoid long question answering data set .", "entities": [[31, 33, "TaskName", "question answering"]]}
{"text": "This is in contrast to existing long - context but factoid QA data sets such as NewsQA ( Trischler et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) , NarrativeQA ( Ko\u010disk\u00fd et al , 2018 ) , DuoRC ( Saha et al , 2018 ) , HotpotQA ( Yang et al , 2018 ) , and Natural Questions ( Kwiatkowski et al , 2019 ) .", "entities": [[16, 17, "DatasetName", "NewsQA"], [25, 26, "DatasetName", "TriviaQA"], [34, 35, "DatasetName", "NarrativeQA"], [43, 44, "DatasetName", "DuoRC"], [52, 53, "DatasetName", "HotpotQA"], [62, 64, "DatasetName", "Natural Questions"]]}
{"text": "In particular , Natural Questions covers two types of short and long answers .", "entities": [[3, 5, "DatasetName", "Natural Questions"]]}
{"text": "Furthermore , although a small portion ( 13 % ) of Natural Questions samples have only long answers , they are still spans of simple facts .", "entities": [[11, 13, "DatasetName", "Natural Questions"]]}
{"text": "In most existing QA data sets such as SQuAD , crowd - workers generate questions based on provided short passages and extract answers from the passages ( Rajpurkar et al , 2016 ) .", "entities": [[8, 9, "DatasetName", "SQuAD"]]}
{"text": "This method of question generation can make QA samples trivial because models can simply detect the most related span to the question by guessing based on shallow pattern matching ( Ko\u010disk\u00fd et al , 2018 ) .", "entities": [[3, 5, "TaskName", "question generation"]]}
{"text": "NLQuAD , unlike MS MARCO ( Bajaj et al , 2016 ) and ELI5 ( Fan et al , 2019 ) , does not use information retrieval ( IR ) methods to collect supporting documents .", "entities": [[3, 5, "DatasetName", "MS MARCO"], [13, 14, "DatasetName", "ELI5"], [25, 27, "TaskName", "information retrieval"]]}
{"text": "With an average document length and answer length of 877 and 175 words , respectively , it exceeds the maximum input length of the state of the art QA models such as BERT ( Devlin et al , 2018 ) and RoBERTa ( Liu et al , 2019 ) due to their memory and computational requirements .", "entities": [[32, 33, "MethodName", "BERT"], [41, 42, "MethodName", "RoBERTa"]]}
{"text": "We also show the shortcomings of the F1 score and ROUGE - N scores in evaluating long sequences .", "entities": [[7, 9, "MetricName", "F1 score"]]}
{"text": "There is a higher chance of overlap between the word N - grams in two long sequences causing F1 and ROUGE - N to over - estimate the performance .", "entities": [[18, 19, "MetricName", "F1"]]}
{"text": "Therefore , we propose to use Intersection over Union ( IoU ) measuring position - sensitive overlap between two spans .", "entities": [[10, 11, "MetricName", "IoU"]]}
{"text": "To handle the input length limitations of BERT and RoBERTa , we pro - pose to train these models in a sliding - window approach ; ( 4 ) We finally show that the state - of - the - art models have limited performance in the non - factoid long QA task .", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "MethodName", "RoBERTa"]]}
{"text": "SQuAD ( Rajpurkar et al , 2016 ) is a factoid span detection data set with short answers .", "entities": [[0, 1, "DatasetName", "SQuAD"]]}
{"text": "DROP ( Dua et al , 2019 ) makes the problem more challenging by adversarially - created questions requiring discrete reasoning over the text .", "entities": [[0, 1, "DatasetName", "DROP"]]}
{"text": "SQuAD and DROP use Wikipedia pages as context passages whereas SearchQA ( Dunn et al , 2017 ) uses IR approaches to collect context passages .", "entities": [[0, 1, "DatasetName", "SQuAD"], [2, 3, "DatasetName", "DROP"], [10, 11, "DatasetName", "SearchQA"]]}
{"text": "Answer generation based on a set of passages is another approach to address this task .", "entities": [[0, 2, "TaskName", "Answer generation"]]}
{"text": "MS MARCO ( Bajaj et al , 2016 ) consists of real - world search queries and retrieved documents corresponding to the queries .", "entities": [[0, 2, "DatasetName", "MS MARCO"]]}
{"text": "There is also a range of multiple - choice QA tasks such as RACE ( Lai et al , 2017 ) , ARC ( Clark et al , 2018 ) , SWAQ ( Zellers et al , 2018 ) , and COS - MOS QA ( Huang et al , 2019 ) that are clustered together with the short - context QA data sets .", "entities": [[13, 14, "DatasetName", "RACE"], [22, 23, "DatasetName", "ARC"]]}
{"text": "NewsQA ( Trischler et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) , NarrativeQA ( Ko\u010disk\u00fd et al , 2018 ) , and DuoRC ( Saha et al , 2018 ) fall into this category and their documents are extracted from news articles , stories , and movie plots , respectively .", "entities": [[0, 1, "DatasetName", "NewsQA"], [9, 10, "DatasetName", "TriviaQA"], [18, 19, "DatasetName", "NarrativeQA"], [28, 29, "DatasetName", "DuoRC"]]}
{"text": "DuReader ( He et al , 2018 ) consists of real - word Chinese queries and corresponding retrieved documents .", "entities": [[0, 1, "DatasetName", "DuReader"]]}
{"text": "HotpotQA ( Yang et al , 2018 ) is a multi - hop data set , but the answer length of its factoid questions is as limited as that of short - context QA data sets .", "entities": [[0, 1, "DatasetName", "HotpotQA"]]}
{"text": "Natural Questions ( Kwiatkowski et al , 2019 ) is a factoid QA task with much longer documents and two types of answer lengths .", "entities": [[0, 2, "DatasetName", "Natural Questions"]]}
{"text": "ELI5 ( Fan et al , 2019 ) consists of real - world questions with answers provided by the Reddit community .", "entities": [[0, 1, "DatasetName", "ELI5"], [19, 20, "DatasetName", "Reddit"]]}
{"text": "Table 1 compares existing long - context question answering data sets along with SQuAD and MS MARCO .", "entities": [[7, 9, "TaskName", "question answering"], [13, 14, "DatasetName", "SQuAD"], [15, 17, "DatasetName", "MS MARCO"]]}
{"text": "To investigate the difficulty level of NLQuAD for state - of - the - art QA systems and to establish baseline results , we evaluate the performance of BERT ( Devlin et al , 2018 ) , RoBERTa ( Liu et al , 2019 ) , and Longformer ( Beltagy et al , 2020 ) .", "entities": [[28, 29, "MethodName", "BERT"], [37, 38, "MethodName", "RoBERTa"], [47, 48, "MethodName", "Longformer"]]}
{"text": "Longformer is a scalable model for processing long documents and has been used for long sequences such as document classification ( Beltagy et al , 2020 ) and document re - ranking ( Sekuli\u0107 et al , 2020 ) .", "entities": [[0, 1, "MethodName", "Longformer"], [18, 20, "TaskName", "document classification"]]}
{"text": "The BERT QA model concatenates question and document pairs into a single sequence and predicts the answer span by a dot product between the final hidden vectors , a start vector and an end vector ( Devlin et al , 2018 ) .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "Due to the memory and computational requirements , BERT can encode sequences with a maximum length of 512 tokens that is less than the average sample length in NLQuAD .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "We train BERT on the segments independently .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "RoBERTa has the same model architecture and input length limitation as BERT but with a robustly optimized pre - training scheme allowing it to generalize better to downstream tasks such as QA ( Liu et al , 2019 ) .", "entities": [[0, 1, "MethodName", "RoBERTa"], [11, 12, "MethodName", "BERT"]]}
{"text": "We apply the same sliding window approach for RoBERTa .", "entities": [[8, 9, "MethodName", "RoBERTa"]]}
{"text": "In order to process the question and entire documents at the same time , we use the Longformer model .", "entities": [[17, 18, "MethodName", "Longformer"]]}
{"text": "It employs an attention mechanism scaling linearly with the sequence length which enables Longformer to process up to 4 , 096 tokens .", "entities": [[13, 14, "MethodName", "Longformer"]]}
{"text": "Exact Match determines if the prediction exactly matches the target which can be a too strict criterion for long answers .", "entities": [[0, 2, "MetricName", "Exact Match"]]}
{"text": "The F1 score measures the overlap between the words in the prediction and the target .", "entities": [[1, 3, "MetricName", "F1 score"]]}
{"text": "The same holds for ROUGE - L with the Longest Common Sub - sequence ( LCS ) because of a high chance of longer LCSs between two long sequences .", "entities": [[4, 7, "MetricName", "ROUGE - L"]]}
{"text": "To better take sequence similarities into account , we propose to evaluate models with the Intersection over Union ( IoU ) score , also known as Jaccard Index .", "entities": [[19, 20, "MetricName", "IoU"]]}
{"text": "IoU is defined as follows : IoU =", "entities": [[0, 1, "MetricName", "IoU"], [6, 7, "MetricName", "IoU"]]}
{"text": "Prediction : The group was set up more than 50 years ago in the era of Spanish dictator General Franco , who repressed the Basques politically and culturally .", "entities": [[18, 19, "DatasetName", "General"]]}
{"text": "The F1 and ROUGE - N scores are always higher than IoU , but the metrics perform similarly in their higher values .", "entities": [[1, 2, "MetricName", "F1"], [11, 12, "MetricName", "IoU"]]}
{"text": "We manually inspected the spans with F1>0 and IoU=0 and saw no significant semantic similarity between the predicted answer span and the target span .", "entities": [[13, 15, "TaskName", "semantic similarity"]]}
{"text": "We analyze the performance of BERT and RoBERTa with different hyper - parameters on the development set in Table 5 .", "entities": [[5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "RoBERTa"]]}
{"text": "RoBERTa constantly outperforms BERT , which is to be expected as RoBERTa is optimized robustly during the pretraining .", "entities": [[0, 1, "MethodName", "RoBERTa"], [3, 4, "MethodName", "BERT"], [11, 12, "MethodName", "RoBERTa"]]}
{"text": "We use the official AllenAI Longformer code 5 to train Longformer on NLQuAD .", "entities": [[5, 6, "MethodName", "Longformer"], [10, 11, "MethodName", "Longformer"]]}
{"text": "While Longformer significantly outperforms BERT and RoBERTa , its performance , particularly in terms of IoU and EM , is far from perfect .", "entities": [[1, 2, "MethodName", "Longformer"], [4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "RoBERTa"], [15, 16, "MetricName", "IoU"], [17, 18, "MetricName", "EM"]]}
{"text": "ing the best human answer in terms of our primary evaluation metric ( IoU ) for each sample .", "entities": [[13, 14, "MetricName", "IoU"]]}
{"text": "Figure 6 shows that the target answers are preferred in 37 % and 64 % of cases over the Longformer and RoBERTa predictions , respectively .", "entities": [[19, 20, "MethodName", "Longformer"], [21, 22, "MethodName", "RoBERTa"]]}
{"text": "Figure 7 compares the performance of BERT , RoBERTa , and Longformer for instances with different document and answer lengths .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "RoBERTa"], [11, 12, "MethodName", "Longformer"]]}
{"text": "Surprisingly , BERT and RoBERTa outperform Longformer for longer answers .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "RoBERTa"], [6, 7, "MethodName", "Longformer"]]}
{"text": "The same pattern occurs for F1 and EM ( not shown in the figure ) .", "entities": [[5, 6, "MetricName", "F1"], [7, 8, "MetricName", "EM"]]}
{"text": "Figure 7 ( right ) shows that RoBERTa and BERT behave completely differently compared to Longformer for longer answer lengths .", "entities": [[7, 8, "MethodName", "RoBERTa"], [9, 10, "MethodName", "BERT"], [15, 16, "MethodName", "Longformer"]]}
{"text": "The former models have a bias to predict longer spans while Longformer under - estimates the length of the answer span .", "entities": [[11, 12, "MethodName", "Longformer"]]}
{"text": "This different behaviour might be due to the sliding window approach and the prediction aggregation in the RoBERTa and BERT models and the attention dilation strategy in Longformer .", "entities": [[17, 18, "MethodName", "RoBERTa"], [19, 20, "MethodName", "BERT"], [27, 28, "MethodName", "Longformer"]]}
{"text": "We introduce NLQuAD , a non - factoid long question answering data set from BBC news articles .", "entities": [[9, 11, "TaskName", "question answering"]]}
{"text": "We propose to use Intersection over Union ( IoU ) as an evaluation metric for long question answering .", "entities": [[8, 9, "MetricName", "IoU"], [16, 18, "TaskName", "question answering"]]}
{"text": "To establish a baseline performance , we experimented with the BERT , RoBERTa , and Longformer question answering models .", "entities": [[10, 11, "MethodName", "BERT"], [12, 13, "MethodName", "RoBERTa"], [15, 16, "MethodName", "Longformer"], [16, 18, "TaskName", "question answering"]]}
{"text": "We hope NLQuAD will inspire more research in the area of document - level language understanding and question answering .", "entities": [[17, 19, "TaskName", "question answering"]]}
{"text": "To do so , we extend the POLAR - framework that transforms word embeddings to interpretable counterparts and apply it to word - emoji embeddings trained on four years of messaging data from the Jodel social network .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "We devise a crowdsourced human judgement experiment to study six usecases , evaluating against words only , what role emoji can play in adding interpretability to word embeddings .", "entities": [[26, 28, "TaskName", "word embeddings"]]}
{"text": "Word embeddings create a vector - space representation in which words with a similar meaning are in close proximity .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "Yet , emoji are widely used in casual communication , e.g. , Online Social Networks ( OSN ) , and are known to extend textual expressiveness , demonstrated to benefit , e.g. , sentiment analysis ( Novak et al , 2015 ;", "entities": [[33, 35, "TaskName", "sentiment analysis"]]}
{"text": "We raise the question if we can leverage the expressiveness of emoji to make word embeddings - and thus also emoji - interpretable .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "Motivated and based upon POLAR ( Mathew et al , 2020 ) , we deploy a revised variant POLAR \u03c1 that transforms arbitrary word embeddings into interpretable counterparts .", "entities": [[23, 25, "TaskName", "word embeddings"]]}
{"text": "We design a crowdsourced human judgement experiment ( 4 ) to study if adding emoji to word embeddings and POLAR \u03c1 in particular increases the interpretability - while also answering how to describe emoji best .", "entities": [[16, 18, "TaskName", "word embeddings"]]}
{"text": "Our human judgement experiment involves six campaigns explaining Words ( W/ * ) or Emoji ( E/ * ) with Words , Figure 1 : The POLAR - framework ( Mathew et al , 2020 ) makes word embeddings interpretable leveraging polar opposites .", "entities": [[37, 39, "TaskName", "word embeddings"]]}
{"text": "It provides a new interpretable embedding subspace with systematic polar opposite scales : Along six use - cases , we evaluate which role emoji expressiveness plays in adding interpretability to word embeddings .", "entities": [[30, 32, "TaskName", "word embeddings"]]}
{"text": "That is , starting with a corpus and its vocabulary V , a word embedding created by an algorithm a ( e.g. , Word2Vec or GloVe ) assigns vectors \u2212 W a v R d on d dimensions to all words v V according to an optimization function ( usually word co - occurrence ) .", "entities": [[25, 26, "MethodName", "GloVe"]]}
{"text": "Extremal Word Score ( EWSO ) .", "entities": [[2, 3, "MetricName", "Score"]]}
{"text": "We next propose an approach to improve the interpretability of word embeddings by adding emoji .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "It uses our extended version POLAR \u03c1 and adds emoji to the POLAR space by creating word embeddings that include emoji .", "entities": [[16, 18, "TaskName", "word embeddings"]]}
{"text": "Ethics .", "entities": [[0, 1, "DatasetName", "Ethics"]]}
{"text": "Prior work showed that the interpretation of emoji varies ( Miller et al , 2016 ; Kimura - Thollander and Kumar , 2019 ) , also between cultures ( Guntuku et al , 2019 ; Gupta et al , 2021 ) .", "entities": [[20, 21, "DatasetName", "Kumar"]]}
{"text": "Interpretable word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "Word embeddings are a common approach to capture meaning ; they are a learned vector space representation of text that carries semantic relationships as distances between the embedded words .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "A rich body of work aims at making word embeddings interpretable , e.g. , via contextual ( Subramanian et al , 2018 ) , sparse embeddings ( Panigrahi et al , 2019 ) , or learned ( Senel et al , 2018 transformations ( Mathew et al , 2020 ) - all focus on text only .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "The POLAR approach is similar to SEMCAT ( Senel et al , 2018 ) , but is based on the concept of semantic differentials ( Osgood et al , 1957 ) for creating a polar subspace .", "entities": [[6, 7, "DatasetName", "SEMCAT"]]}
{"text": "Few works focused on using word embeddings for creating emoji representations , e.g. , ( Eisner et al , 2016 ) or ( Reelfs et al , 2020 ) .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "Yet , the general question if the interpretability of word embeddings can be improved by adding emoji and if different meaning of emoji can be captured remains still open .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "In this work , we adapt the POLAR interpretability approach to emoji and study in a human subject experiment if word embeddings can be made interpretable by adding emoji and how emoji can be interpretated by emoji .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "We raise the question whether we can leverage the expressiveness of emoji to make word embeddings interpretable .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "Thus , we use the POLAR framework ( Mathew et al , 2020 ) that creates interpretable word embeddings through semantic differentials , polar opposites .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "We employ a revised POLAR \u03c1 method that transforms arbitrary word embeddings to interpretable counterparts to which we added emoji .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "We thank Felix Dommes , who was instrumental for this work by developing and implementing the POLAR \u03c1 projection approach and the Extremal Word Score in his Master Thesis .", "entities": [[24, 25, "MetricName", "Score"]]}
{"text": "Dual Transfer for Very Low Resource Supervised Machine Translation", "entities": [[7, 9, "TaskName", "Machine Translation"]]}
{"text": "This paper describes the NoahNMT system submitted to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .", "entities": [[18, 20, "TaskName", "Machine Translation"]]}
{"text": "The system is a standard Transformer model equipped with our recent technique of dual transfer .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "It also employs widely used techniques that are known to be helpful for neural machine translation , including iterative backtranslation , selected finetuning , and ensemble .", "entities": [[14, 16, "TaskName", "machine translation"]]}
{"text": "The final submission achieves the top BLEU for three translation directions .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "The shared task features both unsupervised machine translation and very low resource supervised machine translation .", "entities": [[5, 8, "TaskName", "unsupervised machine translation"], [13, 15, "TaskName", "machine translation"]]}
{"text": "As our core technique is mainly suitable for low resource supervised machine translation , we participated in four translation directions between Chuvash - Russian ( chv - ru ) and Upper Sorbian - German ( hsb - de ) .", "entities": [[11, 13, "TaskName", "machine translation"]]}
{"text": "Our core technique is called dual transfer ( Zhang et al , 2021 ) , which belongs to the family of transfer learning .", "entities": [[21, 23, "TaskName", "transfer learning"]]}
{"text": "It transfers from both high resource neural machine translation model and pretrained language model to improve the quality of low resource machine translation .", "entities": [[7, 9, "TaskName", "machine translation"], [21, 23, "TaskName", "machine translation"]]}
{"text": "During the preparation for the shared task , we conducted additional experiments that supplement the original paper , including the choice of parent language , the validation of Transformer big model , and the usage of dual transfer along with iterative back - translation .", "entities": [[28, 29, "MethodName", "Transformer"]]}
{"text": "Our final submission achieves the top BLEU on the blind test sets for three translation directions : chv ru , ru chv , and hsb de .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "In this shared task , when training the high resource translation model , we always initialize the shared language side with the pretrained language model BERT ( Devlin et al , 2019 ) .", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "Each language was encoded with byte pair encoding ( BPE ) ( Sennrich et al , 2016b ) .", "entities": [[5, 8, "MethodName", "byte pair encoding"], [9, 10, "MethodName", "BPE"]]}
{"text": "The BPE codes and vocabularies were learned on each language 's monolingual data , and then used to segment parallel data .", "entities": [[1, 2, "MethodName", "BPE"]]}
{"text": "After BPE segmentation , we discarded sentences with more than 128 subwords , and cleaned parallel data with length ratio 1.5 .", "entities": [[1, 2, "MethodName", "BPE"]]}
{"text": "We use Transformer ( Vaswani et al , 2017 ) as our translation model , but with slight modifications that follow the implementation of BERT 5 .", "entities": [[2, 3, "MethodName", "Transformer"], [24, 25, "MethodName", "BERT"]]}
{"text": "The absolute position embeddings are also learned as in BERT .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "We use LazyAdam as the optimizer .", "entities": [[5, 6, "HyperparameterName", "optimizer"]]}
{"text": "Hyperparameters for BERT are the same as in the original paper ( Zhang et al , 2021 ) .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "The checkpoint with the highest validation BLEU is kept .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "We conducted an experiment with Transformer base .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "Considering that we plan to use Transformer big for which data amount is likely to play a more important role , we decided to use English as the parent language for Chuvash .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "The original paper ( Zhang et al , 2021 ) evaluated dual transfer only with Transformer base .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "In this shared task , we scale up to Transformer big .", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "Results in Table 3 show that Transformer big brings consistent improvements .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "We also report the runtime of each step in dual transfer for NMT chv ru with Transformer big in Table 4 for reference , but the numbers can vary depending on implementation and data size .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "In the following experiments and our final submission , we use Transformer big models .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "The best BLEU scores are attained with two or three iterations .", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "We finetune five times with different random seeds for model ensemble .", "entities": [[7, 8, "DatasetName", "seeds"]]}
{"text": "In this paper , we describe a series of experiments that contribute to our submission to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .", "entities": [[26, 28, "TaskName", "Machine Translation"]]}
{"text": "Towards Generative Aspect - Based Sentiment Analysis *", "entities": [[2, 7, "TaskName", "Aspect - Based Sentiment Analysis"]]}
{"text": "Aspect - based sentiment analysis ( ABSA ) has received increasing attention recently .", "entities": [[0, 5, "TaskName", "Aspect - based sentiment analysis"]]}
{"text": "Two types of paradigms , namely annotation - style and extraction - style modeling , are designed to enable the training process by formulating each ABSA task as a text generation problem .", "entities": [[29, 31, "TaskName", "text generation"]]}
{"text": "Aspect - based sentiment analysis ( ABSA ) , aiming at mining fine - grained opinion information towards specific aspects , has attracted increasing attention in recent years ( Liu , 2012 ) .", "entities": [[0, 5, "TaskName", "Aspect - based sentiment analysis"]]}
{"text": "Motivated by recent success in formulating sev - eral language understanding problems such as named entity recognition , question answering , and text classification as generation tasks ( Raffel et al , 2020 ; Athiwaratkun et al , 2020 ) , we propose to tackle various ABSA problems in a unified generative approach in this paper .", "entities": [[14, 17, "TaskName", "named entity recognition"], [18, 20, "TaskName", "question answering"], [22, 24, "TaskName", "text classification"]]}
{"text": "In order to enable the Generative Aspect - based Sentiment analysis ( GAS ) , we tailor - make two paradigms , namely annotation - style and extractionstyle modeling to transform the original task as a generation problem .", "entities": [[6, 11, "TaskName", "Aspect - based Sentiment analysis"]]}
{"text": "We investigate four ABSA tasks including Aspect Opinion Pair Extraction ( AOPE ) , Unified ABSA ( UABSA ) , Aspect Sentiment Triplet Extraction ( ASTE ) , and Target Aspect Sentiment Detection ( TASD ) with the proposed unified GAS framework to verify its effectiveness and generality .", "entities": [[20, 24, "TaskName", "Aspect Sentiment Triplet Extraction"]]}
{"text": "Similarly , we replace each aspect term as [ aspect | sentiment polarity ] under the annotation - style formulation and treat the desired pairs as the target output in the extraction - style paradigm to reformulate the UABSA task as a text generation problem .", "entities": [[42, 44, "TaskName", "text generation"]]}
{"text": "Given the input sentence x , we generate a target sequence y , which is either based on the annotationstyle or extraction - style paradigm as described in the last section , with a text generation model f ( ) .", "entities": [[34, 36, "TaskName", "text generation"]]}
{"text": "We adopt the pre - trained T5 model ( Raffel et al , 2020 ) as the generation model f ( ) , which closely follows the encoder - decoder architecture of the original Transformer ( Vaswani et al , 2017 ) .", "entities": [[6, 7, "MethodName", "T5"], [34, 35, "MethodName", "Transformer"]]}
{"text": "Therefore , by formulating these ABSA tasks as a text generation problem , we can tackle them in a unified sequence - to - sequence framework without taskspecific model design .", "entities": [[9, 11, "TaskName", "text generation"]]}
{"text": "We adopt F1 scores as the main evaluation metrics for all tasks .", "entities": [[2, 3, "MetricName", "F1"]]}
{"text": "We adopt the T5 base model from huggingface Transformer library 2 for 2 https://github.com/huggingface/ transformers", "entities": [[3, 4, "MethodName", "T5"], [8, 9, "MethodName", "Transformer"]]}
{"text": "Baseline ( Brun and Nikoulina , 2018 ) - 38.10 TAS - LPM - CRF ( Wan et al , 2020 ) 54.76 64.66 TAS - SW - CRF ( Wan et al , 2020 ) 57.51 65.89 TAS - SW - TO ( Wan et al , 2020 ) 58.09 65.44 all experiments .", "entities": [[12, 13, "MethodName", "LPM"], [14, 15, "MethodName", "CRF"], [28, 29, "MethodName", "CRF"]]}
{"text": "T5 closely follows the original encoder - decoder architecture of the Transformer model , with some slight differences such as different position embedding schemes .", "entities": [[0, 1, "MethodName", "T5"], [11, 12, "MethodName", "Transformer"]]}
{"text": "Therefore , the encoder and decoder of it have similar parameter size as the BERT - BASE model .", "entities": [[14, 15, "MethodName", "BERT"], [16, 17, "MethodName", "BASE"]]}
{"text": "For example , the method fixes \" Bbq rib \" to \" BBQ rib \" ( # 1 ) and \" repeat \" to \" repeats \" ( # 2 ) .", "entities": [[7, 8, "DatasetName", "Bbq"], [12, 13, "DatasetName", "BBQ"]]}
{"text": "Our work is an initial attempt on transforming ABSA tasks , which are typically treated as classification problems , into text generation problems .", "entities": [[20, 22, "TaskName", "text generation"]]}
{"text": "State - of - the - art dialogue models still often stumble with regards to factual accuracy and self - contradiction .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "In particular , they are likely to take on the role of their interlocutor ; for example , if an agent 's partner says they are a software engineer , the agent is likely to say it is a software engineer too ( Roller et al , 2021 ) , or worse , appropriate their partners just told tale of a trip to NAACL as their own .", "entities": [[20, 21, "DatasetName", "agent"], [31, 32, "DatasetName", "agent"]]}
{"text": "These include multi - objective training , unlikelihood training , classifier - assisted re - ranking based generation , and several forms modifying the attention mechanisms of the decoder in a sequence to sequence model .", "entities": [[31, 34, "MethodName", "sequence to sequence"]]}
{"text": "A common paradigm in the state of the art of open - domain dialogue involves concatenating all relevant contextual information as input to a sequence to sequence neural model ( e.g. , transformers ( Vaswani et al , 2017 ) ) to obtain a conditioned response .", "entities": [[24, 27, "MethodName", "sequence to sequence"]]}
{"text": "To quantify the character identity problem , we take a state - of - the - art dialogue agent ( specifically , BlenderBot ( Roller et al , 2021 ) )", "entities": [[18, 19, "DatasetName", "agent"]]}
{"text": "fine - tuned on the LIGHT dialogue dataset and ask human annotators if the agent mistakes its identity based on its utterances in context .", "entities": [[14, 15, "DatasetName", "agent"]]}
{"text": "The agent conditions its response on the LIGHT context and prior utterances in the dialogue history .", "entities": [[1, 2, "DatasetName", "agent"]]}
{"text": "BlenderBot uses a Byte - Level BPE tokenizer ( Radford et al , 2019 ) ; an artifact from the Blender - Bot pre - training is that it only considers 128 such tokens in the past , and thus has no mechanism for recovering truncated information about the LIGHT context in later conversational turns .", "entities": [[6, 7, "MethodName", "BPE"], [20, 21, "MethodName", "Blender"]]}
{"text": "Our second baseline lengthens the input context to 1024 BPE tokens , which allows the entire context for every example to fit into the truncation length of the model ; we follow methods employed in to extend the positional embeddings of the model .", "entities": [[9, 10, "MethodName", "BPE"]]}
{"text": "We first define a metric , role - playing accuracy ( RPA ) , to denote how often a model 's responses are \" in - character \" ; by this , we mean how often the model 's response could feasibly be said by their character , given their assigned character identity .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "In this section we describe several strategies for improving the role - playing accuracy of dialogue agents , specifically ways to improve our transformer baselines .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "We can employ an RPA classifier in response generation by using it to rank candidate model outputs .", "entities": [[7, 9, "TaskName", "response generation"]]}
{"text": "We can control these hyperparameters to explore the speed vs. accuracy trade - off .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "We explore utilizing an unlikelihood ( UL ) loss While training on the LIGHT dataset with standard NLL loss , with some fixed probability we consider a candidate model generation for UL loss .", "entities": [[8, 9, "MetricName", "loss"], [17, 18, "MetricName", "NLL"], [18, 19, "MetricName", "loss"], [32, 33, "MetricName", "loss"]]}
{"text": "We apply UL loss to tokens that yield the wrong character classification .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "However , the RPA classifier models are trained explicitly for this task , whereas the seq2seq models are trained only to generate a plausible continuation of a dialogue history .", "entities": [[15, 16, "MethodName", "seq2seq"]]}
{"text": "M O additional transformer layers , where we vary n M O { 0 , 2 } .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "After initializing the model weights with those trained on the LIGHT response generation task , we then train only the extra layers with only the character classification objective ; once the classifier achieves suitable performance on the task , we can begin to back - propagate the character classification objective multi - tasking with the dialogue task itself to the generation model directly , in the hope that the model learns to update its internal representations of the context and/or the decoded response .", "entities": [[11, 13, "TaskName", "response generation"]]}
{"text": "Profile Grounding Inspired by models demonstrating good performance in knowledge - grounded dialogue ( Zheng and Zhou , 2019 ; Ye et al , 2020 ; Prabhumoye et al , 2021 ; Wang et al , 2019 ) , we propose a simple extension to the transformer seq2seq architecture , specifically the decoder , to ensure the model knows to condition on the pro - file .", "entities": [[2, 3, "DatasetName", "Inspired"], [47, 48, "MethodName", "seq2seq"]]}
{"text": "The standard transformer decoder first uses self - attention over the decoded response , and then cross - attention over the encoder outputs .", "entities": [[2, 4, "MethodName", "transformer decoder"]]}
{"text": "Specifically , we feed the context through a linear projection layer followed by a softmax to select the top - k tokens .", "entities": [[14, 15, "MethodName", "softmax"]]}
{"text": "We experiment with either 0 , 4 , or All prior context utterances .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "We next train baseline models for the dialogue generation task itself .", "entities": [[7, 9, "TaskName", "dialogue generation"]]}
{"text": "Automated results show a slight bump in F1 on the LIGHT valid set , and indeed a bump in RPA .", "entities": [[7, 8, "MetricName", "F1"]]}
{"text": "The RPA UL methods suffer compared to the baselines in terms of PPL and F1 , yet they retain similar RPA metrics .", "entities": [[14, 15, "MetricName", "F1"]]}
{"text": "We hypothesize that while the UL loss can adjust the model to refrain from generating outof - character responses , there are still far too many other tokens that may yield similar outcomes that are not penalized .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "hits@1 for the best model ) , this does not translate to substantial RPA improvements over the baseline .", "entities": [[0, 1, "MetricName", "hits@1"]]}
{"text": "Each model was pre - trained on 1.5B training examples from pushshift.io Reddit ( Baumgartner et al , 2020 ) , with BlenderBot additionally fine - tuned on the BST tasks ( see Roller et al ( 2021 ) for more details ) , before training on LIGHT .", "entities": [[12, 13, "DatasetName", "Reddit"]]}
{"text": "For the multiobjective models , we used the same loss ( and negative - sampling ) setup as the RPA classifiers for the character accuracy objective .", "entities": [[9, 10, "MetricName", "loss"], [24, 25, "MetricName", "accuracy"]]}
{"text": "We experiment with either 0 , 4 , or N \u2212 2 prior utterances ( dubbed \" All \" in relevant tables ) , where N is the total number of utterances ( N \u2212 2 allows the last turn for each speaker to be a candidate utterance ) .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "For any value 0 <", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "d i with N utterances { u 0 , u 1 , ... , u N } .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "i , ... , u i+n } \u2200 0 \u2264 i \u2264", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "For the baseline and re - ranker models , beam search yields the highest F1 scores ; RPA can be improved with the other inference methods when combined with a re - ranker .", "entities": [[14, 15, "MetricName", "F1"]]}
{"text": "Perplexity perplexity appears to be positively correlated with mistaken identity , and negatively correlated with engagingness .", "entities": [[0, 1, "MetricName", "Perplexity"], [1, 2, "MetricName", "perplexity"]]}
{"text": "So , perplexity is a good indicator of how fluent and engaging the model is in conversation , and can indirectly point to a better understanding of the role - playing task .", "entities": [[2, 3, "MetricName", "perplexity"]]}
{"text": "F1 F1 word overlap is positively correlated with engagingness as well , so F1 may be a good proxy of model performance .", "entities": [[0, 1, "MetricName", "F1"], [1, 2, "MetricName", "F1"], [13, 14, "MetricName", "F1"]]}
{"text": "Correlation with mistaken identity is negative here , implying that better F1 corresponds with better role - playing ability .", "entities": [[11, 12, "MetricName", "F1"]]}
{"text": "Human The human outputs are most often correct on the first turn , with gradual decay of accuracy throughout the conversation ( according to RPA ) .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "The vanilla baseline suffers a pretty dramatic drop off after the first couple of turns ; the long - context model achieves slightly higher character accuracy overall", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "Dynamic Sentence Boundary Detection for Simultaneous Translation", "entities": [[2, 4, "TaskName", "Boundary Detection"], [6, 7, "TaskName", "Translation"]]}
{"text": "Simultaneous Translation is a great challenge in which translation starts before the source sentence finished .", "entities": [[1, 2, "TaskName", "Translation"]]}
{"text": "In this paper , we propose a novel method for sentence boundary detection that takes it as a multi - class classification task under the endto - end pre - training framework .", "entities": [[11, 13, "TaskName", "boundary detection"], [18, 22, "TaskName", "multi - class classification"]]}
{"text": "Simultaneous Translation aims to translate the speech of a source language into a target language as quickly as possible without interrupting the speaker .", "entities": [[1, 2, "TaskName", "Translation"]]}
{"text": "Typically , a simultaneous translation system is comprised of an auto - speech - recognition ( ASR ) model and a machine translation ( MT ) model .", "entities": [[12, 15, "TaskName", "speech - recognition"], [21, 23, "TaskName", "machine translation"]]}
{"text": "Therefore , sentence boundary detection ( or sentence segmentation ) 1 plays an important role to narrow the gap between the ASR and transcription .", "entities": [[3, 5, "TaskName", "boundary detection"], [7, 9, "TaskName", "sentence segmentation"]]}
{"text": "Studies of sentence segmentation falls into one of the following two bins : The strategy performs segmentation from a speech perspective .", "entities": [[2, 4, "TaskName", "sentence segmentation"]]}
{"text": "F\u00fcgen et al ( 2007 ) and Bangalore et al ( 2012 ) used prosodic pauses in speech recognition as segmentation boundaries .", "entities": [[17, 19, "TaskName", "speech recognition"]]}
{"text": "According to Venuti ( 2012 ) , silence - based chunking accounts for only 6.6 % , 10 % , and 17.1 % in English , French , and German , respectively .", "entities": [[10, 11, "TaskName", "chunking"]]}
{"text": "The studies considered the problem as classification or sequence labeling , based on SVM , conditional random filed ( CRFs ) ( Lu and Ng , 2010 ; Wang et al , 2012 ; Ueffing et al , 2013 ) .", "entities": [[13, 14, "MethodName", "SVM"]]}
{"text": "In this paper , we use classification to solve the problem of sentence segmentation from the perspective of text .", "entities": [[12, 14, "TaskName", "sentence segmentation"]]}
{"text": "Inspired by the recent pre - training techniques ( Devlin et al , 2019 ; Sun et al , 2019 ) that successfully used in many NLP tasks , we used a pre - trained model for initialization and fine - tune the model on the source side of the sentence .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Overall , the contributions are as follows : We propose a novel sentence segmentation method based on pre - trained language representations , which have been successfully used in various NLP tasks .", "entities": [[12, 14, "TaskName", "sentence segmentation"]]}
{"text": "Our method dynamically predicts the boundary at multiple locations , rather than a specific location , achieving high accuracy with low latency .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "Devlin et al ( 2019 ) proposed a generalized framework BERT , to learn language representations based on a deep Transformer ( Vaswani et al , 2017 ) encoder .", "entities": [[10, 11, "MethodName", "BERT"], [20, 21, "MethodName", "Transformer"]]}
{"text": "Rather than traditionally train a language model from - left - to - right or from - rightto - left , they proposed a masked language model ( MLM ) that randomly replace some tokens in a sequence by a placeholder ( mask ) and trained the model to predict the original tokens .", "entities": [[28, 29, "DatasetName", "MLM"]]}
{"text": "Given a streaming input x = { x 1 , ... , x t , ... , x T } , the task of sentence segmentation is to determine whether x t x is the end of a sentence .", "entities": [[24, 26, "TaskName", "sentence segmentation"]]}
{"text": "Thus we should limit the context size and make a decision dynamically .", "entities": [[5, 7, "HyperparameterName", "context size"]]}
{"text": "As the input is a word streaming , the sentence boundary detection problem can be transformed as , whether there exists a sentence boundary until the current word x t .", "entities": [[10, 12, "TaskName", "boundary detection"]]}
{"text": "We propose a multi - class classification model to predict the probability of a few words before x t as sentence boundaries ( Section 3.1 ) .", "entities": [[3, 7, "TaskName", "multi - class classification"]]}
{"text": "We use the ERNIE framework to first pre - train a language representation and then fine - tune it to sentence boundary detection ( Section 3.2 ) .", "entities": [[21, 23, "TaskName", "boundary detection"]]}
{"text": "The classes are as follows : 0 1 2 \u2026 \u22122 \u22121 \u2026 \u210e \u210e 0 1 2 \u2026 \u22122 \u22121 \u2026 \u2026 \u2026 \u03d5 0 \u2212 1 \u2212 2 Classes Masked Language Model \u2026 \u210e \u210e \u2026 \u210e \u210e \" . \"", "entities": [[6, 7, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [25, 26, "DatasetName", "0"]]}
{"text": "y = \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u03c6 , no sentence boundary detected 0 , x t is the end of a sentence \u22121 ,", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "If the output class is 0 , indicating that the current word x t is the end of a sentence and we put a period after the word .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Then we define the loss function as follows : J ( \u03b8 )", "entities": [[4, 5, "MetricName", "loss"], [11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "= ( x , r ) D log ( r\u22121 t=1 p ( y t = \u03c6 | x \u2264t ; \u03b8 ) + r+M t = r p ( y t = \u2212 ( t \u2212 r ) )", "entities": [[21, 22, "HyperparameterName", "\u03b8"]]}
{"text": "| x \u2264t ; \u03b8 ) )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "predicts whether a word x t labeled as the end of a sentence or not by a binary classification : p ( y t = 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "where y t = 0 means x t is not the end of a sentence and", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "The performance of these methods is limited by incomplete semantics , without considering global boundary detection .", "entities": [[14, 16, "TaskName", "boundary detection"]]}
{"text": "= 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "If more than one sentence boundary probabilities for x t\u2212M , ... , x t exceeds the threshold \u03b8 T h at the same time , we choose the front - most position as a sentence boundary .", "entities": [[18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "From another point of view , the strategy can also compensate for some incorrect suppression of adjacent boundaries , thereby improving online prediction accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "We use two parallel corpus from machine translation task : WMT 14 3 and IWSLT 14 4 .", "entities": [[6, 8, "TaskName", "machine translation"]]}
{"text": "And our sentence boundary detection model is trained on the source transcription of IWSLT 14 unless otherwise specified ( Section 4.3 ) .", "entities": [[3, 5, "TaskName", "boundary detection"]]}
{"text": "We evaluate our model and two existing methods listed below : dynamic - base is our proposed method that detect sentence boundaries dynamically using a multi - class classification .", "entities": [[25, 29, "TaskName", "multi - class classification"]]}
{"text": "T - LSTM uses a RNN - based classification model with two classes .", "entities": [[2, 3, "MethodName", "LSTM"]]}
{"text": "In the fine - tuning stage , we use a learning rate of 2e \u22125 .", "entities": [[10, 12, "HyperparameterName", "learning rate"]]}
{"text": "Table 2 reports the results of source sentence segmentation on En - De translation , where the latency is measured by Consecutive Wait ( CW ) ( Gu et al , 2017 ) , the number of words between two translate actions .", "entities": [[7, 9, "TaskName", "sentence segmentation"]]}
{"text": "Better performance expect high F - score , BLEU , and low latency ( CW ) .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "The translation effect obtained by using the groundtruth period as the sentence segmentation is shown in the first line of Oracle .", "entities": [[11, 13, "TaskName", "sentence segmentation"]]}
{"text": "The N - gram method calculate the probability of add ( p add ) and not add ( p not ) period at each position , and decide whether to chunk by comparing whether p add /p not exceeds \u03b8 T h .", "entities": [[39, 40, "HyperparameterName", "\u03b8"]]}
{"text": "Accurate sentence segmentation brings better performance in translation , bringing an improvement of 1.55 over T - LSTM .", "entities": [[1, 3, "TaskName", "sentence segmentation"], [17, 18, "MethodName", "LSTM"]]}
{"text": "It is interesting to note that , dynamic - force performs better than dynamic - base , in terms of latency and BLEU .", "entities": [[22, 23, "MetricName", "BLEU"]]}
{"text": "This suggests the effectiveness of the force segmentation strategy , that is , select the chunking location with a sentence length limitation will not affect the accuracy of segmentation , and would enhance the translation effect .", "entities": [[15, 16, "TaskName", "chunking"], [26, 27, "MetricName", "accuracy"]]}
{"text": "Surprisingly , the Sort approach is prominent in both segmentation accuracy and translation performance .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "This may be due to the following reasons : 1 ) Sentence classification is not a difficult task , especially when M = 1 for 3 - class classification ( y [ \u03c6 , 0 , \u22121 ] ) , making the task easy to over - fit . 2 )", "entities": [[11, 13, "TaskName", "Sentence classification"], [34, 35, "DatasetName", "0"]]}
{"text": "Intuitively , larger dataset provides more diverse samples , but due to domain changes , it does not necessarily lead to improvements in accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "The performance of various models trained on WMT14 is shown in Table 3 .", "entities": [[7, 8, "DatasetName", "WMT14"]]}
{"text": "On the contrary , N - gram and T - LSTM is hardly affected .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "For T - LSTM , it even improves a little compared with its in - domain performance .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "0.19 M sentences of IWSLT2014 is insufficient to fit the parameters of T - LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "Next , we discuss the effect of changing \u03b8 .", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "Smaller \u03b8 l brings shorter latency , as well as worse performance .", "entities": [[1, 2, "HyperparameterName", "\u03b8"]]}
{"text": "Accordingly , we only use the samples shorter than a fixed value : \u03b8 l in training phrase .", "entities": [[13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "At inference time , we use both dynamic - force with the same sentence length constraint \u03b8 l and dynamic - base to predict sentence boundaries .", "entities": [[16, 17, "HyperparameterName", "\u03b8"]]}
{"text": "This demonstrates the main reason for the poor performance with small \u03b8 l is not the training - testing discrepancy but lies in the first reason that the force constraint is too harsh .", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "We experiment with M from 0 to 5 .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Sentence boundary detection has been explored for years , but the majority of these work focuses on offline punctuation restoration , instead of applied in simultaneous translation .", "entities": [[1, 3, "TaskName", "boundary detection"]]}
{"text": "The main deficiency of this method is that the dependencies outside the input window are lost , resulting in low accuracy .", "entities": [[20, 21, "MetricName", "accuracy"]]}
{"text": "Peitz et al ( 2011 ) and Cho et al ( 2012 ) treats this problem as a machine translation task , training to translate non - punctuated transcription into punctuated text .", "entities": [[18, 20, "TaskName", "machine translation"]]}
{"text": "In this paper , we propose an online sentence boundary detection approach .", "entities": [[9, 11, "TaskName", "boundary detection"]]}
{"text": "By adding this adjacent position constraint and using dynamic prediction , our method achieves higher accuracy with lower latency .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "Our experiments demonstrate robust scores through pretrained BERT ( Bidirectional Encoder Representations from Transformers ) embeddings for word representation , and more so the importance of manual features - once again highlighting the syntactic and semantic characteristics of the ellipsis phenomenon .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "For the classification decision , we notice that a simple Multilayar Perceptron ( MLP ) works well for the detection of ellipsis ; however , Recurrent Neural Networks ( RNN ) are a better choice for the much harder resolution step .", "entities": [[13, 14, "DatasetName", "MLP"]]}
{"text": "Computational approaches to the ellipsis phenomenon majorly focus on the Verb Phrase Ellipsis ( VPE ) along with a few related phenomenon such as gapping , sluicing and do - so anaphora , for instance , the detection of VPE in the Penn Treebank using pattern match ( Hardt , 1992 ) , a transformation learning - based approach to generated patterns for VPE resolution ( Hardt , 1998 ) , the domain independent VPE detection and resolution using machine learning ( Nielsen , 2003 ) , automatically parsed text ( Nielsen , 2004b ) , sentence trimming methods ( McShane et al , 2015 ) , linguistic principles ( McShane and Babkin , 2016 ) , improved parsing techniques that encode elided material dependencies for reconstruction of sentences containing gapping ( Schuster et al , 2018 ) , discriminative and margin infused algorithms", "entities": [[42, 44, "DatasetName", "Penn Treebank"]]}
{"text": "( Dean et al , 2016 ) , Multilayer Perceptrons ( MLP ) and Transformers ( Zhang et al , 2019 ) .", "entities": [[11, 12, "DatasetName", "MLP"]]}
{"text": "For the resolution process , we previously proposed a rule based system ( Khullar et al , 2019 ) that detects noun ellipsis using syntactic constraints on licensors of ellipsis and resolves them by matching Part - of - Speech ( POS ) tag similarity between the licensor of ellipsis and the modifier of the antecedent .", "entities": [[35, 38, "DatasetName", "Part - of"]]}
{"text": "We use the NoEl corpus ( Khullar et al , 2020 ) that marks noun ellipsis instances as a separate layer ( using the stand - off annotation scheme ) on the Cornell Movie Dialogs corpus ( Danescu - Niculescu - Mizil and Lee , 2011 ) .", "entities": [[32, 33, "DatasetName", "Cornell"]]}
{"text": "( l i , s ) \u2212 { 0 , 1 } where 1 denotes that l", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "i is a licensor in s , and 0 otherwise .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "We experiment with both static and contextualised word embeddings for word and context representation .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "For the former , we choose pretrained fastText ( FT ) word embeddings ( Bojanowski et al , 2016 ) as they provide representations for rare and unknown words that might be frequent in the movie dialogues .", "entities": [[7, 8, "MethodName", "fastText"], [11, 13, "TaskName", "word embeddings"]]}
{"text": "For the latter , we use pretrained BERT embeddings from the BERT base uncased wordpiece model for English ( Devlin et al , 2019 ) , as these currently offer the most powerful embeddings taking into account a large left and right context .", "entities": [[7, 8, "MethodName", "BERT"], [11, 12, "MethodName", "BERT"], [14, 15, "MethodName", "wordpiece"]]}
{"text": "fastText We take pretrained FT word embeddings for the noun modifier and sentence in which it is present and sum pool to obtain a single vector that we use to train our classifiers .", "entities": [[0, 1, "MethodName", "fastText"], [5, 7, "TaskName", "word embeddings"]]}
{"text": "For the statistical models , we choose Naive Bayes and Linear Support Vector Machine ( SVM ) , and use scikit learn ( Pedregosa et al , 2011 ) with 5 - fold cross validation for training and testing .", "entities": [[11, 14, "MethodName", "Support Vector Machine"], [15, 16, "MethodName", "SVM"]]}
{"text": "We choose a BERT We separate the sentence and the licensor with a [ SEP ] token and keep the sequence length to 300 as this is the maximum sentence length in the training data .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "After creating the concatenated set of tokens , if the number of tokens are greater than 300 , we clip it to 300 , otherwise we add [ PAD ] tokens which correspond to the embedding of 768 dimensional zero - vector .", "entities": [[28, 29, "DatasetName", "PAD"]]}
{"text": "The [ CLS ] output of the BERT model ( Devlin et al , 2019 ) is then fed into Naive Bayes , Linear SVM , MLP and bi - LSTM networks as above .", "entities": [[7, 8, "MethodName", "BERT"], [24, 25, "MethodName", "SVM"], [26, 27, "DatasetName", "MLP"], [30, 31, "MethodName", "LSTM"]]}
{"text": "the licensor l i from the detection step , and the antecedent candidate a j ; the noun ellipsis resolution task can be defined as follows : f ( a j , l i , s ) \u2212 { 0 , 1 } where 1 denotes that the antecedent candidate a j is the actual resolution of the ellipsis licensed by l i , and 0 otherwise .", "entities": [[39, 40, "DatasetName", "0"], [65, 66, "DatasetName", "0"]]}
{"text": "Embeddings Similar to the detection step , we take pretrained fastText word embeddings for the licensor , antecedent candidate and context , and sum pool to obtain a single vector .", "entities": [[10, 11, "MethodName", "fastText"], [11, 13, "TaskName", "word embeddings"]]}
{"text": "In case of BERT , we separate the sentence , the licensor and the antecedent candidate with a [ SEP ] token and follow the same steps as in the detection step .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "For MLP , we take a simple , two - layer feedforward network ( FFNN ) or two layers of multiple computational units interconnected in a feed - forward way without loops .", "entities": [[1, 2, "DatasetName", "MLP"], [11, 13, "MethodName", "feedforward network"]]}
{"text": "The network has a softmax output layer .", "entities": [[4, 5, "MethodName", "softmax"]]}
{"text": "For bi - LSTM , we have embedding layer , time - distributed translate layer , Bi - LSTM ( RNN ) layer , batch normalization layer , dropout layer and prediction layer .", "entities": [[3, 4, "MethodName", "LSTM"], [18, 19, "MethodName", "LSTM"], [24, 26, "MethodName", "batch normalization"]]}
{"text": "The activation used is Softmax .", "entities": [[4, 5, "MethodName", "Softmax"]]}
{"text": "The loss function is calculated with cross entropy .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "We train in batch sizes of 16 and early stopping with max epochs of 100 .", "entities": [[8, 10, "MethodName", "early stopping"]]}
{"text": "We use default values for the learning rate .", "entities": [[6, 8, "HyperparameterName", "learning rate"]]}
{"text": "Our experiments show that for the detection task , BERT embeddings with a simple MLP gives best scores .", "entities": [[9, 10, "MethodName", "BERT"], [14, 15, "DatasetName", "MLP"]]}
{"text": "This is expected because , BERT currently provides the most powerful contextual word representations , using 12 separate attention mechanism for each layer , where , at each layer , each token can focus on 12 distinct aspects of other tokens .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "al , 2017 ) use many distinct attention heads ( 12 * 12=144 for the base BERT model ) , each head can focus on a different kind of constituent combinations , making BERT broadly attending over the whole sentence .", "entities": [[16, 17, "MethodName", "BERT"], [33, 34, "MethodName", "BERT"]]}
{"text": "The even higher accuracy on the curated dataset can be explained by the nature of the sentences in this dataset which are from textbooks , and , hence , free of grammatical errors , etc . - resulting into improved parser performance in the pre - processing step .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "As with several other NLP tasks , the contextual nature of BERT is useful for noun ellipsis resolution too , making robust predictions with simple neural classifiers .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "Finally , addition of manual features boosts the performance of all classifiers including those that use BERT , highlighting that ellipsis is a syntactically constrained phenomenon .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "Moreover , we find that though the attention mechanism can help improve the performance for text classification in our experiments , it may focus on the irrelevant information .", "entities": [[15, 17, "TaskName", "text classification"]]}
{"text": "For example , in the sentence \" A very funny movie . \" , the long short - term memory model with standard attention ( LSTM - ATT ) infers a correct sentiment label while pays more attention to the irrelevant word \" movie \" , making the result difficult to explain .", "entities": [[15, 20, "MethodName", "long short - term memory"], [25, 26, "MethodName", "LSTM"]]}
{"text": "Specifically , we propose a framework consisting of a learner and a compressor , which enhances the performance and interpretability of the attention model for text classification 1 .", "entities": [[25, 27, "TaskName", "text classification"]]}
{"text": "To evaluate the effectiveness of our proposed approach , we adapt two advanced neural models ( LSTM and BERT ) within the framework and conduct experiments on eight benchmark datasets .", "entities": [[16, 17, "MethodName", "LSTM"], [18, 19, "MethodName", "BERT"]]}
{"text": "Li and Eisner ( 2019 ) compressed the pre - trained embedding ( e.g. , BERT , ELMO ) , remaining only the information that helps a discriminative parser through variational IB .", "entities": [[15, 16, "MethodName", "BERT"], [17, 18, "MethodName", "ELMO"]]}
{"text": "As far as we are aware , we are the first ones to leverage IB into attention mechanisms to train more interpretable attention with better accuracy .", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "The model is optimized by cross - entropy loss to learn the label - relevant information .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "Then , we calculate Z by compressing the text representation R that is the weighted sum of X based on the attention \u03b1 , while remaining the maximum information to judge Y by inputting Z into a MLP classifier for predicting .", "entities": [[22, 23, "HyperparameterName", "\u03b1"], [37, 38, "DatasetName", "MLP"]]}
{"text": "We adopt LSTM and BERT models as our encoder , and other models can also be applied to our framework .", "entities": [[2, 3, "MethodName", "LSTM"], [4, 5, "MethodName", "BERT"]]}
{"text": "i\"1 \u03b1", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "i \u03b1 i \" softmaxpv J", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "Finally , we input the text representation R into a multi - layer perceptron ( MLP ) to predict the probability .", "entities": [[15, 16, "DatasetName", "MLP"]]}
{"text": "The cross - entropy loss is used to optimize the model .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "The learner optimizes the sentence representations by minimizing the cross - entropy loss , which does not restrict the model to ignore the useless information .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "Specifically , we regard ppyq as constant and then minimize E p \u03b8 py , zq rlog q \u03c6 py |", "entities": [[12, 13, "HyperparameterName", "\u03b8"]]}
{"text": "Since we must first sample r to sample y , z from p \u03b8 pr , y , zq , the lower bound of IpZ ; Y q is computed as , IpZ ; Y q \u011b E ppr , yq rE p \u03b8 pz | rq rlog q \u03c6 py |", "entities": [[13, 14, "HyperparameterName", "\u03b8"], [43, 44, "HyperparameterName", "\u03b8"]]}
{"text": "The first component in L is to keep the most useful information in p \u03b8 pz | rq for inferring y , while the second one is to regularize p \u03b8 pz | rq with a predefined prior distribution", "entities": [[14, 15, "HyperparameterName", "\u03b8"], [30, 31, "HyperparameterName", "\u03b8"]]}
{"text": "In particular , two MLP are used to predict u and \u03c3 .", "entities": [[4, 5, "DatasetName", "MLP"]]}
{"text": "Finally , we input the z into a MLP to predict q \u03c6 py | zq and optimize the attention 's parameter via Equation 8 .", "entities": [[8, 9, "DatasetName", "MLP"]]}
{"text": "For LSTM - based models , we use GloVe embedding ( Pennington et al , 2014 ) with 300 - dimension to initialize the word embedding and fine - tune it during the training .", "entities": [[1, 2, "MethodName", "LSTM"], [8, 9, "MethodName", "GloVe"]]}
{"text": "For the BERT - based models , we fine - tune pre - trained BERT - base model .", "entities": [[2, 3, "MethodName", "BERT"], [14, 15, "MethodName", "BERT"]]}
{"text": "From these results , we find the following observations : 1 ) our models ( LSTM / BERT - VAT ) outperform all the corresponding baselines over all the eight datasets , which denotes the effectiveness of our VAT on both LSTM and BERT - based models ; 2 ) compared with attention - based models ( LSTM / BERT - ATT ) , our models obtain better results .", "entities": [[15, 16, "MethodName", "LSTM"], [17, 18, "MethodName", "BERT"], [41, 42, "MethodName", "LSTM"], [43, 44, "MethodName", "BERT"], [57, 58, "MethodName", "LSTM"], [59, 60, "MethodName", "BERT"]]}
{"text": "Furthermore , we visualize the sentence representations obtained from LSTM / BERT - ATT and - VAT models ( Figure 3 ) .", "entities": [[9, 10, "MethodName", "LSTM"], [11, 12, "MethodName", "BERT"]]}
{"text": "For example , it is hard to split the positive samples from the negative ones based on the representations obtained from LSTM - ATT for the IMDB dataset , while the divider line based on our VAT is clear .", "entities": [[21, 22, "MethodName", "LSTM"], [26, 27, "DatasetName", "IMDB"]]}
{"text": "Note that well - trained LSTM / BERT - base is used for evaluating the performance of classification . AOPC .", "entities": [[5, 6, "MethodName", "LSTM"], [7, 8, "MethodName", "BERT"]]}
{"text": "It calculates the average change of accuracy over test data by deleting top K words via attentive weights .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "From the results , we observe that : 1 ) basic attention - based models ( LSTM / BERT - ATT ) can find the important words in the sentence to some extent .", "entities": [[16, 17, "MethodName", "LSTM"], [18, 19, "MethodName", "BERT"]]}
{"text": "Comparing with random ( Random ) , LSTM / BERT - ATT obtains significant improvement ; 2 ) Our models ( LSTM / BERT - VAT ) outperform the standard attention - based models .", "entities": [[7, 8, "MethodName", "LSTM"], [9, 10, "MethodName", "BERT"], [21, 22, "MethodName", "LSTM"], [23, 24, "MethodName", "BERT"]]}
{"text": "It indicates that integrating VIB into the attention mechanism can help improve the interpretability of the models by filtering the useless information ; 3 ) BERT model is sensitive to the context ; deleting the words will destroy the semantic information of the sentence and significantly affect the model 's performance .", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "Intuitively , the more words we delete , the larger accuracy the models reduce .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "Post - hoc Accuracy .", "entities": [[3, 4, "MetricName", "Accuracy"]]}
{"text": "Additionally , we obtain comparable results with only five words for SST - 1 , SST - 2 , and Twitter datasets .", "entities": [[11, 12, "DatasetName", "SST"], [15, 16, "DatasetName", "SST"]]}
{"text": "Second , for BERT - based models , the context words are also important for classification even though they may not be task - specific .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "The LSTM - base model with top - 10 words selected by our LSTM - VAT model can achieve comparable results with the original samples in most cases .", "entities": [[1, 2, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "We use the SemEval 2013 Twitter dataset , which contains word - level sentiment annotation .", "entities": [[3, 5, "DatasetName", "SemEval 2013"]]}
{"text": "Also , LSTM - based models outperform BERT - based models for this task in most cases .", "entities": [[2, 3, "MethodName", "LSTM"], [7, 8, "MethodName", "BERT"]]}
{"text": "It is because that BERT learns much semantic information from the text , and context information plays a vital role in prediction .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "Positive Negative P@1 R@1 F1@1", "entities": [[2, 3, "MetricName", "P@1"], [3, 4, "MetricName", "R@1"]]}
{"text": "P@5 R@5", "entities": [[1, 2, "MetricName", "R@5"]]}
{"text": "F1@5 P@1 R@1", "entities": [[1, 2, "MetricName", "P@1"], [2, 3, "MetricName", "R@1"]]}
{"text": "To understand why our proposed VAT model is more effective than the standard attention - based model , we visualize two examples of LSTM - based models using attention heatmaps ( Figure 7 ) .", "entities": [[23, 24, "MethodName", "LSTM"]]}
{"text": "First , the standard attention - based LSTM model focuses on the wrong words ( e.g. , \" this \" , \" work \" ) even though it predicts the right sentiment while our VAT model finds the correct words ( e.g. , \" admired \" , \" lot \" ) .", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "The experimental results on eight benchmark datasets for text classification verify the effectiveness of our models within this framework .", "entities": [[8, 10, "TaskName", "text classification"]]}
{"text": "In the future , we will investigate the effectiveness of our proposed attention framework for other tasks and areas , such as machine translation and visual question answering .", "entities": [[22, 24, "TaskName", "machine translation"], [25, 28, "DatasetName", "visual question answering"]]}
{"text": "Current state - of - the - art machine translation systems are based on encoder - decoder architectures , that first encode the input sequence , and then generate an output sequence based on the input encoding .", "entities": [[8, 10, "TaskName", "machine translation"]]}
{"text": "Deep neural networks have made a profound impact on natural language processing technology in general , and machine translation in particular ( Blunsom , 2013 ; Cho et al , 2014 ; Jean et al , 2015 ; LeCun et al , 2015 ) .", "entities": [[17, 19, "TaskName", "machine translation"]]}
{"text": "Machine translation ( MT ) can be seen as a sequenceto - sequence prediction problem , where the source and target sequences are of different and variable length .", "entities": [[0, 2, "TaskName", "Machine translation"]]}
{"text": "The predominant neural architectures in machine translation are recurrent encoder - decoder networks ( Graves , 2012 ; Cho et al , 2014 ) .", "entities": [[5, 7, "TaskName", "machine translation"]]}
{"text": "Gehring et al ( 2017b ) outperformed deep LSTMs for machine translation 1D CNNs with gated linear units ( Meng et al , 2015 ; Oord et al , 2016c ; Dauphin et al , 2017 ) in both the encoder and decoder modules .", "entities": [[10, 12, "TaskName", "machine translation"]]}
{"text": "treat the sentence alignment as a latent variable which they infer using a variational inference network during training to optimize a variational lower - bound on the log - likelihood .", "entities": [[13, 15, "MethodName", "variational inference"], [27, 30, "MetricName", "log - likelihood"]]}
{"text": "proposed a 2D LSTM model similar to our 2D CNN for machine translation .", "entities": [[3, 4, "MethodName", "LSTM"], [11, 13, "TaskName", "machine translation"]]}
{"text": "In a second LSTM stream , each cell takes input from its left and top neighbor , as well as from the corresponding cell in the first stream .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "They do not use masked convolutions , since their CNN is used to predict if a given sourcetarget pair is a human or machine translation .", "entities": [[23, 25, "TaskName", "machine translation"]]}
{"text": "The word embeddings { x 1 , . . .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": ", y | t | } are then concatenated to form a 3D tensor X R | t | \u00d7 | s | \u00d7f 0 , with f 0", "entities": [[24, 25, "DatasetName", "0"], [28, 29, "DatasetName", "0"]]}
{"text": "We use the DenseNet convolutional architecture , which is the state of the art for image classification tasks .", "entities": [[3, 4, "MethodName", "DenseNet"], [15, 17, "TaskName", "image classification"]]}
{"text": "Each layer first batch - normalizes ( Ioffe and Szegedy , 2015 ) its input and apply a ReLU ( Nair and Hinton , 2010 )", "entities": [[18, 19, "MethodName", "ReLU"]]}
{"text": "To reduce the computation cost , each layer first computes 4 g channels using a 1\u00d71 convolution from the f 0 + ( l \u2212 1 )", "entities": [[16, 17, "MethodName", "convolution"], [20, 21, "DatasetName", "0"]]}
{"text": "This is followed by a second batch - normalization and ReLU non - linearity .", "entities": [[10, 11, "MethodName", "ReLU"]]}
{"text": "The second convolution has ( k \u00d7 k 2 ) kernels , i.e. masked as illustrated in Figure 1 , and generates the g output features maps to which we apply dropout ( Srivastava et al , 2014 ) .", "entities": [[2, 3, "MethodName", "convolution"]]}
{"text": "Starting from the initial f 0 feature maps , each layer l { 1 , . . .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": ", L } of our DenseNet produces a tensor H l of size | t | \u00d7 | s | \u00d7 f l , where f l is the number of output channels of that layer .", "entities": [[5, 6, "MethodName", "DenseNet"]]}
{"text": "Thus the probability distribution over V for the i - th output token is obtained as p i = SoftMax ( EH pool i ) .", "entities": [[19, 20, "MethodName", "SoftMax"]]}
{"text": "This reduces the number of parameters and generally improves the performance .", "entities": [[3, 6, "HyperparameterName", "number of parameters"]]}
{"text": "The energy that enters into the softmax to predict token w V for the i - th output position is given by e iw = d { 1 , ... , f L } E wd H pool i d ( 5 ) = j { 1 , ... , | s | } d B ij E wd H L ijd .", "entities": [[6, 7, "MethodName", "softmax"]]}
{"text": "As we will show experimentally in the next section , visualizing the values \u03b1 ij for the groundtruth output tokens , we can recover an implicit sentence alignment used by the model .", "entities": [[13, 14, "HyperparameterName", "\u03b1"]]}
{"text": "i = SoftMax H L i w", "entities": [[2, 3, "MethodName", "SoftMax"]]}
{"text": "For open - vocabulary translation , we segment sequences using joint byte pair encoding ( Sennrich et al , 2016 ) with 14 K merge operations on the concatenation of source and target languages .", "entities": [[11, 14, "MethodName", "byte pair encoding"]]}
{"text": "For comparison with state - of - theart architectures , we implemented a bidirectional LSTM encoder - decoder model with dotproduct attention ( Bahdanau et al , 2015 ;", "entities": [[13, 15, "MethodName", "bidirectional LSTM"]]}
{"text": "Luong et al , 2015 ) using PyTorch ( Paszke et al , 2017 ) , and used Facebook AI Research Sequence - to - Sequence Toolkit ( Gehring et al , 2017b ) to train the ConvS2S and Transformer ( Vaswani et al , 2017 ) models on our data .", "entities": [[39, 40, "MethodName", "Transformer"]]}
{"text": "For the Bi - LSTM encoder - decoder , the encoder is a single layer bidirectional LSTM with input embeddings of size 128 and a hidden state of size with different pooling operators and using gated convolutional units .", "entities": [[4, 5, "MethodName", "LSTM"], [15, 17, "MethodName", "bidirectional LSTM"]]}
{"text": "The decoder is a single layer LSTM with similar input size and a hidden size of 256 , the target input embeddings are also used in the pre - softmax projection .", "entities": [[6, 7, "MethodName", "LSTM"], [29, 30, "MethodName", "softmax"]]}
{"text": "For regularization , we apply a dropout of rate 0.2 to the inputs of both encoder and decoder and to the output of the decoder prior to softmax .", "entities": [[27, 28, "MethodName", "softmax"]]}
{"text": "Each convolution uses 3\u00d71 filters and is followed by a gated linear unit with a total of 2 \u00d7 256 channels .", "entities": [[1, 2, "MethodName", "convolution"], [10, 13, "MethodName", "gated linear unit"]]}
{"text": "For the inner layer in the per - position feed - forawrd network we use d f f = 2048 .", "entities": [[19, 20, "DatasetName", "2048"]]}
{"text": "In this section we explore the impact of several parameters of our model : the token embedding dimension , depth , growth rate and filter sizes .", "entities": [[16, 18, "HyperparameterName", "embedding dimension"]]}
{"text": "In each chosen setting , we train five models with different initializations and report the mean and standard deviation of the BLEU scores .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "Adding gated linear units on top of each convolutional layer does not improve the BLEU scores , but increases the variance due to the additional parameters .", "entities": [[14, 15, "MetricName", "BLEU"]]}
{"text": "For some models we additionally report results obtained with sequence level estimation ( SLE , e.g. using reinforcement learning approaches ) , typically aiming directly to optimize the BLEU measure rather than the likelihood of correct translation .", "entities": [[28, 29, "MetricName", "BLEU"]]}
{"text": "First of all we find that all results obtained using byte - pair encodings ( BPE ) are superior to wordbased results .", "entities": [[15, 16, "MethodName", "BPE"]]}
{"text": "In Figure 5 we consider translation quality as a function of sentence length , and compare our model to RNNsearch , ConvS2S and Transformer .", "entities": [[23, 24, "MethodName", "Transformer"]]}
{"text": "Our model gives the best results across all sentence lengths , except for the longest ones where ConvS2S and Transformer are better .", "entities": [[19, 20, "MethodName", "Transformer"]]}
{"text": "The first example illustrates the ability of the model to translate proper names by breaking them down into BPE units .", "entities": [[18, 19, "MethodName", "BPE"]]}
{"text": "In the second example the German word Karriereweg is broken into the four BPE units karri , er , e , weg .", "entities": [[13, 14, "MethodName", "BPE"]]}
{"text": "In all these cases the phrase seems to be decoded as a unit , where features are first taken across the entire corresponding source Figure 6 : Implicit BPE token - level alignments produced by our Pervasive Attention model .", "entities": [[28, 29, "MethodName", "BPE"]]}
{"text": "For the maxpooling aggregation we visualize \u03b1 obtained with Eq .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "( Bahdanau et al , 2017 ) 27.56 Bi - GRU ( MLE+SLE ) ( Bahdanau et al , 2017 ) 28.53 Conv - LSTM ( deep+pos ) ( Gehring et al , 2017a ) 30.4 NPMT + language model ( Huang et al , 2018 )", "entities": [[10, 11, "MethodName", "GRU"], [24, 25, "MethodName", "LSTM"]]}
{"text": "RNNsearch * ( Bahdanau et al , 2015 ) 31.02 1.79 6 M 25.92 7 M Varational attention 33.10 Transformer * * ( Vaswani et al , 2017 ) 32.83 3.53 59 M 27.68 61 M ConvS2S * * ( MLE ) ( Gehring et al , 2017b ) 32 ( Gehring et al , 2017b ) . phrase , and progressively from the part of the source phrase that remains to be decoded .", "entities": [[19, 20, "MethodName", "Transformer"]]}
{"text": "We presented a novel neural machine translation architecture that departs from the encoder - decoder paradigm .", "entities": [[5, 7, "TaskName", "machine translation"]]}
{"text": "The model is implemented as 2D CNN based on DenseNet , with masked convolutions to ensure a proper autoregressive factorization of the conditional probabilities .", "entities": [[9, 10, "MethodName", "DenseNet"]]}
{"text": "We obtain excellent BLEU scores that compare favorably with the state of the art , while using a conceptually simpler model with fewer parameters .", "entities": [[3, 4, "MetricName", "BLEU"]]}
{"text": "In the future , we plan to explore hybrid approaches in which the input to our joint encoding model is not provided by tokenembedding vectors , but the output of 1D source and target embedding networks , e.g. ( bi - ) LSTM or 1D convolutional .", "entities": [[42, 43, "MethodName", "LSTM"]]}
{"text": "In this paper , we describe our participation in the 2021 Workshop on Asian Translation ( team ID : tpt_wat ) .", "entities": [[14, 15, "TaskName", "Translation"]]}
{"text": "The field of machine translation has seen rapid innovation in the last few years , with new model architectures , pre - training regimens , and computational algorithms emerging at a dizzying pace .", "entities": [[3, 5, "TaskName", "machine translation"]]}
{"text": "Companies utilizing these techniques must take into account considerations such as deployment costs ( model speed and size ) , scalability , explainability , the complexity of training regimens ( resource constraints limiting independent hyperparameter optimization for all language pairs ) , and risk management , against which advances yielding performance gains must be weighed .", "entities": [[34, 36, "TaskName", "hyperparameter optimization"]]}
{"text": "For our participation in the 2021 Workshop on Asian Translation shared task on patent translation , we have applied a single , standardized data preparation and model training pipeline as a way of benchmarking the performance of this process .", "entities": [[9, 10, "TaskName", "Translation"]]}
{"text": "Our NMT systems are standard base Transformer ( Vaswani et al , 2017 ) models , which were trained using only the data resources provided by the task organizers .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "These models used shared subword vocabularies created with SentencePiece ( Kudo and Richardson , 2018 ) .", "entities": [[8, 9, "MethodName", "SentencePiece"]]}
{"text": "The data were encoded using subword encodings learned from the corpora using the unigram model trainer provided by SentencePiece ( Kudo and Richardson , 2018 ) .", "entities": [[18, 19, "MethodName", "SentencePiece"]]}
{"text": "For the English Japanese , Korean Japanese , and Chinese Japanese language pairs , we supplemented the corpora with back translation ( from Japanese into each language ) , which is a common data augmentation technique in NMT ( Sennrich et al , 2016 ) .", "entities": [[33, 35, "TaskName", "data augmentation"]]}
{"text": "Our NMT systems were standard base Transformer models trained using the Marian NMT framework ( Junczys - Dowmunt et al , 2018 ) .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "Hyperparameters such as label smoothing , dropout , learning rate , batch size , number of encoder / decoder layers , number of attention heads , embedding dimensionality , etc . , were held fixed across all language pairs .", "entities": [[3, 5, "MethodName", "label smoothing"], [8, 10, "HyperparameterName", "learning rate"], [11, 13, "HyperparameterName", "batch size"]]}
{"text": "Our models were trained on AWS P3 instances using 4 NVIDIA Tesla V100 GPUs .", "entities": [[6, 7, "DatasetName", "P3"]]}
{"text": "As different language pairs require different hyperparameters , any parameter that can be held fixed during the experimentation stage can create significant savings for companies training their own machine translation models .", "entities": [[28, 30, "TaskName", "machine translation"]]}
{"text": "For instance , variation in parameters such as learning rate , dropout , embedding dimensions , and tying the weights of the source and target embedding layers seemed to have similar effects on performance across all language pairs that we tested .", "entities": [[8, 10, "HyperparameterName", "learning rate"]]}
{"text": "The large variance in the relative performance of these systems shows that no \" onesize - fits - all \" yet exists for the problem of machine translation .", "entities": [[26, 28, "TaskName", "machine translation"]]}
{"text": "A Transition - based System for Universal Dependency Parsing", "entities": [[7, 9, "TaskName", "Dependency Parsing"]]}
{"text": "This paper describes the system for our participation of team Wanghao - ftd - SJTU in the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[28, 30, "DatasetName", "Universal Dependencies"]]}
{"text": "In this work , we design a system based on UDPipe 1 for universal dependency parsing , where transitionbased models are trained for different treebanks .", "entities": [[14, 16, "TaskName", "dependency parsing"]]}
{"text": "For the special surprise languages for this task , we adopt a delexicalized strategy and predict based on transfer learning from other related languages .", "entities": [[18, 20, "TaskName", "transfer learning"]]}
{"text": "Universal Dependencies ( UD ) ( Nivre et al , 2016", "entities": [[0, 2, "DatasetName", "Universal Dependencies"], [3, 4, "DatasetName", "UD"]]}
{"text": "( Nivre et al , , 2017b and universal dependency parsing take efforts to build cross - linguistically treebank annotation and develop cross - lingual learning to parse many languages even low - resource languages .", "entities": [[9, 11, "TaskName", "dependency parsing"]]}
{"text": "Universal Dependencies release 2.0 2 ( Nivre et al , 2017b ) includes rich languages and treebanks resources and the parsing task in CoNLL 2017 is based on this dataset .", "entities": [[0, 2, "DatasetName", "Universal Dependencies"]]}
{"text": "In fact , dependency parsing has been adopted as topic of the shared task in CoNLL - X andCoNLL - 2007 ( Buchholz andMarsi , 2006 ; Nivre et al , 2007 ) , which have been the milestones for the researching field of parsing .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}
{"text": "In this paper , we describe the system of team Wanghao - ftd - SJTU for the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies ( Zeman et al , 2017 ) .", "entities": [[28, 30, "DatasetName", "Universal Dependencies"]]}
{"text": "For dependency parsing , there have been two major parsing methods : graph - based and transition - based .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "Transition - based dependency parsing takes linear time complexity and utilizes rich features to make structural prediction ( Zhang and Clark , 2008 ; Zhang and Nivre , 2011 ) .", "entities": [[0, 5, "TaskName", "Transition - based dependency parsing"]]}
{"text": "Specifically , a buffer for input words , a stack for partially built structure and shift - reduce actions are basic elements in a transition - based dependency parsing .", "entities": [[24, 29, "TaskName", "transition - based dependency parsing"]]}
{"text": "For the transition systems of dependency parsing , there have been two major ones : arc - standard and arc - eager ( Nivre , 2008 ) .", "entities": [[5, 7, "TaskName", "dependency parsing"]]}
{"text": "w i | w j , \u03b2 , A \u03c3 | w i , \u03b2 , A \u222a r ( w i , w j ) Finish : \u03c3 = [ w ] , \u03b2 = where \u03c3 , \u03b2 , A represent the stack , queue and the actions respectively .", "entities": [[6, 7, "HyperparameterName", "\u03b2"], [14, 15, "HyperparameterName", "\u03b2"], [34, 35, "HyperparameterName", "\u03b2"], [39, 40, "HyperparameterName", "\u03b2"]]}
{"text": "To tackle this problem , we exploit the universal part - of - speech ( POS ) tags , which could be represented as crosslingual knowledge to avoid language - specific information , and adopting a delexicalized and crosslingual method , which relies solely on universal POS tags and annotated data in close - related languages .", "entities": [[9, 12, "DatasetName", "part - of"]]}
{"text": "Lemma : Lemma or stem of word forms .", "entities": [[0, 1, "DatasetName", "Lemma"], [2, 3, "DatasetName", "Lemma"]]}
{"text": "After we projected features to embeddings and concatenated the generated embeddings to representations of features , the vector representations of the input are fed to a hidden layer activated with tanh , and the output layer is softmax indicating the probabilities of each possible transition actions .", "entities": [[37, 38, "MethodName", "softmax"]]}
{"text": "The parser supports projective and nonprojective dependency parsing , which is configured by the option transition system .", "entities": [[6, 8, "TaskName", "dependency parsing"]]}
{"text": "In Universal Dependencies release 2.0 , only UD Japanese and UD Galician have no non - projective dependency trees ; while UD Chinese , UD Polish and UD Hebrew have a few non - projective trees , around 1 % in the treebanks .", "entities": [[1, 3, "DatasetName", "Universal Dependencies"], [7, 8, "DatasetName", "UD"], [10, 11, "DatasetName", "UD"], [21, 22, "DatasetName", "UD"], [24, 25, "DatasetName", "UD"], [27, 28, "DatasetName", "UD"]]}
{"text": "According to the projective tree quantities of the whole treebanks 5 , we train non - projective parsing for most treebanks except UD Japanese and UD Galician .", "entities": [[22, 23, "DatasetName", "UD"], [25, 26, "DatasetName", "UD"]]}
{"text": "This follows the method of ( Zeman and Resnik , 2008 ) , which shows that transfer learning for another language based on delexicalized parser can perform well .", "entities": [[16, 18, "TaskName", "transfer learning"]]}
{"text": "The Strength of the Weakest Supervision : Topic Classification Using Class Labels", "entities": [[7, 9, "TaskName", "Topic Classification"]]}
{"text": "Experiments on a variety of topic classification data sets show that learning from class labels can save significant initial labeling effort , essentially providing a \" free \" warm start to the topic classifier .", "entities": [[5, 7, "TaskName", "topic classification"]]}
{"text": "When developing topic classifiers for real - world tasks , such as news categorization , query intent detection , and user - generated content analysis , practitioners often begin by crafting a succinct definition , or a class label , to define each class .", "entities": [[16, 18, "TaskName", "intent detection"]]}
{"text": "Unfortunately , these carefully written class labels are completely ignored by supervised topic classification models .", "entities": [[12, 14, "TaskName", "topic classification"]]}
{"text": "Since class labels are the starting point of any topic classification task , they can be viewed as the earliest hence weakest supervision signal .", "entities": [[9, 11, "TaskName", "topic classification"]]}
{"text": "On six topic classification data sets , we evaluate a suite of existing approaches and the proposed approach .", "entities": [[2, 4, "TaskName", "topic classification"]]}
{"text": "Baeza - Yates et al ( 2011 ) called this approach \" naive text classification \" .", "entities": [[13, 15, "TaskName", "text classification"]]}
{"text": "Jagarlamudi et al ( 2012 ) and Hingmire and Chakraborti ( 2014 ) proposed Seeded LDA to incorporate labeled words / topics into statistical topic modeling .", "entities": [[15, 16, "MethodName", "LDA"]]}
{"text": "Zero - shot learning aims to classify visual objects from a new class using only word descriptions of that class ( Socher et al , 2013 ) .", "entities": [[0, 4, "TaskName", "Zero - shot learning"]]}
{"text": "Most research on zero - shot learning focuses on image classification , but the same principle applies to text classification as well ( Pushp and Srivastava , 2017 ) .", "entities": [[3, 7, "TaskName", "zero - shot learning"], [9, 11, "TaskName", "image classification"], [18, 20, "TaskName", "text classification"]]}
{"text": "Our proposed method constructs a new classifier by composing learned word embeddings in a probabilistic manner .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "Since the new classifier transfers semantic knowledge in word embedding to topic classification tasks , it is broadly related to transfer learning ( Pan and Yang , 2010 ) .", "entities": [[11, 13, "TaskName", "topic classification"], [20, 22, "TaskName", "transfer learning"]]}
{"text": "The main difference is that in transfer learning the information about the new task is in the form of labeled data , not class definition words .", "entities": [[6, 8, "TaskName", "transfer learning"]]}
{"text": "When true labels { ( x i , y i ) } n i=1 are available , we can train a new discriminative logistic regression classifier p", "entities": [[23, 25, "MethodName", "logistic regression"]]}
{"text": "\u03b8 ( y | x ) using both true and pseudo labels ( \u03b8 is the model parameter ) : J ( \u03b8 )", "entities": [[0, 1, "HyperparameterName", "\u03b8"], [13, 14, "HyperparameterName", "\u03b8"], [22, 23, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b8 ( y | x i )", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b8 ( y | x j ) .", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "We compare a variety of methods on six topic classification data sets .", "entities": [[8, 10, "TaskName", "topic classification"]]}
{"text": "ST - 1 : ST - 0 retrained on 10 most confident documents predicted by itself .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "GE : a logistic regression classifier trained using generalized expectation criteria ( Druck et al , 2008 ) .", "entities": [[3, 5, "MethodName", "logistic regression"]]}
{"text": "sLDA : a supervised topic model trained using seeded LDA ( Jagarlamudi et al , 2012 ) .", "entities": [[9, 10, "MethodName", "LDA"]]}
{"text": "WENB+LR : a logistic regression classifier trained only on pseudo labels produced by WENB ( Section 3.1 , n = 0 ) .", "entities": [[3, 5, "MethodName", "logistic regression"], [20, 21, "DatasetName", "0"]]}
{"text": "For medical domain tasks , we take raw text from MEDLINE abstracts ( NLM , 2018 ) to train word vectors .", "entities": [[1, 3, "DatasetName", "medical domain"]]}
{"text": "We consider six topic classification data sets with different document lengths and application domains .", "entities": [[3, 5, "TaskName", "topic classification"]]}
{"text": "( 3 ) Med WSD : The MeSH word sense disambiguation ( WSD ) data set ( Jimeno - Yepes et al , 2011 ) .", "entities": [[8, 11, "TaskName", "word sense disambiguation"]]}
{"text": "Such performance gain essentially comes \" for free \" , as any text classification task has to start by defining classes .", "entities": [[12, 14, "TaskName", "text classification"]]}
{"text": "In Table 3 , we report the number of true labels needed for a logistic regression model to achieve the same performance as WENB+LR .", "entities": [[14, 16, "MethodName", "logistic regression"]]}
{"text": "The proposed methods ( WENB and WENB+LR ) show robust performance , because pretrained word vectors can capture semantic similarity even without any word overlap between a class label and a document .", "entities": [[18, 20, "TaskName", "semantic similarity"]]}
{"text": "Learning from class labels themselves provides very limited help ( IR and ST - 0 ) .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "Using class labels as search queries and labeled documents are closely related : IR and ST - 0 perform similarly ; so do IR+NB and ST - 1 .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "Since WENB uses the semantic knowledge in word vectors to infer pseudo labels , the quality of class label word vectors will affect the pseudo label accuracy .", "entities": [[26, 27, "MetricName", "accuracy"]]}
{"text": "First , we introduce a new perspective on text classification : can we build a text classifier by just providing a short description of each class ?", "entities": [[8, 10, "TaskName", "text classification"]]}
{"text": "Second , future work can investigate tasks such as sentiment and emotion classification , which are more challenging than topic classification tasks .", "entities": [[11, 13, "TaskName", "emotion classification"], [19, 21, "TaskName", "topic classification"]]}
{"text": "STIL - Simultaneous Slot Filling , Translation , Intent Classification , and Language Identification : Initial Results using mBART on MultiATIS++", "entities": [[3, 5, "TaskName", "Slot Filling"], [6, 7, "TaskName", "Translation"], [8, 10, "TaskName", "Intent Classification"], [12, 14, "TaskName", "Language Identification"], [18, 19, "MethodName", "mBART"]]}
{"text": "Slot - filling , Translation , Intent classification , and Language identification , or STIL , is a newly - proposed task for multilingual Natural Language Understanding ( NLU ) .", "entities": [[4, 5, "TaskName", "Translation"], [6, 8, "TaskName", "Intent classification"], [10, 12, "TaskName", "Language identification"], [24, 27, "TaskName", "Natural Language Understanding"]]}
{"text": "By performing simultaneous slot filling and translation into a single output language ( English in this case ) , some portion of downstream system components can be monolingual , reducing development and maintenance cost .", "entities": [[3, 5, "TaskName", "slot filling"]]}
{"text": "Results are given using the multilingual BART model ( Liu et al , 2020 ) fine - tuned on 7 languages using the MultiATIS++ dataset .", "entities": [[6, 7, "MethodName", "BART"]]}
{"text": "When no translation is performed , mBART 's performance is comparable to the current state of the art system ( Cross - Lingual BERT by Xu et al ( 2020 ) )", "entities": [[6, 7, "MethodName", "mBART"], [23, 24, "MethodName", "BERT"]]}
{"text": "Multilingual Natural Language Understanding ( NLU ) , also called cross - lingual NLU , is a technique by which an NLU - based system can scale to multiple languages .", "entities": [[1, 4, "TaskName", "Natural Language Understanding"]]}
{"text": "Two typical tasks for goal - based systems , such as virtual assistants and chatbots , are intent classification and slot filling ( Gupta et al , 2006 ) .", "entities": [[17, 19, "TaskName", "intent classification"], [20, 22, "TaskName", "slot filling"]]}
{"text": "Though intent classification creates a language agnostic output ( the intent of the user ) , slot filling does not .", "entities": [[1, 3, "TaskName", "intent classification"], [16, 18, "TaskName", "slot filling"]]}
{"text": "Machine translation could be performed before the slot filling model at system runtime , though the latency would be fully additive , and some amount of information useful to the slotfilling model may be lost .", "entities": [[0, 2, "TaskName", "Machine translation"], [7, 9, "TaskName", "slot filling"]]}
{"text": "Thus , language identification is necessary so that the system can communicate back to the user in the correct language .", "entities": [[2, 4, "TaskName", "language identification"]]}
{"text": "sification , and Language identification ( STIL ) ; ( 2 ) both non - translated and STIL results using the mBART model ( Liu et al , 2020 ) trained using a fully text - to - text data format ; and ( 3 ) public release of source code used in this study , with a goal toward reproducibility and future work on the STIL task 1 .", "entities": [[3, 5, "TaskName", "Language identification"], [21, 22, "MethodName", "mBART"]]}
{"text": "The Airline Travel Information System ( ATIS ) dataset is a classic benchmark for goal - oriented NLU ( Price , 1990 ;", "entities": [[6, 7, "DatasetName", "ATIS"]]}
{"text": "ATIS was localized to Turkish and Hindi in 2018 , forming MultiATIS ( Upadhyay et al , 2018 ) , and then to Spanish , Portuguese , German , French , Chinese , and Japanese in 2020 , forming Multi - ATIS++ ( Xu et al , 2020 ) .", "entities": [[0, 1, "DatasetName", "ATIS"]]}
{"text": "In this work , Portuguese was excluded due to a lack of Portuguese pretraining in the publicly available mBART model , and Japanese was excluded due to a current lack of alignment between Japanese and English samples in MultiATIS++ .", "entities": [[18, 19, "MethodName", "mBART"]]}
{"text": "Hindi and Turkish data were taken from Multi - ATIS , and the training data were upsampled by 3x for Hindi and 7x for Turkish .", "entities": [[9, 10, "DatasetName", "ATIS"]]}
{"text": "Previous approaches for intent classification and slot filling have used either ( 1 ) separate models for slot filling , including support vector machines ( Moschitti et al , 2007 ) , conditional random fields ( Xu and Sarikaya , 2014 ) , and recurrent neural networks of various types ( Kurata et al , 2016 ) or ( 2 ) joint models that diverge into separate decoders or layers for intent classification and slot filling ( Xu and Sarikaya , 2013 ; Guo et al , 2014 ; Liu and Lane , 2016 ; Hakkani - T\u00fcr et al , 2016 ) or that share hidden states ( Wang et al , 2018 ) .", "entities": [[3, 5, "TaskName", "intent classification"], [6, 8, "TaskName", "slot filling"], [17, 19, "TaskName", "slot filling"], [71, 73, "TaskName", "intent classification"], [74, 76, "TaskName", "slot filling"]]}
{"text": "In this work , a fully text - to - text approach similar to that of the T5 model was used , such that the model would have maximum information sharing across the four STIL sub - tasks .", "entities": [[17, 18, "MethodName", "T5"]]}
{"text": "Encoder - decoder models , first introduced in 2014 ( Sutskever et al , 2014 ) , are a mainstay of neural machine translation .", "entities": [[22, 24, "TaskName", "machine translation"]]}
{"text": "Since then , much of the work on transformers focuses on models with only an encoder pretrained with autoencoding techniques ( e.g. BERT by Devlin et al ( 2018 ) ) or auto - regressive models with only a decoder ( e.g. GPT by Radford ( 2018 ) ) .", "entities": [[22, 23, "MethodName", "BERT"], [42, 43, "MethodName", "GPT"]]}
{"text": "In this work , it was assumed that encoder - decoder models , such as BART ( Lewis et al , 2019 ) and T5 ( Raffel et al , 2019 ) , are the best architectural candidates given the translation component of the STIL task , as well as past state of the art advancement by encoder - decoder models on ATIS , cited above .", "entities": [[15, 16, "MethodName", "BART"], [24, 25, "MethodName", "T5"], [62, 63, "DatasetName", "ATIS"]]}
{"text": "The multilingual BART ( mBART ) model architecture was used ( Liu et al , 2020 ) , as well as the pretrained mBART.cc25 model described in the same paper .", "entities": [[2, 3, "MethodName", "BART"], [4, 5, "MethodName", "mBART"]]}
{"text": "The mBART.cc25 model was trained on 25 languages for 500k steps using a 1.4 TB corpus of scraped website data taken from Common Crawl ( Wenzek et al , 2019 ) .", "entities": [[22, 24, "DatasetName", "Common Crawl"]]}
{"text": "SentencePiece tokenization ( Kudo and Richardson , 2018 ) was used for mBART.cc25 with a sub - word vocabulary size of 250k .", "entities": [[0, 1, "MethodName", "SentencePiece"]]}
{"text": "The same vocabulary as that of the pretrained model was used for this work , and SentencePiece tokenization was performed on the full sequence , including the slot tags , intent tags , and language tags .", "entities": [[16, 17, "MethodName", "SentencePiece"]]}
{"text": "For all mBART experiments and datasets , data from all languages were shuffled together .", "entities": [[2, 3, "MethodName", "mBART"]]}
{"text": "Adding 105 more Hindi and 12 more Turkish training examples results in improved performance for the translated , STIL mBART model .", "entities": [[19, 20, "MethodName", "mBART"]]}
{"text": "By adding these 117 samples , the STIL mBART model matches the performance ( within confidence intervals ) of the non - translated mBART model .", "entities": [[8, 9, "MethodName", "mBART"], [23, 24, "MethodName", "mBART"]]}
{"text": "This finding suggests that the STIL models may require more training data than traditional , non - translated slot filling models .", "entities": [[18, 20, "TaskName", "slot filling"]]}
{"text": "Additionally , by adding more Hindi and Turkish data , both the intent accuracy and the slot filling F1 improves for every individual language of the translated , STIL models , suggesting that some portion of the internal , learned representation is language agnostic .", "entities": [[13, 14, "MetricName", "accuracy"], [16, 18, "TaskName", "slot filling"], [18, 19, "MetricName", "F1"]]}
{"text": "Finally , the results suggest that there is a trainingsize - dependent performance advantage in using a single output language , as contrasted with the nontranslated mBART model , for which the intent classification accuracy and slot F1 does not improve ( with statistical significance ) when using the additional Hindi and Turkish training samples .", "entities": [[26, 27, "MethodName", "mBART"], [32, 34, "TaskName", "intent classification"], [34, 35, "MetricName", "accuracy"], [37, 38, "MetricName", "F1"]]}
{"text": "( Qin et al , 2019 ) 97.5 Joint BERT + CRF ( Chen et al , 2019 ) 97.9 Non - translated mBART , with Perfect performance on Chinese and Hindi is unsurprising given their unique scripts versus the other languages tested .", "entities": [[9, 10, "MethodName", "BERT"], [11, 12, "MethodName", "CRF"], [23, 24, "MethodName", "mBART"]]}
{"text": "This preliminary work demonstrates that a single NLU model can perform simultaneous slot filling , translation , intent classification , and language identification across 7 languages using MultiATIS++ .", "entities": [[12, 14, "TaskName", "slot filling"], [17, 19, "TaskName", "intent classification"], [21, 23, "TaskName", "language identification"]]}
{"text": "Until then , work remains to achieve parity with English - only ATIS models .", "entities": [[12, 13, "DatasetName", "ATIS"]]}
{"text": "K i k=1 P ( s ik | c i ; \u03b8 c i )", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "We refer to these as WORD features below .", "entities": [[5, 6, "DatasetName", "WORD"]]}
{"text": "We use a simple multinomial parameterization where each pixel width is treated as a separate outcome up to some maximum allowable width : s ik | c i \u223c M ult ( \u03b8 c i ) Here , \u03b8 c represents the vector of multinomial spacing parameters corresponding to compositor c.", "entities": [[32, 33, "HyperparameterName", "\u03b8"], [38, 39, "HyperparameterName", "\u03b8"]]}
{"text": "In order to fit the model to an input document we estimate the orthographic preference parameters , w c , and spacing preference parameters , \u03b8 c , for each compositor using EM .", "entities": [[25, 26, "HyperparameterName", "\u03b8"], [32, 33, "MetricName", "EM"]]}
{"text": "The M - step for spacing parameters , \u03b8 c , uses the standard multinomial update .", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "We try ablations of WORD and EDIT features , as well as model variants with and without the spacing generation component ( referred to as SPACE . )", "entities": [[4, 5, "DatasetName", "WORD"], [25, 26, "DatasetName", "SPACE"]]}
{"text": "Including WORD features on top of this leads to slightly reduced performance , perhaps as a result of the substantially increased number of free parameters .", "entities": [[1, 2, "DatasetName", "WORD"]]}
{"text": "In the OCR scenario , the addition of WORD features on top of EDIT decreases accuracy , unlike the same experiment with the manual transcription .", "entities": [[8, 9, "DatasetName", "WORD"], [15, 16, "MetricName", "accuracy"]]}
{"text": "Particularly interesting is the result that spacing , rarely a factor considered in NLP models , improves the accuracy significantly for our system when compared with EDIT features alone .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "Specifically , given a text corpus containing news articles from both sides , we first extract a set of topics utilizing LDA topic modeling ( Blei et al , 2003 ) .", "entities": [[21, 22, "MethodName", "LDA"]]}
{"text": "Focusing on the differences in the languages of liberals and conservatives , KhudaBukhsh et al ( 2020 ) analyze political polarization on YouTube using machine translation tools .", "entities": [[24, 26, "TaskName", "machine translation"]]}
{"text": "al ( 2019 ) first measure topic - wise polarization using the leave - out estimator proposed by ; however , they use a token frequency vector to represent an article , which is less expressive and fails to make use of the rich semantics in the context and the pre - knowledge in pretrained language models ( Devlin et al , 2018 ;", "entities": [[54, 57, "TaskName", "pretrained language models"]]}
{"text": "Liu et al , 2019 ) or pretrained word embeddings ( Mikolov et al , 2013 ; Pennington et al , 2014 ) ; furthermore , they represent the topic using the token frequency vector of the entire document , thus incurring noisy information that might smooth over the target semantics in the locality of topic keywords .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "Some works have proposed contextualized embeddings to enhance the quality of neural topic models ( Bianchi et al , 2020 ;", "entities": [[12, 14, "TaskName", "topic models"]]}
{"text": "The proposed PaCTE framework consists of four components : 1 ) LDA Topic Modeling , 2 ) Partisanship Learning , 3 ) Partisanship - aware Contextualized Topic Embedding Generation , and 4 ) Measuring Polarization and Ranking Topics .", "entities": [[11, 12, "MethodName", "LDA"]]}
{"text": "f We train an LDA model on the combined corpus and extract 2 topics .", "entities": [[4, 5, "MethodName", "LDA"]]}
{"text": "= ( t k ) K k=1 , i > j \u21d4 \u03b2 ( t i , D L , D R ) <", "entities": [[12, 13, "HyperparameterName", "\u03b2"]]}
{"text": "We train an LDA topic model using the the combined corpus D C = D L \u222aD R and extract K topics T = { t i } K i=1 , where t i is a topic .", "entities": [[3, 4, "MethodName", "LDA"]]}
{"text": "The idea is inspired by static word embedding models like GloVe ( Pennington et al , 2014 ) , where the authors measure the similarity between words by the cosine similarity between the word embeddings .", "entities": [[10, 11, "MethodName", "GloVe"], [33, 35, "TaskName", "word embeddings"]]}
{"text": "To fit the pretrained language model on the news corpora , we can use one of the two training tasks : masked language modeling or partisanship recognition .", "entities": [[21, 24, "TaskName", "masked language modeling"]]}
{"text": "To obtain the DC topic embedding , Demszky et al ( 2019 ) use word frequency vectors ; Grootendorst ( 2020 ) takes the [ CLS ] embedding of a pretrained language model that gives a holistic document embedding without encoding the context of a topic .", "entities": [[37, 39, "TaskName", "document embedding"]]}
{"text": "In addition , a document is likely to be associated with multiple topics according to the LDA topic model , and therefore using the holistic document embedding as the topic embedding regardless of the specific topic results in identical embeddings for different topics on the same document ; moreover , even if a document is only associated with one topic , it might contain information not relevant to that topic and thus the holistic document embedding will encode noisy information .", "entities": [[16, 17, "MethodName", "LDA"], [25, 27, "TaskName", "document embedding"], [74, 76, "TaskName", "document embedding"]]}
{"text": "( H D L ( t i ) , H D R ( t i ) ) , \u03b2 ( D L , D R , t i )", "entities": [[18, 19, "HyperparameterName", "\u03b2"]]}
{"text": "[ 0 , 1 ] .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Therefore , the polarization - based ranked topic list f ( D L , D R , T ) is computed based on the corresponding polarization scores ( \u03b2 ( D L , D R , t i ) )", "entities": [[28, 29, "HyperparameterName", "\u03b2"]]}
{"text": "We perform lemmatization via LDA Topic Modeling .", "entities": [[2, 3, "TaskName", "lemmatization"], [4, 5, "MethodName", "LDA"]]}
{"text": "In fact , finetuning a BERT model to recognize differences only between CNN vs. Fox is likely to make it end up capturing the writing style differences and ignoring political differences , since the former is an easier task .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "Denoting the number of negative labels ( 0 ) and positive labels ( 1 ) in corpus D on topic t as N t D ( 0 ) and N t D ( 1 ) respectively , the leaning of the corpus on the topic is quantified as le ( D , t )", "entities": [[7, 8, "DatasetName", "0"], [26, 27, "DatasetName", "0"]]}
{"text": "= ( N t D ( 1 ) \u2212N t D ( 0 ) ) /", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "Accordingly , the ground - truth polarization score between a liberal corpus D L and a conservative corpus D R on topic t is computed as the difference between the leanings of the two corpora , such that \u03b1 ( D L , D R , t )", "entities": [[38, 39, "HyperparameterName", "\u03b1"]]}
{"text": "[ 0 , 1 ] .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "As a result , the ground - truth polarization - based topic ranked list l gt ( D L , D R , T labeled ) between a liberal corpus D L and a conservative corpus D R is computed based on the corresponding ground - truth polarization scores ( \u03b1 ( D L , D R , t )", "entities": [[50, 51, "HyperparameterName", "\u03b1"]]}
{"text": "For a pair of news corpora D L and D R and a given topic t , we take the top - 10 most relevant documents from each corpus and feed the token frequency vectors of the documents into the leave - out estimator ( Demszky et al , 2019 ) , from which we use estimated partisanship as the polarization score ( [ 0 , 1 ] ) of topic t between D L and D R , following the idea of measuring within - topic polarization in their paper .", "entities": [[64, 65, "DatasetName", "0"]]}
{"text": "Note that different from their method that extracts topic using embedding - based topic assignment , we use the same LDA topic model in PaCTE to extract topics , so as to ensure fair comparison between PaCTE and LOE .", "entities": [[20, 21, "MethodName", "LDA"]]}
{"text": "On all topics and in all partisan news source pairs , the polarization scores given by PaCTE\u00acFT are below 0.1 ( the full range is [ 0 , 1 ] ) which indicates significant alignment .", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "However , neither PaCTE\u00acFT nor LOE makes use of information about news partisanship , and compared to PaCTE 0", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "1/3 1/3 0 1/3 1/3 0 1/3 1/3 2/3 Huff 1/3 1/3 1/3 2/3 2/3 0 1/3 1/3 0 0 1/3 2/3 NYT 1/3 0 1/3 1 1/3 0 1/3 1/3 0 1/3 0 1/3 Table 3 : Recall@3 on polarized topics retrieval in nine partisan news source pairs using different methods , where we use the polarization - based topic ranked list from a model predictions f pred ( D L , D R , T labeled ) to retrieve the top - 3 topics from the ground - truth ranked list l gt ( D L , D R , T labeled ) .", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [31, 32, "DatasetName", "0"], [33, 34, "DatasetName", "0"]]}
{"text": "In Section 3.4 we propose to use the DC topic embedding to represent the ideology of a document on a topic , instead of using the holistic document embedding .", "entities": [[27, 29, "TaskName", "document embedding"]]}
{"text": "First , we show the results of polarized topics retrieval using PaCTE - DE and PaCTE in three partisan news source pairs in Table 4 . Method CNN vs. Fox Huff vs. Breit NYT vs. NYP PaCTE - DE 0 0 0", "entities": [[39, 40, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [41, 42, "DatasetName", "0"]]}
{"text": "We observe that the polarization scores given by PaCTE - DE in three source pairs on all topics are above 0.98 ( the range is [ 0 , 1 ] ) , suggesting that all topics are highly polarized .", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "However , the output polarization scores from PaCTE are more evenly distributed in [ 0 , 1 ] , and thus are more robust to perturbations during partisanship learning ; a small perturbation on a polarization score does not affect the output ranking .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "Among them topic 0 , 3 , 4 , 14 , 16 , 26 , 35 , 36 , 37 are not used in further analysis because after reading relevant articles we find that they are more about advertisements , sport events , gossip news and recipes and etc . , which are more factual and convey limited media ideologies .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "This project was funded in part by DARPA under contract HR001121C0168 .", "entities": [[7, 8, "DatasetName", "DARPA"]]}
{"text": "We do not annotate all topics because 1 ) it is difficult for humans to discern the two political stances on some topics , especially when such two stances do not exist at all ; 2 ) we use the vanilla LDA topic modeling which is not the state - of - the - art , so the modeled topics will change using different topic models , in which case the annotating step should be repeated .", "entities": [[41, 42, "MethodName", "LDA"], [64, 66, "TaskName", "topic models"]]}
{"text": "Given a topic t from T labeled , the defined two stances , and its 60 most relevant documents ( 10 from each of the six news sources ) , for each document , we ask the annotators to label which stance it belongs to and label it as 0 or 1 ; if the annotator is not able to perceive a clear political stance , then the annotator will label it as - 1 .", "entities": [[49, 50, "DatasetName", "0"]]}
{"text": "Beyond Laurel / Yanny : An Autoencoder - Enabled Search for Polyperceivable Audio", "entities": [[6, 7, "MethodName", "Autoencoder"]]}
{"text": "The clips we consider consist of audio signals synthesized by the Amazon Polly speech synthesis system with a slightly perturbed playback rate ( i.e. a slight slowing - down of the clip ) .", "entities": [[13, 15, "TaskName", "speech synthesis"]]}
{"text": "To sample clips that were likely candidates , we trained a simple autoencoder for audio clips of single words synthesized using the Amazon Polly speech synthesis system .", "entities": [[12, 13, "MethodName", "autoencoder"], [24, 26, "TaskName", "speech synthesis"]]}
{"text": "Treating the autoencoder 's low - dimensional latent space as a proxy for perceptual space , we searched for clips that travel through more of the space as the playback rate is slowed from 1.0\u00d7 to 0.6\u00d7.", "entities": [[2, 3, "MethodName", "autoencoder"]]}
{"text": "Autoencoder details Our autoencoder operates on one - second audio clips sampled at 22 , 050 Hz , which are converted to spectrograms with a window size of 256 and then flattened to vectors in R 90 , 000 .", "entities": [[0, 1, "MethodName", "Autoencoder"], [3, 4, "MethodName", "autoencoder"]]}
{"text": "The encoder is a linear map to R 512 with ReLU activations , and the decoder is a linear map back to R 90 , 000 space with pointwise squaring .", "entities": [[10, 11, "MethodName", "ReLU"]]}
{"text": "Thus , importance sampling increased our procedure 's recall by almost 3\u00d7. For a more quantitative understanding , we analyzed the relationship between \" autoencoder path length \" S and \" perceptual path length \" T .", "entities": [[24, 25, "MethodName", "autoencoder"]]}
{"text": "While the initial work on adversarial examples focused on computer vision , more recent work shows the presence of such examples across other settings , including reinforcement learning ( Huang et al , 2017 ) , reading comprehension ( Jia and Liang , 2017 ) , and speech recognition ( Carlini and Wag - ner , 2018 ; Qin et al , 2019 ) .", "entities": [[36, 38, "TaskName", "reading comprehension"], [47, 49, "TaskName", "speech recognition"]]}
{"text": "Recent research on adversarial robustness of ML models has provided a trove of new tools and perspectives for probing classifiers and exploring the geometry of decision boundaries .", "entities": [[3, 5, "TaskName", "adversarial robustness"]]}
{"text": "Such phenomena raise questions about how our autoencoder - based method can be extended to search for \" priming - sensitive \" polyperceivability .", "entities": [[7, 8, "MethodName", "autoencoder"]]}
{"text": "By modeling perceptual space as the latent space of an autoencoder , we were able to discover dozens of new polyper - ceivable instances , which were validated with Mechanical Turk experiments .", "entities": [[10, 11, "MethodName", "autoencoder"]]}
{"text": "These problems can be decomposed into three subtasks : word extraction using regrex , relation extraction ( Zelenko et al , 2003 ) , ( Bunescu and Mooney , 2005 ) , and classifying the logistics between them .", "entities": [[14, 16, "TaskName", "relation extraction"]]}
{"text": "The latest model DeBERTaV3 ( He et al , 2021 ) has outstanding performance on cross - linguistic tasks , which outperforms BERT and DeBERTa on many tasks .", "entities": [[22, 23, "MethodName", "BERT"], [24, 25, "MethodName", "DeBERTa"]]}
{"text": "The De - BERTaV3 model shares the same pre - trained data with ERNIE - M called XNLI ( Conneau et al , 2018 ) , which can improve the performance and robustness as well .", "entities": [[17, 18, "DatasetName", "XNLI"]]}
{"text": "Two additional datasets were imported , i.e. , the translated dataset from Google translation which is translated from three languages , and the XNLI dataset .", "entities": [[12, 13, "DatasetName", "Google"], [23, 24, "DatasetName", "XNLI"]]}
{"text": "We compared the performance of the ERNIE - M model on four sets of data : the given data , the given data with translated data , the given data with XNLI augmentation , and the given data with both the translated data and XNLI data .", "entities": [[31, 32, "DatasetName", "XNLI"], [44, 45, "DatasetName", "XNLI"]]}
{"text": "Cross - Attention masked language modeling ( CAMLM ) is to align cross - language semantic representations on parallel corpora .", "entities": [[3, 6, "TaskName", "masked language modeling"]]}
{"text": "Back - Translation masked language modeling ( BTMLM ) is trained to generate pseudo - parallel sentences from monolingual sentences .", "entities": [[2, 3, "TaskName", "Translation"], [3, 6, "TaskName", "masked language modeling"]]}
{"text": "The generated pairs are then used as the input of the model to further align the cross - lingual semantics , thus enhancing the multilingual representation . DeBERTaV3 presents a new pre - trained language model , which improves the original De - BERTa model by replacing mask language modeling ( MLM ) with replaced token detection ( RTD ) , a more sample - efficient pre - training task .", "entities": [[51, 52, "DatasetName", "MLM"]]}
{"text": "To extenuate over - fitting for a specific language , our team uses a multi - language ensemble learning strategy that includes a pre - trained language model and a multilingual language model .", "entities": [[17, 19, "TaskName", "ensemble learning"]]}
{"text": "We tried multilingual masked language modeling ( MMLM ) , translation language modeling ( TLM ) , and crossattention masked language modeling ( CAMLM ) have been tried .", "entities": [[3, 6, "TaskName", "masked language modeling"], [19, 22, "TaskName", "masked language modeling"]]}
{"text": "In the process of using multilingual language models , we mainly adopt random search to finetune the ERNIE - M model and data augmentation methods are used for model training .", "entities": [[12, 14, "MethodName", "random search"], [22, 24, "TaskName", "data augmentation"]]}
{"text": "Cross - lingual natural language inference ( XNLI ) dataset is used and the English training set is translated to Italian ( E2I set ) .", "entities": [[0, 6, "TaskName", "Cross - lingual natural language inference"], [7, 8, "DatasetName", "XNLI"]]}
{"text": "Firstly , by using different random seeds , we divided the training set which included all three languages ten times .", "entities": [[6, 7, "DatasetName", "seeds"]]}
{"text": "During the finetuning process , we used random search to optimize hyper - parameters like epochs , learning rate , and batch size .", "entities": [[7, 9, "MethodName", "random search"], [17, 19, "HyperparameterName", "learning rate"], [21, 23, "HyperparameterName", "batch size"]]}
{"text": "By using F1 - Score as our evaluation metric , the best model at all the ten - fold of training is saved .", "entities": [[2, 5, "MetricName", "F1 - Score"]]}
{"text": "To enhance the effect in a single language subtask , we consider using an enhanced mask decoder and a disentangled attention mechanism to improve the effect .", "entities": [[19, 22, "MethodName", "disentangled attention mechanism"]]}
{"text": "DeBERTaV3 meets our needs by using Electra - style pre - training and gradient unwrapping embedding sharing .", "entities": [[6, 7, "MethodName", "Electra"]]}
{"text": "The training set which includes all three languages is divided randomly 10 times by setting different random seeds .", "entities": [[17, 18, "DatasetName", "seeds"]]}
{"text": "To increase the size of training data , we use the following data augmentation methods : 1 ) translate English data into French and Italian by using Baidu translate 2 ) translate English data into French and Italian by using Google translate 3 )", "entities": [[12, 14, "TaskName", "data augmentation"], [40, 41, "DatasetName", "Google"]]}
{"text": "translate French and Italian data into English by using Google translate .", "entities": [[9, 10, "DatasetName", "Google"]]}
{"text": "Because the trail dataset provided by organizers is only 5838 in each language , to increase the amount of data and make the model better , we use Google translator and Baidu translator to translate the English dataset into French and Italian again .", "entities": [[28, 29, "DatasetName", "Google"]]}
{"text": "The other part is that we use the public dataset - XNLI .", "entities": [[11, 12, "DatasetName", "XNLI"]]}
{"text": "We use XNLI dataset because it is often used in similar cross - language tasks .", "entities": [[2, 3, "DatasetName", "XNLI"]]}
{"text": "The XNLI dataset contains a total of 15 languages , and each language contains 7500 pairs of data .", "entities": [[1, 2, "DatasetName", "XNLI"]]}
{"text": "Because the XNLI dataset itself does not contain Italian datasets , we translated the English dataset into Italian and then used the three languages in ERNIE - M model training .", "entities": [[2, 3, "DatasetName", "XNLI"]]}
{"text": "We think this is due to the following reasons : Firstly , Italian is not included in the original XNLI dataset .", "entities": [[19, 20, "DatasetName", "XNLI"]]}
{"text": "Furthermore , to solve the issue of the small dataset , we use various strategies , such as K - ford cross - validation , translating the dataset using different translators , and introducing an external dataset - XNLI , a dataset commonly used in multilingual problems .", "entities": [[38, 39, "DatasetName", "XNLI"]]}
{"text": "multi - task learning , or adversarial training .", "entities": [[0, 4, "TaskName", "multi - task learning"]]}
{"text": "Multilingual Code - Switching for Zero - Shot Cross - Lingual Intent Prediction and Slot Filling", "entities": [[14, 16, "TaskName", "Slot Filling"]]}
{"text": "Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding ( NLU ) .", "entities": [[15, 18, "TaskName", "Natural Language Understanding"]]}
{"text": "In the context of zero - shot learning , this task is typically approached using representations from pre - trained multilingual language models such as mBERT or by fine - tuning on data automatically translated into the target language .", "entities": [[4, 8, "TaskName", "zero - shot learning"], [25, 26, "MethodName", "mBERT"]]}
{"text": "Furthermore , we present an application of our method for crisis informatics using a new human - annotated tweet dataset of slot filling in English and Haitian Creole , collected during the Haiti earthquake .", "entities": [[21, 23, "TaskName", "slot filling"]]}
{"text": "( Nguyen et al , 2017 ; Krishnan et al , 2020 ) or detecting hate speech ( Pamungkas and Patti , 2019 ; Stappen et al , 2020 ) , where the target language might be of low - resource or unknown .", "entities": [[15, 17, "DatasetName", "hate speech"]]}
{"text": "Intent prediction and slot filling are two NLU tasks , usually solved jointly , which learn to model the intent ( sentence - level ) and slot ( word - level ) labels .", "entities": [[3, 5, "TaskName", "slot filling"]]}
{"text": "Such models are currently used extensively for goal - oriented dialogue systems , such as Amazon 's Alexa , Apple 's Siri , Google Assistant , and Microsoft 's Cortana .", "entities": [[7, 12, "TaskName", "goal - oriented dialogue systems"], [23, 24, "DatasetName", "Google"]]}
{"text": "Highly effective transformer - based multilingual models such as mBERT ( Devlin et al , 2019 ) and XLM - R ( Conneau et al , 2020a ) have found success across several multilingual tasks in recent years .", "entities": [[9, 10, "MethodName", "mBERT"], [18, 19, "MethodName", "XLM"]]}
{"text": "In the zero - shot cross - lingual transfer setting with an unknown target language , a typical solution is to use pre - trained transformer models and fine - tune to the downstream task using the monolingual source data .", "entities": [[2, 9, "TaskName", "zero - shot cross - lingual transfer"]]}
{"text": "However , Pires et al ( 2019 ) showed that existing transformer - based represen - Figure 1 : t - SNE plot of embeddings across the 12 multi - head attention layers of multilingual BERT .", "entities": [[28, 32, "MethodName", "multi - head attention"], [35, 36, "MethodName", "BERT"]]}
{"text": "In the above code - switching example , the chunks are in Chinese , Punjabi , Spanish , English , Arabic , and Russian .", "entities": [[14, 15, "DatasetName", "Punjabi"]]}
{"text": "Figure 1 also verifies that the representations across the 12 multi - head attention layers of mBERT are still not shared across languages , instead forming clearly distinguishable clusters per language .", "entities": [[10, 14, "MethodName", "multi - head attention"], [16, 17, "MethodName", "mBERT"]]}
{"text": "To this goal , we introduce a data augmentation method via multilingual codeswitching , where the original sentence in English is code - switched into randomly selected languages .", "entities": [[7, 9, "TaskName", "data augmentation"]]}
{"text": "We show that mBERT can be fine - tuned for many languages starting only with monolingual source - language data , leading to better performance in zero - shot settings .", "entities": [[3, 4, "MethodName", "mBERT"]]}
{"text": "We present a data augmentation method via multilingual code - switching to enhance the language neutrality of transformerbased language models such as mBERT for finetuning to a downstream NLU task of intent prediction and slot filling .", "entities": [[3, 5, "TaskName", "data augmentation"], [22, 23, "MethodName", "mBERT"], [34, 36, "TaskName", "slot filling"]]}
{"text": "We release a new human - annotated tweet dataset , collected during Haiti earthquake disaster , for intent prediction and slot filling in English and Haitian Creole .", "entities": [[20, 22, "TaskName", "slot filling"]]}
{"text": "Multilingual masked language models , such as mBERT ( Devlin et al , 2019 ) , are trained using large datasets of publicly available unlabeled corpora such as Wikipedia .", "entities": [[7, 8, "MethodName", "mBERT"]]}
{"text": "When using slot filling datasets , slot labels that are grouped by BIO ( Ramshaw and Marcus , 1999 ) tags constitute natural chunks , as shown in Figure 2 .", "entities": [[2, 4, "TaskName", "slot filling"]]}
{"text": "To summarize the algorithm , we take a sentence , take each chunk from that sentence , perform a translation into a random language using Google 's NMT system ( Wu et al , 2016 ) , and align the slot labels to fit the translation , i.e. , label propagation through alignment as the translated sentence do not preserve the", "entities": [[25, 26, "DatasetName", "Google"]]}
{"text": "Afro - Asiatic Arabic ( ar ) , Amharic ( am ) , Hebrew ( he ) , Somali ( so ) Germanic German ( de ) , Dutch ( nl ) , Danish ( da ) , Swedish ( sv ) , Norwegian ( no ) Indo - Aryan Hindi ( hi ) , Bengali ( bn ) , Marathi ( mr ) , Nepali ( ne ) , Gujarati ( gu ) , Punjabi ( pa )", "entities": [[75, 76, "DatasetName", "Punjabi"]]}
{"text": "Joint training is traditionally used for intent prediction and slot filling to exploit the correlation between the two tasks .", "entities": [[9, 11, "TaskName", "slot filling"]]}
{"text": "So , a standard joint model loss can be defined as a combination of intent ( L i ) and slot ( L sl ) losses .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "i.e. , L = \u03b1L i + \u03b2L sl , where \u03b1 and \u03b2 are corresponding task weights .", "entities": [[11, 12, "HyperparameterName", "\u03b1"], [13, 14, "HyperparameterName", "\u03b2"]]}
{"text": "Prior works ( Goo et al , 2018 ; Schuster et al , 2019 ; Liu and Lane , 2016 ; Haihong et al , 2019 ) that use BiL - STM or RNN are now modified to BERT - based implementations explored in more recent works Hardalov et al , 2020 ; .", "entities": [[38, 39, "MethodName", "BERT"]]}
{"text": "A standard Joint model consists of BERT outputs from the final hidden state ( classification ( CLS ) token for intent and m word tokens for slots ) fed to linear layers to get intent and slot predictions .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Assuming h cls represents the CLS token and h m represents a token from the remaining word - level tokens , the BERT model outputs are defined as : p", "entities": [[22, 23, "MethodName", "BERT"]]}
{"text": "i = sof tmax (", "entities": [[2, 3, "DatasetName", "sof"]]}
{"text": "The output of Algorithm 1 will be the input used for joint training on BERT for code - switched experiments .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "We use the latest multilingual benchmark dataset of MultiATIS++ , which was created by manually translating the original ATIS ( Price , 1990 ) dataset from English ( en ) to 8 other languages :", "entities": [[18, 19, "DatasetName", "ATIS"]]}
{"text": "The zero - shot baselines for the codeswitching experiments use an English - Only ) model , which is fine - tuned over the pre - trained mBERT separately for each task and an English - only Joint model .", "entities": [[27, 28, "MethodName", "mBERT"]]}
{"text": "From the significance tests , except for Spanish and German , all other languages were helped by code - switching for intent detection .", "entities": [[21, 23, "TaskName", "intent detection"]]}
{"text": "For slot filling , improvement on Portuguese and French is insignificant .", "entities": [[1, 3, "TaskName", "slot filling"]]}
{"text": "Evaluation on Disaster Dataset .", "entities": [[2, 3, "DatasetName", "Disaster"]]}
{"text": "We found that disaster data is more challenging than the ATIS dataset for transfer learning in NLU .", "entities": [[10, 11, "DatasetName", "ATIS"], [13, 15, "TaskName", "transfer learning"]]}
{"text": "This might be due to the lack of strong representation for Haitian Creole in the pre - trained model , although it is similar to French , or due to the limitation of the machine translation system .", "entities": [[34, 36, "TaskName", "machine translation"]]}
{"text": "Overall , we see improvement for both tasks , with Slot F1 plateauing earlier .", "entities": [[11, 12, "MetricName", "F1"]]}
{"text": "mBERT versus XLM - R. Additional performance evaluations and benefits of code - switching on XLM - R ( Conneau et al , 2020a ) , a stronger multilingual language model , are provided in", "entities": [[0, 1, "MethodName", "mBERT"], [2, 3, "MethodName", "XLM"], [15, 16, "MethodName", "XLM"]]}
{"text": "Appendix A. Note that XLM - R is trained using Common - Crawl and is likely to be exposed to some code - switched data .", "entities": [[4, 5, "MethodName", "XLM"]]}
{"text": "Thus , we focus primarily on mBERT which largely remains monolingual at the sentence - level to identify the unbiased impact of code - switching during fine - tuning .", "entities": [[6, 7, "MethodName", "mBERT"]]}
{"text": "Furthermore , runtime and hyperparameter tuning along with insights into layers to freeze before training are shown in Appendix B. Error Analysis .", "entities": [[20, 21, "MetricName", "Error"]]}
{"text": "Cross - Lingual Transfer .", "entities": [[0, 4, "TaskName", "Cross - Lingual Transfer"]]}
{"text": "Researchers have studied cross - lingual tasks in various settings such as sentiment / sequence classification ( Wan , 2009 ; Eriguchi et al , 2018 ; , named entity recognition ( Zirikly and Hagiwara , 2015 ; Tsai et Xie et al , 2018 ) , parts - of - speech tagging ( Yarowsky et al , 2001 ;", "entities": [[28, 31, "TaskName", "named entity recognition"]]}
{"text": "T\u00e4ckstr\u00f6m et al , 2013 ; Plank and Agi\u0107 , 2018 ) , and natural language understanding ( He et al , 2013 ; Upadhyay et al , 2018 ; .", "entities": [[14, 17, "TaskName", "natural language understanding"]]}
{"text": "The methodology for most of the current approaches for cross - lingual tasks can be categorizes as : a ) multilingual representations from pre - trained or fine - tuned models such as mBERT ( Devlin et al , 2019 ) or XLM - R ( Conneau et al , 2020a ) , b ) machine translation followed by alignment ( Shah et al , 2010 ;", "entities": [[33, 34, "MethodName", "mBERT"], [42, 43, "MethodName", "XLM"], [55, 57, "TaskName", "machine translation"]]}
{"text": "Before transformer models , effective approaches included domain adversarial training to extract language - agnostic features ( Ganin et al , 2016 ; and word alignment methods such as MUSE ( Conneau et al , 2017 ) to align fastText word vectors ( Bojanowski et al , 2017 ) .", "entities": [[24, 26, "TaskName", "word alignment"], [29, 30, "DatasetName", "MUSE"], [39, 40, "MethodName", "fastText"]]}
{"text": "Recently , Conneau et al , 2020b show that having shared parameters in the top layers of the multilingual encoders can be used to align different languages quite effectively on tasks such as XNLI ( Conneau et al , 2018 ) .", "entities": [[33, 34, "DatasetName", "XNLI"]]}
{"text": "Monolingual models for joint slot filling and intent prediction have used attention - based RNN ( Liu and Lane , 2016 ) and attention - based BiLSTM with a slot gate ( Goo et al , 2018 ) on benchmark datasets ( Price , 1990 ;", "entities": [[4, 6, "TaskName", "slot filling"], [26, 27, "MethodName", "BiLSTM"]]}
{"text": "These methods have shown that a joint method can enhance both tasks and slot filling can be conditioned on the learned intent .", "entities": [[13, 15, "TaskName", "slot filling"]]}
{"text": "Recently , BERT - based approaches ( Hardalov et al , 2020 ; have improved results .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "Recently , monolingual models have been adapted to code - switched text in entity recognition ( Aguilar and Solorio , 2019 ) , part - ofspeech tagging ( Soto and Hirschberg , 2018 ; Ball and Garrette , 2018 ) , sentiment analysis ( Joshi et al , 2016 ) and language identification ( Mave et al , 2018 ; Yirmibe\u015foglu and Eryigit , 2018 ; Mager et al , 2019 ) .", "entities": [[41, 43, "TaskName", "sentiment analysis"], [51, 53, "TaskName", "language identification"]]}
{"text": "We consider this distinction essential to evaluate a true zero - shot learning scenario and prevent any bias when comparing with translate - and - train .", "entities": [[9, 13, "TaskName", "zero - shot learning"]]}
{"text": "Contemporary work by Tan and Joty , 2021 makes use of both word and phrase - level code - mixing to switch to a set of languages to perform adversarial training for XNLI .", "entities": [[32, 33, "DatasetName", "XNLI"]]}
{"text": "Code - switching and other data augmentation techniques have been applied to the pre - training stage in recent works ( Chaudhary et al , 2020 ; Kale and Siddhant , 2021 ; Dufter and Sch\u00fctze , 2020 ) .", "entities": [[5, 7, "TaskName", "data augmentation"]]}
{"text": "In addition to studying cross - lingual slot filling and language families , another key distinction of our method is that we completely ignore the target language during training to represent a fully zero - shot scenario .", "entities": [[7, 9, "TaskName", "slot filling"]]}
{"text": "This approach enhanced the generalizability of pre - trained language models such as mBERT when fine - tuning for downstream tasks of intent detection and slot filling .", "entities": [[13, 14, "MethodName", "mBERT"], [22, 24, "TaskName", "intent detection"], [25, 27, "TaskName", "slot filling"]]}
{"text": "We conduct an additional analysis on XLM - R ( Conneau et al , 2020a ) and compare it with mBERT ( Devlin et al , 2019 ) .", "entities": [[6, 7, "MethodName", "XLM"], [20, 21, "MethodName", "mBERT"]]}
{"text": "The implementation is very similar in PyTorch ( Paszke et al , 2019 ) but using the pre - trained xlm - roberta - base with RobertaForSequenceClassification ( Wolf et al , 2020 ) as the XLM - R model .", "entities": [[20, 21, "MethodName", "xlm"], [36, 37, "MethodName", "XLM"]]}
{"text": "Individually , XLM - R improved Chinese , Japanese , Portuguese , and Turkish for Intent Prediction and German , Chinese , Japanese , Portuguese , and Hindi for Slot Filling as shown in Figure 7 .", "entities": [[2, 3, "MethodName", "XLM"], [29, 31, "TaskName", "Slot Filling"]]}
{"text": "We observe a trend similar to mBERT with k on XLM - R shown in Figure 8 .", "entities": [[6, 7, "MethodName", "mBERT"], [10, 11, "MethodName", "XLM"]]}
{"text": "However , for XLM - R , we observe that randomized code - switching did not help Chinese for Intent Prediction and Hindi for Slot F1 .", "entities": [[3, 4, "MethodName", "XLM"], [25, 26, "MetricName", "F1"]]}
{"text": "A deeper dive into XLM - R and language families are left for future work .", "entities": [[4, 5, "MethodName", "XLM"]]}
{"text": "For joint training with same task weights , we tuned \u03b1 and \u03b2 using grid search to see the strength of correlation between the tasks .", "entities": [[10, 11, "HyperparameterName", "\u03b1"], [12, 13, "HyperparameterName", "\u03b2"]]}
{"text": "Automatic Error Analysis for Document - level Information Extraction", "entities": [[1, 2, "MetricName", "Error"]]}
{"text": "In particular , the precision / recall / F1 scores typically reported provide few insights on the range of errors the models make .", "entities": [[8, 9, "MetricName", "F1"]]}
{"text": "We build on the work of Kummerfeld and Klein ( 2013 ) to propose a transformation - based framework for automating error analysis in document - level event and ( N - ary ) relation extraction .", "entities": [[34, 36, "TaskName", "relation extraction"]]}
{"text": "Although information extraction ( IE ) research has almost uniformly focused on sentence - level relation and event extraction ( Grishman , 2019 ) , the earliest research in the area formulated the task at the document level .", "entities": [[17, 19, "TaskName", "event extraction"]]}
{"text": "Each involved a complex document - level event extraction task : there were 24 types of events , over a dozen event arguments ( or roles ) to be identified for each event ; documents could contain zero to tens of events , and extracting argument entities ( or role fillers ) required noun phrase coreference resolution to ensure interpretability for the end - user ( e.g. to ensure that multiple distinct mentions of the same entity in the output were not misinterpreted as references to distinct entities ) .", "entities": [[4, 9, "TaskName", "document - level event extraction"], [55, 57, "TaskName", "coreference resolution"]]}
{"text": "First , despite the relative complexity of the task , approaches are only evaluated with respect to their overall performance scores ( e.g. precision , recall , and F1 ) .", "entities": [[28, 29, "MetricName", "F1"]]}
{"text": "In this work , we first introduce a framework for automating error analysis for document - level event and relation extraction , casting both as instances of a general role - filling , or template - filling task ( Jurafsky and Martin , 2021 ) .", "entities": [[19, 21, "TaskName", "relation extraction"]]}
{"text": "Next , we employ the error analysis framework in a comparison of two state - of - the - art documentlevel neural template - filling approaches , DyGIE++ and GTT ( Du et al , 2021b ) , across three template - filling datasets ( SciREX , ProMED ( Patwardhan and Riloff , 2009 ) 3 , and MUC - 4 ) .", "entities": [[45, 46, "DatasetName", "SciREX"]]}
{"text": "Zhou et al , 2008 ; Farr\u00fas et al , 2010 ; Kholy and Habash , 2011 ; Zeman et al , 2011 ; Popovi\u0107 and Ney , 2011 ) , coreference resolution ( Uryupina , 2008 ; Kummerfeld and Klein , 2013 ; Martschat and Strube , 2014 ; Martschat et al , 2015 ) and parsing ( Kummerfeld et al , 2012 ) .", "entities": [[31, 33, "TaskName", "coreference resolution"]]}
{"text": "Recently , generalized automated error analysis frameworks involving human - in - the - loop testing like Errudite ( Wu et al , 2019 ) , CHECK - LIST ( Ribeiro et al , 2020 ) , CrossCheck ( Arendt et al , 2021 ) , and AllenNLP Interpret ( Wallace et al , 2019 ) have successfully been applied to tasks like machine comprehension and relation extraction ( Alt et al , 2020 ) .", "entities": [[39, 40, "DatasetName", "Arendt"], [66, 68, "TaskName", "relation extraction"]]}
{"text": "As in Jurafsky and Martin ( 2021 ) , we will refer to document - level information extraction tasks as template - filling tasks and use the term going forward to refer to both event extraction and documentlevel relation extraction tasks .", "entities": [[34, 36, "TaskName", "event extraction"], [38, 40, "TaskName", "relation extraction"]]}
{"text": "Notably , in the general case , n \u2265 0 and is not specified as part of the input .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "In particular , the overall F1 score for a given document can vary based on how a predicted template is individually matched with a gold template ( or left unmatched ) .", "entities": [[5, 7, "MetricName", "F1 score"]]}
{"text": ", if G \u2212 P \u2265 0 1 + P 1 G + P 2 G ( G \u2212 1 ) + ...", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": ", if G \u2212 P < 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "Using precision and recall , the total F1 score across all the slots / roles is calculated and the template matching with the highest total F1 score is chosen .", "entities": [[7, 9, "MetricName", "F1 score"], [25, 27, "MetricName", "F1 score"]]}
{"text": "6 SCS can be interpreted as distance and is 0 between two identical spans , and 1 for non - overlapping spans .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "If si is the length of the intersection of x and y , and neither x nor y have length 0 , SCS is calculated as shown below ; otherwise , SCS is 1 . overlap = min", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "\u2212 max ( x start , y start ) si = max ( 0 , overlap ) SCS = 1 \u2212 si 2 length ( x ) * length ( y )", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "If there is some overlap between the spans , the SCS is between 0 and 1 ( not inclusive ) , and if there is no overlap between the spans , the SCS is 1 .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "The roles chosen from the MUC - 4 dataset are PERPIND ( individual perpetrator ) , PERPORG ( organization perpetrator ) , TARGET ( physical target ) , VICTIM ( human target ) , and WEAPON which are all string - fill roles , as well as INCIDENT We focus specifically on its 4 - ary relation extraction subtask .", "entities": [[53, 58, "TaskName", "4 - ary relation extraction"]]}
{"text": "In our experiments , we train and test two neuralbased IE models , described briefly below , on the MUC - 4 , ProMED , and SciREX datasets .", "entities": [[26, 27, "DatasetName", "SciREX"]]}
{"text": "Note that ( 2019 ) for the SciREX dataset and 11 tokens for the ProMED dataset .", "entities": [[7, 8, "DatasetName", "SciREX"]]}
{"text": "We use bert - base - cased and allenai / scibert_scivocab_uncased for the base BERT and SciBERT models respectively , which both have a maximum input sequence length of 512 tokens .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "For the SciREX dataset , we adopt a heuristic approach that assumes there is only one template per document , and in that template , we assign the named entities predicted by DyGIE++ for a document to the predicted role types .", "entities": [[2, 3, "DatasetName", "SciREX"]]}
{"text": "For the MUC - 4 and SciREX datasets , GTT is run for 20 epochs , while for ProMED it is run for 36 epochs , to adjust for the smaller size of the dataset .", "entities": [[6, 7, "DatasetName", "SciREX"]]}
{"text": "We use the same BERT and SciBERT base models as described in the DyGIE++ architecture above , both with a maximum input sequence length of 512 tokens .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "We first discuss the results of DyGIE++ and GTT on SciREX , ProMED , and MUC - 4 ; and then examine the performance of these newer neural models on the 1992 MUC - 4 dataset vs. a few of the bestperforming IE systems at the time .", "entities": [[10, 11, "DatasetName", "SciREX"]]}
{"text": "Table 2 shows the results of evaluating DyGIE++ and GTT on the SciREX , ProMED , and MUC - 4 datasets .", "entities": [[12, 13, "DatasetName", "SciREX"]]}
{"text": "We can see that all models perform substantially worse on scientific texts ( ProMED , SciREX ) as compared to news ( MUC - 4 ) , likely because the model base is pretrained for generalpurpose NLP applications ( BERT ) or there are not enough examples of scientific - style text in the pretraining corpus ( SciBERT ) .", "entities": [[15, 16, "DatasetName", "SciREX"], [39, 40, "MethodName", "BERT"]]}
{"text": "In addition , models seem to perform better on the news - style ProMED dataset than the scientific - paper - based long - text SciREX dataset .", "entities": [[25, 26, "DatasetName", "SciREX"]]}
{"text": "This can be explained by the fact that all four models handle a maximum of 512 tokens as inputs , while the average length of a SciREX document is 5401 tokens .", "entities": [[26, 27, "DatasetName", "SciREX"]]}
{"text": "Nevertheless , we see an increase in F1 scores for all SciBERT - based models when compared to their BERT counterparts for the SciREX dataset .", "entities": [[7, 8, "MetricName", "F1"], [19, 20, "MethodName", "BERT"], [23, 24, "DatasetName", "SciREX"]]}
{"text": "This can be explained by the fact that GTT ( SciBERT ) has more Missing Template errors than GTT ( BERT ) .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "So even if GTT ( SciBERT ) performs better on the scientific slot VICTIMS , i.e. it extracts more scientific information , it does not identify relevant events as well as GTT ( BERT ) , reducing the F1 score across the remaining slots .", "entities": [[33, 34, "MethodName", "BERT"], [38, 40, "MetricName", "F1 score"]]}
{"text": "However , there is no significant difference in the number of missing templates between the two models on the ProMED and SciREX datasets .", "entities": [[21, 22, "DatasetName", "SciREX"]]}
{"text": "We can also conclude that DyGIE++ is worse at coreference resolution when compared to GTT as DyGIE++ makes more Duplicate Role Filler errors across all datasets .", "entities": [[9, 11, "TaskName", "coreference resolution"]]}
{"text": "Currently , the error analysis tool reports exact match precision / recall / F1 which is more suitable for string - fill roles .", "entities": [[7, 9, "MetricName", "exact match"], [13, 14, "MetricName", "F1"]]}
{"text": "The GTT ( BERT ) model on the MUC - 4 dataset took 1 hour and 21 minutes to train and around 11 minutes to test on Google Colab ( GPU ) .", "entities": [[3, 4, "MethodName", "BERT"], [27, 28, "DatasetName", "Google"]]}
{"text": "The GTT ( BERT ) model on the ProMED dataset took around 24 minutes to train and 4 minutes to test , while the GTT ( SciBERT ) model on the ProMED dataset took around 13 minutes to train and 4 minutes to test , both on Google Colab ( GPU ) .", "entities": [[3, 4, "MethodName", "BERT"], [47, 48, "DatasetName", "Google"]]}
{"text": "The DyGIE++ ( BERT ) model on the ProMED dataset took around 50 minutes to train , while the DyGIE++ ( SciBERT ) model on the ProMED dataset took around 1 hour and 30 minutes to train , both on a NVIDIA V100 GPU .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "For the SciREX dataset , it took around 10 - 20 minutes to run the GTT ( BERT ) and GTT ( SciBERT ) models on a NVIDIA V100 GPU .", "entities": [[2, 3, "DatasetName", "SciREX"], [17, 18, "MethodName", "BERT"]]}
{"text": "It is worth noting that since the GTT model embeds all inputs before training and SciREX documents are extremely long , more than 25 GB of memory needs to be allocated at the embedding phrase .", "entities": [[15, 16, "DatasetName", "SciREX"]]}
{"text": "The DyGIE++ ( BERT ) model took around 2 hours to train , while the DyGIE++ ( SciBERT ) model took around 4 hours to train , both on a NVIDIA V100 GPU .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Our research was supported , in part , by NSF CISE Grant 1815455 and the Cornell CS Department CSURP grants for undergraduate research .", "entities": [[15, 16, "DatasetName", "Cornell"], [16, 17, "DatasetName", "CS"]]}
{"text": "Each singleton Alter Span transformation is mapped to a Span Error .", "entities": [[10, 11, "MetricName", "Error"]]}
{"text": "A Span Error occurs when a predicted role filler becomes an exact match to the a gold role filer only upon span alteration .", "entities": [[2, 3, "MetricName", "Error"], [11, 13, "MetricName", "exact match"]]}
{"text": "This happens when the system fails at coreference resolution .", "entities": [[7, 9, "TaskName", "coreference resolution"]]}
{"text": "This happens when the system fails at correct span extraction and coreference resolution .", "entities": [[11, 13, "TaskName", "coreference resolution"]]}
{"text": "Intrinsic Evaluation of Summarization Datasets", "entities": [[3, 4, "TaskName", "Summarization"]]}
{"text": "Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees .", "entities": [[3, 4, "TaskName", "summarization"]]}
{"text": "In spite of this , data quality has gone largely unquestioned for many recent summarization datasets .", "entities": [[14, 15, "TaskName", "summarization"]]}
{"text": "We perform the first large - scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets .", "entities": [[9, 10, "TaskName", "summarization"]]}
{"text": "We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the datasets employed .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "From the perspective of datasets , several works have shown that standard datasets in areas such as visual question answering Kafle and Kanan , 2017 ) , natural language inference ( Gururangan et al , 2018 ; Poliak et al , 2018 ) , and reading comprehension ( Kaushik and Lipton , 2018 ) contain annotation artifacts that often give rise to these spurious correlations or reasoning shortcuts .", "entities": [[17, 20, "DatasetName", "visual question answering"], [27, 30, "TaskName", "natural language inference"], [45, 47, "TaskName", "reading comprehension"]]}
{"text": "In this work , we extend these efforts towards the setting of summarization .", "entities": [[12, 13, "TaskName", "summarization"]]}
{"text": "We find this to be particularly timely since several summarization datasets have been released in recent years with little discussion of data quality .", "entities": [[9, 10, "TaskName", "summarization"]]}
{"text": "While prior work on evaluating NLP datasets has focused on their difficulty , transparency , or bias , we consider broadly the overall quality of the dataset - in our case , for the task of summarization .", "entities": [[36, 37, "TaskName", "summarization"]]}
{"text": "With this in mind , we present a multiaspect large - scale study of summarization datasets that dissects summarization into 5 properties that are evaluated across 10 datasets spanning multiple summarization domains .", "entities": [[14, 15, "TaskName", "summarization"], [18, 19, "TaskName", "summarization"], [30, 31, "TaskName", "summarization"]]}
{"text": "Most strikingly , we show that quantifiable aspects of summarization datasets are inconsistent with their use by the NLP community in several instances .", "entities": [[9, 10, "TaskName", "summarization"]]}
{"text": "In particular , without evaluating for the quality and accuracy of data used to test models , it is impossible to be certain that progress is being made and that successive iterations of models truly make progress on the underlying task or linguistic phenomena of interest .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "Within NLP , iconic datasets such as the Penn Treebank ( Marcus et al , 1993 ) have sustained subareas such as language modelling , part - of - speech tagging , and syntactic parsing for years due to the painstaking annotation efforts put into making these high - fidelity resources .", "entities": [[8, 10, "DatasetName", "Penn Treebank"], [22, 24, "TaskName", "language modelling"], [25, 31, "TaskName", "part - of - speech tagging"]]}
{"text": "And in the context of summarization , initial datasets , such as those produced during the Document Understanding Conference ( DUC ) and Text Analysis Conference ( TAC ) evaluations , implemented fine - grained verification of data quality .", "entities": [[5, 6, "TaskName", "summarization"]]}
{"text": "Nonetheless , several recent natural language understanding datasets ( Bowman et al , 2015 ; Rajpurkar et al , 2016 ; Suhr et al , 2017 ) institute explicit qualitycontrol procedures in crowd - sourcing dataset construction ( Zaidan and Callison - Burch , 2011 ; Yan et al , 2014 ; Callison - Burch et al , 2015 ) , such as using additional annotators to validate annotations ( c.f . Geva et", "entities": [[4, 7, "TaskName", "natural language understanding"], [46, 49, "DatasetName", "Yan et al"]]}
{"text": "In the sibling subfield of machine translation , which often shares similar modelling challenges and evaluation regimes as summarization due to the shared nature of being sequence - to - sequence natural language generation tasks , the annual WMT conference 3 consistently furnishes high quality data .", "entities": [[5, 7, "TaskName", "machine translation"], [18, 19, "TaskName", "summarization"]]}
{"text": "And in comparison with other subareas of NLP , we argue that summarization has lagged behind in rigorously ensuring the quality of widely - used datasets .", "entities": [[12, 13, "TaskName", "summarization"]]}
{"text": "While I ( S ; P ) may be greater than zero ( e.g. language modelling pretraining provides statistical information that may facilitate a model to avoid a priori unlikely summaries ) , standard pretraining regimes such as large - scale language modelling over generic text corpora ( Devlin et al , 2019 ; Raffel et al , 2019 ) are likely insufficient to meaningfully learn to summarize .", "entities": [[14, 16, "TaskName", "language modelling"], [41, 43, "TaskName", "language modelling"]]}
{"text": "We hypothesize that the quality of the training dataset T is highly correlated with its mutual information with respect to the summarization task S , I ( S ; T ) .", "entities": [[21, 22, "TaskName", "summarization"]]}
{"text": "Sp\u00e4rck Jones ( 1999 ) famously argued that summarization systems should be understood conditional on the context in which they will be used .", "entities": [[8, 9, "TaskName", "summarization"]]}
{"text": "In recent years , the field has significantly departed from this perspective and primarily studied \" general - purpose summarization \" ( Kryscinski et al , 2019 ) , which she denounced as ignis fatuus .", "entities": [[19, 20, "TaskName", "summarization"]]}
{"text": "With our work , we adopt the perspective that for all datasets it is strictly preferable to have all properties quantified ; it is the responsibility of practitioners building summarization systems to accurately weight different metrics based on their ultimate goals and use cases .", "entities": [[29, 30, "TaskName", "summarization"]]}
{"text": "As such , we refrain from providing prescriptive domain - agnostic or context - agnostic notions of summarization .", "entities": [[17, 18, "TaskName", "summarization"]]}
{"text": "Nevertheless , for summarization , our insight is that various aspects of a summarization example ( a document - summary pair ) can be reliably estimated by re - purposing existing NLP methods .", "entities": [[3, 4, "TaskName", "summarization"], [13, 14, "TaskName", "summarization"]]}
{"text": "We are guided by pioneering work ( Luhn , 1958 ; Edmundson , 1969 ; Mani , 1999 ) that defined core properties of summarization systems and influential sub - sequent work ( Radev et al , 2002 ; Nenkova , 2006 ; Nenkova and McKeown , 2012 ; Peyrard , 2019a ) that refined and extended these properties .", "entities": [[24, 25, "TaskName", "summarization"]]}
{"text": "Each metric assigns a value x [ 0 , 1 ] to every ( D i , S i ) where 1 is the maximal score and example - level scores are averaged to yield a dataset - level score .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "i | M for a given summary and document : TS ( D i , S i )", "entities": [[10, 11, "MethodName", "TS"]]}
{"text": "\u03b8 S", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "RED ( S i )", "entities": [[0, 1, "DatasetName", "RED"]]}
{"text": "We evaluate the semantic coherence of multi - sentence summaries by predicting the probability of each successive sentence conditioned on the previous one using a powerful language model , BERT ( Devlin et al , 2019 ) , pretrained with precisely this objective .", "entities": [[29, 30, "MethodName", "BERT"]]}
{"text": "| S | | j=2 1 BERT ( S", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "i | | \u2212 1 ( 6 ) 4 Data We study the following 10 summarization datasets that have been frequently used in recent years .", "entities": [[15, 16, "TaskName", "summarization"]]}
{"text": "CNN - DM ( Hermann et al , 2015 ; Nallapati et al , 2016 ) is a dataset composed of CNN and Daily Mail news articles with summaries that are a concatenated list of highlight bullet points .", "entities": [[0, 3, "DatasetName", "CNN - DM"]]}
{"text": "GW ( Graff and Cieri , 2003 ) is the Gigaword headline generation dataset that some refer to as a summarization dataset ( Rush et al , 2015 ;", "entities": [[20, 21, "TaskName", "summarization"]]}
{"text": "XSum ( Narayan et al , 2018 ) is an extreme summarization dataset where BBC articles are paired with single - sentence summaries written generally by the author of the article that tries to motivate the BBC audience to read the article by answering \" What is the article about ? \" .", "entities": [[0, 1, "DatasetName", "XSum"], [10, 12, "TaskName", "extreme summarization"]]}
{"text": "PeerRead ( Kang et al , 2018 ) is a dataset of paper drafts from top - tier computer science venues as well as arXiv .", "entities": [[0, 1, "DatasetName", "PeerRead"], [24, 25, "DatasetName", "arXiv"]]}
{"text": "8 Consistent with its use in the summarization community , we consider the full introduction to be the source document and the ab - stract to be the target summary .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "Unlike PeerRead , the full paper is taken as the document but the summary is still specified as the abstract .", "entities": [[1, 2, "DatasetName", "PeerRead"]]}
{"text": "TL ; DR ( V\u00f6lske et al , 2017 ) is a dataset of userwritten articles from the social media platform Reddit along with the author - provided courtesy summaries that tend to be multi - sentence .", "entities": [[21, 22, "DatasetName", "Reddit"]]}
{"text": "MovieScript ( Gorinski and Lapata , 2015 ) is a dataset of movie scripts drawn from the Script - Base corpus that are aligned with user - written summaries sourced either from Wikipedia or IMDB .", "entities": [[34, 35, "DatasetName", "IMDB"]]}
{"text": "Various additional data provided within the Movi - eScript dataset is neglected in this work .", "entities": [[6, 7, "DatasetName", "Movi"]]}
{"text": "Compression scores quantitatively disambiguate summarization tasks .", "entities": [[4, 5, "TaskName", "summarization"]]}
{"text": "Concretely , we observe GW has the lowest compression scores and while GW is sometimes described as a summarization dataset ( Rush et al , 2015 ; Chopra et al , 2016 ) , it is better seen as a headline generation dataset that is more in the style of sentence compression ( as is suggested by S i", "entities": [[18, 19, "TaskName", "summarization"], [50, 52, "DatasetName", "sentence compression"]]}
{"text": "Conversely , AMI and Movi - eScript achieve the highest scores by a substantial margin and are long - document summarization datasets .", "entities": [[4, 5, "DatasetName", "Movi"], [19, 21, "TaskName", "document summarization"]]}
{"text": "Classifying new summarization datasets accurately may prove useful given that successful methods from one domain often do not extend to another and this shortcoming in generalization can be attributed to the differences in compression requirements ( Cohan et al , 2018 ) .", "entities": [[2, 3, "TaskName", "summarization"]]}
{"text": "Given the goals stated in the XSum dataset paper , TL ; DR may be a better choice than XSum .", "entities": [[6, 7, "DatasetName", "XSum"], [19, 20, "DatasetName", "XSum"]]}
{"text": "In particular , Narayan et al ( 2018 ) introduce XSum as a large dataset that legitimately requires abstraction .", "entities": [[10, 11, "DatasetName", "XSum"]]}
{"text": "While XSum is more abstractive than other News datasets ( barring GW ) and is relatively large , TL ; DR displays greater abstractivity , similar length summaries , and is 15 times larger .", "entities": [[1, 2, "DatasetName", "XSum"]]}
{"text": "That said , Narayan et al ( 2018 ) explore topic - oriented strategies in their work and such methods may be better suited to XSum given the TS scores .", "entities": [[25, 26, "DatasetName", "XSum"], [28, 29, "MethodName", "TS"]]}
{"text": "CNN - DM and NYT are suboptimal for studying abstractive / extractive systems respectively .", "entities": [[0, 3, "DatasetName", "CNN - DM"]]}
{"text": "Several recent works ( See et al , 2017 ; Paulus et al , 2018 ; Li et al , 2018 ) have used CNN - DM to build and evaluate abstractive systems .", "entities": [[24, 27, "DatasetName", "CNN - DM"]]}
{"text": "Given our findings , we find both of these trends to be inconsistent with dataset properties and suboptimal given other preferable datasets for these purposes : CNN - DM is one of the least abstractive datasets and there are larger and more extractive alternatives to NYT such as NWS .", "entities": [[26, 29, "DatasetName", "CNN - DM"]]}
{"text": "Especially in the case of CNN - DM , we note that training learning - based systems ( e.g. neural methods ) using data with limited abstractivity implies the resulting summarizers will be limited in their ability to generate genuinely abstractive text .", "entities": [[5, 8, "DatasetName", "CNN - DM"]]}
{"text": "This is validated by empirical findings as both See et al ( 2017 ) and Zhang et al ( 2018 ) observe limited abstractivity in abstractive systems trained on CNN - DM .", "entities": [[29, 32, "DatasetName", "CNN - DM"]]}
{"text": "9 CNN - DM is not a representative benchmark for summarization as a whole .", "entities": [[1, 4, "DatasetName", "CNN - DM"], [10, 11, "TaskName", "summarization"]]}
{"text": "Recent work ( Kryscinski et al , 2019 ; Raffel et al , 2019 ) has explicitly portrayed CNN - DM as the benchmark dataset for summarization ; the field has implicitly done this for several years ( Kryscinski et al , 2019 ) .", "entities": [[18, 21, "DatasetName", "CNN - DM"], [22, 24, "DatasetName", "the benchmark"], [26, 27, "TaskName", "summarization"]]}
{"text": "While there is clear value in evaluating pretrained representations on summarization datasets , we caution against using CNN - DM as a stand - in for the entire summarization subfield .", "entities": [[10, 11, "TaskName", "summarization"], [17, 20, "DatasetName", "CNN - DM"], [28, 29, "TaskName", "summarization"]]}
{"text": "While this adds additional overhead , this cost is necessary to draw meaningful conclusions about the impact of advances on summarization broadly given the pronounced diversity in summarization datasets ( Table 1 ) .", "entities": [[20, 21, "TaskName", "summarization"], [27, 28, "TaskName", "summarization"]]}
{"text": "Specifically , Nenkova ( 2006 ) argues that redundancy is a clear inhibitor for practical application of summarization systems .", "entities": [[17, 18, "TaskName", "summarization"]]}
{"text": "We observe that the Scientific summaries ( which are abstracts of published papers ) are clearly more coherent than the author - generated summaries in TL ; DR , the fragmented summaries in AMI , and the concatenated bullet - point summaries in CNN - DM .", "entities": [[43, 46, "DatasetName", "CNN - DM"]]}
{"text": "We find that this distinction is captured by the SC measure using BERT .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "While the properties we evaluate for do not exhaust all aspects of summarization that may be of interest , it is unclear to what extent different measures overlap in judgments .", "entities": [[12, 13, "TaskName", "summarization"]]}
{"text": "10 \u03c1 = 1 indicates perfect positive correlation ( which is why we see this for all diagonal entries ) and \u03c1 < 0 indicates the metrics are anti - correlated .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "Additionally , since we measure topic similarity using LDA and unigram count statistics , it is not surprising that extractions may correlate with high topic similarity .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "In particular , while we find the semantic coherence scores are appropriate for most examples we manually inspected , this suggests that BERT relies upon word - level overlaps in making next - sentence judgments ( similar to behaviors seen in other sentence - pair tasks such as natural language inference , c.f Gururangan et al , 2018 )", "entities": [[22, 23, "MethodName", "BERT"], [48, 51, "TaskName", "natural language inference"]]}
{"text": "Crucial information in the summary does not appear in the reference document and is not general knowledge .", "entities": [[15, 17, "TaskName", "general knowledge"]]}
{"text": "In particular , we unsurprisingly found that academic papers in the Science domain generally have highly grammatical summaries whereas the bullet - point summaries in CNN - DM and the author - written summaries in TL ;", "entities": [[25, 28, "DatasetName", "CNN - DM"]]}
{"text": "On the other hand , heavily compressed summaries in NWS and XSum often are just category labels ( e.g. Sports ) , in TL ;", "entities": [[11, 12, "DatasetName", "XSum"]]}
{"text": "Highly abstractive summaries in PeerRead are often translated to English from the reference document 's language and discuss results that do not appear in the introduction but likely appear later in the paper .", "entities": [[4, 5, "DatasetName", "PeerRead"]]}
{"text": "Within the context of our sample of examples , we find that eight of the ten summarization datasets ( all but AMI , MovieScript ) contain at least 8 % low quality examples , the majority contain at least 14 % low quality examples , and that these low quality examples can be detected using our compression and abstractivity metrics .", "entities": [[16, 17, "TaskName", "summarization"]]}
{"text": "In general , we find that the low quality TL ; DR \" summaries \" we detect often serve a different rhetorical purpose than summarization ( e.g. attention grabbing , responding to a previous post that is not available in the dataset , sarcasm / humor ) .", "entities": [[24, 25, "TaskName", "summarization"]]}
{"text": "As an alternative to automated evaluation , Chen et al ( 2016 ) and Yatskar ( 2019 ) conduct human evaluations of standard datasets in reading comprehension and question answering .", "entities": [[25, 27, "TaskName", "reading comprehension"], [28, 30, "TaskName", "question answering"]]}
{"text": "Within the context of summarization , the special case of quality estimation regarding factual consistency / faithfulness has been of recent interest ( Wang et al , 2020 ;", "entities": [[4, 5, "TaskName", "summarization"]]}
{"text": "Summarization Practices .", "entities": [[0, 1, "TaskName", "Summarization"]]}
{"text": "Several analyses and critiques exist for different aspects of the summarization pipeline .", "entities": [[10, 11, "TaskName", "summarization"]]}
{"text": "From an evaluation perspective , several works have discussed the shortcomings of ROUGE / automated evaluation ( Liu and Liu , 2008 ; Chaganty et al , 2018 ; Hashimoto et al , 2019 ; Peyrard , 2019b ) as well proposed alternative metrics for summarization or natural language generation more broadly ( Clark et al , 2019 ; Zhang et al , 2020 ; Sellam et al , 2020 ) .", "entities": [[45, 46, "TaskName", "summarization"]]}
{"text": "al ( 2019 ) provide a critical reevaluation of summarization research .", "entities": [[9, 10, "TaskName", "summarization"]]}
{"text": "Most relevant to our work , they show that web - scraped datasets , specifically CNN - DM and NWS , contain a nontrivial fraction of examples ( approx .", "entities": [[15, 18, "DatasetName", "CNN - DM"]]}
{"text": "Jung et al ( 2019 ) provide an aspect - level evaluation of both summarization datasets and systems .", "entities": [[14, 15, "TaskName", "summarization"]]}
{"text": "In their work , the dataset analyses center on biases in the data ( e.g. positional biases , which are often seen in news summarization ) , which is reminiscent of the annotation artifacts seen in other NLP tasks ( Gururangan et al , 2018 ; Niven and Kao , 2019 ) .", "entities": [[23, 25, "TaskName", "news summarization"]]}
{"text": "Our results demonstrate that a sizeable fraction of examples in most summarization datasets are low quality .", "entities": [[11, 12, "TaskName", "summarization"]]}
{"text": "Alternatively , they can serve as additional metrics for the ( possibly unsupervised ) evaluation of summarization systems , potentially mitigating deficiencies in standard metrics , such as ROUGE , by directly penalizing redundancy and semantic incoherence .", "entities": [[16, 17, "TaskName", "summarization"]]}
{"text": "In this work , we restrict ourselves to single - document single - reference English language summarization datasets .", "entities": [[16, 17, "TaskName", "summarization"]]}
{"text": "While the datasets we study constitute a considerable fraction of dataset usage in the summarization community , several multi - document summarization datasets have been introduced ( e.g. Fabbri et al , 2019 ; Antognini and Faltings , 2020 ) and multi - reference summarization datasets have often been argued to be desirable due to under - constrained nature of the summarization task ( Kryscinski et al , 2019 ) and the ideal evaluation paradigm for ROUGE ( Lin , 2004 ) .", "entities": [[14, 15, "TaskName", "summarization"], [18, 22, "TaskName", "multi - document summarization"], [44, 45, "TaskName", "summarization"], [61, 62, "TaskName", "summarization"]]}
{"text": "Beyond English , both large summarization datasets ( Nguyen and Daum\u00e9 III , 2019 ; Varab and Schluter , 2020 ) and more general language resources / technologies ( Joshi et al , 2020 ) are less available , which may heighten the need for data quality assurance .", "entities": [[5, 6, "TaskName", "summarization"]]}
{"text": "More broadly , the measures that we introduce are automated , and therefore non - human , judgments of the quality of summarization data .", "entities": [[22, 23, "TaskName", "summarization"]]}
{"text": "Additionally , since we principally envision applying these metrics to datasets , we make no efforts to make these metrics robust to adversarially - crafted data and they are likely quite susceptible to adversarial attack .", "entities": [[33, 35, "TaskName", "adversarial attack"]]}
{"text": "In this work , we demonstrate that various aspects of summarization datasets can be intrinsically evaluated for .", "entities": [[10, 11, "TaskName", "summarization"]]}
{"text": "Our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future .", "entities": [[14, 15, "TaskName", "summarization"], [28, 29, "TaskName", "summarization"]]}
{"text": "We fix the number of training documents across datasets to attempt to control for the confound of larger datasets inducing higher quality topic models .", "entities": [[22, 24, "TaskName", "topic models"]]}
{"text": "In the main paper , we compute redundancy scores for each distinct sentence pair using ROUGE - L Fmeasure and then average these individual values to get a score for the entire summary .", "entities": [[15, 18, "MetricName", "ROUGE - L"]]}
{"text": "Alternatively , we considered other ROUGE scores ( specifically ROUGE - 1 and ROUGE - 2 ) as well as max pooling the sentence pair scores .", "entities": [[20, 22, "MethodName", "max pooling"]]}
{"text": "We report these results below in Table 7 . We do not observe significant changes with the specific ROUGE metric considered ( i.e. a Spearman \u03c1 of 1.0 which indicates a perfect correlation in the case of max pooling across the ROUGE variants ) .", "entities": [[37, 39, "MethodName", "max pooling"]]}
{"text": "We do see substantial differences between averaging and max pooling ; we find that max pooling turns out to precisely correlate ( \u03c1 = 1.0 ) with the average summary length .", "entities": [[8, 10, "MethodName", "max pooling"], [14, 16, "MethodName", "max pooling"]]}
{"text": "We therefore chose to report redundancy scores using averaging as we also qualitatively found them to be more useful and characteristic , especially for datasets such as AMI and the Scientific datasets as max pooling was overly aggressive .", "entities": [[33, 35, "MethodName", "max pooling"]]}
{"text": "While the nuances of the specific ROUGE variant did not significantly impact trends in redundancy scores , we chose to report the ROUGE - L scores in the main paper as we ( highly subjectively ) found the values to be most interpretable / consistent with values we would have assigned .", "entities": [[22, 25, "MetricName", "ROUGE - L"]]}
{"text": "We evaluate for semantic coherence between successive pairs of sentences , exploiting the auxiliary training objective of BERT beyond its masked language modeling objective .", "entities": [[17, 18, "MethodName", "BERT"], [20, 23, "TaskName", "masked language modeling"]]}
{"text": "In particular , we were especially interested in this given that many systems are designed with explicit handling of sentence boundaries ( e.g. more extractive systems first rank extractive sentences and then order a thresholded subset ) and datasets such as CNN - DM , which are artificially concatenated , may not be inherently coherent across sentence - boundaries .", "entities": [[41, 44, "DatasetName", "CNN - DM"]]}
{"text": "Our observations regarding the measure of coherence provided by BERT 's next - sentence predictions seem to contradict existing findings .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "In particular , introduce RoBERTa as a direct followup study to BERT and find that the next - sentence prediction objective is not an effective pretraining objective for improving representations for natural language understanding ; Yang et al ( 2019 ) also provide similar evidence .", "entities": [[4, 5, "MethodName", "RoBERTa"], [8, 9, "DatasetName", "followup"], [11, 12, "MethodName", "BERT"], [31, 34, "TaskName", "natural language understanding"]]}
{"text": "However , our findings do not contest these conclusions but instead suggest that , nonetheless , BERT is a strong next - sentence predictor and that these predictions are still useful for measuring coherence across sentences .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "While we considered word or subword measures of coherence , we did not consider alternative pretrained models that are pretrained on other objectives related to inter - sentence coherence such as ALBERT ( Lan et al , 2020 ) .", "entities": [[31, 32, "MethodName", "ALBERT"]]}
{"text": "Concurrent work by Prabhumoye et al ( 2020 ) also substantiates the usefulness of BERT - based nextsentence prediction for measuring coherence and ranking sentences orders .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "We did consider this and found language modeling scores ( e.g. surprisal ) assigned via a pretrained high - quality causal lan - guage model ( GPT - 2 ) to be inconsistent with our human judgments .", "entities": [[26, 27, "MethodName", "GPT"]]}
{"text": "We believe language modeling scores in this sense are likely highly sensitive to the domain ( and even within - domain effects , e.g. lexical variation for XSum which is fairly limited given all articles are sourced from the BBC whereas for Newsroom the variation is greater given the heterogeneous group of publishers with more diversified writing styles ) .", "entities": [[27, 28, "DatasetName", "XSum"]]}
{"text": "We use the versions of GW and CNN - DM dataset released by Gehrmann et al ( 2018 ) .", "entities": [[7, 10, "DatasetName", "CNN - DM"]]}
{"text": "14 Sentence boundary tokens inserted by Gehrmann et al ( 2018 ) to improve summarization quality were removed to ensure fair comparison in our work .", "entities": [[14, 15, "TaskName", "summarization"]]}
{"text": "An important distinction in the use of the CNN - DM dataset for modeling is whether the entity - anonymized or non - anonymized version was used .", "entities": [[8, 11, "DatasetName", "CNN - DM"]]}
{"text": "We found only examples in CNN - DM failed this criterion and this constituted less than 0.1 % 114 287227 of the dataset .", "entities": [[5, 8, "DatasetName", "CNN - DM"]]}
{"text": "All results were reported then on the standard training set if we were aware of a standard split used consistently in the summarization system literature .", "entities": [[22, 23, "TaskName", "summarization"]]}
{"text": "We use LDA ( Blei et al , 2003 ) to learn all topic models and rely on the implementation in Gensim ( \u0158eh\u016f\u0159ek and Sojka , 2010 ) based on specification of Hoffman et al ( 2010 ) .", "entities": [[2, 3, "MethodName", "LDA"], [13, 15, "TaskName", "topic models"]]}
{"text": "We compute semantic coherence by predicting the probability of a sentence conditional on the preceding sentence using BERT .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "BERT was pretrained with exactly this objective ( beyond its masked language modeling objective ) and we use the released model as - is with no further fine - tuning .", "entities": [[0, 1, "MethodName", "BERT"], [10, 13, "TaskName", "masked language modeling"]]}
{"text": "The only model with a nontrivial number of parameters used in this work is the bert - base - uncased models we use in measuring semantic coherence .", "entities": [[6, 9, "HyperparameterName", "number of parameters"]]}
{"text": "Figure 3 : Dataset : PeerRead .", "entities": [[5, 6, "DatasetName", "PeerRead"]]}
{"text": "Specifically , since authors on the social discussion platform Reddit choose to provide these summaries at their discretion , we often find the \" summaries \" are attention - grabbing and serve a starkly different rhetorical purpose from how summaries are generally conceived .", "entities": [[9, 10, "DatasetName", "Reddit"]]}
{"text": "Detector : Extremely High Compression Figure 9 : Dataset : XSum .", "entities": [[10, 11, "DatasetName", "XSum"]]}
{"text": "We observe this trend quite frequently in XSum .", "entities": [[7, 8, "DatasetName", "XSum"]]}
{"text": "Intuitively , the claim is that the uncertainty about the summarization task that is reduced by the model ( which is uniquely determined by its training data , pretraining data , and architecture ) is at most what can be cumulatively reduced by the training data , pretraining data , and", "entities": [[10, 11, "TaskName", "summarization"]]}
{"text": "We do note that this second hypothesis may be false given the partial evidence of GPT - 3 ( Brown et al , 2020 ) and the successes it enjoys in few - shot learning due to pretraining at unprecedented scale .", "entities": [[15, 16, "MethodName", "GPT"], [31, 35, "TaskName", "few - shot learning"]]}
{"text": "However , no evaluation is conducted on summarization data in that work .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "We thank Ge Gao , Esin Durmus , and members of the Cornell and Stanford NLP groups for their valuable advice .", "entities": [[12, 13, "DatasetName", "Cornell"]]}
{"text": "CUNI - KIT System for Simultaneous Speech Translation Task at IWSLT 2022", "entities": [[7, 8, "TaskName", "Translation"]]}
{"text": "In this paper , we describe our submission to the Simultaneous Speech Translation at IWSLT 2022 .", "entities": [[12, 13, "TaskName", "Translation"]]}
{"text": "This paper describes the CUNI - KIT submission to the Simultaneous Speech Translation task at IWSLT 2022 ( Anastasopoulos et al , 2022 ) by Charles University ( CUNI ) and Karlsruhe Institute of Technology ( KIT ) .", "entities": [[12, 13, "TaskName", "Translation"]]}
{"text": "Recent work on end - to - end ( E2E ) simultaneous speech - to - text translation ( ST ) is focused on training specialized models specifically for this task .", "entities": [[9, 10, "DatasetName", "E2E"], [12, 18, "TaskName", "speech - to - text translation"]]}
{"text": "In this work , we base our system on a robust multilingual offline ST model that leverages pretrained wav2vec 2.0 ( Baevski et al , 2020 ) and mBART ( Liu et al , 2020b ) .", "entities": [[28, 29, "MethodName", "mBART"]]}
{"text": "Nguyen et al ( 2021 ) proposed a hypothesis stability detection for automatic speech recognition ( ASR ) .", "entities": [[12, 15, "TaskName", "automatic speech recognition"]]}
{"text": "Liu et al ( 2020a ) explore such strategies in the context of speech recognition and translation .", "entities": [[13, 15, "TaskName", "speech recognition"]]}
{"text": "Speech recognition and translation use chunking for simultaneous inference with various chunk sizes ranging from 300 ms to 2 seconds ( Liu , 2020 ; Nguyen et al , 2021 ) although the literature suggests that the turn - taking in conversational speech is shorter , around 200 ms ( Levinson and Torreira , 2015 ) .", "entities": [[0, 2, "TaskName", "Speech recognition"], [5, 6, "TaskName", "chunking"]]}
{"text": "= W 0 : max ( 0 , | W | \u2212n ) , ( 1 ) where W c best is the best hypothesis obtained in the beam search of c - th chunk .", "entities": [[2, 3, "DatasetName", "0"], [6, 7, "DatasetName", "0"]]}
{"text": "It reports the quality metric BLEU ( Papineni et", "entities": [[5, 6, "MetricName", "BLEU"]]}
{"text": "al , 2002 ; Post , 2018 ) and latency metrics Average Proportion ( AP , Cho and Esipova 2016 ) , Average Lagging ( AL , Ma et al 2019 ) , and Differentiable Average Lagging ( DAL , Cherry and Foster 2019 ) modified for speech source .", "entities": [[14, 15, "DatasetName", "AP"]]}
{"text": "Specifically , we implement an Agent class .", "entities": [[5, 6, "DatasetName", "Agent"]]}
{"text": "We have to implement two important functions : policy ( state ) and predict ( state ) , where state is the state of the agent ( e.g. , read processed input , emitted tokens , ... ) .", "entities": [[25, 26, "DatasetName", "agent"]]}
{"text": "The policy function returns the action of the agent : ( 1 ) READ to request more input , ( 2 ) WRITE to emit new hypothesis tokens .", "entities": [[8, 9, "DatasetName", "agent"]]}
{"text": "As soon as our agent emits an endof - sequence ( EOS ) token , the inference of the utterance is finished by the SimulEval .", "entities": [[4, 5, "DatasetName", "agent"]]}
{"text": "if | hypothesis | > 0 then return W RIT E end if end if return READ", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "First , we do experiments with a monolingual Model A , then for the submission , we use a multilingual and more robust Model B. 4 Model A is the KIT IWSLT 2020 model for the Offline Speech Translation task .", "entities": [[38, 39, "TaskName", "Translation"]]}
{"text": "Specifically , it is an end - to - end English to German Transformer model with relative attention .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "wav2vec 2.0 is a Transformer encoder model which receives raw waveforms as input and generates high - level representations .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "The architecture consists of two main components : first , a convolution - based feature extractor downsamples long audio waveforms into features that have similar lengths with spectrograms .", "entities": [[11, 12, "MethodName", "convolution"]]}
{"text": "After that , a deep Transformer encoder uses self - attention and feedforward neural network blocks to transform the features without further downsampling .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "During the self - supervised training process , the network is trained with a contrastive learning strategy ( Baevski et al , 2020 ) , in which the already downsampled features are randomly masked and the model learns to predict the quantized latent representation of the masked time step .", "entities": [[14, 16, "MethodName", "contrastive learning"]]}
{"text": "We fine - tune all of the weights in the Transformer encoder .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "Moreover , to make the model more robust to the fluctuation in absolute positions and durations when it comes to audio signals , we added the relative position encodings ( Dai et al , 2019 ; Pham et al , 2020a ) to alleviate this problem .", "entities": [[26, 29, "MethodName", "relative position encodings"]]}
{"text": "The mBART50 model follows the Transformer encoder and decoder ( Vaswani et al , 2017 ) .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "For the onlinization experiments , we use MuST - C ( Cattoni et al , 2021 ) tst - COMMON from the v2.0 release .", "entities": [[7, 10, "DatasetName", "MuST - C"]]}
{"text": "The results on MuST - C tst - COMMON are in Table 2 .", "entities": [[3, 6, "DatasetName", "MuST - C"]]}
{"text": "Later , we implemented the same onlinization approach using wav2vec 2.0 and mBART from Huggingface Transformers ( Wolf et al , 2020 ) .", "entities": [[12, 13, "MethodName", "mBART"]]}
{"text": "A ) and the submitted system ( Model B ) on the MuST - C v2 tst - COMMON .", "entities": [[12, 15, "DatasetName", "MuST - C"]]}
{"text": ", the grant 19 - 26934X ( NEUREM3 ) of the Czech Science Foundation , the European Union 's Horizon 2020 Research and Innovation Programme under Grant Agreement No 825460 ( ELITR ) , and partly supported by a Facebook Sponsored Research Agreement \" Language Similarity in Machine Translation \" .", "entities": [[47, 49, "TaskName", "Machine Translation"]]}
{"text": "We also train a bilingual model incorporating back translation and knowledge distillation then combine the two models using sequence - to - sequence mapping .", "entities": [[10, 12, "MethodName", "knowledge distillation"]]}
{"text": "Neural machine translation ( NMT ) witnessed a lot of success in the past few years especially for high resource languages ( Vaswani et al , 2017 ) .", "entities": [[1, 3, "TaskName", "machine translation"]]}
{"text": "Some of the popular techniques are adding high resource helper languages as in multilingual neural machine translation ( MNMT )", "entities": [[15, 17, "TaskName", "machine translation"]]}
{"text": "al , 2016 ; Johnson et al , 2017 ; Arivazhagan et al , 2019 ) , using monolingual data including pre - training ( Liu et al , 2020 ) , multi - task learning ( Wang et al , 2020 ) , back translation ( Sennrich et al , 2016 ) or any combination of these methods ( Barrault et al , 2020 ) and system combination of multiple systems ( Liu et al , 2018 ) .", "entities": [[32, 36, "TaskName", "multi - task learning"]]}
{"text": "Using knowledge distillation ( Freitag et al , 2017 ) to create bilingual baselines from the original multilingual model and combining it with the multilingual model .", "entities": [[1, 3, "MethodName", "knowledge distillation"]]}
{"text": "For Bengali , English , Hindi and German , we apply fastText 1 language identification on the monolingual data to remove sentences which are not predicted as the expected language .", "entities": [[11, 12, "MethodName", "fastText"], [13, 15, "TaskName", "language identification"]]}
{"text": "We do the same for Hausa , Xhosa and Zulu using Polyglot 2 because fastText does not cover these three languages .", "entities": [[14, 15, "MethodName", "fastText"]]}
{"text": "The models use a transformer base architecture comprising 6 encoder and 6 decoder layers and a 24 K joint vocabulary built for Bengali \u2194 Hindi , a 8 K joint vocabulary built for English \u2194 Hausa and a 4 K joint vocabulary built for Xhosa \u2194 Zulu using sentencepiece ( Kudo and Richardson , 2018 ) to learn these subword units to tokenize the sentences .", "entities": [[48, 49, "MethodName", "sentencepiece"]]}
{"text": "The model uses a 64 K joint vocabulary constructed using sentencepiece ( Kudo and Richardson , 2018 ) from a subset of the monolingual data of each language as described in Section 2 .", "entities": [[10, 11, "MethodName", "sentencepiece"]]}
{"text": "The objective comprises the usual parallel data likelihood referred to as MT , a masked language model ( MLM ) at the encoder and a denoising auto - encoder ( DAE ) ( similar to mBART ( Liu et al , 2020 ) ) at the decoder side .", "entities": [[18, 19, "DatasetName", "MLM"], [25, 26, "TaskName", "denoising"], [35, 36, "MethodName", "mBART"]]}
{"text": "For example , individual models obtained from different checkpoints during the same training or by training models sharing the same vocab and architecture using different data or simply different random seeds can be combined using model averaging techniques .", "entities": [[30, 31, "DatasetName", "seeds"]]}
{"text": "We also use the resulting multilingual model to create a bilingual model incorporating back translation and knowledge distillation .", "entities": [[16, 18, "MethodName", "knowledge distillation"]]}
{"text": "RetNRef ignores the exemplar during response generation , RetNRef \u03b1 generates the response highly over - fitted to the exemplar , and RetNRef trained with our training method ( CORGE )", "entities": [[5, 7, "TaskName", "response generation"], [9, 10, "HyperparameterName", "\u03b1"]]}
{"text": "Knowledge - grounded generation models that utilize retrieved results ( e.g. , relevant documents from Wikipedia ) to generate informative responses have been proposed to perform knowledge - intensive NLP tasks ( e.g. , open - domain question answering ) .", "entities": [[34, 39, "TaskName", "open - domain question answering"]]}
{"text": "Guu et al ( 2020 ) show the effectiveness of pre - training a knowledge retriever with the largescale language model for open - domain question answering , and Lewis et al ( 2020 ) demonstrate that knowledge - grounded generative models produce more informative and diverse sentences than vanilla generative models on a wide range of knowledgeintensive NLP tasks .", "entities": [[22, 27, "TaskName", "open - domain question answering"]]}
{"text": "Fan et al ( 2021 ) similarly propose a knowledge - grounded generative model for response generation , but they do not focus on the open - domain conversation .", "entities": [[15, 17, "TaskName", "response generation"]]}
{"text": "As mentioned in Roller et al ( 2021 ) , the primitive exemplar - based generative model tends to ignore the retrieved exemplar dur - ing response generation due to the one - to - many problem in open - domain conversation ( Li et al , 2016 ) .", "entities": [[26, 28, "TaskName", "response generation"]]}
{"text": "However , this searching process could return a significantly different exemplar z from the gold response r i , and it induces the generator G to ignore the retrieved exemplar during response generation .", "entities": [[31, 33, "TaskName", "response generation"]]}
{"text": "z i , j , c i ) per each selected exemplar z i , j , then apply the softmax function to the relevance score to 1 Note that S R", "entities": [[20, 21, "MethodName", "softmax"]]}
{"text": "Our final training objective is to minimize the loss function L = n i=1 L ( r i , c i )", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "where \u03b1 \u22121 = z Z i P R ( z , c i ) P G ( r i | c i , z ) .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "Difference between CORGE and Knowledgegrounded generative models The way of leveraging the relevance scores is already employed by knowledge - grounded generative models ( Lewis et al , 2020 ; Sachan et al , 2021 ) in open - domain question answering .", "entities": [[37, 42, "TaskName", "open - domain question answering"]]}
{"text": "We utilize the following four datasets used in Roller et al ( 2021 ) , which are Blended Skill Talk ( BST ) ( Smith et al , 2020 ) , ConvAI2 ( Zhang et al , 2018 ) , Empathetic Dialogues ( ED ) ( Rashkin et al , 2019 ) , and Wizard of Wikipedia ( WoW ) .", "entities": [[17, 20, "DatasetName", "Blended Skill Talk"], [31, 32, "DatasetName", "ConvAI2"], [54, 57, "DatasetName", "Wizard of Wikipedia"]]}
{"text": "al , 2018 ) and Blender 90 M ( Roller et al , 2021 ) are considered as a baseline retrieval model and a baseline generative model .", "entities": [[5, 6, "MethodName", "Blender"]]}
{"text": "Knowledge - grounded Generative Models Although RAG ( Lewis et al , 2020 ) and KIF ( Fan et al , 2021 ) are proposed to perform knowledgegrounded generation tasks , we employ RAG and KIF as baselines since they have a similar form with exemplar - based generative models .", "entities": [[6, 7, "MethodName", "RAG"], [33, 34, "MethodName", "RAG"]]}
{"text": "We use Amazon Mechanical Turk to collect the annotations , and more details are described in the Supplementary Material .", "entities": [[17, 19, "DatasetName", "Supplementary Material"]]}
{"text": "We also employ the automatic evaluation metrics , Perplexity ( PPL ) , Dist - n , and BLEU ( Papineni et", "entities": [[8, 9, "MetricName", "Perplexity"], [18, 19, "MetricName", "BLEU"]]}
{"text": "BLEU ( z , r ) is adopted to measure the degree of the token overlap between the provided exemplar and the generated response pair ( z , r ) .", "entities": [[0, 1, "MetricName", "BLEU"]]}
{"text": "A higher BLEU ( z , r ) score indicates that the generator copies more from the provided exemplar while generating the response .", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "We provide the details of our implementation in the Supplementary Material .", "entities": [[9, 11, "DatasetName", "Supplementary Material"]]}
{"text": "This result demonstrates that RetNRef \u03b1 does not make good use of the retrieved exemplar except when the gold response is given as the retrieved exemplar .", "entities": [[5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "From this observation , we claim that RetNRef \u03b1 generates a response highly over - fitted to the selected exemplar , which is caused by utilizing the gold response as an exemplar in the training phase .", "entities": [[8, 9, "HyperparameterName", "\u03b1"]]}
{"text": "Higher Dist - n of RetNRef + CORGE and Mat - ToGen+CORGE compared to Blender 90 M shows that our exemplar - based generative models produce more diverse responses than the vanilla generative model .", "entities": [[14, 15, "MethodName", "Blender"]]}
{"text": "Although RetNRef \u03b1 is the only one that achieves comparable", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "Average BLEU ( z , r ) scores implicitly measure the overlap between the retrieved exemplar and the generated response ; thus , a higher degree of BLEU ( z , r ) indicates that the generator depends more on the retrieved exemplar .", "entities": [[1, 2, "MetricName", "BLEU"], [27, 28, "MetricName", "BLEU"]]}
{"text": "RetNRef shows a negligible BLEU ( z , r ) score , which reaffirms that the model is almost not utilizing the retrieved exemplar .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "RetNRef \u03b1 and MatToGen have higher BLEU ( z , r ) scores compared to RetNRef + CORGE and MatToGen+CORGE , respectively , which verifies that the former depends more on the retrieved exemplar than the latter .", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [6, 7, "MetricName", "BLEU"]]}
{"text": "PPL gold , PPL ret , and Dist - n of RAG and KIF have a similar degree to those of Blender 90 M , which implies that the exemplars are not providing useful information while generating the response .", "entities": [[11, 12, "MethodName", "RAG"], [21, 22, "MethodName", "Blender"]]}
{"text": "The average BLEU ( z , r ) score also has a poor degree , indicating almost no overlap between the retrieved exemplars and the generated responses .", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "As shown in Figure 4 , the standard deviation of normalized relevance scores P R ( z , c ) computed by the retriever almost gets near zero when the retriever of RAG is jointly trained .", "entities": [[32, 33, "MethodName", "RAG"]]}
{"text": "As a result , the retriever collapses , which means the retriever may return inappropriate exemplars to the generator ( also shown in the example of KIF and RAG in Table 4 ) .", "entities": [[28, 29, "MethodName", "RAG"]]}
{"text": "Intriguingly , jointly training the retriever with CORGE also causes the retriever scores to be flattened , as shown in Figure 4 , and we empirically observe the minor collapse of the retriever as we experienced in RAG as well .", "entities": [[37, 38, "MethodName", "RAG"]]}
{"text": "= BERT r ( z ) , d ( r ) = BERT r ( r ) , q ( c )", "entities": [[1, 2, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"]]}
{"text": "= BERT c ( c ) , ( 3 )", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "where d ( z ) and d ( r ) are encoded vectors produced by response encoder BERT r and q ( c ) is an encoded vector produced by context encoder BERT c .", "entities": [[17, 18, "MethodName", "BERT"], [32, 33, "MethodName", "BERT"]]}
{"text": "As we mentioned in Section 5.2 , we employ Biencoder 256 M and Blender 90 M as a retriever and a generator of each exemplar - based generative model , respectively .", "entities": [[13, 14, "MethodName", "Blender"]]}
{"text": "For MatToGen , additional MLP layers are added to the retriever , as follows the details in Cai et al ( 2019b ) .", "entities": [[4, 5, "DatasetName", "MLP"]]}
{"text": "When training the models , weights of the retriever and the generator are initialized with the pre - trained Bi - encoder 256 M and Blender 90 M , respectively , For Blender 90 M , we use the model released by ParlAI ( Miller et al , 2017 )", "entities": [[25, 26, "MethodName", "Blender"], [32, 33, "MethodName", "Blender"]]}
{"text": "Following ( Roller et al , 2021 ) , we choose a minimum beam length and a beam size as 20 BPE tokens and 10 , respectively , and use tri - gram beam blocking on context and response blocks .", "entities": [[21, 22, "MethodName", "BPE"]]}
{"text": "When generating the response , Blender 90 M takes 0.481 seconds , and Ret - NRef + CORGE takes 0.523 seconds per instance .", "entities": [[5, 6, "MethodName", "Blender"]]}
{"text": "There is only an 8.7 % amount of inference time gap between Blender 90 M and RetNRef + CORGE .", "entities": [[12, 13, "MethodName", "Blender"]]}
{"text": "Blender 90 M That 's good to hear , I ' ve a dog too , he ' s my best friend .", "entities": [[0, 1, "MethodName", "Blender"]]}
{"text": "Are you doing anything cool for halloween RAG", "entities": [[7, 8, "MethodName", "RAG"]]}
{"text": "Author now at Google .", "entities": [[3, 4, "DatasetName", "Google"]]}
{"text": "Hierarchical correspondences between abstract and detailed representations of concepts and events were an important aspect of the original formulation of scripts for natural language understanding ( Schank and 1 .", "entities": [[22, 25, "TaskName", "natural language understanding"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "z q 7 x d 2 d n d 2 z 9 w D 4 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R", "entities": [[20, 21, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "f 5 O B 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "z q 7 x d 2 d n d 2 z 9 w D 4 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R", "entities": [[20, 21, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "f 5 O B 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "z q 7 x d 2 d n d 2 z 9 w D 4 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R", "entities": [[20, 21, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "f 5 O B 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "z q 7 x d 2 d n d 2 z 9 w D 4 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R", "entities": [[20, 21, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "f 5 O B 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "Every instruction is paired with the original YouTube video and YouCook caption so the annotator could see how 1 Notable exceptions include the hierarchical instructions of ( Regneri et al , 2013 ) and ( Bisk et al , 2016 the action was performed , rather than hallucinating additional details .", "entities": [[10, 11, "DatasetName", "YouCook"]]}
{"text": "The average caption is approximately 4x longer than a YouCook caption .", "entities": [[9, 10, "DatasetName", "YouCook"]]}
{"text": "Closely related architectures have been proposed for segmental sequence modeling ( Wang et al , 2017 ) and phrase - based neural machine translation ( Huang et al , 2018 ) .", "entities": [[22, 24, "TaskName", "machine translation"]]}
{"text": "We train the transducer models using Viterbi EM ( after doing marginal likelihood training for the initial iterations ) , as we found it gave higher predictive accuracy than marginal likelihood training only .", "entities": [[7, 8, "MetricName", "EM"], [27, 28, "MetricName", "accuracy"]]}
{"text": "4 We also evaluate the performance of a language modelling baseline and a seq2seq model without attention ( Sutskever et al , 2014 ) , to compare the effect of not modeling alignment at all .", "entities": [[8, 10, "TaskName", "language modelling"], [13, 14, "MethodName", "seq2seq"]]}
{"text": "S k S C U b T S g + u 6 / W r N c 7 0 5 y C r x C 1 K D A o 1 + 9 a s 3 S F g W c 4 V M U", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "p + T M K g M S J d q W Q j J X f 0 /", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "S k S C U b T S g + u 6 / W r N c 7 0 5 y C r x C 1 K D A o 1 + 9 a s 3 S F g W c 4 V M U", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "p + T M K g M S J d q W Q j J X f 0 /", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "S k S C U b T S g + u 6 / W r N c 7 0 5 y C r x C 1 K D A o 1 + 9 a s 3 S F g W c 4 V M U", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "p + T M K g M S J d q W Q j J X f 0 /", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "p Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k i 0 W R Z k", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ A O 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "p Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k i 0 W R Z k", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ A O 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "p Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k i 0 W R Z k", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ A O 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "p Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k i 0 W R Z k", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ A O 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0", "entities": [[20, 21, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "k 3 b p Z h N 2 N 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "g f H j U 0 n G q G D Z Z L G L V C a h G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "p H s t H M 0 n Q j + h Q 8 p", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Z l 0 l q U L L F o j", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0", "entities": [[20, 21, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "k 3 b p Z h N 2 N 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "g f H j U 0 n G q G D Z Z L G L V C a h G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "p H s t H M 0 n Q j + h Q 8 p", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Z l 0 l q U L L F o j", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0", "entities": [[20, 21, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "k 3 b p Z h N 2 N 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "g f H j U 0 n G q G D Z Z L G L V C a h G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "p H s t H M 0 n Q j + h Q 8 p", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Z l 0 l q U L L F o j", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0", "entities": [[20, 21, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "k 3 b p Z h N 2 N 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "g f H j U 0 n G q G D Z Z L G L V C a h G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "p H s t H M 0 n Q j + h Q 8 p", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Z l 0 l q U L L F o j", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "p Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k i 0 W R Z k", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ A O 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "p Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k i 0 W R Z k", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ A O 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "p Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k i 0 W R Z k", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ A O 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h 6 9 L B b B U 0 l E 0", "entities": [[8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 6 G E /", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "a P T d r 9 4 g Y V m M 0 j B B t e 7 6 X m q C n C r D m c B p p Z d p T C", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "k b 0 y F 2 L Z U 0 R h 3 k 8 1 O n 5 M w q", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "p Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "k i 0 W R Z k", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "x a S 0 4 x c w x / 4 H z", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ A O 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0", "entities": [[20, 21, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "k 3 b p Z h N 2 N 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "g f H j U 0 n G q G D Z Z L G L V C a h G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "p H s t H M 0 n Q j + h Q 8 p", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Z l 0 l q U L L F o j", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0", "entities": [[20, 21, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "k 3 b p Z h N 2 N 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "g f H j U 0 n G q G D Z Z L G L V C a h G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "p H s t H M 0 n Q j + h Q 8 p", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Z l 0 l q U L L F o j", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q s e i F 4 8 V 7 Q e 0", "entities": [[20, 21, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "k 3 b p Z h N 2 N 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "g f H j U 0 n G q G D Z Z L G L V C a h G", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "p H s t H M 0 n Q j + h Q 8 p", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Z l 0 l q U L L F o j", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "f H j U 0 n G q G D Z Z L G L V C a h G", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "p H s t H M 0 n Q j + h Q 8 p", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "A z a q z 0 g P 1 a v 1 x x q + 4 c Z J V 4 O a l", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Z l 0 l q U L L F o j", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "We evaluate the OpenAI GPT transformer language model ( Radford et al , 2018 ) with and without fine - tuning .", "entities": [[4, 5, "MethodName", "GPT"]]}
{"text": "The left - hand side of the table shows words from the abstract YouCook annotations and corresponding phrases in the concrete annotation .", "entities": [[13, 14, "DatasetName", "YouCook"]]}
{"text": "Finally , Table 5 shows three randomly chosen examples ( from the validation set ) of greedy decodings for slot filling with GPT fine - tuned on our dataset .", "entities": [[19, 21, "TaskName", "slot filling"], [22, 23, "MethodName", "GPT"]]}
{"text": "These examples demonstrate that , first , there are cases where GPT is successful or produces a semantically valid answer ( e.g. fully vs completely ) .", "entities": [[11, 12, "MethodName", "GPT"]]}
{"text": "describe a similar corpus of natural descriptions of composite actions , useful for activity recognition in video .", "entities": [[13, 15, "TaskName", "activity recognition"]]}
{"text": "= j p ( a j | a j\u22121 , x 1 : a j\u22121 , y 1 : j\u22121 ) \u00d7 p ( y j | a j , x 1 : a j , y 1 : j\u22121 ) ( 2 ) The abstract and concrete sequences are both encoded with LSTM Recurrent Neural Networks ( Hochreiter and Schmidhuber , 1997 ) .", "entities": [[53, 54, "MethodName", "LSTM"]]}
{"text": "Due to the small size of the training data , words in both sequences are embedded using fixed GloVe embeddings ( Pennington et al , 2014 ) .", "entities": [[18, 20, "MethodName", "GloVe embeddings"]]}
{"text": "The word emission probability is then defined as p ( yj | aj , x1 : a j , y1 : j\u22121 ) = softmax ( MLP ( ea j , dj ) )", "entities": [[24, 25, "MethodName", "softmax"], [26, 27, "DatasetName", "MLP"]]}
{"text": "The parameterized alignment model contains as special cases two degenerate solutions : ( 1 ) an unconditional language model and ( 2 ) a seq2seq model .", "entities": [[24, 25, "MethodName", "seq2seq"]]}
{"text": "To make the implementation GPU - efficient , we vectorize the computation of \u03b1 .", "entities": [[13, 14, "HyperparameterName", "\u03b1"]]}
{"text": "We perform batched Viterbi EM training by computing the Viterbi alignments for a batch , and then performing a gradient step based on treating those alignments as observations .", "entities": [[4, 5, "MetricName", "EM"]]}
{"text": "We follow a two - stage training procedure : we first directly optimize the marginal likelihood with batched SGD to find a reasonable initial distribu - tion over alignments , before switching to Viterbi EM training .", "entities": [[18, 19, "MethodName", "SGD"], [34, 35, "MetricName", "EM"]]}
{"text": "Improving Joint Training of Inference Networks and Structured Prediction Energy Networks", "entities": [[7, 9, "TaskName", "Structured Prediction"]]}
{"text": "However , their alternating optimization approach suffers from instabilities during training , requiring additional loss terms and careful hyperparameter tuning .", "entities": [[14, 15, "MetricName", "loss"]]}
{"text": "In this paper , we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction .", "entities": [[22, 24, "TaskName", "structured prediction"]]}
{"text": "Belanger and McCallum ( 2016 ) formulated deep energy - based models for structured prediction , which they called structured prediction energy networks ( SPENs ) .", "entities": [[13, 15, "TaskName", "structured prediction"], [19, 21, "TaskName", "structured prediction"]]}
{"text": "During training with hinge loss , the inference network is actually trained to do \" costaugmented \" inference .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "We empirically validate our strategies on two sequence labeling tasks from natural language processing ( NLP ) , namely part - of - speech tagging and named entity recognition .", "entities": [[19, 25, "TaskName", "part - of - speech tagging"], [26, 29, "TaskName", "named entity recognition"]]}
{"text": "While SPENs have been used for multiple NLP tasks , including multi - label classification ( Belanger and McCallum , 2016 ) , part - of - speech tagging ( Tu and Gimpel , 2018 ) , and semantic role labeling ( Belanger et al , 2017 ) , they are not widely used in NLP .", "entities": [[11, 15, "TaskName", "multi - label classification"], [23, 29, "TaskName", "part - of - speech tagging"], [38, 41, "TaskName", "semantic role labeling"]]}
{"text": "Structured prediction is extremely common in NLP , but is typically approached using methods that are more limited than SPENs ( such as conditional random fields ) or models that suffer from a train / test mismatch ( such as most auto - regressive models ) .", "entities": [[0, 2, "TaskName", "Structured prediction"]]}
{"text": "SPENs offer a maximally expressive framework for structured prediction while avoiding the train / test mismatch and therefore offer great potential for NLP .", "entities": [[7, 9, "TaskName", "structured prediction"]]}
{"text": "While we have found benefit from training inference networks for machine translation in recent work ( Tu et al , 2020b ) , that work assumed a fixed , pretrained energy function .", "entities": [[10, 12, "TaskName", "machine translation"]]}
{"text": "( x ) E \u0398 ( x , y )", "entities": [[4, 5, "HyperparameterName", "\u0398"]]}
{"text": "They solved the relaxed problem by using gradient descent to iteratively minimize the energy with respect to y. The energy function parameters \u0398 are trained using a structured hinge loss which requires repeated cost - augmented inference during training .", "entities": [[22, 23, "HyperparameterName", "\u0398"], [29, 30, "MetricName", "loss"]]}
{"text": "\u2248 arg min y Y R ( x ) ( E \u0398 ( x , y )", "entities": [[11, 12, "HyperparameterName", "\u0398"]]}
{"text": "Here is the specific objective to jointly train \u0398 ( parameters of the energy function ) and \u03a6 ( parameters of the cost - augmented inference network ) :", "entities": [[8, 9, "HyperparameterName", "\u0398"]]}
{"text": "min \u0398 max \u03a6 x i , y", "entities": [[1, 2, "HyperparameterName", "\u0398"]]}
{"text": "\u2212 E \u0398 ( x i , F \u03a6 ( x i ) )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x i , y i ) ]", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ ( 4 ) where D is the set of training pairs , [ h ] + = max ( 0 , h ) , and is a structured cost function that computes the distance between its two arguments .", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "More specifically , for the fine - tuning step , we first initialize \u03a8 with \u03a6 ; next , we do gradient descent according to the following objective to learn \u03a8 : \u03a8 arg min \u03a8 x X E \u0398 ( x , A \u03a8 ( x ) ) where X is a set of training or validation inputs .", "entities": [[39, 40, "HyperparameterName", "\u0398"]]}
{"text": "In particular , we use a \" compound \" objective that combines two widely - used losses in structured prediction .", "entities": [[18, 20, "TaskName", "structured prediction"]]}
{"text": "\u0398 x", "entities": [[0, 1, "HyperparameterName", "\u0398"]]}
{"text": "\u2212E \u0398 ( x i , y )", "entities": [[1, 2, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x i , y i ) )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ margin - rescaled hinge loss + \u03bb max y ( \u2212E \u0398", "entities": [[5, 6, "MetricName", "loss"], [12, 13, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x i , y i ) )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "The margin - rescaled hinge loss contains cost - augmented inference , shown as part of Eq .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "The perceptron loss contains the test - time inference problem , which is shown in Eq .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "We avoid this issue by training two inference networks , A \u03a8 for test - time inference and F \u03a6 for cost - augmented inference : min \u0398 max \u03a6 , \u03a8", "entities": [[27, 28, "HyperparameterName", "\u0398"]]}
{"text": "\u2212E \u0398 ( x i , F \u03a6 ( x i ) )", "entities": [[1, 2, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x i , y i ) ]", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ + \u03bb [ \u2212E \u0398 ( x i , A \u03a8 ( x i ) )", "entities": [[5, 6, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x i , y i ) ]", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "We use minibatch stochastic gradient descent and alternately optimize \u0398 , \u03a6 , and \u03a8.", "entities": [[3, 6, "MethodName", "stochastic gradient descent"], [9, 10, "HyperparameterName", "\u0398"]]}
{"text": "The objective for the energy parameters \u0398 in minibatch M is : \u0398 arg min", "entities": [[6, 7, "HyperparameterName", "\u0398"], [12, 13, "HyperparameterName", "\u0398"]]}
{"text": "\u0398 x", "entities": [[0, 1, "HyperparameterName", "\u0398"]]}
{"text": "\u2212E \u0398 ( x i , F \u03a6 ( x i ) )", "entities": [[1, 2, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x i , y i )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ \u03bb \u2212E \u0398", "entities": [[3, 4, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x i , y i )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ When we remove 0 - truncation ( see Sec .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "\u2212 E \u0398 ( x i , F \u03a6 ( x i ) )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "\u2212 \u03bbE \u0398", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "That is , denoting the vector at position t of the cost - augmented network output by F \u03a6 ( x , y ) t , we have : F \u03a6 ( x , y ) t = softmax", "entities": [[38, 39, "MethodName", "softmax"]]}
{"text": "One motivation for these parameterizations is to reduce the total number of parameters in the procedure .", "entities": [[10, 13, "HyperparameterName", "number of parameters"]]}
{"text": "Generally , the number of parameters is expected to decrease when moving from separated to shared to stacked .", "entities": [[3, 6, "HyperparameterName", "number of parameters"]]}
{"text": "We will compare the three options empirically in our experiments , in terms of both accuracy and number of parameters .", "entities": [[15, 16, "MetricName", "accuracy"], [17, 20, "HyperparameterName", "number of parameters"]]}
{"text": "When describing them , we will illustrate their impact by showing training trajectories for the Twitter part - of - speech tagging task .", "entities": [[16, 22, "TaskName", "part - of - speech tagging"]]}
{"text": "l 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "\u2212 E \u0398 ( x , F \u03a6 ( x ) )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x , y ) ]", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ where [ h ] + = max ( 0 , h ) .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "However , there are two potential reasons why l 0 will equal zero and trigger no gradient update .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "First , E \u0398 ( the energy function , corresponding to the discriminator in a GAN ) may already be well - trained , and it can easily separate the gold standard output from the costaugmented inference network output .", "entities": [[3, 4, "HyperparameterName", "\u0398"], [15, 16, "MethodName", "GAN"]]}
{"text": "Second , the cost - augmented inference network ( corresponding to the generator in a GAN ) could be so poorly trained that the energy of its output is very large , leading the margin constraints to be satisfied and l 0 to be zero .", "entities": [[15, 16, "MethodName", "GAN"], [41, 42, "DatasetName", "0"]]}
{"text": "In standard margin - rescaled max - margin learning in structured prediction ( Taskar et al , 2004 ; Tsochantaridis et al , 2004 ) , the cost - augmented inference step is performed exactly ( or approximately with reasonable guarantee of effectiveness ) , ensuring that when l 0 is zero , the energy parameters are well trained .", "entities": [[10, 12, "TaskName", "structured prediction"], [49, 50, "DatasetName", "0"]]}
{"text": "However , in our case , l 0 may be zero simply because the cost - augmented inference network is undertrained , which will be the case early in training .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "Then , when using zero truncation , the gradient of the inference network parameters will be 0 .", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "This is likely why Tu and Gimpel ( 2018 ) found it important to add several stabilization terms to the l 0 objective .", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "Tu and proposed adding a local cross entropy ( CE ) loss , which is the sum of the label cross entropy losses over all positions in the sequence , to stabilize inference network training .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "We similarly find this term to help speed up convergence and improve accuracy .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "l 0 = [ ( F \u03a6 ( x ) , y )", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "\u2212 E \u0398 ( x , F \u03a6 ( x ) )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "+ E \u0398 ( x , y ) ]", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "In the GAN objective , the discriminator D is updated in the inner loop , and they alternate between multiple update steps for D and one update step for G.", "entities": [[2, 3, "MethodName", "GAN"]]}
{"text": "However , the analogy is limited , since GAN training involves sampling noise vectors and using them to generate data , while there are no noise vectors or explicitly - generated samples in our framework .", "entities": [[8, 9, "MethodName", "GAN"]]}
{"text": "In the original output space Y ( x ) , y t , j is 1 for a single j and 0 for all others .", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "R d denotes the \" input feature vector \" for position t. We define b to be the d - dimensional BiLSTM ( Hochreiter and Schmidhuber , 1997 )", "entities": [[21, 22, "MethodName", "BiLSTM"]]}
{"text": "The full set of energy parameters \u0398 includes the U j vectors , W , and the parameters of the BiLSTM .", "entities": [[6, 7, "HyperparameterName", "\u0398"], [20, 21, "MethodName", "BiLSTM"]]}
{"text": "We use h to denote an LSTM tag language model ( TLM ) that takes a sequence of labels as input and returns a distribution over next labels .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "= h ( y 0 , . . .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": ", y t\u22121 ) to be the distribution given the preceding label vectors ( under a LSTM language model ) .", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "= \u2212 T +1 t=1 log y t y t ( 8 ) where y 0 is the start - of - sequence symbol and y T +1 is the end - of - sequence symbol .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "h ( y 0 , . . . , y t\u22121 ) where h is an LSTM TLM that takes a sequence of labels as input and returns a distribution over next labels .", "entities": [[3, 4, "DatasetName", "0"], [16, 17, "MethodName", "LSTM"]]}
{"text": "We define y t = g ( x 0 , ... ,", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "x t\u22121 , y 0 , ... , y t\u22121 ) where g is a forward LSTM TLM .", "entities": [[4, 5, "DatasetName", "0"], [16, 17, "MethodName", "LSTM"]]}
{"text": "Here \u03b3 is a hyperparameter that is tuned .", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "Twitter part - of - speech ( POS ) tagging ( Gimpel et al , 2011 ) and named entity recognition ( NER ; Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[1, 4, "DatasetName", "part - of"], [18, 21, "TaskName", "named entity recognition"], [22, 23, "TaskName", "NER"]]}
{"text": "Twitter Part - of - Speech ( POS ) Tagging .", "entities": [[1, 4, "DatasetName", "Part - of"]]}
{"text": "Like Tu and Gimpel ( 2018 ) , we use a BiLSTM to compute the input feature vector for each position , using hidden size 100 .", "entities": [[11, 12, "MethodName", "BiLSTM"]]}
{"text": "The output of the inference network is a softmax function , so the inference network will produce a distribution over labels at each position .", "entities": [[8, 9, "MethodName", "softmax"]]}
{"text": "We train the inference network using stochastic gradient descent ( SGD ) with momentum and train the energy parameters using Adam ( Kingma and Ba , 2014 ) .", "entities": [[6, 9, "MethodName", "stochastic gradient descent"], [10, 11, "MethodName", "SGD"], [20, 21, "MethodName", "Adam"]]}
{"text": "We also explore training the inference network using Adam when not using the local CE loss .", "entities": [[8, 9, "MethodName", "Adam"], [15, 16, "MetricName", "loss"]]}
{"text": "Named Entity Recognition ( NER ) .", "entities": [[0, 3, "TaskName", "Named Entity Recognition"], [4, 5, "TaskName", "NER"]]}
{"text": "We use the CoNLL 2003 English dataset ( Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[3, 5, "DatasetName", "CoNLL 2003"]]}
{"text": "We use the BIOES tagging scheme , following previous work ( Ratinov and Roth , 2009 ) , resulting in 17 NER labels .", "entities": [[21, 22, "TaskName", "NER"]]}
{"text": "We use 100 - dimensional pretrained GloVe embeddings ( Pennington et al , 2014 ) .", "entities": [[6, 8, "MethodName", "GloVe embeddings"]]}
{"text": "The task is evaluated using F1 score computed with the conlleval script .", "entities": [[5, 7, "MetricName", "F1 score"]]}
{"text": "We use a dropout keep - prob of 0.7 for all LSTM cells .", "entities": [[11, 12, "MethodName", "LSTM"]]}
{"text": "We use Adam ( Kingma and Ba , 2014 ) and do early stopping on the development set .", "entities": [[2, 3, "MethodName", "Adam"], [12, 14, "MethodName", "early stopping"]]}
{"text": "We consider three NER modeling configurations .", "entities": [[3, 4, "TaskName", "NER"]]}
{"text": "NER uses only words as input and pretrained , fixed Tu and Gimpel ( 2018 ) .", "entities": [[0, 1, "TaskName", "NER"]]}
{"text": "The inference network architecture is a one - layer BiLSTM .", "entities": [[9, 10, "MethodName", "BiLSTM"]]}
{"text": "GloVe embeddings .", "entities": [[0, 2, "MethodName", "GloVe embeddings"]]}
{"text": "NER+ uses words , the case of the first letter , POS tags , and chunk labels , as well as pretrained GloVe embeddings with fine - tuning .", "entities": [[22, 24, "MethodName", "GloVe embeddings"]]}
{"text": "For example , on NER , using the CE term reduces the number of epochs chosen by early stopping from \u223c100 to \u223c25 .", "entities": [[4, 5, "TaskName", "NER"], [12, 15, "HyperparameterName", "number of epochs"], [17, 19, "MethodName", "early stopping"]]}
{"text": "For POS , using the CE term reduces the number of epochs from \u223c150 to \u223c60 .", "entities": [[9, 12, "HyperparameterName", "number of epochs"]]}
{"text": "For the separated parameterization , the performance drops slightly for NER , likely due to the larger number of parameters .", "entities": [[10, 11, "TaskName", "NER"], [17, 20, "HyperparameterName", "number of parameters"]]}
{"text": "Notably , Wasserstein GANs provided the first convergence measure in GAN training using Wasserstein distance .", "entities": [[10, 11, "MethodName", "GAN"]]}
{"text": "al ( 2018 ) proposed a novel weight normalization technique called spectral normalization .", "entities": [[7, 9, "MethodName", "weight normalization"], [11, 13, "MethodName", "spectral normalization"]]}
{"text": "Future work will explore other structured prediction tasks , such as parsing and generation .", "entities": [[5, 7, "TaskName", "structured prediction"]]}
{"text": "We have taken initial steps in this direction , considering constituency parsing with the sequence - to - sequence model of Tran et al ( 2018 ) .", "entities": [[10, 12, "TaskName", "constituency parsing"]]}
{"text": "We linearize the constituency parsing outputs , similar to Tran et al ( 2018 ) .", "entities": [[3, 5, "TaskName", "constituency parsing"]]}
{"text": "Here , b has a seq2seq - with - attention architecture identical to Tran et", "entities": [[5, 6, "MethodName", "seq2seq"]]}
{"text": "YNU - HPCC at SemEval - 2021 Task 6 : Combining ALBERT and Text - CNN for Persuasion Detection in Texts and Images", "entities": [[11, 12, "MethodName", "ALBERT"]]}
{"text": "For propaganda technology detection in text , we propose a combination model of both AL - BERT and Text - CNN for text classification , as well as a BERT - based multi - task sequence labeling model for propaganda technology coverage span detection .", "entities": [[16, 17, "MethodName", "BERT"], [22, 24, "TaskName", "text classification"], [29, 30, "MethodName", "BERT"]]}
{"text": "For the meme classification task involved in text understanding and visual feature extraction , we designed a parallel channel model divided into text and image channels .", "entities": [[2, 4, "TaskName", "meme classification"]]}
{"text": "Propaganda techniques generally include the use of logical fallacies and appeal to the emotions of the audience .", "entities": [[7, 9, "TaskName", "logical fallacies"]]}
{"text": "The detection of propaganda techniques in texts is similar to a text sentiment analysis , and both can be attributed to text classification tasks .", "entities": [[12, 14, "TaskName", "sentiment analysis"], [21, 23, "TaskName", "text classification"]]}
{"text": "In a previous study , Peng et al ( 2020 ) used the adversarial learning of sentiment word representations for a sentiment analysis .", "entities": [[21, 23, "TaskName", "sentiment analysis"]]}
{"text": "A tree - structured regional CNN - LSTM and dynamic routing in a tree - structured LSTM ( Wang et al , 2019 ) were used for a dimensional sentiment analysis .", "entities": [[7, 8, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"], [29, 31, "TaskName", "sentiment analysis"]]}
{"text": "In previous SemEval competitions , Dao et al ( 2020 ) used GloVe - LSTM and BERT - LSTM models , and Paraschiv et al ( 2020 ) used an ensemble model containing BERT and BiLSTM to detect both spans and categories of propaganda techniques in news articles .", "entities": [[12, 13, "MethodName", "GloVe"], [14, 15, "MethodName", "LSTM"], [16, 17, "MethodName", "BERT"], [18, 19, "MethodName", "LSTM"], [33, 34, "MethodName", "BERT"], [35, 36, "MethodName", "BiLSTM"]]}
{"text": "In addition , in multimodal analysis combining images and text , Yuan et al ( 2020 ) proposed a parallel channel ensemble model combining BERT embedding , BiLSTM , attention and CNN , and ResNet for a sentiment analysis of memes .", "entities": [[24, 25, "MethodName", "BERT"], [27, 28, "MethodName", "BiLSTM"], [34, 35, "MethodName", "ResNet"], [37, 39, "TaskName", "sentiment analysis"]]}
{"text": "Li et al ( 2019 ) proposed a Visual BERT model that aligns and fuses text and image information using transformers ( Vaswani et al , 2017 ) .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "For subtask 1 , we added a Text - CNN layer after the pre - trained model ALBERT to fine - tune it for a multi - label classification of text .", "entities": [[17, 18, "MethodName", "ALBERT"], [25, 29, "TaskName", "multi - label classification"]]}
{"text": "BERT was used in the model for text feature extraction followed by multi - task sequence labeling , and the results of each task were combined to obtain the final results .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "The text channel used both the ALBERT and Text - CNN models to extract features of text in the meme , and the image channel used ResNet and VGGNet for image feature extraction .", "entities": [[6, 7, "MethodName", "ALBERT"], [26, 27, "MethodName", "ResNet"]]}
{"text": "First , section 2 describes the details of the w1 , w2 , w3 , w4 , \u2026 \u2026 , wn - 1 , wn ALBERT and Text - CNN used in our system .", "entities": [[25, 26, "MethodName", "ALBERT"]]}
{"text": "This is a multi - label classification problem for text , based on the pre - trained ALBERT model and added a Text - CNN layer .", "entities": [[3, 7, "TaskName", "multi - label classification"], [17, 18, "MethodName", "ALBERT"]]}
{"text": "As illustrated in Figure 2 , the proposed model includes an ALBERT layer , a Text - CNN layer , a fully connected layer , and an output layer .", "entities": [[11, 12, "MethodName", "ALBERT"]]}
{"text": "With our model , the pretrained ALBERT model is fine - tuned to obtain a 512 \u00d7 768 hidden representation matrix for subsequent multi - label classification of text .", "entities": [[6, 7, "MethodName", "ALBERT"], [23, 27, "TaskName", "multi - label classification"]]}
{"text": "Text - CNN ( Kim , 2014 ) is a convolutional neural network applied to a text classification task , using multiple kernels of different sizes to extract key information in sentences , and is thus able to better capture the local relevance .", "entities": [[16, 18, "TaskName", "text classification"]]}
{"text": "In this layer , we used three different sizes of onedimensional convolution kernels , i.e. , 3 , 4 , and 5 , to extract information from the hidden representation matrix output from the ALBERT layer for the final multi - label text classification .", "entities": [[11, 12, "MethodName", "convolution"], [34, 35, "MethodName", "ALBERT"], [39, 44, "TaskName", "multi - label text classification"]]}
{"text": "We built the model by converting the problem to detect the coverage of each propagation technique separately for the input sequence , and built a multi - task sequence labeling model based on a fine - tuning of BERT .", "entities": [[38, 39, "MethodName", "BERT"]]}
{"text": "As illustrated in Figure 3 , the input sequence was first obtained using the pre - trained BERT ( Devlin et al , 2019 ) model with a hidden representation matrix with dimensions of 512 \u00d7 768 .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "We used a parallel channel model of text and image channels , and then concatenated the text and image features extracted by the two parallel channels to apply multi - label meme classification .", "entities": [[31, 33, "TaskName", "meme classification"]]}
{"text": "In the text channel , we used the ALBERT - Text - CNN model used in subtask 1 , taking the text part of the meme content as an input to obtain a 768 - dimensional text feature vector as the output .", "entities": [[8, 9, "MethodName", "ALBERT"]]}
{"text": "In the image channel , we used ResNet and VGGNet , taking the image part of the meme content as input to obtain a 512 - dimensional image feature vector as the output .", "entities": [[7, 8, "MethodName", "ResNet"]]}
{"text": "The ResNet model ( He et al , 2016 ) is a deep residual learning model for image recognition , and presents the interlayer residual jump connection and solves the deep vanishing gradient problem .", "entities": [[1, 2, "MethodName", "ResNet"], [17, 19, "TaskName", "image recognition"]]}
{"text": "VGGNet ( Simonyan and Zisserman , 2015 ) is a deep convolutional neural network with small - sized convolutional kernels and a regular network structure , in which the size of the convolution kernels used in VGG16 in our experiment is 3 \u00d7 3 , and the pooling kernels is 2 \u00d7 2 .", "entities": [[32, 33, "MethodName", "convolution"]]}
{"text": "Furthermore , only the structures of the ResNet and VGGNet were used in our experiment , and the pre - training weights were not applied .", "entities": [[7, 8, "MethodName", "ResNet"]]}
{"text": "All models used the TensorFlow2 backend , and all BERT - based models were implemented using the HuggingFace Transformers toolkit ( Wolf et al , 2020 ) .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "We conducted experiments on several pre - trained models including BERT , RoBERTa ( Liu et al , 2019 ) , and ALBERT combined with the Text - CNN layer , and observed that the ALBERT and Text - CNN models achieved the best performance , the reason for which may be that the training datasets are small , and a serious overfitting will occur by directly finetuning BERT .", "entities": [[10, 11, "MethodName", "BERT"], [12, 13, "MethodName", "RoBERTa"], [22, 23, "MethodName", "ALBERT"], [35, 36, "MethodName", "ALBERT"], [68, 69, "MethodName", "BERT"]]}
{"text": "Furthermore , the experiments show that the ALBERT model has fewer parameters and performs better on small datasets .", "entities": [[7, 8, "MethodName", "ALBERT"]]}
{"text": "Adding a Text - CNN layer after the BERT - based model can better extract the local relevance information of the text , which not only effectively alleviates the overfitting phenomenon it also effectively improves the model performance .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "It can be observed that ResNet18 works better than VGG16 when using both ALBERT and ALBERT - Text - CNN models .", "entities": [[13, 14, "MethodName", "ALBERT"], [15, 16, "MethodName", "ALBERT"]]}
{"text": "In this paper , we presented our system for the SemEval - 2021 Task 6 , the experimental results in subtasks 1 and 3 show that our proposed ALBERT - Text - CNN model and the parallel channel model achieved a good performance in the detection of persuasion techniques in texts and images .", "entities": [[28, 29, "MethodName", "ALBERT"]]}
{"text": "Mclarty et al ( 2018 ) trained a Support Vector Machine ( SVM ) on pre - vocalic /r/ and vowels , and their approach did quite well in classifying prevocalic /r / s.", "entities": [[8, 11, "MethodName", "Support Vector Machine"], [12, 13, "MethodName", "SVM"]]}
{"text": "so the accuracy is expected to decrease .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "On tokens where there was no ground truth , humans only agreed with the SVM classification about 55 % of the time .", "entities": [[14, 15, "MethodName", "SVM"]]}
{"text": "Following standard methods of Automatic Speech Recognition , we converted the audio to 12 Mel - Frequency - Cepstral - Coefficients ( MFCCs ) .", "entities": [[4, 7, "TaskName", "Automatic Speech Recognition"]]}
{"text": "The Gated Recurrent Unit is shown in more detail in figure 2 , where we can see the input from the previous timestep and layer , and how this is filtered through gates using tanh and sigmoid activation functions .", "entities": [[1, 4, "MethodName", "Gated Recurrent Unit"], [36, 38, "MethodName", "sigmoid activation"]]}
{"text": "For regularization we used a kernel L2 regularization for the dense layer and we used both activation L2 and Recurrent L2 for the GRU layer .", "entities": [[6, 8, "HyperparameterName", "L2 regularization"], [23, 24, "MethodName", "GRU"]]}
{"text": "By gathering more data , we would expect that our accuracy would improve and eventually reach a plateau where additional speakers would not affect anything .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "When automated methods for rhoticity reach the accuracy level of humans , along with consistency and full replicability , this will open the floodgates to large amounts of /r/ data and greatly expand sociolinguistic knowledge of dialect variation around the world , efficiently allowing studies to be replicated across research groups .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "PubMedQA : A Dataset for Biomedical Research Question Answering", "entities": [[0, 1, "DatasetName", "PubMedQA"], [7, 9, "TaskName", "Question Answering"]]}
{"text": "We introduce PubMedQA , a novel biomedical question answering ( QA ) dataset collected from PubMed abstracts .", "entities": [[2, 3, "DatasetName", "PubMedQA"], [7, 9, "TaskName", "question answering"]]}
{"text": "PubMedQA has 1k expert - annotated , 61.2k unlabeled and 211.3k artificially generated QA instances .", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}
{"text": "Each PubMedQA instance is composed of ( 1 ) a question which is either an existing research article title or derived from one , ( 2 ) a context which is the corresponding abstract without its conclusion , ( 3 ) a long answer , which is the conclusion of the abstract and , presumably , answers the research question , and ( 4 ) a yes / no / maybe answer which summarizes the conclusion .", "entities": [[1, 2, "DatasetName", "PubMedQA"]]}
{"text": "PubMedQA is publicly available at https://pubmedqa.github.io .", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}
{"text": "A long - term goal of natural language understanding is to build intelligent systems that can reason and infer over natural language .", "entities": [[6, 9, "TaskName", "natural language understanding"]]}
{"text": "The question answering ( QA ) task , in which models learn how to answer questions , is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems .", "entities": [[1, 3, "TaskName", "question answering"]]}
{"text": "Yang et al , 2018 ; , the largest annotated biomedical QA dataset , BioASQ ( Tsatsaronis et al , 2015 ) has less than 3k training instances , most of which are simple factual questions .", "entities": [[14, 15, "DatasetName", "BioASQ"]]}
{"text": "Interestingly , more than half of the question titles of PubMed articles can be briefly answered by yes / no / maybe , which is significantly higher than the proportions of such questions in other datasets , e.g. : just 1 % in Natural Questions and 6 % in HotpotQA ( Yang et al , 2018 ) .", "entities": [[43, 45, "DatasetName", "Natural Questions"], [49, 50, "DatasetName", "HotpotQA"]]}
{"text": "To this end , we present PubMedQA , a biomedical QA dataset for answering research questions using yes / no / maybe .", "entities": [[6, 7, "DatasetName", "PubMedQA"]]}
{"text": "Unlike other QA datasets in which questions are asked by crowd - workers for existing contexts ( Rajpurkar et al , 2016 ; Yang et al , 2018 ; Ko\u010disk\u1ef3 et al , 2018 ) , in PubMedQA contexts are generated to answer the questions and both are written by the same authors .", "entities": [[37, 38, "DatasetName", "PubMedQA"]]}
{"text": "This consistency assures that contexts are perfectly related to the questions , thus making PubMedQA an ideal benchmark for testing scientific reasoning abilities .", "entities": [[14, 15, "DatasetName", "PubMedQA"]]}
{"text": "As an attempt to solve PubMedQA and provide a strong baseline , we fine - tune BioBERT on different subsets in a multi - phase style with additional supervision of long answers .", "entities": [[5, 6, "DatasetName", "PubMedQA"]]}
{"text": "In 2006 and 2007 , TREC 2 held QA challenges on genomics corpus ( Hersh et al , 2006 ( Hersh et al , , 2007 , where the task is to retrieve relevant documents for 36 and 38 topic questions , respectively .", "entities": [[5, 6, "DatasetName", "TREC"]]}
{"text": "The QA task of BioASQ ( Tsatsaronis et al , 2015 ) has phases of ( a ) retrieve question - related documents and ( b ) using related documents as contexts to answer yes / no , factoid , list or summary questions .", "entities": [[4, 5, "DatasetName", "BioASQ"]]}
{"text": "BioASQ 2019 has a training set of 2 , 747 QA instances and a test set of 500 instances .", "entities": [[0, 1, "DatasetName", "BioASQ"]]}
{"text": "Several large - scale automatically collected biomedical QA datasets have been introduced : emrQA ( Pampari et al , 2018 ) is an extractive QA dataset for electronic medical records ( EHR ) built by re - purposing existing annotations on EHR corpora .", "entities": [[13, 14, "DatasetName", "emrQA"]]}
{"text": "Yes / No QA : Datasets such as HotpotQA ( Yang et al , 2018 ) , Natural Questions , ShARC ( Saeidi et al , 2018 ) and BioASQ ( Tsatsaronis et al , 2015 ) contain yes / no questions as well as other types of questions .", "entities": [[8, 9, "DatasetName", "HotpotQA"], [17, 19, "DatasetName", "Natural Questions"], [20, 21, "DatasetName", "ShARC"], [29, 30, "DatasetName", "BioASQ"]]}
{"text": "BoolQ ( Clark et al , 2019 ) specifically focuses on naturally occurring yes / no questions , and those questions are shown to be surprisingly difficult to answer .", "entities": [[0, 1, "DatasetName", "BoolQ"]]}
{"text": "We add a \" maybe \" choice in PubMedQA to cover uncertain instances .", "entities": [[8, 9, "DatasetName", "PubMedQA"]]}
{"text": "Typical neural approaches to answering yes / no questions involve encoding both the question and context , and decoding the encoding to a class output , which is similar to the well - studied natural language inference ( NLI ) task .", "entities": [[34, 37, "TaskName", "natural language inference"]]}
{"text": "Recent breakthroughs of pre - trained language models like ELMo ( Peters et al , 2018 ) and BERT ( Devlin et al , 2018 ) show significant performance i m - provements on NLI tasks .", "entities": [[9, 10, "MethodName", "ELMo"], [18, 19, "MethodName", "BERT"]]}
{"text": "In this work , we use domain specific versions of them to set baseline performance on PubMedQA .", "entities": [[16, 17, "DatasetName", "PubMedQA"]]}
{"text": "3 PubMedQA Dataset", "entities": [[1, 2, "DatasetName", "PubMedQA"]]}
{"text": "PubMedQA is split into three subsets : labeled , unlabeled and artificially generated .", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}
{"text": "We show the architecture of PubMedQA dataset in Fig .", "entities": [[5, 6, "DatasetName", "PubMedQA"]]}
{"text": "We show the basic statistics of three PubMedQA subsets in Table 1 . Instance Topics : PubMed abstracts are manually annotated by medical librarians with Medical Subject Headings ( MeSH ) 6 , which is a controlled vocabulary designed to describe the topics of biomedical texts .", "entities": [[7, 8, "DatasetName", "PubMedQA"]]}
{"text": "The main metrics of PubMedQA are accuracy and macro - F1 on PQA - L test set using question and context as input .", "entities": [[4, 5, "DatasetName", "PubMedQA"], [6, 7, "MetricName", "accuracy"], [8, 11, "MetricName", "macro - F1"]]}
{"text": "BioBERT is initialized with BERT ( Devlin et al , 2018 ) and further pretrained on PubMed abstracts and PMC 7 articles .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "Expectedly , it vastly outperforms BERT in various biomedical NLP tasks .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "While fine - tuning , we feed PubMedQA questions and contexts ( or long answers ) , separated 7 https://www.ncbi.nlm.nih.gov/pmc/ by the special [ SEP ] token , to BioBERT .", "entities": [[7, 8, "DatasetName", "PubMedQA"]]}
{"text": "The yes / no / maybe labels are predicted using the special [ CLS ] embedding using a softmax function .", "entities": [[18, 19, "MethodName", "softmax"]]}
{"text": "Cross - entropy loss of predicted and true label distribution is denoted as L QA .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "We use them as an additional signal for training : similar to Ma et al ( 2018 ) regularizing neural machine translation models with binary bag - of - word ( BoW ) statistics , we fine - tune BioBERT with an auxiliary task of predicting the binary BoW statistics of the long answers , also using the special [ CLS ] embedding .", "entities": [[20, 22, "TaskName", "machine translation"]]}
{"text": "We minimize binary crossentropy loss of this auxiliary task :", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "( 1 \u2212 b i ) log ( 1 \u2212b i ) where b i andb i are ground - truth and predicted probability of whether token i is in the long answers ( i.e. : b i { 0 , 1 } andb i [ 0 , 1 ] ) , and N is the BoW vocabulary size .", "entities": [[39, 40, "DatasetName", "0"], [46, 47, "DatasetName", "0"]]}
{"text": "The total loss is : L = L QA + \u03b2L BoW", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "\u2713 0 \u2713 0 \u2713 I \u2713 II \u2713 F \u2713 B 1 \u2713 B 2 Fine - tuning Supervision", "entities": [[1, 2, "DatasetName", "0"], [3, 4, "DatasetName", "0"]]}
{"text": "BiLSTM : We simply concatenate the question and context / long answer with learnable segment embeddings appended to the biomedical word2vec embeddings ( Pyysalo et al , 2013 ) of each token .", "entities": [[0, 1, "MethodName", "BiLSTM"]]}
{"text": "ESIM with BioELMo : Following the state - ofthe - art recurrent architecture of NLI ( Peters et al , 2018 ) , we use pre - trained biomedical contextualized embeddings BioELMo ( Jin et al , 2019 ) for word representations .", "entities": [[0, 1, "MethodName", "ESIM"]]}
{"text": "Then we apply the ESIM model ( Chen et al , 2016 ) , where a biLSTM is used to encode the question and context / long answer , followed by an attentional local inference layer and a biLSTM inference composition layer .", "entities": [[4, 5, "MethodName", "ESIM"], [16, 17, "MethodName", "biLSTM"], [38, 39, "MethodName", "biLSTM"]]}
{"text": "After pooling , a softmax output unit is applied for predicting the yes / no / maybe label .", "entities": [[4, 5, "MethodName", "softmax"]]}
{"text": "Comparison of Models : A trend of BioBERT > ESIM w/ BioELMo > BiLSTM > shallow features > majority , conserves across different training schedules on both accuracy and macro - F1 .", "entities": [[9, 10, "MethodName", "ESIM"], [13, 14, "MethodName", "BiLSTM"], [27, 28, "MetricName", "accuracy"], [29, 32, "MetricName", "macro - F1"]]}
{"text": "Fine - tuned BioBERT is better than state - of - theart recurrent model of ESIM w/ BioELMo , probably because BioELMo weights are fixed while all BioBERT parameters can be fine - tuned , which better benefit from the pre - training settings .", "entities": [[15, 16, "MethodName", "ESIM"]]}
{"text": "In phase I + Final setting where models are pre - trained on PQA - A , we observe significant improvements on accuracy and macro - F1 and some models even achieve their best accuracy under this setting .", "entities": [[22, 23, "MetricName", "accuracy"], [24, 27, "MetricName", "macro - F1"], [34, 35, "MetricName", "accuracy"]]}
{"text": "Other models have better accuracy and especially macro - F1 than majority baseline .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 10, "MetricName", "macro - F1"]]}
{"text": "We present PubMedQA , a novel dataset aimed at biomedical research question answering using yes / no / maybe , where complex quantitative reasoning is required to solve the task .", "entities": [[2, 3, "DatasetName", "PubMedQA"], [11, 13, "TaskName", "question answering"]]}
{"text": "PubMedQA has substantial automatically collected instances as well as the largest size of expert annotated", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}
{"text": "There are several interesting future directions to explore on PubMedQA , e.g. : ( 1 ) about 21 % of PubMedQA contexts contain no natural language descriptions of numbers , so how to properly handle these numbers is worth studying ; ( 2 ) we use binary BoW statistics prediction as a simple demonstration for additional supervision of long answers .", "entities": [[9, 10, "DatasetName", "PubMedQA"], [20, 21, "DatasetName", "PubMedQA"]]}
{"text": "Learning a harder but more informative auxiliary task of long answer generation might lead to further improvements .", "entities": [[10, 12, "TaskName", "answer generation"]]}
{"text": "Articles of PubMedQA are biased towards clinical study - related topics ( described in Appendix B ) , so PubMedQA has the potential to assist evidence - based medicine , which seeks to make clinical decisions based on evidence of high quality clinical studies .", "entities": [[2, 3, "DatasetName", "PubMedQA"], [19, 20, "DatasetName", "PubMedQA"]]}
{"text": "Generally , PubMedQA can serve as a benchmark for testing scientific reasoning abilities of machine reading comprehension models .", "entities": [[2, 3, "DatasetName", "PubMedQA"], [14, 17, "TaskName", "machine reading comprehension"]]}
{"text": "Clinical study - related topics are over - represented in PubMedQA : we found proportions of MeSH terms like : \" Pregnancy Outcome \" \" Socioeconomic Factors \" \" Risk Assessment \" \" Survival Analysis \" \" Prospective Studies \" \" Case - Control Studies \" \" Reference Values \" are significantly higher in the PubMedQA articles than those in 200k most recent general PubMed articles ( significance is defined by p < 0.05 in twoproportion z - test ) .", "entities": [[10, 11, "DatasetName", "PubMedQA"], [33, 35, "TaskName", "Survival Analysis"], [55, 56, "DatasetName", "PubMedQA"]]}
{"text": "The dataset consists of 6 , 567 labels for Reddit posts and comments .", "entities": [[9, 10, "DatasetName", "Reddit"]]}
{"text": "Notably , de - spite social scientific studies that show online misogyny is pervasive on some Reddit communities , to date a training dataset for misogyny has not been created with Reddit data .", "entities": [[16, 17, "DatasetName", "Reddit"], [31, 32, "DatasetName", "Reddit"]]}
{"text": "In this paper we seek to address the limitations of previous research by presenting a dataset of Reddit content with expert labels for misogyny that can be used to develop more accurate and nuanced classification models .", "entities": [[17, 18, "DatasetName", "Reddit"]]}
{"text": "Third , we present a dataset of 6 , 383 entries from Reddit .", "entities": [[12, 13, "DatasetName", "Reddit"]]}
{"text": "However , social scientific and ethnographic research shows that Reddit is increasingly home to numerous misogynistic communities .", "entities": [[9, 10, "DatasetName", "Reddit"]]}
{"text": "Reddit is a social news website organised in to topic - based communities .", "entities": [[0, 1, "DatasetName", "Reddit"]]}
{"text": "Recent research suggests that the rate of misogynistic content in the Reddit manosphere is growing and such content is increasingly more violent ( Farrell et al , 2019 ) .", "entities": [[11, 12, "DatasetName", "Reddit"]]}
{"text": "Waseem and Hovy ( 2016 ) provided a widelyused dataset for abusive language classification .", "entities": [[11, 13, "TaskName", "abusive language"]]}
{"text": "However 85 % of the disagreements between annotators were over sexism labels , which shows that even experienced coders of abusive language can have difficultly identifying gendered abuse .", "entities": [[20, 22, "TaskName", "abusive language"]]}
{"text": "A shared task confirmed that the dataset could be used to distinguish misogynistic and non - misogynistic content with high accuracy , but performance was lower in differentiating between types of misogyny .", "entities": [[20, 21, "MetricName", "accuracy"]]}
{"text": "In Lynn et al ( 2019a ) they show that deep learning techniques had greater accuracy in detecting misogyny than conventional machine learning techniques .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "We collected conversation threads from Reddit .", "entities": [[5, 6, "DatasetName", "Reddit"]]}
{"text": "Posts and comments were collected from February to May 2020 using the python package PRAW , a wrapper for the Reddit API ( Boe , 2020 ) .", "entities": [[20, 21, "DatasetName", "Reddit"]]}
{"text": "Posts on Reddit have a text title and a body which can be text , an image , or a link .", "entities": [[2, 3, "DatasetName", "Reddit"]]}
{"text": "That said , it can include other forms of abusive language which are not misogynistic .", "entities": [[9, 11, "TaskName", "abusive language"]]}
{"text": "A key difficulty in the formation of abusive language training datasets is producing high quality annotations .", "entities": [[7, 9, "TaskName", "abusive language"]]}
{"text": "Deciding between similar categories , such as ' hate speech ' versus ' offensive language ' can be difficult ( Waseem et al , 2017 ) .", "entities": [[8, 10, "DatasetName", "hate speech"]]}
{"text": "For instance , found that crowdsourced annotators were more likely to label sexist content as merely ' offensive ' while racist and homophobic content was considered ' hate speech ' .", "entities": [[27, 29, "DatasetName", "hate speech"]]}
{"text": "Annotators reviewed the disagreements in weekly meetings which were overseen by an expert facilitator , a PhD researcher who had developed the annotation taxonomy and was familiar with the literature on online misogyny and hate speech classification .", "entities": [[34, 36, "DatasetName", "hate speech"]]}
{"text": "Gomez et al ( 2020 ) have a Kappa of 0.15 in the \" MMH150 \" dataset of hateful memes .", "entities": [[18, 20, "DatasetName", "hateful memes"]]}
{"text": "Further , we evaluate two uncased BERT - base models ( Devlin et al , 2019 ) - one unweighted , the other using class weights emphasising the minority class , i.e. misogynistic content , to account for class imbalance .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "We make use of the more granular secondary labels in our taxonomy to conduct an error analysis for the weighted BERT model .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "Precision Recall F1 score Accuracy", "entities": [[0, 1, "MetricName", "Precision"], [1, 2, "MetricName", "Recall"], [2, 4, "MetricName", "F1 score"], [4, 5, "MetricName", "Accuracy"]]}
{"text": "In many cases , the derogation depends on the context of the earlier conversation thread , thus the BERT - model , which does not explicitly take into account prior entries in the thread , can not recognise the misogyny in isolation .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "In this paper we have presented a hierarchical granular taxonomy for misogyny and have described a dataset containing high quality , expert labels of misogynistic content from Reddit .", "entities": [[27, 28, "DatasetName", "Reddit"]]}
{"text": "The more granular subcategories in the taxonomy may be too small to classify separately , but they provide insights into the relative frequency of different forms of misogynistic content on Reddit and enable detailed error analysis .", "entities": [[30, 31, "DatasetName", "Reddit"]]}
{"text": "The two datasets include labels for 6 , 383 unique Reddit entries ( i.e. posts or comments ) across 672 conversation threads collected .", "entities": [[10, 11, "DatasetName", "Reddit"]]}
{"text": "Logistic regression with l1 - regularisation is implemented in R using the ' glmnet ' package ( Friedman et al , 2010 ) on a unigram representation of the data .", "entities": [[0, 2, "MethodName", "Logistic regression"]]}
{"text": "Model Architecture We implement uncased BERT - base models ( Devlin et al , 2019 ) using the transformers Python library ( Wolf et al , 2020 ) .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "For sequence classification , we add a linear layer with softmax output .", "entities": [[7, 9, "MethodName", "linear layer"], [10, 11, "MethodName", "softmax"]]}
{"text": "d A unique string assigned to every comment and post by Reddit .", "entities": [[11, 12, "DatasetName", "Reddit"]]}
{"text": "author The Reddit username of the entry author .", "entities": [[2, 3, "DatasetName", "Reddit"]]}
{"text": "We aim to examine the performance of current discourse parsing models via gradual domain shift : within the same corpus , on in - domain texts , and on out - of - domain texts , and discuss the differences between the transformer - based models and the previous models in predicting different types of implicit relations both interand intra - sentential .", "entities": [[8, 10, "TaskName", "discourse parsing"], [55, 57, "TaskName", "implicit relations"]]}
{"text": "The value of predicting the semantic classes of coherence relations has been demonstrated in several applications , including sentiment analysis ( Marcu , 2000 ;", "entities": [[18, 20, "TaskName", "sentiment analysis"]]}
{"text": "Bhatia et al , 2015 ) , machine comprehension ( Narasimhan and Barzilay , 2015 ) , summarization ( Cohan et al , 2018 ; Marcu , 1999 ; Xu et", "entities": [[17, 18, "TaskName", "summarization"]]}
{"text": "Yet shallow discourse parsing is a very difficult task ; more than 10 years after the introduction of the Penn Discourse Treebank ( Eleni Miltsakaki , 2004 ) , performance for English implicit discourse relation recognition has gone from 40.2 F - 1 ( Lin et al , 2009 ) to 47.8 ( Lee et al , 2020 ) , less than 8 percentage points ; a similar story could be said about the relation prediction performance of RST parsers .", "entities": [[2, 4, "TaskName", "discourse parsing"]]}
{"text": "Although domain difference is a recognized issue in shallow discourse parsing by existing work ( Braud et al , 2017 ; Liu et al , 2016 ) , we still have little understanding of the types of distributional shift that matter and by how much , even within one language .", "entities": [[9, 11, "TaskName", "discourse parsing"]]}
{"text": "This position paper seeks to shed some light on our current state in discourse parsing in English .", "entities": [[13, 15, "TaskName", "discourse parsing"]]}
{"text": "Surprisingly , we found that parsers have some issues even within the same news source as the training set ( WSJ ) ; the differences in accuracy were not significant between indomain and out - of - domain data for the qualitative examples that we looked at , although the distribution of errors tend to be different .", "entities": [[26, 27, "MetricName", "accuracy"]]}
{"text": "Additionally , as part of our evaluation , we asked linguists to perform manual annotation , which allowed us to evaluate the accuracy of these parsers on plain , unlabeled text , and gain some insight about the mistakes made by the parsers .", "entities": [[22, 23, "MetricName", "accuracy"]]}
{"text": "We urge future researchers to consider developing contextaware models for shallow discourse parsing moving forward .", "entities": [[11, 13, "TaskName", "discourse parsing"]]}
{"text": "The RST Discourse Treebank ( Carlson et al , 2003 ) contains 385 Wall Street Journal articles from the Penn Treebank ( Marcus et al , 1993 ) which have been split into elementary discourse units and annotated according to Rhetorical Structure Theory , where discourse relations are annotated in a tree structure across the whole document .", "entities": [[19, 21, "DatasetName", "Penn Treebank"]]}
{"text": "The Penn Discourse Treebank ( PDTB ) ( Eleni Miltsakaki , 2004 ; Rashmi Prasad , 2008 ; Prasad et al , 2018 ) , which also uses Penn Treebank Wall Street Journal articles , contains discourse relations annotated in a shallow , non - hierarchical manner .", "entities": [[28, 30, "DatasetName", "Penn Treebank"]]}
{"text": "In this paper , we focus on implicit relations , where a connective can be inserted between the two arguments that indicates a discourse relation .", "entities": [[7, 9, "TaskName", "implicit relations"]]}
{"text": "Liang et al ( 2020 ) identify locating the position of relations as a new challenge in the PDTB - 3 , due to the significantly increased number of intra - sentential implicit relations annotated .", "entities": [[32, 34, "TaskName", "implicit relations"]]}
{"text": "Techniques of discourse parsing range from supervised Mabona et al , 2019 ; Lin et al , 2019 ; Zhang et al , 2020 ; Kobayashi et al , 2020 ) and weakly supervised and unsupervised approaches ( Lee et al , 2020 ; Nishida and Nakayama , 2020 ; Kurfal\u0131 and\u00d6stling , 2019 ) ; recent developments such as word / contextual embeddings have improved parser performance , although not as significantly as other tasks ( Shi and Demberg , 2019 ; Chen et al , 2019 )", "entities": [[2, 4, "TaskName", "discourse parsing"]]}
{"text": "We provide a comparative genrebased analysis on distributionally shifted text data and present a qualitative analysis of the impact of the practical choices that these models make while doing discourse parsing across frameworks .", "entities": [[29, 31, "TaskName", "discourse parsing"]]}
{"text": "3 Where are we in discourse parsing ?", "entities": [[5, 7, "TaskName", "discourse parsing"]]}
{"text": "The data we examine is : WSJ texts outside of the Penn Treebank , other news texts , and the GUM corpus ( Zeldes , 2017 ) .", "entities": [[11, 13, "DatasetName", "Penn Treebank"], [20, 21, "DatasetName", "GUM"]]}
{"text": "Note that none of these texts contain gold PDTB annotations , and only the GUM corpus contains gold RST annotations .", "entities": [[14, 15, "DatasetName", "GUM"]]}
{"text": "The former is needed in order to parse unlabeled text , and the latter is a more accurate BERT - based implicit sense classifier ( implicit sense classification is the most difficult PDTB parsing task ) .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "We also evaluate it on the gold labels from the GUM corpus ( but trained on the RST ) .", "entities": [[10, 11, "DatasetName", "GUM"]]}
{"text": "Because GUM is annotated with 20 different discourse relations which do not precisely map to the conventional 18 types used in the Wang et al ( 2017 ) parser , we map the ones that do n't match these types or the more fine - grained relations in the following manner , following Braud et", "entities": [[1, 2, "DatasetName", "GUM"]]}
{"text": "Transformer - based models perform better on linguistically different intra - sentential relations than they do on inter - sentential relations .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "Table 1 shows the accuracies of the base and large BERT model ( Chen et al , 2019 ) on the implicit relations in the two versions of the PDTB .", "entities": [[10, 11, "MethodName", "BERT"], [21, 23, "TaskName", "implicit relations"]]}
{"text": "These results drive home the influence of the linguistic and structural differences between intra - and inter - sentence implicit relations on the performance of the parsers .", "entities": [[19, 21, "TaskName", "implicit relations"]]}
{"text": "Parsers struggle to identify implicit relations from less frequent classes .", "entities": [[4, 6, "TaskName", "implicit relations"]]}
{"text": "In order to capture this , we measure the performance across several domain shifts from the PDTB - 2 using three datasets : WSJ articles from the COHA corpus ( Davies , 2012 ) , other news articles from COHA , and the GUM corpus ( Zeldes , 2017 ) .", "entities": [[43, 44, "DatasetName", "GUM"]]}
{"text": "The GUM corpus , our out - of - domain dataset , contains data from eight domains : Academic , Bio , Fiction , Interview , News , Travel , How - to guides , and Forum Discussions .", "entities": [[1, 2, "DatasetName", "GUM"], [20, 21, "DatasetName", "Bio"], [24, 25, "DatasetName", "Interview"]]}
{"text": "Cause was 0.33 , 0.14 , and 0.33 for WSJ articles , non - WSJ news articles , and the GUM corpus respectively .", "entities": [[20, 21, "DatasetName", "GUM"]]}
{"text": "This likely contributed to the low accuracy for these documents .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "For the GUM corpus , segmentation was especially an issue in the travel genre , where headers or captions would be labeled as part of an argument .", "entities": [[2, 3, "DatasetName", "GUM"]]}
{"text": "As is shown in Table 5 , the percentage of implicit relations that the parsers got right on the second level appeared to decrease on average as the domain shifted .", "entities": [[10, 12, "TaskName", "implicit relations"]]}
{"text": "However , this was a very slight decrease ; they had roughly the same level of accuracy across all datasets , which was very low .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "The results of running the state - of - the - art Wang et al ( 2017 ) parser on the gold labels of the RST and GUM corpus are shown in Figure 2 .", "entities": [[27, 28, "DatasetName", "GUM"]]}
{"text": "However , in order for discourse parsers to be useful for applications outside of the news domain , models that can more easily adapt to the target domain must be developed . 0 - 2 % 1 2 2 2 3 3 2 - 5 % 1 0 0 2 2 2 > 5 % 3 3 3 3 3 3", "entities": [[32, 33, "DatasetName", "0"], [47, 48, "DatasetName", "0"], [48, 49, "DatasetName", "0"]]}
{"text": "While inspecting the results of the annotations , we found several helpful phenomena for developing future models , including observations regarding the role of context in shallow discourse parsing and errors that current RST parsers are making .", "entities": [[27, 29, "TaskName", "discourse parsing"]]}
{"text": "For the qualitative analysis , we ask two annotators ( a faculty member and a graduate student from linguistics departments ) to provide annotations for the data , as none of the texts contain gold PDTB labels and only the GUM corpus contains gold RST labels .", "entities": [[40, 41, "DatasetName", "GUM"]]}
{"text": "In order for the annotators to annotate this corpus , discourse relations were randomly chosen from Wall Street Journal articles , other news articles , and the GUM corpus .", "entities": [[27, 28, "DatasetName", "GUM"]]}
{"text": "With a few exceptions ( Dai and Huang , 2018 ; Shi and Demberg , 2019 ; Zhang et al , 2021 ) , existing models for shallow discourse parsing mostly do not Figure 3 : RST parse tree containing a segment of the relations that were examined in the qualitative analysis .", "entities": [[28, 30, "TaskName", "discourse parsing"]]}
{"text": "Cause and the BERT parser predicting it as Expansion .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "These observations provide insights as to why contextual embeddings with document context such as the next sentence prediction task helps with implicit discourse relation classification ( Shi and Demberg , 2019 ) .", "entities": [[21, 25, "TaskName", "implicit discourse relation classification"]]}
{"text": "More generally , we believe future work on discourse parsing should look beyond only the arguments of a relation because of the different interpretations one would give when taking the relation in vs. out of context .", "entities": [[8, 10, "TaskName", "discourse parsing"]]}
{"text": "Attachment issues are a particular problem for RST parsing due to its hierarchical nature ; one at - tachment issue can lead to error propagation where the accuracy of the attachments further in the tree is impacted by that of the current one .", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "Discourse parsing for text has seen a recent surge in experimental approaches .", "entities": [[0, 2, "TaskName", "Discourse parsing"]]}
{"text": "The conclusions drawn above from these experiments make it clear that discourse parsing , though it has come a long way in the past decade or so , still has a long way to go , particularly with respect to parsing on out - ofdomain texts and addressing issues of class imbalances , although the BERT - based model has made some improvements in this area .", "entities": [[11, 13, "TaskName", "discourse parsing"], [55, 56, "MethodName", "BERT"]]}
{"text": "Additionally , we investigated how and when PDTB - 3 can help in improving the prediction of intra - sentential implicit relations .", "entities": [[20, 22, "TaskName", "implicit relations"]]}
{"text": "There are several promising future directions for the area of discourse parsing .", "entities": [[10, 12, "TaskName", "discourse parsing"]]}
{"text": "A model that detects intra - sentential implicit relations is necessary in order to be able to parse on the PDTB - 3 .", "entities": [[7, 9, "TaskName", "implicit relations"]]}
{"text": "Importance - based Neuron Allocation for Multilingual Neural Machine Translation", "entities": [[8, 10, "TaskName", "Machine Translation"]]}
{"text": "Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages .", "entities": [[2, 4, "TaskName", "machine translation"]]}
{"text": "However , the current multilingual translation paradigm often makes the model tend to preserve the general knowledge , but ignore the language - specific knowledge .", "entities": [[15, 17, "TaskName", "general knowledge"]]}
{"text": "The general part is responsible for preserving the general knowledge and participating in the translation of all the languages , while the language - specific part is responsible for preserving the languagespecific knowledge and participating in the translation of some specific languages .", "entities": [[8, 10, "TaskName", "general knowledge"]]}
{"text": "Neural machine translation ( NMT ) ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al , 2014 ; Bahdanau et al , 2015 ; Gehring et", "entities": [[1, 3, "TaskName", "machine translation"]]}
{"text": "A typical solution to reduce the model size and the training cost is to handle multiple languages in a single multilingual neural machine translation ( MNMT ) model ( Ha et al , 2016 ;", "entities": [[22, 24, "TaskName", "machine translation"]]}
{"text": "Therefore , the MNMT model trained on the combined data generally captures the general knowledge , but ignores the language - specific knowledge , rendering itself sub - optimal for the translation of a specific language ( Sachan and Neubig , 2018 ;", "entities": [[13, 15, "TaskName", "general knowledge"]]}
{"text": "However , these methods suffer from the parameter increment problem , because the number of parameters increases linearly with the number of languages .", "entities": [[13, 16, "HyperparameterName", "number of parameters"]]}
{"text": "To achieve this , we propose to divide the model neurons into two parts based on their importance : the general neurons which are used to retain the general knowledge of all the languages , and the language - specific neurons which are used to retain the language - specific knowledge .", "entities": [[28, 30, "TaskName", "general knowledge"]]}
{"text": "We show that some modules tend to capture the general knowledge while some modules are more essential for capturing the languagespecific knowledge .", "entities": [[9, 11, "TaskName", "general knowledge"]]}
{"text": "In this section , we will give a brief introduction to the Transformer model ( Vaswani et al , 2017 ) and the Multilingual translation .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "Transformer is a stacked network with N identical layers containing two or three basic blocks in each layer .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "For the importance evaluation function \u0398 ( ) , we adopt two schemes : one is based on the Taylor Expansion and the other is based on the Absolute Value .", "entities": [[5, 6, "HyperparameterName", "\u0398"]]}
{"text": "Assuming the independence of each neuron in the model , the change of loss when removing a certain neuron can be represented as : | \u2206L ( h i )", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "i = 0 )", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Then , approximating L ( H , h i = 0 ) with a firstorder Taylor polynomial where h i equals zero : L ( H , h", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "i = 0 )", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Thus , we can ignore the remainder and get the importance evaluation function as follows : \u0398 TE ( i )", "entities": [[16, 17, "HyperparameterName", "\u0398"]]}
{"text": "Finally , the evaluation function is shown as : \u0398 m TE ( i l )", "entities": [[9, 10, "HyperparameterName", "\u0398"]]}
{"text": "\u0398 m AV", "entities": [[0, 1, "HyperparameterName", "\u0398"]]}
{"text": "Therefore , we rank the neurons in each layer based on the importance and make the top \u03c1 percentage as general neurons that are responsible for capturing general knowledge .", "entities": [[27, 29, "TaskName", "general knowledge"]]}
{"text": "Sentences of all languages were tokenized by the Moses scripts 2 and further segmented into subword symbols using Byte - Pair Encoding ( BPE ) rules ( Sennrich et al , 2016 ) with 40 K merge operations for all languages jointly .", "entities": [[23, 24, "MethodName", "BPE"]]}
{"text": "We tokenize and truecase the sentences with Moses scripts and apply a jointly - learned set of 90k BPE obtained from the merged source and target sides of the training data for all twelve language pairs .", "entities": [[18, 19, "MethodName", "BPE"]]}
{"text": "+ TS ( Blackwood et al , 2018 )", "entities": [[1, 2, "MethodName", "TS"]]}
{"text": "+ Adapter", "entities": [[1, 2, "MethodName", "Adapter"]]}
{"text": "For fair comparisons , we implement the proposed method and other contrast methods on the advanced Transformer model using the open - source toolkit Fairseq - py ( Ott et al , 2019 ) .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "We can see that the improvements brought by + TS and + Adapter methods are not large .", "entities": [[9, 10, "MethodName", "TS"], [12, 13, "MethodName", "Adapter"]]}
{"text": "For the + TS method , attention module may be not essential to capture language - specific knowledge , and thus it is difficult to converge to good optima .", "entities": [[3, 4, "MethodName", "TS"]]}
{"text": "For the + Adapter method , adding an adapter module to the end of each layer may be not appropriate for some languages and hence has a loose capture to the specific features .", "entities": [[3, 4, "MethodName", "Adapter"]]}
{"text": "When we expand the model capacity to the level of + Adapter , our approach can achieve better translation performance , which demonstrates the effectiveness of our method .", "entities": [[11, 12, "MethodName", "Adapter"]]}
{"text": "To better show the overall impact of the hyperparameter k , we vary it from 0 to 1 and the results are shown in Figure 4 .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "The main idea of our method is to let the general knowledge and the language - specific knowledge be captured by different neurons of our method .", "entities": [[10, 12, "TaskName", "general knowledge"]]}
{"text": "For the general knowledge , we randomly erase 20 % general neurons of the best checkpoint of our method , which means we mask the output value of these neurons to 0 , then generate translation using it .", "entities": [[2, 4, "TaskName", "general knowledge"], [31, 32, "DatasetName", "0"]]}
{"text": "While this paradigm fully explores the general knowledge between languages and hard to obtain the specific knowledge of each language ( Tan et al , 2019 ; Aharoni et al , 2019 ) , the subsequent researches resort to Language - specific modeling , trying to find a better trade - off between sharing and specific .", "entities": [[6, 8, "TaskName", "general knowledge"]]}
{"text": "Besides , Lan et al ( 2020 ) presents two parameter reduction techniques to lower memory consumption and increase the training speed of BERT .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "The current standard models of multilingual neural machine translation fail to capture the characteristics of specific languages , while the latest researches focus on the pursuit of specific knowledge while increasing the capacity of the model and requiring fine manual design .", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "We divide neurons to general neurons and language - specific neurons to retain general knowledge and capture language - specific knowledge without model capacity incremental and specialized design .", "entities": [[13, 15, "TaskName", "general knowledge"]]}
{"text": "By introducing the graph structure , the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution .", "entities": [[35, 36, "MethodName", "convolution"]]}
{"text": "Probabilistic topic models ( Blei , 2012 ) are tools for discovering main themes from large corpora .", "entities": [[1, 3, "TaskName", "topic models"]]}
{"text": "The popular Latent Dirichlet Allocation ( LDA ) ( Blei et al , 2003 ) and its variants ( Lin and He , 2009 ; Zhao et al , 2010 ; Zhou", "entities": [[6, 7, "MethodName", "LDA"]]}
{"text": "Notably , NVDM ( Miao et al , 2016 ) employs variational autoencoder ( VAE ) ( Kingma and Welling , 2013 ) to model topic inference and document generation .", "entities": [[11, 13, "MethodName", "variational autoencoder"], [14, 15, "MethodName", "VAE"]]}
{"text": "W - LDA ( Nan et al , 2019 ) models topics in the Wasserstein autoencoders ( Tolstikhin et al , 2017 ) framework and achieves distribution matching by minimizing their Maximum Mean Discrepancy ( MMD )", "entities": [[2, 3, "MethodName", "LDA"], [15, 16, "MethodName", "autoencoders"], [35, 36, "DatasetName", "MMD"]]}
{"text": "( Gretton et al , 2012 ) , while adversarial topic model ( Wang et al , 2019a ( Wang et al , , b , 2020 directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network ( GAN )", "entities": [[48, 51, "MethodName", "Generative Adversarial Network"], [52, 53, "MethodName", "GAN"]]}
{"text": "In GTM , the topical representation of a document node is aggregated from its multi - hop neighborhood , including both document and word nodes , using Graph Convolutional Network ( GCN ) ( Kipf and Welling , 2016 ) .", "entities": [[27, 30, "MethodName", "Graph Convolutional Network"], [31, 32, "MethodName", "GCN"]]}
{"text": "As GCN is able to capture high - order neighborhood relationships , GTM is essentially capable of modeling both word - word and doc - doc relationships .", "entities": [[1, 2, "MethodName", "GCN"]]}
{"text": "TF - IDF ij , i D and j V TF - IDF ji , i V and j D 1 , i = j 0 , otherwise ( 1 ) where A is the adjacency matrix of G and TF - IDF", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "L rec ( X , X ) and MMD ( P Z , Q Z ) are training objectives .", "entities": [[8, 9, "DatasetName", "MMD"]]}
{"text": "The encoder network E maps nodes in G to their topic distributions by iteratively applying graph convolution to the node features .", "entities": [[16, 17, "MethodName", "convolution"]]}
{"text": "Following ( Kipf and Welling , 2016 ) , the layer - wise propagation rule of the graph convolution at layer l", "entities": [[18, 19, "MethodName", "convolution"]]}
{"text": "j A ij , W ( l ) R d ( l ) \u00d7d ( l+1 ) is a layerspecific weight matrix where d ( l ) is the output size of layer l , and \u03c3 denotes an activation function that is LeakyReLU", "entities": [[39, 41, "HyperparameterName", "activation function"]]}
{"text": "H ( l ) R N \u00d7d ( l ) is the activations of all nodes at layer l and H ( 0 )", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "At each encoder layer , what the graph convolution does is aggregating node features from a node 's first - order neighborhood , which consequently enlarges the receptive field of the central node and enables the information propagation between relevant nodes .", "entities": [[8, 9, "MethodName", "convolution"]]}
{"text": "After successively applying L graph convolution layers , the encoding of a node essentially involves its L th - order neighborhood .", "entities": [[5, 6, "MethodName", "convolution"]]}
{"text": "We also add a batch normalization ( Ioffe and Szegedy , 2015 ) after each graph convolution .", "entities": [[4, 6, "MethodName", "batch normalization"], [16, 17, "MethodName", "convolution"]]}
{"text": "After the graph encoding , a softmax is further applied to the node features of a document to produce a multinomial topic distribution\u1e91 R K , where K is the topic number .", "entities": [[6, 7, "MethodName", "softmax"]]}
{"text": "To achieve this goal , we employ a 2 - layer MLP with LeakyReLU activation and batch normalization in the first layer .", "entities": [[11, 12, "DatasetName", "MLP"], [16, 18, "MethodName", "batch normalization"]]}
{"text": "The output of the MLP decoder is then softmax - normalized to generate a word distributionx R V .", "entities": [[4, 5, "DatasetName", "MLP"], [8, 9, "MethodName", "softmax"]]}
{"text": "The reconstruction loss is defined as L rec ( X , X )", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "Following W - LDA ( Nan et al , 2019 ) , we achieve this goal by minimizing the Maximum Mean Discrepancy ( MMD ) ( Gretton et al , 2012 ) between the distribution Q\u1e90 of inferred topic distributions\u1e91 and the Dirichlet prior P Z from which we draw multinomial noises z : MMD ( PZ , Q\u1e90 )", "entities": [[3, 4, "MethodName", "LDA"], [23, 24, "DatasetName", "MMD"], [54, 55, "DatasetName", "MMD"]]}
{"text": "We use the information diffusion kernel ( Lebanon and Lafferty , 2003 ) as in W - LDA : k ( z , z )", "entities": [[17, 18, "MethodName", "LDA"]]}
{"text": "We compare the performance of our model with LDA ( Blei et al , 2003 ) , NVDM ( Miao et al , 2016 ) , ProdLDA", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "( Srivastava and Sutton , 2017 ) , GraphBTM ( Zhu et al , 2018 ) , ATM ( Wang et al , 2019a ) and W - LDA", "entities": [[28, 29, "MethodName", "LDA"]]}
{"text": "We use 2 graph convolution layers with output dimensions of 100 and K respectively in the encoder .", "entities": [[4, 5, "MethodName", "convolution"]]}
{"text": "W - LDA , ATM , LDA , and GraphBTM alternately achieve the second - best but they are always under - performed compared to our model .", "entities": [[2, 3, "MethodName", "LDA"], [6, 7, "MethodName", "LDA"]]}
{"text": "As described in section 2 , GTM is an extension to W - LDA with the main difference that GTM models topics in a larger context and incorporates more global information with the graph encoder .", "entities": [[13, 14, "MethodName", "LDA"]]}
{"text": "Therefore the improvements of GTM over W - LDA indicate the effectiveness of such information for topic modeling .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "LDA has a slightly higher NPMI score on 20Newsgroups dataset with 75 and 100 topics , nevertheless , GTM outperforms all baseline models with a relatively large margin on other settings of 20Newsgroups .", "entities": [[0, 1, "MethodName", "LDA"]]}
{"text": "Notably , W - LDA , GraphBTM , and LDA obtain the second - best overall C P , C A , and NPMI scores respectively .", "entities": [[4, 5, "MethodName", "LDA"], [9, 10, "MethodName", "LDA"]]}
{"text": "As a comparison , GraphBTM 's rec.autos topic mixes up automobiles and criminals , W - LDA 's misc.forsale topic is difficult to identify with too many offtopic words , while LDA can not distinguish between rec.autos and misc.forsale well thus recog - nizes them as the same topic .", "entities": [[16, 17, "MethodName", "LDA"], [31, 32, "MethodName", "LDA"]]}
{"text": "Replacing GCN in GTM with more advanced graph neural networks is another promising research direction .", "entities": [[1, 2, "MethodName", "GCN"]]}
{"text": "With the introduction of connotative valid facts , knowledge inference on knowledge graph improves the performance of many downstream applications , such as vertical search and question answering ( Dong et al , 2015 ; Lukovnikov et al , 2017 ) .", "entities": [[26, 28, "TaskName", "question answering"]]}
{"text": "Actually , in YAGO ( Suchanek et al , 2007 ) and Wikidata ( Vrande\u010di\u0107 and Kr\u00f6tzsch , 2014 ) , a primary triple is identified for each n - ary fact .", "entities": [[3, 4, "DatasetName", "YAGO"]]}
{"text": "The quintessential one of tensor / matrix based methods is RESCAL ( Nickel et al , 2011 ) .", "entities": [[10, 11, "MethodName", "RESCAL"]]}
{"text": "Similarly , ComplEx ( Trouillon et al , 2016 ) relates each relation to a matrix of head and tail entities , which is decomposed and learned like RESCAL .", "entities": [[28, 29, "MethodName", "RESCAL"]]}
{"text": "Translation based methods date back to TransE ( Bordes et al , 2013 ) .", "entities": [[0, 1, "TaskName", "Translation"], [6, 7, "MethodName", "TransE"]]}
{"text": "This matrix is fed into a convolution layer , followed by a concatenation layer and a fully - connected layer to generate a validity score .", "entities": [[6, 7, "MethodName", "convolution"]]}
{"text": "ConvE ( Dettmers et al , 2018 ) models entity inference process via 2D convolution over the reshaped then concatenated embedding of the known entity and relation .", "entities": [[14, 15, "MethodName", "convolution"]]}
{"text": "ConvR ( Jiang et al , 2019 ) further adaptively constructs convolution filters from relation embedding and applies these filters across entity embedding to generate convolutional features .", "entities": [[11, 12, "MethodName", "convolution"]]}
{"text": "Specifically , RAE applies a fully - connected neural network to model the above likelihood .", "entities": [[2, 3, "MethodName", "RAE"]]}
{"text": "Then , convolution is adopted to get the embeddings of the attribute - value pairs , and a fully - connected neural network is applied to evaluate their relatedness and finally to obtain the validity score of the input n - ary fact .", "entities": [[2, 3, "MethodName", "convolution"]]}
{"text": "These two scores are used to generate the final score of F ct by weighted sum and further compute the loss .", "entities": [[20, 21, "MetricName", "loss"]]}
{"text": "Note that , following RAE ( Zhang et al , 2018 ) and NaLP ( Guan et al , 2019 ) , we only apply fully - connected neural networks in NeuInfer .", "entities": [[4, 5, "MethodName", "RAE"]]}
{"text": "This component estimates the validity of ( h , r , t ) , including the acquisition of its interaction vector and the assessment of its validity , corresponding to \" hrt - FCNs \" and \" FCN 1 \" in Figure 1 , respectively .", "entities": [[37, 38, "MethodName", "FCN"]]}
{"text": "W 1 , n 1 + b 1 , n 1 ) , ( 1 ) where f ( ) is the ReLU function ; n 1 is the number of the neural network layers ; { W 1 , 1 , W 1 , 2 , . . .", "entities": [[22, 23, "MethodName", "ReLU"]]}
{"text": "+ b val ) , ( 2 ) where W val and b val are the weight matrix and bias variable , respectively ; \u03c3 ( x ) = 1 1+e \u2212x is the sigmoid function , which constrains val hrt ( 0 , 1 ) .", "entities": [[42, 43, "DatasetName", "0"]]}
{"text": "For simplicity , the number of hidden nodes in each fully - connected layer of \" hrt - FCNs \" and \" FCN 1 \" gradually reduces with the same difference between layers .", "entities": [[22, 23, "MethodName", "FCN"]]}
{"text": ", m ) , the acquisition of the overall interaction vector , and the assessment of the compatibility of F ct , corresponding to \" hrtav - FCNs \" , \" min \" and \" FCN 2 \" in Figure 1 , respectively .", "entities": [[35, 36, "MethodName", "FCN"]]}
{"text": "Then , similar to \" FCN 1 \" , we obtain the compatibility score comp F ct of F ct : comp F ct = \u03c3 ( o hrtav W comp + b comp ) , ( 5 ) where W comp of dimension d \u00d7 1 and b comp are the weight matrix and bias variable , respectively .", "entities": [[5, 6, "MethodName", "FCN"]]}
{"text": "The final score s F ct of F ct is the weighted sum of the above validity score and compatibility score : s F ct = val hrt comp F ct = w val hrt + ( 1 \u2212 w ) comp F ct , ( 6 ) where w ( 0 , 1 ) is the weight factor .", "entities": [[51, 52, "DatasetName", "0"]]}
{"text": "0 .", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "The two primary attributes are selected based on RAE ( Zhang et al , 2018 ) .", "entities": [[8, 9, "MethodName", "RAE"]]}
{"text": "The statistics of the datasets after conversion or reorganization are outlined in ciprocal Rank ( MRR ) and Hits@N .", "entities": [[15, 16, "MetricName", "MRR"]]}
{"text": "Then , MRR is the average of these reciprocal ranks , and Hits@N is the proportion of the ranks less than or equal to N .", "entities": [[2, 3, "MetricName", "MRR"]]}
{"text": "For an nary fact , they infer one of the entities / the relation in Method JF17 K WikiPeople MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 the primary triple or the attribute value / attribute in an auxiliary description , given its other information .", "entities": [[19, 20, "MetricName", "MRR"], [20, 21, "MetricName", "Hits@1"], [21, 22, "MetricName", "Hits@3"], [22, 23, "MetricName", "Hits@10"], [23, 24, "MetricName", "MRR"], [24, 25, "MetricName", "Hits@1"], [25, 26, "MetricName", "Hits@3"], [26, 27, "MetricName", "Hits@10"]]}
{"text": "( Wen et al , 2016 ) and its modified version RAE ( Zhang et al , 2018 ) , and the state - of - the - art one is NaLP ( Guan et al , 2019 ) .", "entities": [[11, 12, "MethodName", "RAE"]]}
{"text": "As m - TransH is worse than RAE , following NaLP , we do not adopt it as a baseline .", "entities": [[7, 8, "MethodName", "RAE"]]}
{"text": "Since RAE is deliberately developed only for simple entity inference , we compare NeuInfer only with NaLP on simple relation inference .", "entities": [[1, 2, "MethodName", "RAE"]]}
{"text": "Without loss of generality , here we report only the experimental results on simple entity inference .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "On more difficult WikiPeople , NeuInfer is comparable to the best baseline NaLP on the binary category and gains much better performance on the n - ary category in terms of the fine - grained MRR and Hits@1 .", "entities": [[35, 36, "MetricName", "MRR"], [37, 38, "MetricName", "Hits@1"]]}
{"text": "Transformer based Natural Language Generation for Question - Answering", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "25 50 rank A1 / Bert / Cmbert - base A2 / Bert / flaubert - small A1 / Bert / xlm - roberta - base A1 / Bert / flaubert - base - unc A1 / Bert / xlm - mlm - enfr - 1024 A1 / Bert / xlm - roberta - large A2 / Bert / gpt2 A1 / Bert / bert - base - mlg - unc A1 / Bert / bert - base - mlg A2 / Bert / xlm - clm - enfr - 1024 A1 / Bert / xlm - clm - enfr - 1024 A2 / Bert / xlm - mlm - enfr - 1024 A2 / Bert / flaubert - large A2 / Bert / xlm - roberta - base A1 / Bert / flaubert - large A1 / Bert / flaubert - small A1 / Bert / openai - gpt A2 / Bert / Cmbert - base A2 / Bert / flaubert - base A2 / Bert / flaubert - base - unc A2 / Bert / xlm - roberta - large A1 / Bert / gpt2 A2 / Bert / bert - base - mlg - unc A2 / Bert / gpt2 - medium A1 / Bert / gpt2 - large A2 / Bert / bert - base - mlg A1 / Bert / gpt2 - medium", "entities": [[21, 22, "MethodName", "xlm"], [39, 40, "MethodName", "xlm"], [41, 42, "DatasetName", "mlm"], [50, 51, "MethodName", "xlm"], [84, 85, "MethodName", "xlm"], [95, 96, "MethodName", "xlm"], [106, 107, "MethodName", "xlm"], [108, 109, "DatasetName", "mlm"], [124, 125, "MethodName", "xlm"], [149, 150, "MethodName", "gpt"], [177, 178, "MethodName", "xlm"]]}
{"text": "In what follows , we detail in section 3 the approach we propose for answer generation in Natural Language and we briefly discuss the QAS developed .", "entities": [[14, 16, "TaskName", "answer generation"]]}
{"text": "The QAS cover mainly three tasks : question analysis , information retrieval and answer extraction ( Lopez et al , 2011 ) .", "entities": [[10, 12, "TaskName", "information retrieval"]]}
{"text": "In this article , we particularly focus on the answer generation process .", "entities": [[9, 11, "TaskName", "answer generation"]]}
{"text": "Our answer generation approach differs from these works as it is unsupervised , can be adapted to any type of factual question ( except for why ) and is based only on easily accessible and unannotated data .", "entities": [[1, 3, "TaskName", "answer generation"]]}
{"text": "In what follows , we detail in section 3 the approach we propose for answer generation in Natural Language and we briefly discuss the QAS developed .", "entities": [[14, 16, "TaskName", "answer generation"]]}
{"text": "The answer generation approach proposed is a component of a system which was developed in Rojas Barahona", "entities": [[1, 3, "TaskName", "answer generation"]]}
{"text": "Therefore , we propose an unsupervised approach which integrates the use of Transformer models such as BERT ( Devlin et al , 2019 ) and GPT ( Radford et al , 2018 ) .", "entities": [[12, 13, "MethodName", "Transformer"], [16, 17, "MethodName", "BERT"], [25, 26, "MethodName", "GPT"]]}
{"text": "This approach is composed of two fundamental phases : The dependency analysis of the input question and the answer generation using Transformer models .", "entities": [[18, 20, "TaskName", "answer generation"], [21, 22, "MethodName", "Transformer"]]}
{"text": "UDPipeFuture is a POS tagger and graph parser based dependency parser using a BiLSTM , inspired by Dozat et", "entities": [[13, 14, "MethodName", "BiLSTM"]]}
{"text": "Our modification consisted in adding several contextual word embeddings ( with respect to the language ) .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "In order to find the best configuration we experimented with models like multilingual BERT ( Devlin et al , 2019 ) , XLM - R ( Conneau et al , 2019 ) ( for both , English and French ) , RoBERTA ( Liu et al , 2019 ) ( for English ) , FlauBERT ( Le et al , 2020 ) and CamemBERT ( Martin et al , 2019 ) ( for French ) during the training of the treebanks French - GSD and English - EWT 4 , of the Universal Dependencies project ( UD ) ( Nivre et al , 2016 ) 5 .", "entities": [[13, 14, "MethodName", "BERT"], [22, 23, "MethodName", "XLM"], [92, 94, "DatasetName", "Universal Dependencies"], [96, 97, "DatasetName", "UD"]]}
{"text": "This is the case for all languages ( of the CoNLL shared task ) , where language specific contextual embeddings or multingual ones ( as BERT or XLM - R ) improved parsing ( Heinecke , 2020 )", "entities": [[25, 26, "MethodName", "BERT"], [27, 28, "MethodName", "XLM"]]}
{"text": "In order to parse simple , quiz - like questions , the training corpora of the two UD treebanks are not appropriate ( enough ) , since both treebanks do not contain many questions , if at all 6 .", "entities": [[17, 18, "DatasetName", "UD"]]}
{"text": "An explanation for bad performance on questions of parser models trained on standard UD is the fact , that in both languages , the syntax of questions differs from the syntax of declarative sentences : apart from wh question words , in English the to do periphrasis is nearly always used in questions .", "entities": [[13, 14, "DatasetName", "UD"]]}
{"text": "Table 2 shows the ( much lower ) results when parsing questions using models trained only on the standard UD treebanks .", "entities": [[19, 20, "DatasetName", "UD"]]}
{"text": "For the annotations we followed the general UD guidelines 8 as well as the treebank specific guidelines of En - EWT and Fr - GSD .", "entities": [[7, 8, "DatasetName", "UD"]]}
{"text": "The contextual word embeddings CamemBERT ( for French ) and BERT ( English ) have the biggest impact .", "entities": [[2, 4, "TaskName", "word embeddings"], [10, 11, "MethodName", "BERT"]]}
{"text": "We rely on the UdpipeFuture version which we have improved with BERT ( for English ) /CamemBERT", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "These structures will be evaluated by a Language Model ( LM ) based on Transformer models which will extract the most probable sequence of text fragments that can account for the answer to be sent to the user :", "entities": [[14, 15, "MethodName", "Transformer"]]}
{"text": "Therefore , to predict this missing word , we use BERT as the generation model ( GM ) for its ability to capture bidirectionally the context of a given word within a sentence .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "In case when BERT returns a non - alphabetic character sequence , we assume that the optimal structure , as predicted by the LM , does not need to be completed by an additional word .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "The existing QAS test sets are more tailored to systems which generate the exact short answer to a question or more focused on the Machine Reading Comprehension task where the answer consists of a text passage from a document containing the short answer .", "entities": [[24, 27, "TaskName", "Machine Reading Comprehension"]]}
{"text": "As illustrated in figure 1 , two possible architectures of the approach proposed for answer generation have been evaluated .", "entities": [[14, 16, "TaskName", "answer generation"]]}
{"text": "To evaluate the proposed approach , we have referred to standard metrics defined for NLG tasks such as Automatic Translation and Summarization , as they allow to assess to what extent a generated sentence is similar to the gold sentence .", "entities": [[19, 20, "TaskName", "Translation"], [21, 22, "TaskName", "Summarization"]]}
{"text": "We con - sider three N - gram metrics ( BLEU , METEOR and ROUGE ) and the BERT score metric which exploits the pre - trained embeddings of BERT to calculate the similarity between the answer generated and the gold answer .", "entities": [[10, 11, "MetricName", "BLEU"], [12, 13, "DatasetName", "METEOR"], [18, 19, "MethodName", "BERT"], [29, 30, "MethodName", "BERT"]]}
{"text": "\" BT \" in Column GM stands for BERT - base - multilingual - cased .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "In column LM we use \" CmBT \" for CamemBERT - base , \" BT - ml - c \" for BERT - base - multilingual - cased , \" XRob \" for XLM - RoBERTa - base , \" FBT - s - c \" for FlauBERT - small - cased , \" FBT - b - uc \" for FlauBERT - base - uncased and \" clm - 1024 \" for XLM - clmenfr - 1024 lingual BERT model predicts missing words better than CamemBERT for French sentences .", "entities": [[21, 22, "MethodName", "BERT"], [33, 34, "MethodName", "XLM"], [35, 36, "MethodName", "RoBERTa"], [73, 74, "MethodName", "XLM"], [79, 80, "MethodName", "BERT"]]}
{"text": "These findings are also confirmed by the Friedman test where we can clearly see that the first ranked configuration maps the best configuration selected according to the human accuracy , with a very slight difference for the other four configurations .", "entities": [[28, 29, "MetricName", "accuracy"]]}
{"text": "Let us see if that means that the four metrics are correlated with the human accuracy .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "According to the first three configurations , architecture A2 prevails and the GPT transformer takes over the other lan - guage models .", "entities": [[12, 13, "MethodName", "GPT"]]}
{"text": "These findings mean that we actually can rely on the use of these standard metrics to evaluate the answer generation task for question - answering .", "entities": [[18, 20, "TaskName", "answer generation"]]}
{"text": "When trying to get an insight on the answers generated by the current intelligent systems such as Google assistant and Alexa , we noted that these systems are very accurate when extracting the correct answer to a question and can sometimes generate user - friendly answers that help recall the question context , specially with Alexa .", "entities": [[17, 18, "DatasetName", "Google"]]}
{"text": "In Column GM we use \" BT - ml \" for BERT - base - multilingual - cased and \" BT \" for BERTlarge - cased .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "In column LM \" GPT \" stands for for OpenAI - GPT , \" GPT2 - l \" for GPT2 - large , \" GPT2 - m \" for GPT2medium , \" GPT2 \" for GPT2 , \" BT - b - uc \" for BERT - base - uncased , \" mlm - 2048 \" for XLM - mlm - en - 2048 and \" BT - l - c \" for BERT - large - cased .", "entities": [[4, 5, "MethodName", "GPT"], [11, 12, "MethodName", "GPT"], [45, 46, "MethodName", "BERT"], [52, 53, "DatasetName", "mlm"], [54, 55, "DatasetName", "2048"], [57, 58, "MethodName", "XLM"], [59, 60, "DatasetName", "mlm"], [63, 64, "DatasetName", "2048"], [73, 74, "MethodName", "BERT"]]}
{"text": "To better understand this complex and understudied task , we study the functional structure of long - form answers collected from three datasets , ELI5 ( Fan et al , 2019 ) , We - bGPT ( Nakano et al , 2021 ) and Natural Questions ( Kwiatkowski et al , 2019 ) .", "entities": [[24, 25, "DatasetName", "ELI5"], [44, 46, "DatasetName", "Natural Questions"]]}
{"text": "We develop an ontology of six sentence - level functional roles for long - form answers , and annotate 3.9k sentences in 640 answer paragraphs .", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "We collect discourse annotations on three long - form question answering ( LFQA ) datasets , ELI5 ( Fan et al , 2019 ) , WebGPT ( Nakano et al , 2021 ) and Natural Questions ( NQ ) ( Kwiatkowski et al , 2019 ) .", "entities": [[9, 11, "TaskName", "question answering"], [16, 17, "DatasetName", "ELI5"], [34, 36, "DatasetName", "Natural Questions"], [37, 38, "DatasetName", "NQ"]]}
{"text": "While all three contain paragraph - length answers needed for complex queries , they are collected in distinct mannersanswers in ELI5 are written by Reddit users ; answers in WebGPT are written by annotators who searched documents on a web interface and heavily quoted those documents to form an answer , and answers in NQ are pre - existing paragraphs from Wikipedia corpus .", "entities": [[20, 21, "DatasetName", "ELI5"], [24, 25, "DatasetName", "Reddit"], [54, 55, "DatasetName", "NQ"]]}
{"text": "We also annotate a small number of model - generated answers from a recent long - form question answering ( LFQA ) system ( Krishna et al , 2021 ) and provide rich analysis of their discourse structure .", "entities": [[17, 19, "TaskName", "question answering"]]}
{"text": "Answers in ELI5 contains more examples and elaborations , while answers extracted WebGPT : How much money is needed in order to not have to work for the rest of your life ?", "entities": [[2, 3, "DatasetName", "ELI5"]]}
{"text": "from Wikipedia passages ( NQ ) contain more auxiliary information .", "entities": [[4, 5, "DatasetName", "NQ"]]}
{"text": "Analyzing a subset of ELI5 and WebGPT , we also identify a big gap in lexical overlap between long - form answer and evidence passages across all functional roles .", "entities": [[4, 5, "DatasetName", "ELI5"]]}
{"text": "We further envision using functional roles for controllable long - form generations , concise answer generation , and improved evaluation metrics for LFQA .", "entities": [[14, 16, "TaskName", "answer generation"]]}
{"text": "We developed our ontology by examining longform answers in online community forums ( subreddit", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "Like I 'm Five ( ELI5 ) )", "entities": [[6, 7, "DatasetName", "ELI5"]]}
{"text": "and Wikipedia passages , hence answers derived from different domains ( e.g. , textbooks ) can contain roles beyond our ontology .", "entities": [[20, 21, "MethodName", "ontology"]]}
{"text": "As our ontology does not provide an exhaustive list of the functional roles , we instructed our annotators to annotate other roles not covered by our ontology as Miscellaneous as well .", "entities": [[2, 3, "MethodName", "ontology"], [26, 27, "MethodName", "ontology"], [28, 29, "TaskName", "Miscellaneous"]]}
{"text": "4 ELI5 / ELI5 - model ELI5 consists of QA pairs where the questions and answers are retrieved from the subreddit r / explainlikeimfive .", "entities": [[1, 2, "DatasetName", "ELI5"], [3, 4, "DatasetName", "ELI5"], [6, 7, "DatasetName", "ELI5"]]}
{"text": "The answers in ELI5 are of varying quality and style .", "entities": [[3, 4, "DatasetName", "ELI5"]]}
{"text": "In addition to answers in the original datasets , we annotate a small number of model - generated answers from Krishna et al ( 2021 ) ( we refer this set as ELI5 - model ) , a state - ofthe art LFQA system on ELI5 .", "entities": [[32, 33, "DatasetName", "ELI5"], [45, 46, "DatasetName", "ELI5"]]}
{"text": "While they reuse questions from ELI5 , they newly collect answers from trained human annotators who were instructed to first search for related documents using a search engine and then construct the answers with reference to those documents .", "entities": [[5, 6, "DatasetName", "ELI5"]]}
{"text": "The collected data ( denoted as \" human demonstration \" consisting of question , answer , a set of evidence documents , and mapping from the answer to the evidence document ) are used to finetune GPT - 3 ( Brown et al , 2020 ) to generate long - form answers .", "entities": [[36, 37, "MethodName", "GPT"]]}
{"text": "Natural Questions ( NQ ) NQ contains questions from Google search queries , which is paired with a relevant Wikipedia article and an answer in the article if the article answers the question .", "entities": [[0, 2, "DatasetName", "Natural Questions"], [3, 4, "DatasetName", "NQ"], [5, 6, "DatasetName", "NQ"], [9, 10, "DatasetName", "Google"]]}
{"text": "This process removes 42 % , 28 % and 34 % from ELI5 , WebGPT and NQ respectively .", "entities": [[12, 13, "DatasetName", "ELI5"], [16, 17, "DatasetName", "NQ"]]}
{"text": "4 Our data is sourced from the validation split of ELI5 from the KILT ( Petroni et al , 2021 ) benchmark , the testing portion from WebGPT ( their samples are publicly hosted at https://openaipublic.blob.core.windows .", "entities": [[10, 11, "DatasetName", "ELI5"], [13, 14, "DatasetName", "KILT"]]}
{"text": "net / webgpt - answer - viewer / index.html , which answers questions from the ELI5 test set ) , and the validation split from Natural Questions .", "entities": [[15, 16, "DatasetName", "ELI5"], [25, 27, "DatasetName", "Natural Questions"]]}
{"text": "We create a filtered set of NQ that focuses on paragraph - level answers containing complex queries .", "entities": [[6, 7, "DatasetName", "NQ"]]}
{"text": "5 While many NQ questions can be answered with a short entity ( e.g. , how many episodes in season 2 breaking bad ? )", "entities": [[3, 4, "DatasetName", "NQ"]]}
{"text": "With our annotated data , we study the differences between the three types of long - form answers , namely answers provided by users in online community ( ELI5 ) , answers written by trained annotators through web search ( WebGPT ) , and answers identified in Wikipedia passages ( NQ ) .", "entities": [[28, 29, "DatasetName", "ELI5"], [50, 51, "DatasetName", "NQ"]]}
{"text": "NQ has the highest rate of invalid answer ( 15 % ) .", "entities": [[0, 1, "DatasetName", "NQ"]]}
{"text": "Around 10 % of the answers from ELI5 reject presupposition in the question , which is a common phenomena in information - seeking questions .", "entities": [[7, 8, "DatasetName", "ELI5"]]}
{"text": "NQ shows the highest proportion of auxiliary information , as the paragraphs are written independent of the questions .", "entities": [[0, 1, "DatasetName", "NQ"]]}
{"text": "In contrast , ELI5 contains more answer sentences and examples which provide explanation .", "entities": [[3, 4, "DatasetName", "ELI5"]]}
{"text": "Both ELI5 and WebGPT contain organizational sentences , demonstrating that it is commonly used when answerers assemble answers that cover more than one aspects .", "entities": [[1, 2, "DatasetName", "ELI5"]]}
{"text": "While answers from NQ are directly extracted from Wikipedia passages , both ELI5 and WebGPT are written specifically for the question .", "entities": [[3, 4, "DatasetName", "NQ"], [12, 13, "DatasetName", "ELI5"]]}
{"text": "Answerer ( annotators ) of WebGPT were instructed to answer the question based on the evidence documents returned by a search engine , while answers from ELI5 were written first independently and later paired with relevant Wikipedia passages", "entities": [[26, 27, "DatasetName", "ELI5"]]}
{"text": "Overall , WebGPT answers exhibit more lexical overlap ( unigram : 0.64 , bigram : 0.36 ) with evidence document than ELI5 answers ( unigram : 0.09 , bigram : 0.01 ) .", "entities": [[21, 22, "DatasetName", "ELI5"]]}
{"text": "For ELI5 answers , sentences belonging to answer and summary roles have the highest overlap while example , auxiliary information and miscellaneous sentences are less grounded to external sources .", "entities": [[1, 2, "DatasetName", "ELI5"]]}
{"text": "Their model uses passage retriever ( Guu et al , 2020 ) , and generates answers based on the retrieved passage with a routing transformer model .", "entities": [[23, 25, "MethodName", "routing transformer"]]}
{"text": "As we have annotated more examples from ELI5 dataset ( 411 answer paragraphs compared to around 100 paragraphs in other three datasets ( WebGPT , NQ and ELI5 - model ) ) , we randomly split the ELI5 longform answers into train , validation and test sets with a 70%/15%/15 % ratio , and train the model on the training portion .", "entities": [[7, 8, "DatasetName", "ELI5"], [25, 26, "DatasetName", "NQ"], [27, 28, "DatasetName", "ELI5"], [37, 38, "DatasetName", "ELI5"]]}
{"text": "Metrics We report accuracy with respect to the majority role label ( or adjudicated one , if majority does n't exist )", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "( Acc ) , match on any label from three annotators ( Match ) , F1 score for each role and their macro average score Macro - F1 .", "entities": [[1, 2, "MetricName", "Acc"], [15, 17, "MetricName", "F1 score"], [25, 28, "MetricName", "Macro - F1"]]}
{"text": "Classification Models This baseline classifies each sentence independently .", "entities": [[0, 1, "TaskName", "Classification"]]}
{"text": "We use the [ CLS ] token from RoBERTa - Large model ( Liu et al , 2019 ) which encodes [ question < q > ans 1 ... < start > ans i < end > ... ] , where ans i is the i th sentence in the answer .", "entities": [[8, 9, "MethodName", "RoBERTa"]]}
{"text": "We used AdamW optimizer and a linear learning rate schedule .", "entities": [[2, 3, "MethodName", "AdamW"], [3, 4, "HyperparameterName", "optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}
{"text": "Seq2Seq Models We use two variations ( base , large ) of T5 model ( Raffel et al , 2020 ) , which take the concatenation of question and answer sentences , and output the roles for each sentence sequentially .", "entities": [[0, 1, "MethodName", "Seq2Seq"], [12, 13, "MethodName", "T5"]]}
{"text": "We compare all pairs of annotation and calculate average F1 and accuracy of all pairs .", "entities": [[8, 10, "MetricName", "average F1"], [11, 12, "MetricName", "accuracy"]]}
{"text": "Table 4 reports the results on ELI5 test set .", "entities": [[6, 7, "DatasetName", "ELI5"]]}
{"text": "The sequential prediction model ( T5 ) significantly outperform classification model ( RoBERTa ) which makes a prediction per sentence .", "entities": [[5, 6, "MethodName", "T5"], [12, 13, "MethodName", "RoBERTa"]]}
{"text": "Overall , with a moderate amount of in - domain annotated data , our best model ( T5 - large ) can reliably classify functional roles of sentences in the long - form answers , showing comparable performances to human lower bound .", "entities": [[17, 18, "MethodName", "T5"]]}
{"text": "Table 5 reports the results on the three out - ofdomain datasets , WebGPT , NQ and ELI5 - model ( model - generated answers ) .", "entities": [[15, 16, "DatasetName", "NQ"], [17, 18, "DatasetName", "ELI5"]]}
{"text": "While T5 - large still exhibits the best overall performance , all learned models perform worse , partially as the role distribution has changed .", "entities": [[1, 2, "MethodName", "T5"]]}
{"text": "Despite trained on the ELI5 dataset , role classification model also perform worse on model - generated answers ( ELI5model ) , echoing our observation that human annotators find it challenging to process the discourse structure of model - generated answers .", "entities": [[4, 5, "DatasetName", "ELI5"]]}
{"text": "In news , Choubey et al ( 2020 ) adopted Van Dijk ( 2013 ) 's content schema cataloging events ( e.g. , main event , anecdotal ) , which they showed to improve the performance of event coreference resolution .", "entities": [[37, 40, "TaskName", "event coreference resolution"]]}
{"text": "In scientific writing , content types ( e.g. , background , methodology ) are shown to be useful for summarization ( Teufel and Moens , 2002 ; Cohan et al , 2018 ) , information extraction ( Mizuta et al , 2006 ; Liakata et al , 2012 ) , and information retrieval ( Kircz , 1991 ; Liddy , 1991 ) .", "entities": [[19, 20, "TaskName", "summarization"], [51, 53, "TaskName", "information retrieval"]]}
{"text": "Question Answering .", "entities": [[0, 2, "TaskName", "Question Answering"]]}
{"text": "Recent work ( Cao and Wang , 2021 ) have investigated the ontology of questions , which includes comparison questions , verification questions , judgement questions , etc .", "entities": [[12, 13, "MethodName", "ontology"]]}
{"text": "We construct the ontology of functional roles of answer sentences .", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "One of the roles in our ontology is summary , yielding an extractive summarization dataset .", "entities": [[6, 7, "MethodName", "ontology"], [12, 14, "TaskName", "extractive summarization"]]}
{"text": "This shares motivation with a line of work studying query - focused summarization ( Xu and Lapata , 2020 ) .", "entities": [[12, 13, "TaskName", "summarization"]]}
{"text": "We annotate existing , publicly available long - form question answering datasets which might contain incorrect and outdated information and societal biases .", "entities": [[9, 11, "TaskName", "question answering"]]}
{"text": "A difficulty in repurposing NQ is that not all questions with paragraph answers only actually need multiple sentences .", "entities": [[4, 5, "DatasetName", "NQ"]]}
{"text": "To identify complex questions , we built a simple BERT - based classifier , trained to distinguish NQ questions with short answers ( i.e. , less than five tokens ) and ELI5 questions .", "entities": [[9, 10, "MethodName", "BERT"], [17, 18, "DatasetName", "NQ"], [31, 32, "DatasetName", "ELI5"]]}
{"text": "We use the [ CLS ] token from BERT model to perform prediction .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "We use the original split from the ELI5 dataset , and split the NQ open 's validation set into val and test set .", "entities": [[7, 8, "DatasetName", "ELI5"], [13, 14, "DatasetName", "NQ"]]}
{"text": "We preprocessed the questions by converting to lowercase and exclude punctuation to remove syntactic differences between ELI5 and NQ questions .", "entities": [[16, 17, "DatasetName", "ELI5"], [18, 19, "DatasetName", "NQ"]]}
{"text": "We then run this classifier to select the non factoid questions from NQ questions with long - form answers , which classifies around 10 % , out of the 27 , 752 NQ long questions as non - factoid .", "entities": [[12, 13, "DatasetName", "NQ"], [32, 33, "DatasetName", "NQ"]]}
{"text": "Table 9 : Different reasons for invalid question answer pairs for ELI5 - model and annotator agreement .", "entities": [[11, 12, "DatasetName", "ELI5"]]}
{"text": "Miscellaneous It might have just been a part of their routine , or it might have been a learned behavior , or it might have been something they did because it was the only way they could do it , and they figured it out , and it was just a part of their routine , and they thought it was cool .", "entities": [[0, 1, "TaskName", "Miscellaneous"]]}
{"text": "Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision", "entities": [[4, 6, "TaskName", "Representation Learning"]]}
{"text": "How to automatically construct sentiment lexicons has become a research topic in the field of sentiment analysis and opinion mining .", "entities": [[15, 17, "TaskName", "sentiment analysis"], [18, 20, "TaskName", "opinion mining"]]}
{"text": "Recently there were some attempts to employ representation learning algorithms to construct a sentiment lexicon with sentiment - aware word embedding .", "entities": [[7, 9, "TaskName", "representation learning"]]}
{"text": "Experiments on the SemEval 2013 - 2016 datasets indicate that the sentiment lexicon generated by our approach achieves the state - of - the - art performance in both supervised and unsupervised sentiment classification , in comparison with several strong sentiment lexicon construction methods .", "entities": [[3, 5, "DatasetName", "SemEval 2013"]]}
{"text": "Sentiment lexicon plays an important role in many practical sentiment analysis and opinion mining tasks .", "entities": [[9, 11, "TaskName", "sentiment analysis"], [12, 14, "TaskName", "opinion mining"]]}
{"text": "There were some manually annotated universal sentiment lexicons such as General Inquireer ( GI ) and HowNet .", "entities": [[10, 11, "DatasetName", "General"]]}
{"text": "However , due to the ubiquitous domain diversity and absence of domain prior knowledge , the automatic construction technique for domain - specific sentiment lex - * The corresponding author of this paper . icons has become a challenging research topic in the field of sentiment analysis and opinion mining ( Wang and Xia , 2016 ) .", "entities": [[45, 47, "TaskName", "sentiment analysis"], [48, 50, "TaskName", "opinion mining"]]}
{"text": "The resulting lexicons obtained the best results in SemEval 2013 .", "entities": [[8, 10, "DatasetName", "SemEval 2013"]]}
{"text": "More advanced representation learning models were also utilized , with the aim to construct the sentiment lexicons with efficient word embeddings ( Tang et al , 2014a ; Hamilton et al , 2016 ; Vo and Zhang , 2016 ) .", "entities": [[2, 4, "TaskName", "representation learning"], [19, 21, "TaskName", "word embeddings"]]}
{"text": "The traditional representation learning framework such as Word2Vec only captures the syntactic information in the texts , but ignores the sentiment relations between words .", "entities": [[2, 4, "TaskName", "representation learning"]]}
{"text": "For example , Tang et al ( 2014a ) exploited a dedicated neural architecture to integrate document - level sentiment supervision and the syntactic knowledge for representation learning .", "entities": [[26, 28, "TaskName", "representation learning"]]}
{"text": "In representation learning , the embeddings of words are summed up to represent the document , and the word \" like \" will be falsely associated with the negative sentiment label .", "entities": [[1, 3, "TaskName", "representation learning"]]}
{"text": "Such linguistic phenomena occur frequently in review texts , and makes sentiment - aware word representation learning less effective .", "entities": [[15, 17, "TaskName", "representation learning"]]}
{"text": "To address this problem , in this paper , we propose a new representation learning framework called HSSWE , to learn sentiment - aware word embeddings based on hierarchical sentiment supervision .", "entities": [[13, 15, "TaskName", "representation learning"], [24, 26, "TaskName", "word embeddings"]]}
{"text": "Finally , following Tang et al ( 2014a ) , a simple classifier was constructed to obtain the domainspecific sentiment lexicon by using word embeddings as inputs .", "entities": [[23, 25, "TaskName", "word embeddings"]]}
{"text": "Our approach obtains the state - of - the - art performance in comparison with several strong sentiment lexicon construction methods , on the benchmark SemEval 2013 - 2016 datasets for twitter sentiment classification .", "entities": [[23, 25, "DatasetName", "the benchmark"], [25, 27, "DatasetName", "SemEval 2013"]]}
{"text": "Many variants of PMI were proposed afterwards , for example , positive pointwise mutual information ( PPMI ) , second order co - occurrence PMI ( SOC - PMI ) , etc .", "entities": [[16, 17, "DatasetName", "PPMI"], [26, 27, "DatasetName", "SOC"]]}
{"text": "The key of this method is to build a lexical graph by calculating the PPMI between words .", "entities": [[14, 15, "DatasetName", "PPMI"]]}
{"text": "There were many word representation learning methods such as NNLM ( Bengio et al , 2003 ) and Word2Vec", "entities": [[4, 6, "TaskName", "representation learning"]]}
{"text": "Some work were later proposed to deal with this problem by incorporating the sentiment information during representation learning .", "entities": [[16, 18, "TaskName", "representation learning"]]}
{"text": "In the last module , we construct a sentiment lexicon by using the sentiment - aware word embeddings as the basis .", "entities": [[16, 18, "TaskName", "word embeddings"]]}
{"text": "The bias of document - level softmax layer \u03b8t Weight of word - level softmax layer \u03b8 d Weight of document - level softmax layer p ( c | et )", "entities": [[6, 7, "MethodName", "softmax"], [14, 15, "MethodName", "softmax"], [16, 17, "HyperparameterName", "\u03b8"], [23, 24, "MethodName", "softmax"]]}
{"text": "= \uf8f4 \uf8f2 \uf8f4 \uf8f3 [ 0 , 1 ] , if SO ( t ) > 0", "entities": [[6, 7, "DatasetName", "0"], [17, 18, "DatasetName", "0"]]}
{"text": "[ 1 , 0 ] , if SO ( t ) < 0 random { [ 1 , 0 ] or [ 0 , 1 ] } , otherwise .", "entities": [[3, 4, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "( 1 ) Word - Level Sentiment Supervision We use the word - level sentiment annotation [ p ( \u2212 | t ) , p ( + | t ) ] provided in Section 3.1 to supervise word representation learning at the word level .", "entities": [[38, 40, "TaskName", "representation learning"]]}
{"text": "= sof tmax ( \u03b8 t e + b t ) .", "entities": [[1, 2, "DatasetName", "sof"], [4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "( 2 ) Document - Level Sentiment Supervision We use the document - level sentiment annotations to supervise word representation learning at the document level .", "entities": [[19, 21, "TaskName", "representation learning"]]}
{"text": "= sof tmax ( \u03b8 d de + b d ) .", "entities": [[1, 2, "DatasetName", "sof"], [4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "The weight of f word can be increased by choosing a lager value of \u03b1 .", "entities": [[14, 15, "HyperparameterName", "\u03b1"]]}
{"text": "We train our neural model with stochastic gradient descent and use AdaGrad ( Duchi et", "entities": [[6, 9, "MethodName", "stochastic gradient descent"], [11, 12, "MethodName", "AdaGrad"]]}
{"text": "Thirdly , a traditional logistic regression classifier is trained by using the embeddings of extended sentiment words as the inputs .", "entities": [[4, 6, "MethodName", "logistic regression"]]}
{"text": "We tune the hyper - parameter \u03b1 in the training process .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "We evaluate the sentiment lexicons in both supervised and unsupervised sentiment classification tasks , on the SemEval 2013 - 2016 datasets .", "entities": [[16, 18, "DatasetName", "SemEval 2013"]]}
{"text": "We follow ( Mohammad et al , 2013 ) to extract the lexicon features as follows : Total count of words in the tweet score of which is greater than 0 ; Total count of words in the tweet score of which is less than 0 ; The sum of scores for all word great than 0 ; 2 http://help.sentiment140.com/for - students The sum of scores for all word less than 0 ; The max score greater than 0 ; The min score less than 0 ; Non - zero score of the last positive word in the tweet ; Non - zero score of the last negative word in the tweet .", "entities": [[30, 31, "DatasetName", "0"], [45, 46, "DatasetName", "0"], [56, 57, "DatasetName", "0"], [71, 72, "DatasetName", "0"], [78, 79, "DatasetName", "0"], [85, 86, "DatasetName", "0"]]}
{"text": "We report the performance of SVM by using these lexicon features .", "entities": [[5, 6, "MethodName", "SVM"]]}
{"text": "Unsupervised Sentiment Classification Evaluation : For unsupervised sentiment classification , we sum up the scores of all sentiment words in the document , according to the sentiment lexicon .", "entities": [[2, 3, "TaskName", "Classification"]]}
{"text": "If the sum is greater than 0 , the document will be considered as positive , otherwise negative .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "The unsupervised learning evaluation metric is accuracy .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "We compare our HSSWE method with four sentiment lexicons generated by the related work proposed in recent years : Sentiment140 was constructed by Mohammad et al ( 2013 ) on tweet corpus based on PMI between each word and the emoticons .", "entities": [[19, 20, "DatasetName", "Sentiment140"]]}
{"text": "HIT was constructed by Tang et al ( 2014a ) with a representation learning approach .", "entities": [[12, 14, "TaskName", "representation learning"]]}
{"text": "Supervised Sentiment Classification : We first report the supervised sentiment classification F 1 score of five compared methods on the Semeval 2013 - 2016 datasets in Table 3 .", "entities": [[2, 3, "TaskName", "Classification"], [20, 22, "DatasetName", "Semeval 2013"]]}
{"text": "It outperforms Sentiment140 , HIT , NN and ETSL 1.7 , 2.8 , 1.9 , and 3.2 percentages on the average of four datasets .", "entities": [[2, 3, "DatasetName", "Sentiment140"]]}
{"text": "In can be seen that HSSWE obtains the best performance on Semeval 2013 - 2015 .", "entities": [[11, 13, "DatasetName", "Semeval 2013"]]}
{"text": "Supervised Sentiment Classification : As is shown in Table 5 , two basic models PMI - SO and Doc - Sup show similar performance .", "entities": [[2, 3, "TaskName", "Classification"]]}
{"text": "Unsupervised Sentiment Classification : As is shown in Table 6 , the conclusions are similar with that in supervised sentiment classification : HSS - WE achieves the significantly better performance .", "entities": [[2, 3, "TaskName", "Classification"]]}
{"text": "In this section , we discuss the tradeoff between two parts of supervisions by turning the tradeoff parameter \u03b1 .", "entities": [[18, 19, "HyperparameterName", "\u03b1"]]}
{"text": "In this paper , we proposed to construct sentiment lexicons based on a sentiment - aware word representation learning approach .", "entities": [[17, 19, "TaskName", "representation learning"]]}
{"text": "We proposed word representation learning via hierarchical sentiment supervision , i.e. , under the supervi - sion at both word and document levels .", "entities": [[3, 5, "TaskName", "representation learning"]]}
{"text": "Multi - Domain Named Entity Recognition with Genre - Aware and Agnostic Inference", "entities": [[3, 6, "TaskName", "Named Entity Recognition"]]}
{"text": "Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input .", "entities": [[0, 3, "TaskName", "Named entity recognition"]]}
{"text": "However , domain transfer of NER models with data from multiple genres has not been widely studied .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "To this end , we conduct NER experiments in three predictive setups on data from : a ) multiple domains ; b ) multiple domains where the genre label is unknown at inference time ; c ) domains not encountered in training .", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "We introduce a new architecture tailored to this task by using shared and private domain parameters and multi - task learning .", "entities": [[17, 21, "TaskName", "multi - task learning"]]}
{"text": "Named entity recognition ( NER ) is an important component in several tasks including named entity linking ( Cucerzan , 2007 ) , co - reference resolution ( Ng and Cardie , 2002 ) , question answering ( Krishnamurthy and Mitchell , 2015 ) , relation extraction ( Culotta and Sorensen , 2004 ) and usually sits upstream of analytics such as sentiment ( Pang and Lee , 2004 ) or stance ( Mohammad et al , 2016 ) .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"], [15, 17, "TaskName", "entity linking"], [35, 37, "TaskName", "question answering"], [45, 47, "TaskName", "relation extraction"]]}
{"text": "Building robust NER models to accurately tag and adapt to heterogeneous types of text is thus paramount .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "Recent research focused on improving the overall performance of NER models on specific data sets .", "entities": [[9, 10, "TaskName", "NER"]]}
{"text": "Yet NER models show relatively high variance even when trained on the same data ( Reimers and Gurevych , 2017 ) and poorly generalize when tested on data from different genres 1 , especially if these contain entity mentions unseen in the test data ( Augenstein et al , 2017 ; Agarwal et al , 2020 ) .", "entities": [[1, 2, "TaskName", "NER"]]}
{"text": "Despite this , research on NER models robust to different types of input is usually limited to the standard domain adaptation scenario : a single source domain rich in training data and a single target domain with limited or no training data ( Lin and Lu , 2018 ) .", "entities": [[5, 6, "TaskName", "NER"], [19, 21, "TaskName", "domain adaptation"]]}
{"text": "We argue that this is an over - simplified experimental setup that is not typical for how NER models are used in real - world applications .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "Ideally , NER models use all available data , regardless of genre , and perform inference on data from any genre , even if this was not encountered in training .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "This work introduces three experimental setups for the NER task where models are trained on data from multiple genres and evaluated as follows : a ) Multi - Domain - evaluation is performed across multiple genres , all seen in training .", "entities": [[8, 9, "TaskName", "NER"]]}
{"text": "We propose a neural architecture for NER tailored to these three experimental setups , based on the popular BiLSTM - CRF architecture ( Lample et al , 2016 ) .", "entities": [[6, 7, "TaskName", "NER"], [18, 19, "MethodName", "BiLSTM"], [20, 21, "MethodName", "CRF"]]}
{"text": "Further , we add a multi - task learning objective for domain prediction to guide this separation .", "entities": [[5, 9, "TaskName", "multi - task learning"]]}
{"text": "We compare this model with several competitive methods that use a similar base architecture while holding the embeddings constant ( i.e. GloVe embeddings ) .", "entities": [[21, 23, "MethodName", "GloVe embeddings"]]}
{"text": "These include models trained on data from each domain independently , models that pool all data and models that use domain identities as features through to source - target domain adaptation methods .", "entities": [[29, 31, "TaskName", "domain adaptation"]]}
{"text": "Setups for Domain Adaptation Domain adaptation , formulated as learning a single model for the same task across multiple domains , is a wellstudied research area in NLP ( Chelba and Acero , 2004 ; Florian et al , 2004 ; Blitzer et al , 2006 ; Daum\u00e9 III , 2007 ) .", "entities": [[2, 4, "TaskName", "Domain Adaptation"], [4, 6, "TaskName", "Domain adaptation"]]}
{"text": "The standard setup for domain adaptation is to maximize performance on data from a single low - resource ( target ) domain , by using data from a single high - resource ( source ) domain ( Blitzer et al , 2007 ; Peng and Dredze , 2017 ) .", "entities": [[4, 6, "TaskName", "domain adaptation"]]}
{"text": "The multi - domain text classification task studied in ( Li and Zong , 2008 ; Wu and Huang , 2015 ; Chen and Cardie , 2018 ) is the analogous setup for the text classification task to the first experimental setup we propose for NER .", "entities": [[4, 6, "TaskName", "text classification"], [34, 36, "TaskName", "text classification"], [45, 46, "TaskName", "NER"]]}
{"text": "Multi - Domain Adaptation Methods for multidomain text classification use data fusion either at the feature or classifier level ( Li and Zong , 2008 ) , decomposing the classifier into a shared one and multiple domain - specific ones ( Wu and Huang , 2015 ) , further guided by a domain discriminator ( Chen and Cardie , 2018 ) which is also used in multi - lingual NER ( Chen et al , 2019 ) .", "entities": [[2, 4, "TaskName", "Domain Adaptation"], [7, 9, "TaskName", "text classification"], [69, 70, "TaskName", "NER"]]}
{"text": "Further , Mc - Closky et al ( 2010 ) explored sequence tagging tasks on data from unknown domains and Chen and Cardie ( 2018 ) experiment with sentiment classification on data from unknown domains , similar to our third experimental setup for NER .", "entities": [[43, 44, "TaskName", "NER"]]}
{"text": "Domain Adaptation for NER Models for domain adaptation in NER using neural architectures were studied recently , albeit mostly for covering the single - source and single - target setup .", "entities": [[0, 2, "TaskName", "Domain Adaptation"], [3, 4, "TaskName", "NER"], [6, 8, "TaskName", "domain adaptation"], [9, 10, "TaskName", "NER"]]}
{"text": "For sequence tagging , one CRF for each of the two domains is used to obtain the predictions ( Yang et al , 2017 ) .", "entities": [[5, 6, "MethodName", "CRF"]]}
{"text": "The model adds layers between embeddings and the BiLSTM layers , between the BiL - STM and the CRF for the target domain and separate CRF layers , the latter two of which we adapt to our proposed architecture for multi - domain adaptation .", "entities": [[8, 9, "MethodName", "BiLSTM"], [18, 19, "MethodName", "CRF"], [25, 26, "MethodName", "CRF"], [42, 44, "TaskName", "domain adaptation"]]}
{"text": "Their experiments on NER focused only on three data sets : CoNLL , MUC - 6 and MUC - 7 and only the first of our three setups .", "entities": [[3, 4, "TaskName", "NER"]]}
{"text": "A multi - task domain adaptation method for NER and word segmentation is used in ( Peng and Dredze , 2017 ) .", "entities": [[4, 6, "TaskName", "domain adaptation"], [8, 9, "TaskName", "NER"]]}
{"text": "The output of these linear layers is fed to a CRF .", "entities": [[10, 11, "MethodName", "CRF"]]}
{"text": "We adopt the linear domain projection method , but extend this to also include a shared projection , followed by domain - specific CRFs and multi - task learning .", "entities": [[25, 29, "TaskName", "multi - task learning"]]}
{"text": "Finally , another type of domain adaptation is temporal adaptation of models tested on data that is more recent than the training data , when each temporal slice can be considered as a different domain ( Rijwhani and Preo\u0163iuc - Pietro , 2020 ) .", "entities": [[5, 7, "TaskName", "domain adaptation"]]}
{"text": "This section describes the proposed NER architecture tailored the architecture to our multi - domain experimental setups , which is independent of input embedding representation .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "The basic component of our NER models is an architecture which has reached state - of - the - art performance several times over the last few years ( Lample et al , 2016 ; Peters et al , 2018 ; Akbik et al , 2018 ) .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "Named entity recognition task is a structured prediction task and earlier statistical approaches are based models like Conditional Random Fields ( Lafferty et al , 2001 ) , which rely on features often designed based on domain - specific knowledge ( Luo et al , 2015 ) .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [6, 8, "TaskName", "structured prediction"]]}
{"text": "The current dominant approach to the NER task consists of neural architectures based on recurrent neural networks with different choices of input representations Ma and Hovy , 2016 ;", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "The input consists of a concatenation of pretrained word embeddings and character embeddings .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "Character embeddings are trained using an LSTM from randomly initialized vectors as in ( Lample et al , 2016 ) .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "Word embeddings are derived from a combination GloVe ( Pennington et al , 2014 ) and FastText ( Bojanowski et al , 2017 ) pre - trained word embeddings , as used in ( Ma and Hovy , 2016 ) .", "entities": [[0, 2, "TaskName", "Word embeddings"], [7, 8, "MethodName", "GloVe"], [16, 17, "MethodName", "FastText"], [27, 29, "TaskName", "word embeddings"]]}
{"text": "This representation is passed through two LSTM layers that process the input sequence in differ - .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "The outputs of these layers are concatenated and , in order to map the word representation obtained from the LSTM module into the label distribution , passed to a one - layer feed - forward network .", "entities": [[19, 20, "MethodName", "LSTM"]]}
{"text": "A Conditional Random Field is applied to the class predictions to jointly assign the sequence tags using a transition matrix .", "entities": [[1, 4, "MethodName", "Conditional Random Field"]]}
{"text": "This CRF layer improves performance of the model ( Lample et al , 2016 ) as it ensures the output sequence takes into account dependencies between the tags and also models the constraints the output sequence adheres to ( e.g. I - PER can not follow B - LOC ) .", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "We propose a new architecture based on the BiLSTM - CRF model tailored to the three proposed experimental setups .", "entities": [[8, 9, "MethodName", "BiLSTM"], [10, 11, "MethodName", "CRF"]]}
{"text": "Our proposed architecture enhances the base architecture with three components : a ) domain - specific and - independent feed - forward layers that process the BiLSTM outputs ; b ) domain - specific and - independent feed forward layers CRFs ; c ) a multi - task learning objective that learns domain labels as an auxiliary task .", "entities": [[26, 27, "MethodName", "BiLSTM"], [45, 49, "TaskName", "multi - task learning"]]}
{"text": "We model the shared and private features at both the feature mapping stage connecting the BiLSTM outputs to the CRF ( s ) and at the CRF level .", "entities": [[15, 16, "MethodName", "BiLSTM"], [19, 20, "MethodName", "CRF"], [26, 27, "MethodName", "CRF"]]}
{"text": "We expect the features extracted by the BiLSTM layers to model the structure of the input across all domains .", "entities": [[7, 8, "MethodName", "BiLSTM"]]}
{"text": "In training , the BiLSTM outputs are projected to both the shared layer and the private layer based on the domain label provided in training .", "entities": [[4, 5, "MethodName", "BiLSTM"]]}
{"text": "The CRF layer is used to make a global decision for the entire tag sequence by modelling label dependencies .", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "Hence , each feedforward layer feeds into either private CRFs ( one for each domain ) or a shared CRF .", "entities": [[19, 20, "MethodName", "CRF"]]}
{"text": "The separation of the shared and private layers could happen before the CRF stage ( late separation ) or before the feed - forward layer stage ( early separation ) .", "entities": [[12, 13, "MethodName", "CRF"]]}
{"text": "Multi - Task Learning of Domain Labels Further , to better guide the learning process , we augment our architecture with a multi - task learning objective .", "entities": [[0, 4, "TaskName", "Multi - Task Learning"], [22, 26, "TaskName", "multi - task learning"]]}
{"text": "The architecture uses average pooling on BiLSTM outputs followed by a fully connected layer .", "entities": [[3, 5, "MethodName", "average pooling"], [6, 7, "MethodName", "BiLSTM"]]}
{"text": "Finally , softmax is applied over the learned domain feature to obtain a probability distribution of all domain labels .", "entities": [[2, 3, "MethodName", "softmax"]]}
{"text": "The domain classification objective is to minimize the crossentropy loss L domain ( x , y d ) for an input x with domain label y d .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "The global objective function is the combination of the NER loss function and domain loss : L ( x ; y , y d )", "entities": [[9, 10, "TaskName", "NER"], [10, 11, "MetricName", "loss"], [14, 15, "MetricName", "loss"]]}
{"text": "In addition , in order to test the feasibility of NER tagging in a zero - shot domain setup , we present additional data covering four other genres .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "The data set collection used in learning the multidomain models ( denoted as ' Open Data ' in the rest of the paper ) includes the following three data sets : CoNLL 2003 We use the data set released as part of CoNLL 2003 shared task for English ( Tjong Kim Sang and De Meulder , 2003 ) , which is arguably the most popular data set for NER and is regularly used as a benchmark for this task .", "entities": [[31, 33, "DatasetName", "CoNLL 2003"], [42, 44, "DatasetName", "CoNLL 2003"], [68, 69, "TaskName", "NER"]]}
{"text": "OntoNotes ( six genres )", "entities": [[0, 1, "DatasetName", "OntoNotes"]]}
{"text": "The OntoNotes data set ( Hovy et al , 2006 ) ( Augenstein et al , 2017 ) .", "entities": [[1, 2, "DatasetName", "OntoNotes"]]}
{"text": "Zero Shot Genres Finally , for zero - shot genre NER , we use a collection of internal data sets from four different genres spanning news , closed captions and other documents .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "In case other types of entities exist in the data ( e.g. MISC for CoNLL , dates for OntoNotes ) , these are considered to be not an entity , similar to ( Augenstein et al , 2017 ) .", "entities": [[18, 19, "DatasetName", "OntoNotes"]]}
{"text": "We train our models using the open data sets from CoNLL , Twitter and OntoNotes .", "entities": [[14, 15, "DatasetName", "OntoNotes"]]}
{"text": "The training , development and test splits of CoNLL and OntoNotes follows the standard splits .", "entities": [[10, 11, "DatasetName", "OntoNotes"]]}
{"text": "InDomain trains an individual NER model using the base architecture for each of the known domains .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "InDomain - DomainClassifier uses the same NER models as the InDomain model .", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "We thus build a separate domain classifier using a Bi - LSTM recurrent neural network that feeds the final hidden state into a feed - forward network to recognize the domain of a given input sentence and route it to the appropriate InDomain NER model .", "entities": [[11, 12, "MethodName", "LSTM"], [43, 44, "TaskName", "NER"]]}
{"text": "Data pooling is the standard baseline in most domain adaptation experiments .", "entities": [[8, 10, "TaskName", "domain adaptation"]]}
{"text": "This is similar to the INIT strategy for domain adaptation used in ( Mou et al , 2016 ; .", "entities": [[8, 10, "TaskName", "domain adaptation"]]}
{"text": "The gradient reversal technique aims to confuse the domain discriminator while learning NER with the combination of the training data from all domains .", "entities": [[12, 13, "TaskName", "NER"]]}
{"text": "This is appended to the word - level features that are used as input to the BiLSTM layers .", "entities": [[16, 17, "MethodName", "BiLSTM"]]}
{"text": "This method uses a domain - specific CRF for each domain and a shared CRF for all domains .", "entities": [[7, 8, "MethodName", "CRF"], [14, 15, "MethodName", "CRF"]]}
{"text": "Both the BiLSTM and the feed - forward layers are shared across all domains .", "entities": [[2, 3, "MethodName", "BiLSTM"]]}
{"text": "The learning rate is 1e \u22123 with weight decay value 1e \u22125 .", "entities": [[1, 3, "HyperparameterName", "learning rate"], [7, 9, "MethodName", "weight decay"]]}
{"text": "We use 300 - dimensional pre - trained word embeddings as described in Section 3.1 , whereas the character LSTM is randomly initialized and has a hidden dimension of 64 .", "entities": [[8, 10, "TaskName", "word embeddings"], [19, 20, "MethodName", "LSTM"]]}
{"text": "When training the domain features together with the NER ( PoolDomain+DomainFeat ) , we set the domain embedding size to 128 .", "entities": [[8, 9, "TaskName", "NER"]]}
{"text": "Following , we evaluate the performance of our model on the data used for zero - shot genre NER .", "entities": [[18, 19, "TaskName", "NER"]]}
{"text": "The goal of these experiments is to examine the NER performance across the three proposed experimental setups which focus on model generalizability across multiple domains .", "entities": [[9, 10, "TaskName", "NER"]]}
{"text": "Performance is measured using micro F1 score .", "entities": [[4, 6, "MetricName", "micro F1"]]}
{"text": "The other standard baseline for domain adaptation ( PoolDomain ) obtains a similar performance ( \u22122.19 compared to our method ) to the in - domain approach , which shows the benefits of multidomain adaptation .", "entities": [[5, 7, "TaskName", "domain adaptation"]]}
{"text": "PoolDomain - Init is performing overall poorly , which shows that the INIT transfer learning strategy that is somewhat effective for source - target domain adaptation does not work well in the multidomain setup .", "entities": [[13, 15, "TaskName", "transfer learning"], [24, 26, "TaskName", "domain adaptation"]]}
{"text": "when each of the three components are alternatively turned off : Shared - Private Linear layer , Shared - Private CRF and the domain prediction auxiliary task .", "entities": [[14, 16, "MethodName", "Linear layer"], [20, 21, "MethodName", "CRF"]]}
{"text": "Shared vs. Shared - Private CRF With the rest of the architecture fixed , the results show that the shared - private CRF performs close to the shared CRF when the shared linear layer is used ( 80.08 vs. 80.16 ; 82.04 vs. 82.74 ; all comparisons in this section are on macro - average ) .", "entities": [[5, 6, "MethodName", "CRF"], [22, 23, "MethodName", "CRF"], [28, 29, "MethodName", "CRF"], [32, 34, "MethodName", "linear layer"]]}
{"text": "However , once we use a separate linear layer between the BiLSTM and each CRF , the difference between having the shared and the shared - private CRFs increases drastically ( 81.36 vs. 83.11 ; 82.30 vs. 84.68 ) .", "entities": [[7, 9, "MethodName", "linear layer"], [11, 12, "MethodName", "BiLSTM"], [14, 15, "MethodName", "CRF"]]}
{"text": "With only this late separation , the inputs to CRF decoders are still domain - independent features , which makes it hard for the linear CRF to adapt .", "entities": [[9, 10, "MethodName", "CRF"], [25, 26, "MethodName", "CRF"]]}
{"text": "When the inputs are already domain - dependent , the linear CRF can better use this information in performing the joint inference of the sequence .", "entities": [[11, 12, "MethodName", "CRF"]]}
{"text": "We note that only using shared - private CRF with the base architecture is equivalent to the MultDomain - SP method ( Yang et al , 2017 ) .", "entities": [[8, 9, "MethodName", "CRF"]]}
{"text": "The results show that regardless of the other parameters , adding shared and private linear layers between the BiLSTM layers and the CRF ( s ) is always beneficial ( 80.08 vs. 81.36 ; 80.16 vs. 83.11 ; 82.04 vs. 82.30 ; 82.74 vs. 84.68 ) .", "entities": [[18, 19, "MethodName", "BiLSTM"], [22, 23, "MethodName", "CRF"]]}
{"text": "The improvements are relatively larger when combined with shared and private CRF , as previously seen .", "entities": [[11, 12, "MethodName", "CRF"]]}
{"text": "Multi - Task Learning of Domain Labels", "entities": [[0, 4, "TaskName", "Multi - Task Learning"]]}
{"text": "Finally , we compare the impact of adding the multi - task learning objective .", "entities": [[9, 13, "TaskName", "multi - task learning"]]}
{"text": "We find that , similar to the linear layers , adding the domain prediction task is always beneficial for the model with the increase being larger if is only a shared linear layer .", "entities": [[31, 33, "MethodName", "linear layer"]]}
{"text": "We use an oracle - based selection technique on the in - domain models to select , after the prediction and using the gold labels the model which performed best for each test instance , as selected using F1 score or , if there are no entities , the model with most O predictions .", "entities": [[38, 40, "MetricName", "F1 score"]]}
{"text": "The oracle thus provides the counterfactually \" Optimal \" strategy of model selection for each test instance and represents an upper bound on strategies relying on InDomain models .", "entities": [[11, 13, "TaskName", "model selection"]]}
{"text": "This highlights both the variability of NER models trained on different data sets and that there is potentially more room for improvements in the multi - domain setup .", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "The Supplementary Material shows a breakdown of the domain prediction labels for three methods : domain classification , domain prediction in the proposed MultDomain - SP - Aux model and the oracle in - domain choice on gold data .", "entities": [[1, 3, "DatasetName", "Supplementary Material"]]}
{"text": "The main additions are that of the shared layers and the auxiliary task to the components of the N in - domain models and is thus a constant addition in the number of parameters to the total of N indomain models .", "entities": [[31, 34, "HyperparameterName", "number of parameters"]]}
{"text": "Existing NER approaches are widely faced with limited scalability when applied to data that spans multiple domains .", "entities": [[1, 2, "TaskName", "NER"]]}
{"text": "This paper introduced three experimental setups that provide a framework for evaluating the robustness of NER models .", "entities": [[15, 16, "TaskName", "NER"]]}
{"text": "Future work will focus on domain adaptation at the embedding layer .", "entities": [[5, 7, "TaskName", "domain adaptation"]]}
{"text": "In uniformly many cases , predictions from other in - domain models are better than the existing in - domain one , showing the variability of the NER models .", "entities": [[27, 28, "TaskName", "NER"]]}
{"text": "Analysis of Zero - Shot Crosslingual Learning between English and Korean for Named Entity Recognition", "entities": [[12, 15, "TaskName", "Named Entity Recognition"]]}
{"text": "This paper presents a English - Korean parallel dataset that collects 381 K news articles where 1 , 400 of them , comprising 10 K sentences , are manually labeled for crosslingual named entity recognition ( NER ) .", "entities": [[32, 35, "TaskName", "named entity recognition"], [36, 37, "TaskName", "NER"]]}
{"text": "Three types of crosslingual learning approaches , direct model transfer , embedding projection , and annotation projection , are used to develop zero - shot Korean NER models .", "entities": [[26, 27, "TaskName", "NER"]]}
{"text": "This is pioneering work that explores zero - shot crosslingual learning between English and Korean and provides rich parallel annotation for a core NLP task such as named entity recognition .", "entities": [[27, 30, "TaskName", "named entity recognition"]]}
{"text": "Crosslingual representation learning aims to derive embeddings for words ( or sentences ) from multiple languages that can be projected into a shared vector space ( Conneau et al , 2018 ; Schuster et al , 2019b ; Conneau and Lample , 2019 ) .", "entities": [[1, 3, "TaskName", "representation learning"]]}
{"text": "The latest multilingual transformer encoders such as BERT ( Devlin et al , 2019 ) and XLM ( Conneau et al , 2020 ) have made it possible to develop robust crosslingual models through zero - shot learning that requires no labeled training data on the target side ( Jebbara and Cimiano , 2019 ; Chidambaram et al , 2019 ; Chi et al , 2020 ) .", "entities": [[7, 8, "MethodName", "BERT"], [16, 17, "MethodName", "XLM"], [34, 38, "TaskName", "zero - shot learning"]]}
{"text": "This paper provides a comprehensive analysis of crosslingual zero - shot learning in English and Korean .", "entities": [[8, 12, "TaskName", "zero - shot learning"]]}
{"text": "We first create a new dataset comprising a large number of parallel sentences and annotate them for named entity recognition ( NER ; Sec . 3 ) .", "entities": [[17, 20, "TaskName", "named entity recognition"], [21, 22, "TaskName", "NER"]]}
{"text": "We then adapt the crosslingual approaches and build NER models in Korean through zeroshot learning ( Sec . 4 ) .", "entities": [[8, 9, "TaskName", "NER"]]}
{"text": "Our results are promising although depicting few challenges in zero - shot learning for English and Korean ( Sec . 6 ) .", "entities": [[9, 13, "TaskName", "zero - shot learning"]]}
{"text": "The contributions of this work can be summarized as follows : To create a crosslingual dataset that enables to develop robust zero - shot NER models in Korean .", "entities": [[24, 25, "TaskName", "NER"]]}
{"text": "Aldarmaki and Diab ( 2019 ) derived a context - aware crosslingual mapping from a parallel corpus using word alignment .", "entities": [[18, 20, "TaskName", "word alignment"]]}
{"text": "Schuster et al ( 2019b ) aligned word embeddings from multilingual transformer encoders using context independent embedding anchors .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "Devlin et al ( 2019 ) proposed multilingual BERT that generates contextualized word embeddings for multiple languages in one vector space by simply sharing all languages ' vocabulary .", "entities": [[8, 9, "MethodName", "BERT"], [12, 14, "TaskName", "word embeddings"]]}
{"text": "Conneau and Lample ( 2019 ) extends mBERT by introducing bilingual data and an extra pretraining task ( Translation Language Modeling ) .", "entities": [[7, 8, "MethodName", "mBERT"], [18, 19, "TaskName", "Translation"]]}
{"text": "Luo et al ( 2021 ) adds a crossattention module into the Transformer encoder to explicitly build the interdependence between langauges .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "For cross - lingual NER , Ni et al ( 2017 ) presented weakly supervised crosslingual models using annotation and representation projection .", "entities": [[1, 5, "TaskName", "cross - lingual NER"]]}
{"text": "Huang et al ( 2019 ) made an empirical analysis of how sequential order and multilingual embeddings are used in crosslingual NER .", "entities": [[21, 22, "TaskName", "NER"]]}
{"text": "Artetxe and Schwenk ( 2019 ) presented multilingual transfer models that used few - shot learning adapting supervising BEA , ranking and retraining for massive transfer .", "entities": [[12, 16, "TaskName", "few - shot learning"]]}
{"text": "Wu and Dredze ( 2019 ) and Wu et al ( 2020 ) directly transfers the NER model trained on the source language to the target language using crosslingual representations from multilingual encoders ( Direct model transfer ) .", "entities": [[16, 17, "TaskName", "NER"]]}
{"text": "Although there exist news articles with single sentence after the grouping process , we still include them in the train set in order to make full use of the parallel sentences provided , which will be used to train the word alignment model and the transformation matrix in Section 5 .", "entities": [[40, 42, "TaskName", "word alignment"]]}
{"text": "ELIT 2 using the Flair model trained on OntoNotes ( Pradhan et al , 2013 ) .", "entities": [[8, 9, "DatasetName", "OntoNotes"]]}
{"text": "Korean sentences are tagged by a CRF - based model adapting KoBERT ( Korean BERT ) 3 trained on the corpus distributed by Cheon and Kim ( 2018 ) .", "entities": [[6, 7, "MethodName", "CRF"], [14, 15, "MethodName", "BERT"]]}
{"text": "In addition , Korean sentences are processed by the Mecab morphological analyzer 4 that produces more linguistically sounding tokens than SentencePiece ( Kudo and Richardson , 2018 ) in KoBERT .", "entities": [[20, 21, "MethodName", "SentencePiece"]]}
{"text": "All named entities from the CRF tagger are then remapped to the tokens produced by the Mecab analyzer using heuristics so they can better reflect the previous morphology work in Korean ( Hong , 2009 ) .", "entities": [[5, 6, "MethodName", "CRF"]]}
{"text": "For English , the pseudo - annotated entities are revised by the OntoNotes named entity guidelines ( BBN , 2014 ; Maekawa , 2018 ) , and missing entities are annotated as necessary .", "entities": [[12, 13, "DatasetName", "OntoNotes"]]}
{"text": "We reproduce the previous work which finetunes mBERT on English NER dataset and transfers the trained model to a target language , in our case , Korean .", "entities": [[7, 8, "MethodName", "mBERT"], [10, 11, "TaskName", "NER"]]}
{"text": "We fine - tune on OntoNotes , whereas the previous work fine - tuned on CoNLL 2003 NER dataset .", "entities": [[5, 6, "DatasetName", "OntoNotes"], [15, 17, "DatasetName", "CoNLL 2003"], [17, 18, "TaskName", "NER"]]}
{"text": "The other two approaches that will be experimented are embedding preojection and annotation projection following Ni et al ( 2017 ) , although some modules in the implementation are updated or added : the encoders used to derive the embeddings from sentences , the word alignment tool , the training data selection scheme heuristics .", "entities": [[44, 46, "TaskName", "word alignment"]]}
{"text": "Figure 1 illustrates an overview of two crosslingual learning approaches adapted to develop zero - shot Korean NER models .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "The Korean embeddings generated by individual approaches are fed into a trainer to build the Korean NER models .", "entities": [[16, 17, "TaskName", "NER"]]}
{"text": "No manual annotation is added to the Korean data ; thus they both are zero - shot learning .", "entities": [[14, 18, "TaskName", "zero - shot learning"]]}
{"text": "The NER model is trained on only English sentences represented by the transformed embeddings k * and a pseudo - label annotated with an existing English NER model .", "entities": [[1, 2, "TaskName", "NER"], [26, 27, "TaskName", "NER"]]}
{"text": "The quality of pseudo annotation hugely depends on the performance of word alignment , which is generally not robust for the case of distant language pairs such as English and Korean .", "entities": [[11, 13, "TaskName", "word alignment"]]}
{"text": "Then , the impurity M ( , e ) is measured as follows where \u03b1 is a smoothing factor : M ( , e )", "entities": [[14, 15, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 The relative frequency P ( | e ) and the impurity M ( , e ) are used to assess pseudo - annotation reliability .", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "f ( T i ) \u2265 \u03c6 ; g ( T i ) \u2264 \u03b3 ; | E i | \u2265 \u00b5 ; \u03c8 = T | F Only the target sentences satisfying all of the above constraints are used for training given the hyperparameters", "entities": [[14, 15, "HyperparameterName", "\u03b3"]]}
{"text": "\u03c8 , \u03b1 , \u03c6 , \u03b3 , and \u00b5.", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [6, 7, "HyperparameterName", "\u03b3"]]}
{"text": "We freeze the bottom n layers ( including n ) of mBERT , where layer 0 is the embedding layer .", "entities": [[11, 12, "MethodName", "mBERT"], [15, 16, "DatasetName", "0"]]}
{"text": "The cases of n are { - 1 , 0 , 3 , 6 , 9 } , where - 1 denotes fine - tuning all layers in mBERT .", "entities": [[9, 10, "DatasetName", "0"], [28, 29, "MethodName", "mBERT"]]}
{"text": "For word - level classification , a simple linear classification layer with softmax is added on mBERT .", "entities": [[12, 13, "MethodName", "softmax"], [16, 17, "MethodName", "mBERT"]]}
{"text": "For embedding projection and annotation projection , two types of transformer encoders , mBERT ( Devlin et al 2019 )", "entities": [[13, 14, "MethodName", "mBERT"]]}
{"text": "To evaluate the zero - shot Korean NER model performance ( Table 6 ) when different size of parallel sentences are available , we use different subsets of sentences of increasing sizes ( 0 , 1 K , 10 K , 100 K , 200 K , 400 K , 747 K ; 0 to total # of sentences in TRN ) .", "entities": [[7, 8, "TaskName", "NER"], [33, 34, "DatasetName", "0"], [53, 54, "DatasetName", "0"], [60, 61, "DatasetName", "TRN"]]}
{"text": "Size 0 means the embeddings from source language are not transformed when fed into the NER model for training .", "entities": [[1, 2, "DatasetName", "0"], [15, 16, "TaskName", "NER"]]}
{"text": "Word embeddings from the last hidden layer of each transformer encoder are extracted .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "i be lists of word embeddings of the i'th sentence extracted from the last layer in the source and target encoders , respectively .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "Sentence embeddings are simply created by averaging the word embeddings of parallel source and target sentences .", "entities": [[0, 2, "TaskName", "Sentence embeddings"], [8, 10, "TaskName", "word embeddings"]]}
{"text": "i be lists of word embeddings of the i'th sentence extracted from the last layer in the source and target encoders .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "For each sentence in the source language , embeddings from last hidden layer are transformed by W w | s and fed into the NER model for training ( Section 5.5 ) .", "entities": [[24, 25, "TaskName", "NER"]]}
{"text": "\u03b1 : the smoothing factor to measure the impurity . \u03c6 : retain sentences whose annotation reliability scores by relative frequency \u2265 this threshold .", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b3 : retain sentences whose annotation reliability scores by impurity \u2264 this threshold .", "entities": [[0, 1, "HyperparameterName", "\u03b3"]]}
{"text": "Once the pseudo - annotation is created , all Korean sentences are encoded by mBERT to generate Korean embeddings that are fed into the NER model .", "entities": [[14, 15, "MethodName", "mBERT"], [24, 25, "TaskName", "NER"]]}
{"text": "For embedding and annotation projections , a bidirectional LSTM - based NER tagger using a CRF decoder is adapted to build our NER models ( Lample et al , 2016 ) .", "entities": [[7, 9, "MethodName", "bidirectional LSTM"], [11, 12, "TaskName", "NER"], [15, 16, "MethodName", "CRF"], [22, 23, "TaskName", "NER"]]}
{"text": "Table 5 shows the best result of direct model transfer , mBERT fine - tuned on OntoNotes NER dataset and evaluated on our Korean TST set .", "entities": [[11, 12, "MethodName", "mBERT"], [16, 17, "DatasetName", "OntoNotes"], [17, 18, "TaskName", "NER"]]}
{"text": "The best model is built under the setting when all layers including the embedding layer of mBERT are fine - tuned .", "entities": [[16, 17, "MethodName", "mBERT"]]}
{"text": "Both mBERT and XLM - R models showed a performance improvement over 2 % with embedding transformation .", "entities": [[1, 2, "MethodName", "mBERT"], [3, 4, "MethodName", "XLM"]]}
{"text": "The number of parallel sentences used for training transformation matrix has a considerable impact on the Zero - shot learning .", "entities": [[16, 20, "TaskName", "Zero - shot learning"]]}
{"text": "We expect further improvement of the annotation projection approach when adapting a more accurate word alignment tool or a data selection scheme , which we will further investigate .", "entities": [[14, 16, "TaskName", "word alignment"]]}
{"text": "The grammatical difference between English and Korean , where Korean uses measure words for quantifying the classes of objects while English does not in general , makes it difficult to accurately predict under the zero - shot learning setting .", "entities": [[34, 38, "TaskName", "zero - shot learning"]]}
{"text": "Given this dataset , Korean NER models are built by zero - shot learning using multilingual encoders .", "entities": [[5, 6, "TaskName", "NER"], [10, 14, "TaskName", "zero - shot learning"]]}
{"text": "Our data selection scheme for annotation projection significantly improves the NER performance although it is still suboptimal .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "Our error analysis depicts unique characteristics in Korean that make it hard for zero - shot learning , challenges that we need to overcome in the future work .", "entities": [[13, 17, "TaskName", "zero - shot learning"]]}
{"text": "There are 18 named entity tags annotated in the OntoNotes 5.0 as follows ( Pradhan et al , 2013 ) : 9 CARDINAL : Numerical terms not categorized in other categorizations .", "entities": [[9, 11, "DatasetName", "OntoNotes 5.0"]]}
{"text": "General expressions of dates are included too such as ' few months ' , ' that day ' , ' Next season ' and ' First quarter ' .", "entities": [[0, 1, "DatasetName", "General"]]}
{"text": "LAW : Named documents made into laws .", "entities": [[0, 1, "DatasetName", "LAW"]]}
{"text": "9 https://catalog.ldc.upenn.edu/docs/ LDC2013T19 / OntoNotes - Release - 5.0.pdf PERCENT :", "entities": [[4, 5, "DatasetName", "OntoNotes"]]}
{"text": "PNT : Percentage expressions with % symbol or the word ' percent ' .", "entities": [[0, 1, "DatasetName", "PNT"]]}
{"text": "Table 9 shows the performance of the NER model in the ELIT toolkit on the English development and evaluation sets in our dataset .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "The task specific NER model used : a 2 - layer Bi - LSTM with a hidden size of 768 followed by a CRF layer .", "entities": [[3, 4, "TaskName", "NER"], [13, 14, "MethodName", "LSTM"], [23, 24, "MethodName", "CRF"]]}
{"text": "A dropout rate of 0.5 is applied on the input and the output of the Bi - LSTM .", "entities": [[17, 18, "MethodName", "LSTM"]]}
{"text": "A.4 Comparison of NER performances ( Zero - shot VS Existing )", "entities": [[3, 4, "TaskName", "NER"]]}
{"text": "We compare our best performing Zero - shot Korean NER model with the existing Korean NER model 11 on TST .", "entities": [[9, 10, "TaskName", "NER"], [15, 16, "TaskName", "NER"]]}
{"text": "LAW , PRODUCT , and WORK_OF_ART appear the most in Politics , Sci / Tech , and Lifestyle , that focus on legal issues , tech products , and entertainment ( e.g. , music , movies , shows ) , respectively .", "entities": [[0, 1, "DatasetName", "LAW"]]}
{"text": "This work was partly supported by the Institute of Information and Communications Technology Planning and Evaluation ( IITP ) grant funded by the Korean government ( MSIT ) ( No . 2020 - 0 - 01361 , Artificial Intelligence Graduate School Program ( Yonsei University ) ) .", "entities": [[33, 34, "DatasetName", "0"]]}
{"text": "In my talk , I will discuss which issues still affect the development of abusive language detection systems , for example the problem of dealing with annotators ' disagreement in the creation of training data , and the issues related to contextual information in threads .", "entities": [[14, 16, "TaskName", "abusive language"]]}
{"text": "Sara 's main research interests are related to temporal and event - based processing of texts , especially in the historical domain , and social media processing , including the detection of abusive language .", "entities": [[32, 34, "TaskName", "abusive language"]]}
{"text": "One example can be exact match of the last name .", "entities": [[4, 6, "MetricName", "exact match"]]}
{"text": "Blocking has been widely studied for record linkage and entity disambiguation .", "entities": [[9, 11, "TaskName", "entity disambiguation"]]}
{"text": "= { 0 , 1 } , where r x , r y R. A blocking function f is a boolean logic formula consisting with blocking predicates p 1 , p 2 , , p n , and each predicate is connected with either conjunction or disjunction \u2228.", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Since it is made up of blocking predicates , f ( r x , r y ) = { 0 , 1 } for all r x , r y R.", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "The goal is to find an optimal blocking function f * that covers a minimum number of record pairs while missing up to a fraction \u03b5 of total number of matching record pairs .", "entities": [[25, 26, "HyperparameterName", "\u03b5"]]}
{"text": "LEARNCNF gets 3 inputs , where L are labeled Find T CandN egT erms that maximizes gain function", "entities": [[12, 13, "MethodName", "egT"]]}
{"text": "( P os , N eg , T erm ) 28 : if CALCNEGGAIN ( P os , N eg , T ) > 0", "entities": [[24, 25, "DatasetName", "0"]]}
{"text": "This ensures that we miss less than \u03b5 of the total number of positive samples in the final CNF formula .", "entities": [[7, 8, "HyperparameterName", "\u03b5"]]}
{"text": "Consider pairs in L only for clustering 9 : end for 10 : end function they filter out all terms with pairwise completeness ( PC ) below threshold t. We use the dual of the original function used as a CNF , which is now gain CN F = p+n P + N if n N > t 0 otherwise .", "entities": [[58, 59, "DatasetName", "0"]]}
{"text": "That is , measures that compare the absolute value of blocking key , e.g. exact match of first n characters .", "entities": [[14, 16, "MetricName", "exact match"]]}
{"text": "We evaluate our CNF blocking with reduction ratio ( RR ) , pairs completeness ( PC ) , and F - measure .", "entities": [[9, 10, "DatasetName", "RR"], [19, 22, "MetricName", "F - measure"]]}
{"text": "RR = 1 \u2212 p + n P + N , ( 5 ) P C = p P , ( 6 )", "entities": [[0, 1, "DatasetName", "RR"]]}
{"text": "F = 2 \u00d7 RR \u00d7 P C RR + P C .", "entities": [[4, 5, "DatasetName", "RR"], [8, 9, "DatasetName", "RR"]]}
{"text": "RR measures the efficiency of the blocking function , PC measures the quality of the blocking function .", "entities": [[0, 1, "DatasetName", "RR"]]}
{"text": "F is the harmonic mean of RR and PC .", "entities": [[6, 7, "DatasetName", "RR"]]}
{"text": "exact : Exact match .", "entities": [[2, 4, "MetricName", "Exact match"]]}
{"text": "The parameter \u03b5 is used to vary the PC .", "entities": [[2, 3, "HyperparameterName", "\u03b5"]]}
{"text": "We tested values in [ 0 , 1 ] to get the PC - RR curve .", "entities": [[5, 6, "DatasetName", "0"], [14, 15, "DatasetName", "RR"]]}
{"text": "k is selected experimentally to calculate the maximum reachable F - measure .", "entities": [[9, 12, "MetricName", "F - measure"]]}
{"text": "Figure 1 shows the PC - RR curve tested on three different gain functions .", "entities": [[6, 7, "DatasetName", "RR"]]}
{"text": "As we can see from the results , information gain has highest RR overall .", "entities": [[12, 13, "DatasetName", "RR"]]}
{"text": "Figure 2 shows the PC - RR curve for each method .", "entities": [[6, 7, "DatasetName", "RR"]]}
{"text": "For PC=0.99 , RR for CNF blocking was 0.882 while DNF blocking was 0.745 .", "entities": [[3, 4, "DatasetName", "RR"]]}
{"text": "Although the proposed similarity criterion compatible could catch positive pairs with empty attributes , it allows many negative pairs to pass the criterion , which makes the RR low .", "entities": [[27, 28, "DatasetName", "RR"]]}
{"text": "Canopy clustering was the fastest but generally we saw from the Figure 2 that its RR is much lower in high PC .", "entities": [[15, 16, "DatasetName", "RR"]]}
{"text": "Figure 3 shows the reduction ratio pair completion ( RR - PC ) curve for each method .", "entities": [[9, 10, "DatasetName", "RR"]]}
{"text": "Fisher 's method produced nearly uniformsized blocks , but had limitations in reaching a high PC and had a generally lower RR compared to our method .", "entities": [[21, 22, "DatasetName", "RR"]]}
{"text": "Morphological Inflection Generation with Multi - space Variational Encoder - Decoders", "entities": [[0, 2, "TaskName", "Morphological Inflection"]]}
{"text": "In morphologically rich languages , different affixes ( i.e. prefixes , infixes , suffixes ) can be combined with the lemma to reflect various syntactic and semantic features of a word .", "entities": [[20, 21, "DatasetName", "lemma"]]}
{"text": "The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation ( Chahuneau et al , 2013 ) and information retrieval ( Darwish and Oard , 2007 ) .", "entities": [[16, 18, "TaskName", "machine translation"], [26, 28, "TaskName", "information retrieval"]]}
{"text": "The Universal Morphological Reinflection task at SIGMORPHON 2017 ( Cotterell and Sch\u00fctze , 2017 ) is an evaluation campaign aimed at systems that tackle the task of morphological inflection .", "entities": [[27, 29, "TaskName", "morphological inflection"]]}
{"text": "The continuous latent variable is expected to reflect the lemma form of a word and the discrete variables are used to induce the desired labels of the inflected word .", "entities": [[9, 10, "DatasetName", "lemma"]]}
{"text": "For the supervised part we are reducing the reconstruction error of generating the inflected word given the lemma and corresponding tags .", "entities": [[17, 18, "DatasetName", "lemma"]]}
{"text": "We participated in task 1 , inflection generation , in which the goal is to output the inflected form of a lemma given a set of desired morphological tags .", "entities": [[21, 22, "DatasetName", "lemma"]]}
{"text": "The variational autoencoder ( Kingma and Welling , 2014 ) is an efficient way to handle ( continuous ) latent variables in neural models .", "entities": [[1, 3, "MethodName", "variational autoencoder"]]}
{"text": "The VAE learns a generative model of the probability p ( x ) of observed data x.", "entities": [[1, 2, "MethodName", "VAE"]]}
{"text": "VAE uses the variational inference to approximate the intractable posterior by learning a parametric posterior distribution for all observations .", "entities": [[0, 1, "MethodName", "VAE"], [3, 5, "MethodName", "variational inference"]]}
{"text": "Th learning objective function is the variational lower bound on the marginal log likelihood of data : log p \u03b8 ( x ) \u2265", "entities": [[19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "( z | x ) [ log p \u03b8 ( x | z ) ]", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "We maximize the variational lower bound on the conditional log likelihood of observing x ( t ) and y ( t ) as follows : log p \u03b8 ( x ( t ) ,", "entities": [[27, 28, "HyperparameterName", "\u03b8"]]}
{"text": "\u2265 E z\u223cq \u03c6 ( z | x ( s ) ) log p \u03b8 ( x ( t ) , y ( t ) ,", "entities": [[14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "[ log p \u03b8 ( x ( t ) | y ( t ) , z ) + log p \u03c0 ( y ( t ) )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "For the variational encoder - decoder ( MSVED ) , the variational lower bound on the conditional log likelihood is affected by the recognition model , and thus is computed as : log p \u03b8 ( x ( t )", "entities": [[34, 35, "HyperparameterName", "\u03b8"]]}
{"text": "log p \u03b8 ( x ( t ) , y ( t ) , z |", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "[ E z\u223cq \u03c6 ( z | x ( s ) ) [ log p \u03b8 ( x ( t ) | y ( t ) , z ) ]", "entities": [[15, 16, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b8 ( x , y , z ) q \u03c6 ( y , z | x )", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "[ E z\u223cq \u03c6 ( z | x ) [ log p \u03b8 ( x | z , y ) ]", "entities": [[12, 13, "HyperparameterName", "\u03b8"]]}
{"text": "In this case we also minimize the following cross entropy as the classification loss : D ( x , y )", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "= \u03b1 U ( x ) + L u ( x ( s ) | x ( t ) )", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "To alleviate this problem , we use the recently proposed Gumbel - Softmax trick ( Maddison et al , 2014 ; Gumbel and Lieblein , 1954 ) to create a differentiable estimator for categorical variables .", "entities": [[12, 13, "MethodName", "Softmax"]]}
{"text": "Input Dropout in the Decoder : Besides annealing the KL cost , we also randomly drop out the input token with a probability of \u03b2 at each time step of the decoder during learning .", "entities": [[1, 2, "MethodName", "Dropout"], [24, 25, "HyperparameterName", "\u03b2"]]}
{"text": "k a l b \u2303 ( x ) \u00b5 ( x ) \u270f \u21e0 N ( 0 , 1 ) z < w > k k \u00e4", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "k a l b \u2303 ( x ) \u00b5 ( x ) \u270f \u21e0 N ( 0 , 1 ) z < w > k k a Multinomial Sampling + ...... y12", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "We represent \u00b5 ( u ) and \u03c3 2 ( u ) as MLPs and sample z from N ( \u00b5 ( u ) , diag ( \u03c3 2 ( u ) ) ) , using z = \u00b5 + \u03c3 , where \u223c N ( 0 , I ) .", "entities": [[46, 47, "DatasetName", "0"]]}
{"text": "Similarly , we can obtain the hidden representation of ( t ) and use this as input to the inference model on each label y x ( t ) i , which is also an MLP following a softmax layer to generate the categorical probabilities of target labels .", "entities": [[35, 36, "DatasetName", "MLP"], [38, 39, "MethodName", "softmax"]]}
{"text": "Other experimental setups : We apply temperature annealing in the Gumble - Softmax with the scheme max ( 0.5 , exp ( \u22123e \u2212 5 t ) )", "entities": [[12, 13, "MethodName", "Softmax"]]}
{"text": "We train the model with Adadelta ( Zeiler , 2012 ) and use early - stop with a patience of 5 .", "entities": [[5, 6, "MethodName", "Adadelta"]]}
{"text": "For example , V is a label of Part - of - speech - tagging .", "entities": [[8, 11, "DatasetName", "Part - of"]]}
{"text": "Data Augmentation : We augment the data set in the similar way as Kann and Sch\u00fctze ( 2016 ) .", "entities": [[0, 2, "TaskName", "Data Augmentation"]]}
{"text": "By doing so , the training data is not limited to the form of lemma to inflected word but can also be any word pairs that share the same lemma .", "entities": [[14, 15, "DatasetName", "lemma"], [29, 30, "DatasetName", "lemma"]]}
{"text": "This indicates that for different languages , the continuous space required to encode the lemma and inflected information varies from language to language .", "entities": [[14, 15, "DatasetName", "lemma"]]}
{"text": "First , we first examined the effects of data augmentation and Wiki Data for semi - supervised learning on the performance of our model .", "entities": [[8, 10, "TaskName", "data augmentation"]]}
{"text": "By removing the augmented data from the training set , we observe a large gain in the generation accuracy .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "For example , in Arabic , the distributions of predicted tags with respect to case , possession , part - of - speech , and several other classes differ significantly from the original training data .", "entities": [[18, 21, "DatasetName", "part - of"]]}
{"text": "2 . As shown in the table , we found that the inflections of Latin and Icelandic have more suffix variations from the lemma .", "entities": [[23, 24, "DatasetName", "lemma"]]}
{"text": "Non - Autoregressive Text Generation with Pre - trained Language Models", "entities": [[3, 5, "TaskName", "Text Generation"]]}
{"text": "In this work , we show that BERT can be employed as the backbone of a NAG model to greatly improve performance .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "For a comprehensive evaluation , we test the proposed model on three text generation tasks , including text summarization , sentence compression and machine translation .", "entities": [[12, 14, "TaskName", "text generation"], [17, 19, "TaskName", "text summarization"], [20, 22, "DatasetName", "sentence compression"], [23, 25, "TaskName", "machine translation"]]}
{"text": "Autoregressive generation ( AG ) models achieve state - of - the - art performance on a wide range of text generation tasks , such as machine translation ( Vaswani et al , 2017 ) and text summarization ( Rush et al , 2015 ) .", "entities": [[20, 22, "TaskName", "text generation"], [26, 28, "TaskName", "machine translation"], [36, 38, "TaskName", "text summarization"]]}
{"text": "Specifically , we utilize BERT ( Devlin et al , 2019 ) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer ( Lafferty et al , 2001 ; for better capturing the output - side dependencies .", "entities": [[4, 5, "MethodName", "BERT"], [23, 24, "MethodName", "BERT"], [26, 27, "MethodName", "CRF"]]}
{"text": "Furthermore , for tasks like text summarization , the output sequence ( summary ) is known to be shorter than the source sequence ( article ) .", "entities": [[5, 7, "TaskName", "text summarization"]]}
{"text": "The subset size is jointly determined by the source length T and a predefined ratio \u03b1 that is set based on our prior knowledge from the data statistics .", "entities": [[15, 16, "HyperparameterName", "\u03b1"]]}
{"text": "We evaluate the proposed model on three typical text generation tasks , including text summarization , sentence compression and machine translation .", "entities": [[8, 10, "TaskName", "text generation"], [13, 15, "TaskName", "text summarization"], [16, 18, "DatasetName", "sentence compression"], [19, 21, "TaskName", "machine translation"]]}
{"text": "In summary , our contributions are : ( 1 ) We propose a novel framework that utilizes BERT for text generation under the non - autoregressive generation paradigm ; ( 2 ) We propose a decoding mechanism that allows the model to dynamically determine the output length , and a new context - aware learning objective that reduces errors stemming from the output - side conditional independence assumption ; ( 3 ) We introduce a ratio - first decoding strategy that further improve the model 's inference efficiency .", "entities": [[17, 18, "MethodName", "BERT"], [19, 21, "TaskName", "text generation"]]}
{"text": "First , we describe how to utilize BERT as a non - autoregressive generation model .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "The architecture of the proposed model is presented in Figure 2 , in which the embedding layer and the stack of transformer layers are initialized with BERT ( Devlin et al , 2019 ) .", "entities": [[26, 27, "MethodName", "BERT"]]}
{"text": "Input Representation Following the setup of BERT , we first append a [ cls ] and a [ sep ] token on both sides of the source sequence .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "As a special case , for tasks like text summarization where the source is known to be longer than the target , we do not attach the [ pad ] tokens when constructing the input .", "entities": [[8, 10, "TaskName", "text summarization"]]}
{"text": "Transformer Layers Given the source sequence X , it is processed by a stack of N transformer ( Vaswani et al , 2017 ) layers .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "Formally , the Multi - Head Attention is defined as MultiHead ( Q , K , V ) , where Q , K , V denotes the query , key and value respectively .", "entities": [[3, 7, "MethodName", "Multi - Head Attention"]]}
{"text": "= max ( 0 , xW 1 +", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "The final sequence representation H R T \u00d7d model is the output states of BERT from the last layer , where T is the source sequence length and d model is the model size .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "CRF Layer", "entities": [[0, 1, "MethodName", "CRF"]]}
{"text": "Then , H is passed through a linearchain CRF ( Lafferty et al , 2001 ) .", "entities": [[8, 9, "MethodName", "CRF"]]}
{"text": "Under the CRF framework , the likelihood of the target sequence Y with length T is then modelled as : P CRF ( Y | X )", "entities": [[2, 3, "MethodName", "CRF"], [21, 22, "MethodName", "CRF"]]}
{"text": "In practice , \u03a6 is parameterized by a neural network that maps the BERT output state", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "In the context of text generation , the size of the label space ( vocabulary size ) | V | is typically large , e.g. , 32k .", "entities": [[4, 6, "TaskName", "text generation"]]}
{"text": "During inference , the result\u1ef8 is acquired as Y = arg max Y S ( X , Y ) , where the CRF scoring function S ( X , Y ) in Equation ( 8 ) can be decomposed as : S ( X , Y )", "entities": [[22, 23, "MethodName", "CRF"]]}
{"text": "We note that the outputs of BERT can be divided into two subsets .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "This indicates that it suffices to only consider the beginning part of BERT outputs for improving the inference speed .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "Especially , for tasks like summarization where the target is known to be shorter than the source sequence , we are safe to only use the first [ \u03b1 T ] outputs of BERT to perform inference .", "entities": [[5, 6, "TaskName", "summarization"], [28, 29, "HyperparameterName", "\u03b1"], [33, 34, "MethodName", "BERT"]]}
{"text": "arg max Y F ( X , Y , \u03b1 ) , = arg max Y { [ \u03b1 T ] i=1 \u03a6", "entities": [[9, 10, "HyperparameterName", "\u03b1"], [18, 19, "HyperparameterName", "\u03b1"]]}
{"text": "+ [ \u03b1 T ] i=2 t ( y i\u22121 , y i ) } .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "It should be noted that , [ \u03b1 T ] only constrains the maximum length of the generated result , and the actual output length ( after removing the generated [ eos ] tokens ) is still decided by the model itself .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "= \u2212 T i=1 { log p \u03b8 ( y", "entities": [[7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "The overall learning objective is then defined as L CRF", "entities": [[9, 10, "MethodName", "CRF"]]}
{"text": "Non - Autoregressive generation was first introduced by Gu et al ( 2018 ) to reduce the inference latency in machine translation .", "entities": [[20, 22, "TaskName", "machine translation"]]}
{"text": "Recently , proposed to incorporate a conditional random field into the decoder of a NAG model for better modelling the outputside dependencies .", "entities": [[6, 9, "MethodName", "conditional random field"]]}
{"text": "Our work is different from prior works in two aspects : ( 1 ) we directly utilize a pretrained language model ( BERT ) to perform nonautoregressive generation ; ( 2 ) our model can dynamically generate the output sequence without the need of prespecified output length .", "entities": [[22, 23, "MethodName", "BERT"]]}
{"text": "We evaluate the proposed model on three typical text generation tasks : ( 1 ) text summarization ; ( 2 ) sentence compression and ( 3 ) machine translation .", "entities": [[8, 10, "TaskName", "text generation"], [15, 17, "TaskName", "text summarization"], [21, 23, "DatasetName", "sentence compression"], [27, 29, "TaskName", "machine translation"]]}
{"text": "The BERT model we use is the Huggingface implementation ( Wolf et al , 2019 ) ( bert - base - uncased ) .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "To approximate the transition matrix in the CRF layer , we set the dimension d of matrices E 1 and E 2 as 32 .", "entities": [[7, 8, "MethodName", "CRF"]]}
{"text": "Text summarization aims to automatically generate a compact summary that retains the most important content of the original text document ( Nenkova and McKeown , 2012 ) .", "entities": [[0, 2, "TaskName", "Text summarization"]]}
{"text": "We compare our model with several representative and the latest NAG models , including NAG - NMT ( Gu et al , 2018 ) , NAR - REG ( Wang et al , 2019b ) and NAG - CRF .", "entities": [[25, 26, "DatasetName", "NAR"], [38, 39, "MethodName", "CRF"]]}
{"text": "In addition , to better examine the effect of using BERT in NAG models , we add a BNAG - CRF baseline which adopts the same structure of the NAG - CRF model but using BERT as the encoder .", "entities": [[10, 11, "MethodName", "BERT"], [20, 21, "MethodName", "CRF"], [31, 32, "MethodName", "CRF"], [35, 36, "MethodName", "BERT"]]}
{"text": "Comparing the results of BNAG - CRF and NAG - CRF , we can see that incorporating BERT as encoder helps to improve the model performance .", "entities": [[6, 7, "MethodName", "CRF"], [10, 11, "MethodName", "CRF"], [17, 18, "MethodName", "BERT"]]}
{"text": "Nonetheless , our model still outperforms BNAG - CRF with LPD - 9 decoding .", "entities": [[8, 9, "MethodName", "CRF"]]}
{"text": "It comes from the fact that , for some input documents with length T , the reference summary is longer than [ \u03b1 T ] .", "entities": [[22, 23, "HyperparameterName", "\u03b1"]]}
{"text": "Sentence compression aims at compressing a long sentence into a short one by deleting redundant words .", "entities": [[0, 2, "DatasetName", "Sentence compression"]]}
{"text": "In this experiment , we use the Google sentence compression dataset ( Filippova and Altun , 2013 ) as our benchmark .", "entities": [[7, 8, "DatasetName", "Google"], [8, 10, "DatasetName", "sentence compression"]]}
{"text": "For evaluation , we use the standard token - kept - F1 ( F1 ) score .", "entities": [[11, 12, "MetricName", "F1"], [13, 14, "MetricName", "F1"]]}
{"text": "We also compare our model with several strong autoregressive models , including Bi - LSTM - Dep ( Filippova et al , 2015 ) , Tagger and Tagger+ILP , HiSAN - Dep and HiSAN ( Kamigaito et al , 2018 ) .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "Machine translation aims at translating text from the source language to the target language .", "entities": [[0, 2, "TaskName", "Machine translation"]]}
{"text": "Following previous works , we use the sequence - level knowledge distillation ( Gu et al , 2018 ) during training .", "entities": [[10, 12, "MethodName", "knowledge distillation"]]}
{"text": "In this experiment , we use the BERT model in German language .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "( Guo et al , 2019 ) , NAG - REG ( Wang et al , 2019b ) , NAG - CRF and BNAG - CRF .", "entities": [[21, 22, "MethodName", "CRF"], [25, 26, "MethodName", "CRF"]]}
{"text": "In addition , we compare our model with several strong autoregressive models , including LSTM - based ( Wu et al , 2016 ) , CNN - based ( Gehring et al , 2017 ) and transformer model .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "By removing BERT from the model , we observe notable drop across all metrics .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "This shows that the knowledge of BERT is an important factor of the model 's strong performance .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Comparing with results in Table 1 , it still outperforms vanilla NAG - CRF and performs comparably with NAG - CRF using LPD decoding , which demonstrates the merit of the proposed dynamic length decoding mechanism .", "entities": [[13, 14, "MethodName", "CRF"], [20, 21, "MethodName", "CRF"]]}
{"text": "Another interesting finding is that , by only removing the CRF layer , the most notable drop is observed on the bigram - level metric .", "entities": [[10, 11, "MethodName", "CRF"]]}
{"text": "This shows that the bigram - level dependencies on the output side are mainly captured by the CRF module .", "entities": [[17, 18, "MethodName", "CRF"]]}
{"text": "In addition , by removing both BERT and CRF , all metrics further decrease .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "CRF"]]}
{"text": "R - L denotes the model 's ROUGE - L score .", "entities": [[7, 10, "MetricName", "ROUGE - L"]]}
{"text": "The ROUGE - L results also indicate that the reduction in token repetition can effectively improve the model generation quality .", "entities": [[1, 4, "MetricName", "ROUGE - L"]]}
{"text": "To provide a quantitative analysis , we perform inference on the Gigawords dataset using ratio - first with different \u03b1 .", "entities": [[19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "Therefore , once we have a prior knowledge on the data statistic , we can easily choose a proper \u03b1 that both improves the inference speed whilst maintaining the generation quality .", "entities": [[19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "By setting different \u03b1 , ratio - first provides us an explicit way to control the balance between the inference speed and the generation quality .", "entities": [[3, 4, "HyperparameterName", "\u03b1"]]}
{"text": "In this work , we explored the potential of BERT in various text generation tasks under the NAG framework .", "entities": [[9, 10, "MethodName", "BERT"], [12, 14, "TaskName", "text generation"]]}
{"text": "To reduce errors stemming from the assumption of conditional independence of output tokens , we proposed a context - aware objective as well as using a CRF decoding .", "entities": [[26, 27, "MethodName", "CRF"]]}
{"text": "The collected human - model interaction dataset and system code are available at https://github . com / vipulraheja / IteraTeR.", "entities": [[5, 7, "DatasetName", "interaction dataset"]]}
{"text": "We release our code , revision interface , and collected human - model interaction dataset to promote future research on collaborative text revision .", "entities": [[13, 15, "DatasetName", "interaction dataset"]]}
{"text": "Note that the users were only asked to accept or reject edits , and they had control neither over the number of iterations , nor over the stopping criteria .", "entities": [[20, 23, "HyperparameterName", "number of iterations"]]}
{"text": "We followed the prior work ( Du et al , 2022 ) to collect the text revision data across three domains : ArXiv , Wikipedia and Wikinews .", "entities": [[22, 23, "DatasetName", "ArXiv"]]}
{"text": "For the evaluation of text revisions made by our system ( SYSTEM - HUMAN ) , we only presented the original source document at the initial revision depth ( D 0 ) to our system , and let the system generate edits in the following revision depths , while incorporating the accept / reject decisions on modelgenerated edit suggestions by the users .", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "The output of the model is the revised sentence , and we trained the model with cross - entropy loss .", "entities": [[19, 20, "MetricName", "loss"]]}
{"text": "Table 7 shows an example of iterative text revision in ArXiv domain generated by R3 .", "entities": [[10, 11, "DatasetName", "ArXiv"]]}
{"text": "The quality score ranges between 0 and 1 .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "We evaluated 10 unique documents in ArXiv domain , and took the average score from all 3 annotators .", "entities": [[6, 7, "DatasetName", "ArXiv"]]}
{"text": "Empirical results show that R3 can generate iterative text revisions with acceptance rates comparable or even better than human writers at early revision depths . 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "Here , we develop a novel agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategiesto curb this spread .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "Here , we develop a novel agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategiesto curb this spread .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "Here , we develop a novel detailed agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategiesto curb this spread .", "entities": [[7, 8, "DatasetName", "agent"]]}
{"text": "Here , we develop a novel agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategies .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "Here , we develop a novel n agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategies .", "entities": [[7, 8, "DatasetName", "agent"]]}
{"text": "t HUMAN - HUMAN SYSTEM - HUMAN ( ours ) 0", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "Semantic Modelling of Adjective - Noun Collocations Using FrameNet", "entities": [[8, 9, "DatasetName", "FrameNet"]]}
{"text": "The FrameNet modelling is based on the online resource available at http://framenet.icsi.berkeley.edu .", "entities": [[1, 2, "DatasetName", "FrameNet"]]}
{"text": "Section 3 presents our own proposal of how to deal with semantics of collocations ; we argue that the notion of a semantic frame in the sense of FrameNet ( Ruppenhofer et al , 2016 ) provides a suitably general semantic framework that is applicable to a wide range of semantic fields .", "entities": [[28, 29, "DatasetName", "FrameNet"]]}
{"text": "Furthermore , we argue that collocations offer an interesting empirical domain for validating the structure of semantic frames and for further developing the FrameNet framework itself .", "entities": [[23, 24, "DatasetName", "FrameNet"]]}
{"text": "LFs have been widely used in lexicographic projects on describing French semantic derivations and collocations ( Polguere , 2000 ) , and have also been implemented in the Spanish online dictionary of collocations ( DiCE ) that focuses on describing emotion lexemes ( Vincze et al , 2011 ) .", "entities": [[40, 41, "DatasetName", "emotion"]]}
{"text": "Semantic Frames for English are described in the lexical database FrameNet ( FrameNet - Database ) in terms of Frame Elements ( FEs ) ( Ruppenhofer et al , 2016 ) .", "entities": [[10, 11, "DatasetName", "FrameNet"], [12, 13, "DatasetName", "FrameNet"]]}
{"text": "The further advantage of FrameNet is that it can be adapted for other languages .", "entities": [[4, 5, "DatasetName", "FrameNet"]]}
{"text": "For the reasons outlined above , we will use Frame El - ements in the sense of FrameNet for the semantic modelling of adjective - noun collocations .", "entities": [[17, 18, "DatasetName", "FrameNet"]]}
{"text": "To assess the applicability of FrameNet for modelling of collocations , we have investigated eleven frames for nouns from various semantic fields ( see Table 1 ) .", "entities": [[5, 6, "DatasetName", "FrameNet"]]}
{"text": "As explained in the previous section , we employ English FrameNet for German collocations .", "entities": [[10, 11, "DatasetName", "FrameNet"]]}
{"text": "Semantic Frames in FrameNet describe non - linguistic concepts and deal with meanings rather than with particular lexical units in a language .", "entities": [[3, 4, "DatasetName", "FrameNet"]]}
{"text": "Thus , a correct translation of the target German word into English makes it possible to apply the information contained in the English FrameNet to German data .", "entities": [[23, 24, "DatasetName", "FrameNet"]]}
{"text": "In FrameNet the lexical unit ( LU ) ' chocolate ' evokes the frame \" Food \" with Frame Elements ( FEs ) FOOD , CONSTITUENT PARTS , DESCRIPTOR , and TYPE .", "entities": [[1, 2, "DatasetName", "FrameNet"]]}
{"text": "FrameNet offers a suitable FE TYPE for describing the relation that holds between these adjectives and the noun .", "entities": [[0, 1, "DatasetName", "FrameNet"]]}
{"text": "It is defined in FrameNet as follows : \" This FE identifies a particular Type of the food item \" ( FrameNet - Database ) .", "entities": [[4, 5, "DatasetName", "FrameNet"], [21, 22, "DatasetName", "FrameNet"]]}
{"text": "For instance , according to the FrameNet Database ( FrameNet - Database ) , the frame \" Personal relationship \" evoked by the noun Freund ' friend ' has the following non - core FEs :", "entities": [[6, 7, "DatasetName", "FrameNet"], [9, 10, "DatasetName", "FrameNet"]]}
{"text": "At first glance , the modifier characterizes MAN - NER ; however , in German , the expression fester Freund means ' boyfriend ' that actually refers to the nature of the relationship between the partners .", "entities": [[9, 10, "TaskName", "NER"]]}
{"text": "In FrameNet it evokes the frame \" Emotion directed \" .", "entities": [[1, 2, "DatasetName", "FrameNet"]]}
{"text": "A similar pattern is found for the emotion noun Angst ' fear ' .", "entities": [[7, 8, "DatasetName", "emotion"]]}
{"text": "Consider its collocates : gro\u00df ' strong ' , nackt ' pure ' , h\u00f6llisch ' hellish ' , panisch ' panic ' , pur ' pure ' , unterschwellig ' subconscious ' , blank ' sheer ' , diffus ' vague ' , tief ' deep ' , dumpf ' vague ' , existenziell ' existential ' , krankhaft ' pathological ' The identified relevant FEs are as follows ( FrameNet - Database ) : Degree : The extent to which the Experiencer 's emotion deviates from the norm for the emotion .", "entities": [[71, 72, "DatasetName", "FrameNet"], [85, 86, "DatasetName", "emotion"], [92, 93, "DatasetName", "emotion"]]}
{"text": "The other adjectives do not reveal any information about the intensity of the experienced emotion : blank ' sheer ' , pur ' pure ' , and nackt ' pure ' rather imply that , at a particular moment , fear is the only emotion guiding the behaviour of a person .", "entities": [[14, 15, "DatasetName", "emotion"], [44, 45, "DatasetName", "emotion"]]}
{"text": "All the above described cases demonstrate that semantic roles present in abstract collocations are quite diverse , and the relations can well be generalized using FrameNet 's inventory of frame elements .", "entities": [[25, 26, "DatasetName", "FrameNet"]]}
{"text": "However , two adjectives from this list stand out in their meaning : symbolisch ' symbolic ' and unmenschlich ' inhumane ' , they carry an extra meaning describing a kind of penalty , which is reflected in the FE INSTRUMENT ( \" The Instrument with which the reward or punishment is carried out \" ( FrameNet - Database ) ) .", "entities": [[56, 57, "DatasetName", "FrameNet"]]}
{"text": "Consider the noun ' price ' : it is defined in FrameNet as \" the amount of money expected , required , or given in payment for something \" ( FrameNet - Database ) .", "entities": [[11, 12, "DatasetName", "FrameNet"], [30, 31, "DatasetName", "FrameNet"]]}
{"text": "The most suitable FE in this case is RATE that according to FrameNet describes price or payment per unit of Goods and is therefore the closest to the concept of a scale in this frame .", "entities": [[12, 13, "DatasetName", "FrameNet"]]}
{"text": "FrameNet provides frame semantic information about many lexical units ; however , it is still under development and there are cases , when the frame evoked by a noun does not reflect all the aspects of its meaning .", "entities": [[0, 1, "DatasetName", "FrameNet"]]}
{"text": "More than one thousand frames are described in FrameNet , thus providing a rich coverage of the lexicon .", "entities": [[8, 9, "DatasetName", "FrameNet"]]}
{"text": "The frame evoked by ' future ' in FrameNet is \" Alternatives \" with the following FEs ( FrameNet - Database ) : Agent : An individual involved in the Event .", "entities": [[8, 9, "DatasetName", "FrameNet"], [18, 19, "DatasetName", "FrameNet"], [23, 24, "DatasetName", "Agent"]]}
{"text": "Purpose : The state - of - affairs that the Agent hopes to bring about which is associated with some of the possible Events but not others .", "entities": [[10, 11, "DatasetName", "Agent"]]}
{"text": "The most appropriate FEs appear to be DESCRIPTOR which in FrameNet refers to descriptive characteristics and properties , and TIME .", "entities": [[10, 11, "DatasetName", "FrameNet"]]}
{"text": "We are grateful to the anonymous reviewer for raising an interesting question concerning the applicability of FrameNet 's semantic relations to adjective - noun free phrases as well .", "entities": [[16, 17, "DatasetName", "FrameNet"]]}
{"text": "We will use this dataset to examine FrameNet 's coverage of lexical units from different semantic fields .", "entities": [[7, 8, "DatasetName", "FrameNet"]]}
{"text": "A second important objective of our future research will be to address the question of reliability of annotations for the semantics of collocations on the basis of FrameNet .", "entities": [[27, 28, "DatasetName", "FrameNet"]]}
{"text": "As mentioned in Section 2 , one of the advantages of FrameNet is that it can be adapted for other languages .", "entities": [[11, 12, "DatasetName", "FrameNet"]]}
{"text": "Therefore , it is worthwhile to conduct a comparative study on semantic annotation of collocations based on FrameNet for languages other than German .", "entities": [[17, 18, "DatasetName", "FrameNet"]]}
{"text": "One noteworthy difference that is apparent already at this point is that FrameNet 's semantic relations can also be applied to describe free phrases , whereas the application of LFs is limited to lexically restricted combinations ( Mel'\u010duk , 1995 ; Mel'\u010duk , 2015 ) .", "entities": [[12, 13, "DatasetName", "FrameNet"]]}
{"text": "As social distancing , self - quarantines , and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech .", "entities": [[26, 28, "DatasetName", "hate speech"]]}
{"text": "We propose a novel use of learned feature importance which improves upon the performance of prior state - of - the - art text classification techniques , while producing more easily interpretable decisions .", "entities": [[7, 9, "TaskName", "feature importance"], [23, 25, "TaskName", "text classification"]]}
{"text": "With billions of individuals active on social media , the task of finding reviewing and classifying hate speech online quickly grows to a scale not achievable without the use of machine learning .", "entities": [[16, 18, "DatasetName", "hate speech"]]}
{"text": "Additionally , the definition of hate - speech can be broad and include many nuances , but in general hate speech is defined as communication which disparages or incites violence towards an individual or group based on that person or groups ' cultural / ethnic background , gender or sexual orientation .", "entities": [[19, 21, "DatasetName", "hate speech"]]}
{"text": "In the context of Covid - 19 , the United Nations has released guidelines on Covid - 19 related hatespeech Guidance on COVID - 19 related Hate Speech cautioning that Member States and Social Media companies that with the rise of Covid - 19 cases there has also been an increase of hate speech .", "entities": [[10, 11, "DatasetName", "Nations"], [26, 28, "DatasetName", "Hate Speech"], [52, 54, "DatasetName", "hate speech"]]}
{"text": "The tweets above displays an example of hate speech used for scapegoating by @realDonaldTrump and the response of @ajRAFAEL highlighting the impact this hate speech has on Asian Americans .", "entities": [[7, 9, "DatasetName", "hate speech"], [23, 25, "DatasetName", "hate speech"]]}
{"text": "The importance of identifying hate speech combined with the magnitude of the data makes this an area in which innovations achieved in NLP and AI research can make an impact .", "entities": [[4, 6, "DatasetName", "hate speech"]]}
{"text": "In the context of detecting hate speech detection , this can lead to predictions be largely the outcome of a few key terms ( Davidson et al , 2017 ) .", "entities": [[5, 8, "TaskName", "hate speech detection"]]}
{"text": "Our Contribution : In this research , we merge feature importance with text classification to help decrease false positives .", "entities": [[9, 11, "TaskName", "feature importance"], [12, 14, "TaskName", "text classification"]]}
{"text": "Our method combines the global representation of a term 's feature importance to a predicted class with the local term feature importance of an individual observation .", "entities": [[10, 12, "TaskName", "feature importance"], [20, 22, "TaskName", "feature importance"]]}
{"text": "Each term 's Figure 2 : This is an example of a Covid - 19 Tweet incorrectly classified by our baseline model as \" hate speech towards immigrants \" .", "entities": [[24, 26, "DatasetName", "hate speech"]]}
{"text": "After the applying our prediction enhancement method , the tweet was correctly classified as \" not hate speech \" .", "entities": [[16, 18, "DatasetName", "hate speech"]]}
{"text": "global feature importance is collected from our training dataset and baseline model .", "entities": [[1, 3, "TaskName", "feature importance"]]}
{"text": "Then local feature importance is calculated for each observation on which our trained model makes a prediction .", "entities": [[2, 4, "TaskName", "feature importance"]]}
{"text": "Our algorithm , uses the term level global feature importance to penalize model predictions when an observation 's local term feature importance differs from the global feature importance .", "entities": [[8, 10, "TaskName", "feature importance"], [20, 22, "TaskName", "feature importance"], [26, 28, "TaskName", "feature importance"]]}
{"text": "In the same vein of our research , others have leveraged explainability derived with integrated gradients ( Sundararajan et al , 2017 ) and subject matter experts to create priors for use in text classification .", "entities": [[33, 35, "TaskName", "text classification"]]}
{"text": "One of the more commonly utilized explainability methods , SHAP provides a framework within the feature contrubutions to a a model 's output can be derived by borrowing Aultman - Shapely values from cooperative game theory ( Lundberg and Lee , 2017 ) .", "entities": [[9, 10, "MethodName", "SHAP"]]}
{"text": "While there are several \" explainer \" implementations included with SHAP , Gradint Explainer allowed us to leverage our entire training dataset as the background dataset which allows our global average term values described in our experimental design to represent all terms in the training corpus .", "entities": [[10, 11, "MethodName", "SHAP"]]}
{"text": "SHAP 's Gradient Explainer builds off of integrated gradients and leverages what are called expected gradients .", "entities": [[0, 1, "MethodName", "SHAP"]]}
{"text": "This lead us to combining three datasets in the domain of hate and offensive speech : the collection of racist and sexist tweets presented by Waseem and Hovy ( Waseem and Hovy , 2016 ) , the Offensive Language Identification Dataset ( OLID )", "entities": [[38, 40, "TaskName", "Language Identification"], [42, 43, "DatasetName", "OLID"]]}
{"text": "( Zampieri et al , 2019 ) , and Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter ( HatEval ) ( Basile et al , 2019 ) .", "entities": [[12, 14, "DatasetName", "Hate Speech"]]}
{"text": "First , we leveraged data collected by the Texas Advanced Computing Center ( TAAC ) at the University of Texas at Austin .", "entities": [[8, 9, "DatasetName", "Texas"], [19, 20, "DatasetName", "Texas"]]}
{"text": "In the context of hate speech in Covid - 19 , terms which target countries or ethnicity 's in the labeling of the virus clearly disregard the aforementioned UN guidance on hate speech .", "entities": [[4, 6, "DatasetName", "hate speech"], [31, 33, "DatasetName", "hate speech"]]}
{"text": "Since our research focus is to leverage learned feature importance to enhance predictions , we followed proven methods for hate speech classification ( Gamb\u00e4ck and Sikdar , 2017 ) .", "entities": [[8, 10, "TaskName", "feature importance"], [19, 21, "DatasetName", "hate speech"]]}
{"text": "These embeddings were passed to a Convolution Neural Network classifier which mirrored the same architecture used by Yoon Kim for sentence classification ( Kim , 2014 ) .", "entities": [[6, 7, "MethodName", "Convolution"], [20, 22, "TaskName", "sentence classification"]]}
{"text": "We allowed for parameter tuning with random search , and the final CNN consisted of three convolutional layers of 75 filters with kernel sizes of 3 , 4 and 6 .", "entities": [[6, 8, "MethodName", "random search"]]}
{"text": "These all received one dimensional max pooling and a dropout rate of 0.4 was applied .", "entities": [[5, 7, "MethodName", "max pooling"]]}
{"text": "These parameters were selected by ranking validation AUC .", "entities": [[7, 8, "MetricName", "AUC"]]}
{"text": "To achieve this , we apply SHAP 's Gradient Explainer to our baseline model .", "entities": [[6, 7, "MethodName", "SHAP"]]}
{"text": "Since our method requires positive inputs to measure the percentage difference , these values for every s step across all sequences in the training dataset are scaled between 0 and 1 via min / max scaling .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "We then create a dictionary of terms from the training corpus and store the \" global average \" feature importance of each term .", "entities": [[18, 20, "TaskName", "feature importance"]]}
{"text": "This dictionary of global average feature importance values is used to calculate how far a particular prediction strays from the feature importance represented in our training data .", "entities": [[5, 7, "TaskName", "feature importance"], [20, 22, "TaskName", "feature importance"]]}
{"text": "Difference Multiplier Now that we have the global importance of each term ( feature token ) to each class , we calculate the percentage difference of each term 's local feature importance to that term 's global feature importance by each class .", "entities": [[30, 32, "TaskName", "feature importance"], [37, 39, "TaskName", "feature importance"]]}
{"text": "As you can see above the percentage difference between each local ( s l ) and global ( s g ) feature importance is subtracted from 1 .", "entities": [[21, 23, "TaskName", "feature importance"]]}
{"text": "We hypothesize this is due to the diversity of language and relatively neutral feature importance most terms have on the negative class .", "entities": [[13, 15, "TaskName", "feature importance"]]}
{"text": "The relative neutrality in both global and local feature importance scores can be seen in figure 2 , and this results in a higher overall average for the aggregation of the Term Difference Multiplier .", "entities": [[8, 10, "TaskName", "feature importance"]]}
{"text": "Here we have experimented with a novel method to leverage the global feature importance from a model 's training dataset to reinforce or even penalize new predictions when their local feature importance varies from this learned global value .", "entities": [[12, 14, "TaskName", "feature importance"], [30, 32, "TaskName", "feature importance"]]}
{"text": "We intend to explore further scenarios such as altering the equation used in our Term Difference Multiplier and the datasets used since the global feature importance can greatly influence the multiplier combined with our model 's original predicted probabilities .", "entities": [[24, 26, "TaskName", "feature importance"]]}
{"text": "Bi - Directional Recurrent Neural Ordinary Differential Equations for Social Media Text Classification", "entities": [[11, 13, "TaskName", "Text Classification"]]}
{"text": "Classification of posts in social media such as Twitter is difficult due to the noisy and short nature of texts .", "entities": [[0, 1, "TaskName", "Classification"]]}
{"text": "Our experiments demonstrate that RNODE and Bi - RNODE are effective for the problem of stance classification of rumours in social media .", "entities": [[15, 17, "TaskName", "stance classification"]]}
{"text": "Information disseminated in social media such as Twitter can be useful for addressing several realworld problems like rumour detection , disaster management , and opinion mining .", "entities": [[17, 19, "TaskName", "rumour detection"], [24, 26, "TaskName", "opinion mining"]]}
{"text": "In addition , the character limit ( 240 characters ) imposed by social media such as Twitter make it even harder to perform text classification .", "entities": [[23, 25, "TaskName", "text classification"]]}
{"text": "Social media text classification , such as rumour stance classification 1 ( Qazvinian et al , 1 Rumour stance classification helps to identify the veracity 2011 ;", "entities": [[2, 4, "TaskName", "text classification"], [8, 10, "TaskName", "stance classification"], [18, 20, "TaskName", "stance classification"]]}
{"text": "Zubiaga et al , 2016 ; Lukasik et al , 2019 ) can be addressed effectively using sequence labelling models such as long short term memory ( LSTM ) networks ( Zubiaga et al , 2016 ; Augenstein et al , 2016 ; Kochkina et al , 2017 ; Zubiaga et al , 2018b , a ; Dey et al , 2018 ; Liu et al , 2019 ; Tian et al , 2020 ) .", "entities": [[27, 28, "MethodName", "LSTM"]]}
{"text": "NODE ( Chen et al , 2018 ) is a continuous depth deep learning model that performs transformation of feature vectors in a continuous manner using ordinary differential equation solvers .", "entities": [[0, 1, "MethodName", "NODE"]]}
{"text": "NODEs bring parameter efficiency and address model selection in deep learning to a great extent .", "entities": [[6, 8, "TaskName", "model selection"]]}
{"text": "RNODE generalizes RNN by extending NODE for time - series data by considering temporal information associated with the sequential data .", "entities": [[5, 6, "MethodName", "NODE"]]}
{"text": "We show the effectiveness of the proposed models on the rumour stance classification problem in Twitter using the RumourEval - 2019 ( Derczynski et al , 2019 ) dataset .", "entities": [[11, 13, "TaskName", "stance classification"], [22, 23, "DatasetName", "Derczynski"]]}
{"text": "We found RNODE and Bi - RNODE can improve the social media text classification by effectively making use of the temporal information and is better than LSTMs and gated recurrent units ( GRU ) with temporal features .", "entities": [[12, 14, "TaskName", "text classification"], [32, 33, "MethodName", "GRU"]]}
{"text": "In particular , we consider the rumour stance classification problem in Twitter where one classifies tweets into Support , Query , Deny , and Comment class , thus y", "entities": [[7, 9, "TaskName", "stance classification"]]}
{"text": "NODE were introduced as a continuous depth alternative to Residual Networks ( ResNets )", "entities": [[0, 1, "MethodName", "NODE"]]}
{"text": "Residual block output is computed as h", "entities": [[0, 2, "MethodName", "Residual block"]]}
{"text": "t+1 = h t + f ( h t , \u03b8 t ) , where f ( ) is a neural network ( NN ) parameterized by \u03b8 t and h t representing the hidden representation at depth t.", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [27, 28, "HyperparameterName", "\u03b8"]]}
{"text": "f ( h ( t ) , t , \u03b8 ) .", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "The sequence of residual block operations in ResNets can be seen as a solution to this ODE .", "entities": [[3, 5, "MethodName", "residual block"]]}
{"text": "Using standard cross - entropy loss , the parameters of the models are learnt through backpropagation .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "For Bi - RNODE , an extra neural network f \u03b8 \u2032 ( ) is required to compute hidden representations h b ( t \u2032 i ) in the backward direction .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "Training in Bi - RNODE is done in a similar manner to RNODE , with cross - entropy loss and back - propagation to estimate parameters .", "entities": [[18, 19, "MetricName", "loss"]]}
{"text": "To demonstrate the effectiveness of the proposed approaches , we consider the stance classification problem in Twitter and RumourEval - 2019 ( Derczynski et al , 2019 data set .", "entities": [[12, 14, "TaskName", "stance classification"], [22, 23, "DatasetName", "Derczynski"]]}
{"text": "Features : For dataset preparation , each data point x i associated with a Tweet includes text embedding , retweet count , favourites count , punctuation features , negative and positive word count , presence of hashtags , user mentions , URLs etc . obtained from the tweet .", "entities": [[19, 20, "DatasetName", "retweet"]]}
{"text": "The text embedding of the tweet is obtained by concatenating the word embeddings 2 .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "Each tweet timestamp is converted to epoch time and Min - Max normalization is applied over the time stamps associated with each event to keep the duration of the event in the interval [ 0 , 1 ] .", "entities": [[34, 35, "DatasetName", "0"]]}
{"text": "Baselines : We compared results of our proposed RNODE and Bi - RNODE models with RNN based baselines such LSTM ( Kochkina et al , 2017 ) , Bi - LSTM ( Augenstein et al , 2016 ) , GRU ( Cho et al , 2014 ) , Bi - GRU , and Majority ( labelling with most frequent class ) baseline models .", "entities": [[19, 20, "MethodName", "LSTM"], [30, 31, "MethodName", "LSTM"], [39, 40, "MethodName", "GRU"], [50, 51, "MethodName", "GRU"]]}
{"text": "We also use a variant of LSTM baseline considering temporal information ( Zubiaga et al , 2018b ) ,", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "LSTM - timeGap where the time gap of consecutive data points is included as part of the input data .", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "Evaluation Metrics : We consider the standard evaluation metrics such as precision , recall , F1 and in addition the AUC score to account for the data imbalance .", "entities": [[15, 16, "MetricName", "F1"], [20, 21, "MetricName", "AUC"]]}
{"text": "Different hyperparameters like neural network layers ( 1 , 2 ) , numerical methods ( Euler , RK4 , Dopri5 for RNODE and Bi - RNODE ) and aggregation strategy ( concatenation or averaging for Bi - LSTM Bi - GRU and Bi - RNODE ) are used for all the models and the best configuration is selected from the validation data for different experimental setups and train / test data splits .", "entities": [[37, 38, "MethodName", "LSTM"], [40, 41, "MethodName", "GRU"]]}
{"text": "Through experiments , we show these models perform better than LSTMs on rumour stance classification problem in Twitter", "entities": [[13, 15, "TaskName", "stance classification"]]}
{"text": "AMR Parsing via Graph Sequence Iterative Inference *", "entities": [[0, 2, "TaskName", "AMR Parsing"]]}
{"text": "We propose a new end - to - end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph .", "entities": [[12, 14, "TaskName", "AMR parsing"]]}
{"text": "We design a model based on iterative inference that helps achieve better answers in both perspectives , leading to greatly improved parsing accuracy .", "entities": [[22, 23, "MetricName", "accuracy"]]}
{"text": "Remarkably , without the help of any large - scale pre - trained language model ( e.g. , BERT ) , our model already surpasses previous state - of - the - art using BERT .", "entities": [[18, 19, "MethodName", "BERT"], [34, 35, "MethodName", "BERT"]]}
{"text": "With the help of BERT , we can push the state - of - the - art results to 80.2 % on LDC2017T10 ( AMR 2.0 ) and 75.4 % on LDC2014T12 ( AMR 1.0 ) .", "entities": [[4, 5, "MethodName", "BERT"], [22, 23, "DatasetName", "LDC2017T10"]]}
{"text": "AMR parsing is the task of transforming natural language text into AMR .", "entities": [[0, 2, "TaskName", "AMR parsing"]]}
{"text": "One biggest challenge of AMR parsing is the lack of explicit alignments between nodes ( concepts ) in the graph and words in the text .", "entities": [[4, 6, "TaskName", "AMR parsing"]]}
{"text": "The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Code : 14204418 ) and the Direct Grant of the Faculty of Engineering , CUHK ( Project Code : 4055093 ) .", "entities": [[41, 42, "DatasetName", "CUHK"]]}
{"text": "We introduce a new approach tackling AMR parsing , following the incremental sequence - tograph transduction paradigm .", "entities": [[6, 8, "TaskName", "AMR parsing"]]}
{"text": "Equivalently , we treat AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph .", "entities": [[4, 6, "TaskName", "AMR parsing"]]}
{"text": "On a coarse - grained level , we can categorize existing AMR parsing approaches into two main classes : Two - stage parsing ( Flanigan et al , 2014 ;", "entities": [[11, 13, "TaskName", "AMR parsing"]]}
{"text": "Seq2seq - based parsing ( Barzdins and Gosko , 2016 ; Konstas et al , 2017 ; van Noord and Bos , 2017 ; Peng et al , 2018 ) views parsing as sequence - to - sequence transduction by some linearization of the AMR graph .", "entities": [[0, 1, "MethodName", "Seq2seq"]]}
{"text": "Pust et al ( 2015 ) regard the task as a machine translation problem , while Artzi et al ( 2015 ) adapt combinatory categorical grammar .", "entities": [[11, 13, "TaskName", "machine translation"]]}
{"text": "We call it expansion because the new node , as an abstract concept of some specific text fragments in the input sentence , is derived to complete some missing elements in the current semantic graph .", "entities": [[28, 30, "TaskName", "missing elements"]]}
{"text": "attention x t y t+ 1 \u2026 initial state x 0 f ( G i , x 0 ) f", "entities": [[10, 11, "DatasetName", "0"], [17, 18, "DatasetName", "0"]]}
{"text": "G i W graph memory Figure 2 : Overview of the dual graph - sequence iterative inference for AMR parsing .", "entities": [[18, 20, "TaskName", "AMR parsing"]]}
{"text": "The inference starts with an initial concept decision x 0 and follows the inference chain", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "x 0 f ( G i , x 0 )", "entities": [[1, 2, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "We believe the mutual causalities , as described above , are useful for action disambiguation and harmonious decision making , which eventually result in more accurate parses .", "entities": [[17, 19, "TaskName", "decision making"]]}
{"text": "We formulate AMR parsing as a series of dual graph - sequence decisions and design an iterative inference approach to tackle each of them .", "entities": [[2, 4, "TaskName", "AMR parsing"]]}
{"text": "Formally , the parsing model consists of a series of graph expansion procedures { G 0 . . .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "} , starting from an empty graph G 0 .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "The sequence encoder follows the multi - layer Transformer architecture described in Vaswani et", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "At the bottom layer , each token is firstly transformed into the concatenation of features learned by a character - level convolutional neural network ( charCNN , Kim et al , 2016 ) and randomly initialized embeddings for its lemma , part - of - speech tag , and named entity tag .", "entities": [[39, 40, "DatasetName", "lemma"], [41, 44, "DatasetName", "part - of"]]}
{"text": "Additionally , we also include features learned by pre - trained language model BERT ( Devlin et al , 2019 ) .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "For clarity , we omit the detailed transformations ( Vaswani et al , 2017 ) and denote the final output from our sequence encoder as { h 0 , h 1 , . . .", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": ", h n } R d , where h 0 corresponds the special token BOS and serves as an overall rep - resentation while others are considered as contextualized word representations .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "We employ multi - layer Transformer architecture with masked self - attention and source - attention , which only allows each position in the node sequence to attend to all positions up to and including that position , and every position in the node sequence to attend over all positions in the input sequence .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "Then , a multi - layer Transformer encoder with masked self - attention and sourceattention is applied , resulting in vector representations { s 0 , s 1 , . . .", "entities": [[6, 7, "MethodName", "Transformer"], [24, 25, "DatasetName", "0"]]}
{"text": ", s m } R d , where s 0 represents the special concept BOG and serves as a dummy node while others are considered as contextualized node representations .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "We employ the scaled dot - product attention proposed in Vaswani et", "entities": [[3, 8, "MethodName", "scaled dot - product attention"]]}
{"text": "Concretely , we first calculate an attention distribution over all input tokens : \u03b1 t = softmax ( ( W Q y t )", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [16, 17, "MethodName", "softmax"]]}
{"text": "The attention weights \u03b1 t R n provide a soft alignment between the new concept and the tokens in the input sequence .", "entities": [[3, 4, "HyperparameterName", "\u03b1"]]}
{"text": "First , \u03b1 t is fed through an MLP and softmax to obtain a probability distribution over a pre - defined vocabulary : MLP ( \u03b1 t )", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [8, 9, "DatasetName", "MLP"], [10, 11, "MethodName", "softmax"], [23, 24, "DatasetName", "MLP"], [25, 26, "HyperparameterName", "\u03b1"]]}
{"text": "+ b ( vocab ) ) , where W V R d\u00d7d denotes the learnable linear projection that transforms the text memories into the value subspace , and the value vectors are averaged according to \u03b1 t for concept label prediction .", "entities": [[35, 36, "HyperparameterName", "\u03b1"]]}
{"text": "Third , to address the attribute values such as person names or numerical strings , we also use \u03b1 t for another copy mechanism that directly copies the original strings of input tokens .", "entities": [[18, 19, "HyperparameterName", "\u03b1"]]}
{"text": "The above three channels are combined via a soft switch to control the production of the concept label from different sources : [ p 0 , p 1 , p 2 ] = softmax ( W ( switch )", "entities": [[24, 25, "DatasetName", "0"], [33, 34, "MethodName", "softmax"]]}
{"text": "At each graph reasoning step t , the relation solver receives a state vector x t that carries the latest concept decision and the output graph memories s 0 , s 1 , . . .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "First , a relation identification module points to some preceding nodes as source nodes ; Then , the relation classification module predicts the relation type between the new concept and predicted source nodes .", "entities": [[18, 20, "TaskName", "relation classification"]]}
{"text": "Following Cai and Lam ( 2019 ) , we use multi - head attention for a more compact parsing procedure where multiple source nodes are simultaneously determined .", "entities": [[10, 14, "MethodName", "multi - head attention"]]}
{"text": "5 Formally , our relation identification module employs H different attention heads , for each head h , we calculate an attention distribution over all existing node ( including the dummy node s 0 ) :", "entities": [[33, 34, "DatasetName", "0"]]}
{"text": "\u03b2 h t = softmax ( ( W Q h x t )", "entities": [[0, 1, "HyperparameterName", "\u03b2"], [4, 5, "MethodName", "softmax"]]}
{"text": "T W K h s 0 : m \u221a d k ) .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Then , we take the maximum over different heads as the final edge probabilities : \u03b2 t [ i ]", "entities": [[15, 16, "HyperparameterName", "\u03b2"]]}
{"text": "h=1 \u03b2 h t [ i ] .", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "Specifically , at each spanning step i , we start the iterative inference by setting x 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "= h 0 and solving f", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "( G i , x 0 ) .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "W V h s 0 : n )", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "\u03b2 h t ) , where FFN ( x ) is a feed - forward network and W V h projects graph memories into a value space for each head", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "After N steps of iterative inference , i , e. , x 0 f ( G i , x 0 )", "entities": [[12, 13, "DatasetName", "0"], [19, 20, "DatasetName", "0"]]}
{"text": "The Algorithm 1 AMR Parsing via Graph Sequence Iterative Inference Input : the input sentence W = ( w 1 , w 2 , . . .", "entities": [[3, 5, "TaskName", "AMR Parsing"]]}
{"text": "Output : the corresponding AMR graph G // compute text memories 1 : h 0 , h 1 , . . .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "// initialize graph 2 : G 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "= ( nodes= { BOG } , edges= ) // start graph expansions 3 : i = 0 4 : while True do 5 : s 0 , . . .", "entities": [[17, 18, "DatasetName", "0"], [26, 27, "DatasetName", "0"]]}
{"text": "the graph memories can be computed * incrementally * 6 : x 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "= h 0 // iterative inference 7 : for t 1 to N do 8 :", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "The optimization objective is to maximize the sum of the decomposed step - wise log - likelihood , where each is the sum of concept , edge , and edge label probabilities .", "entities": [[14, 17, "MetricName", "log - likelihood"]]}
{"text": "During testing , our model searches for the best output graph through beam search based on the log - likelihood at each spanning step .", "entities": [[17, 20, "MetricName", "log - likelihood"]]}
{"text": "al , 2014 ) for tokenization , lemmatization , part - of - speech , and named entity tagging .", "entities": [[7, 8, "TaskName", "lemmatization"], [9, 12, "DatasetName", "part - of"]]}
{"text": "Our models are trained using ADAM ( Kingma and Ba , 2014 ) for up to 60 K steps ( first 50 K with the random sibling order and last 10 K with deterministic order ) , with early stopping based on development set performance .", "entities": [[5, 6, "DatasetName", "ADAM"], [38, 40, "MethodName", "early stopping"]]}
{"text": "We fix BERT parameters similar to Zhang et al ( 2019a , b ) due to the GPU memory limit .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "In addition , only recent works ( Zhang et al , 2019a , b ; Lindemann et al , 2019 ; Naseem et al , 2019 ) have started to utilize the large - scale pretrained language model , BERT ( Devlin et al , 2019 ; Wolf et al , 2019 ) .", "entities": [[39, 40, "MethodName", "BERT"]]}
{"text": "( 2 ) How much does BERT help ?", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "The performance of AMR parsing is conventionally evaluated by SMATCH ( F1 ) metric .", "entities": [[3, 5, "TaskName", "AMR parsing"], [11, 12, "MetricName", "F1"]]}
{"text": "Note that even without BERT , our model still outperforms the previous state - of - the - art approaches using BERT ( Zhang et al , 2019b , a ) with 77.3 % .", "entities": [[4, 5, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"]]}
{"text": "This is particularly remarkable since running BERT is computationally expensive .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Besides , again , our model without BERT already surpasses previous state - of - the - art results using BERT .", "entities": [[7, 8, "MethodName", "BERT"], [20, 21, "MethodName", "BERT"]]}
{"text": "For ablated models , it can be observed that our models yield the best results in all settings if there are any competitors , indicating BERT and graph re - categorization are not the exclusive key for our superior performance .", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "it is worth noting that the NER scores are much lower when using graph re - categorization .", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "This is because the rule - based system for NER in graph recategorization does not generalize well to unseen entities , which suggest a potential improvement by adapting better NER taggers .", "entities": [[9, 10, "TaskName", "NER"], [29, 30, "TaskName", "NER"]]}
{"text": "One important point here is that the model size in terms of the number of parameters is constant regardless of the number of inference steps , making it different from general over - parameterized problems .", "entities": [[13, 16, "HyperparameterName", "number of parameters"]]}
{"text": "We illustrate the values of \u03b1 t , \u03b2 t as the iterative inference progresses .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [8, 9, "HyperparameterName", "\u03b2"]]}
{"text": "Speed We also report the parsing speed of our non - optimized code : With BERT , the parsing speed of our system is about 300 tokens / s , while without BERT , it is about 330 tokens / s on a single Nvidia P4 GPU .", "entities": [[15, 16, "MethodName", "BERT"], [32, 33, "MethodName", "BERT"]]}
{"text": "We presented the dual graph - sequence iterative inference method for AMR Parsing .", "entities": [[11, 13, "TaskName", "AMR Parsing"]]}
{"text": "Also , the idea proposed in this paper may be applied to a broad range of structured prediction tasks ( not only restricted to other semantic parsing tasks ) where the complex output space can be divided into two interdependent parts with a similar iterative inference process to achieve harmonious predictions and better performance .", "entities": [[16, 18, "TaskName", "structured prediction"], [25, 27, "TaskName", "semantic parsing"]]}
{"text": "Char - level CNNs and Transformer layers in the sentence encoder and the graph encoder share the same hyper - parameter settings .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "The BERT model ( Devlin et al , 2019 ) we used is the Huggingface 's implementation ( Wolf et al , 2019 ) ( bert - base - cased ) .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "We randomly mask ( replacing inputs with a special UNK token ) the input lemmas , POS tags , and NER tags with a rate of 0.33 .", "entities": [[20, 21, "TaskName", "NER"]]}
{"text": "The learning rate schedule is similar to that in Vaswani", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "et al ( 2017 ) , with warm - up steps being set to 2K. We use early stopping on the development set for choosing the best model .", "entities": [[17, 19, "MethodName", "early stopping"]]}
{"text": "In post - processing , we recover the original AMR format from the compact format , restore Wikipedia links using the DBpedia Spotlight API ( Daiber et al , 2013 ) , add polarity attributes based on rules observed from the training data .", "entities": [[21, 22, "DatasetName", "DBpedia"]]}
{"text": "Each constraint computes the degree to which its context matches every three - segment window in the input ( i.e. , it applies a novel feature based convolution operation to the input ) and imposes its preferred modification in proportion to the degree of match and its strength .", "entities": [[27, 28, "MethodName", "convolution"]]}
{"text": "Using Type Information to Improve Entity Coreference Resolution", "entities": [[6, 8, "TaskName", "Coreference Resolution"]]}
{"text": "Coreference resolution ( CR ) is an essential part of discourse analysis .", "entities": [[0, 2, "TaskName", "Coreference resolution"]]}
{"text": "This paper offers the first such model and evaluation , demonstrating modest gains in accuracy by introducing either gold standard or predicted types .", "entities": [[14, 15, "MetricName", "accuracy"]]}
{"text": "Coreference resolution ( CR ) is an extensively studied problem in computational linguistics and NLP ( Hobbs , 1978 ;", "entities": [[0, 2, "TaskName", "Coreference resolution"]]}
{"text": "Solutions to this problem allow us to make meaningful links between concepts and entities within a discourse and therefore serves as a valuable pre - processing step for downstream tasks like summarization and questionanswering ( Steinberger et al , 2007 ;", "entities": [[31, 32, "TaskName", "summarization"]]}
{"text": "Recently , multiple datasets including Ontonotes ( Pradhan et al , 2012 ) , Litbank ( Bamman et al , 2020 ) , EmailCoref ( Dakle et al , 2020 ) , and WikiCoref ( Ghaddar and Langlais , 2016 ) have been proposed as benchmark datasets for CR , especially in the sub - area of entity anaphora ( Sukthanker et al , 2020b ) .", "entities": [[5, 6, "DatasetName", "Ontonotes"], [14, 15, "DatasetName", "Litbank"], [33, 34, "DatasetName", "WikiCoref"]]}
{"text": "For example , OntoNotes includes 18 types while EmailCoref includes only 4 .", "entities": [[3, 4, "DatasetName", "OntoNotes"]]}
{"text": "Thus , we evaluate the performance of the proposed modeling approach on each dataset both with the set of type tags germaine to the dataset as well as a common set of four basic types ( person , org , location , facility ) inspired from research on Named Entity Recognition ( NER ) ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[48, 51, "TaskName", "Named Entity Recognition"], [52, 53, "TaskName", "NER"]]}
{"text": "Our motivation is similar to ( Durrett and Klein , 2014 ) , which used a structured CRF with handcurated features to jointly - model the tasks of CR , entity typing , and entity linking .", "entities": [[17, 18, "MethodName", "CRF"], [30, 32, "TaskName", "entity typing"], [34, 36, "TaskName", "entity linking"]]}
{"text": "However , our work differs from there 's as we show the benefits of entity - type information in neural models that use contextualized representations like BERT ( Peters et al , 2018 ) .", "entities": [[26, 27, "MethodName", "BERT"]]}
{"text": "Some prior art ( Petroni et al , 2019 ; Roberts et al , 2020 ) argues that contextual - Figure 1 : We improve Bamman et al ( 2020 ) for entity coreference resolution by incorporating type information at two levels .", "entities": [[33, 35, "TaskName", "coreference resolution"]]}
{"text": "However , in this work , we empirically show that access to explicit knowledge about entity - types benefits neural models that use BERT for CR .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "Neural Coreference Resolution :", "entities": [[1, 3, "TaskName", "Coreference Resolution"]]}
{"text": "The SOTA models show impressive performance on state - of - the - art datasets like OntoNotes ( Pradhan et al , 2012 ) and GAP ( Webster et al , 2018 ) .", "entities": [[16, 17, "DatasetName", "OntoNotes"], [25, 26, "DatasetName", "GAP"]]}
{"text": "More recently , Joshi et al ( 2019Joshi et al ( , 2020 showed that use of contextual representations instead of wordembeddings like GloVe ( Pennington et al , 2014 ) can further boost the results over and above those just mentioned .", "entities": [[23, 24, "MethodName", "GloVe"]]}
{"text": "Type Information : Named Entity Recognition datasets ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ;", "entities": [[3, 6, "TaskName", "Named Entity Recognition"]]}
{"text": "Type information as a predictive signal has been shown to be beneficial for NLP tasks like relation extraction ( Soares et al , 2019 ) and entitylinking .", "entities": [[16, 18, "TaskName", "relation extraction"]]}
{"text": "Type Information for CR : Multiple prior works have shown type - information to be a useful feature for shallow coreference resolution classifiers ( Soon et al , 2001 ; Bengtson and Roth , 2008 ; Ponzetto and Strube , 2006 ; Haghighi and Klein , 2010 ; Durrett and Klein , 2014 ) .", "entities": [[20, 22, "TaskName", "coreference resolution"]]}
{"text": "This paper expands on these studies to show that entity - type information is also beneficial for neural models that use contextualized representations like BERT ( Peters et al , 2018 ) , which have been argued to implicitly capture facts and relationships between real - world entities ( Petroni et al , 2019 ; Roberts et al , 2020 ) .", "entities": [[24, 25, "MethodName", "BERT"]]}
{"text": "The model gives stateof - the - art scores on the LitBank corpus ( Bamman et al , 2020 ) and is an end - to - end mention ranking system based on Lee et al ( 2017 ) , which has shown competitive performance on the OntoNotes dataset .", "entities": [[11, 12, "DatasetName", "LitBank"], [47, 48, "DatasetName", "OntoNotes"]]}
{"text": "However , this model differs from Lee et al ( 2017 ) as it uses BERT embeddings , omits author and genre information , and only focuses on the task of mention - linking .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "The BERT embeddings for each token i are passed through a bi - directional LSTM ( x i ) .", "entities": [[1, 2, "MethodName", "BERT"], [14, 15, "MethodName", "LSTM"]]}
{"text": "Second , to check the type consistency ( softly ) between any two mentions under consideration as possibly coreferent , we append a feature ( tc ) in Eq . 2 , which takes the value 0 if both mentions have the same type , and 1 otherwise .", "entities": [[36, 37, "DatasetName", "0"]]}
{"text": "0 . S ( m j , m k )", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "LitBank .", "entities": [[0, 1, "DatasetName", "LitBank"]]}
{"text": "2 Similar to LitBank , it considers a mention to be a span of text that refers to a real - world entity .", "entities": [[3, 4, "DatasetName", "LitBank"]]}
{"text": "Ontonotes .", "entities": [[0, 1, "DatasetName", "Ontonotes"]]}
{"text": "From this multi - lingual dataset , we evaluate on the subset ( english ) from OntoNotes that was used in the CoNLL - 2012 shared task ( Pradhan et al , 2012 ) .", "entities": [[16, 17, "DatasetName", "OntoNotes"]]}
{"text": "The dataset differs from LitBank in its annotation scheme with the biggest difference being the fact that it does not annotate singletons .", "entities": [[4, 5, "DatasetName", "LitBank"]]}
{"text": "However , unlike LitBank and EmailCoref , not all mentions have an associated entity - type .", "entities": [[3, 4, "DatasetName", "LitBank"]]}
{"text": "WikiCoref .", "entities": [[0, 1, "DatasetName", "WikiCoref"]]}
{"text": "This corpus , released by ( Ghaddar and Langlais , 2016 ) , comprises 30 documents from wikipedia annotated for coreference resolution .", "entities": [[20, 22, "TaskName", "coreference resolution"]]}
{"text": "Apart from a common list of types ( like PER , ORG , LOC ) , they also include corpus - specific categories like DIGital ( EmailCoref ) , MONey , and LANG ( OntoNotes ) .", "entities": [[34, 35, "DatasetName", "OntoNotes"]]}
{"text": "The performances are compared on the average F1 of the abovementioned metrics .", "entities": [[6, 8, "MetricName", "average F1"]]}
{"text": "For EmailCoref , OntoNotes , and WikiCoref , we report the mean score of 5 independent runs of the model with different seeds .", "entities": [[3, 4, "DatasetName", "OntoNotes"], [6, 7, "DatasetName", "WikiCoref"], [22, 23, "DatasetName", "seeds"]]}
{"text": "Whereas , for LitBank , we present the 10 - fold cross - validation results .", "entities": [[3, 4, "DatasetName", "LitBank"]]}
{"text": "This suggests that type information is helpful for CR on LitBank despite the heavily skewed distribution of entity - types in this corpus .", "entities": [[10, 11, "DatasetName", "LitBank"]]}
{"text": "apart from the common four ( PERson , ORGanization , LOCation , FACility ) that are often used in the Named Entity Recognition literature ( Tjong Kim Sang , 2002 ) .", "entities": [[20, 23, "TaskName", "Named Entity Recognition"]]}
{"text": "Models trained with common types as features perform worse than + ET ( orig ) which was expected as several original types are now clubbed into a single category ( e.g. LAW - > OTHER , LANG - > OTHER )", "entities": [[31, 32, "DatasetName", "LAW"]]}
{"text": "One surprising observation is the small difference between the performance on OntoNotes dataset , despite the fact that the number of type categories reduce from 18 + Other ( + ET ( orig ) ) to 4 + Other ( + ET ( com ) ) .", "entities": [[11, 12, "DatasetName", "OntoNotes"]]}
{"text": "This could either be because ( 1 ) the entities with corpus - specific types occur less frequently in Ontonotes , or ( 2 ) the baseline model does a good job in resolving them .", "entities": [[19, 20, "DatasetName", "Ontonotes"]]}
{"text": "To evaluate if the F1 score improvements achieved by + ET models are because of fewer type mismatch errors , we report the number of coreference clusters detected by the model that contain at least one element with a type that is different from the others in the cluster .", "entities": [[4, 6, "MetricName", "F1 score"]]}
{"text": "We introduce a baseline approach to infer the type of the mentions and then use these predictions in the + ET models , in place of the gold types , for coreference resolution .", "entities": [[31, 33, "TaskName", "coreference resolution"]]}
{"text": "The new sequence ( S = ... , c \u22122 , c \u22121 , < ENT_START > , e 1 , e 2 , ... , e n , < ENT_END > , c 1 , c 2 , ... ) is tokenized using BERT tokenizer and passed through the BERT encoder .", "entities": [[44, 45, "MethodName", "BERT"], [50, 51, "MethodName", "BERT"]]}
{"text": "This architecture is motivated from ( Soares et al , 2019 ) who show that adding markers around entities before passing the sentence through BERT performs better for relation extraction .", "entities": [[24, 25, "MethodName", "BERT"], [28, 30, "TaskName", "relation extraction"]]}
{"text": "Type Prediction : Our final evaluation of the use of types in coreference is perhaps the most important one as it uses predicted types rather than annotated types , thus demonstrating that the benefits can be achieved in practice .", "entities": [[0, 2, "TaskName", "Type Prediction"]]}
{"text": "Here we use the Type Prediction Model described just above .", "entities": [[4, 6, "TaskName", "Type Prediction"]]}
{"text": "We limit the length of the input sequence to 128 tokens and use BERT - base - cased model for our type - prediction experiments .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "Since all four datasets suffer from class - imbalance , we report both Macro F1 score as well as the accuracy for the model .", "entities": [[13, 15, "MetricName", "Macro F1"], [20, 21, "MetricName", "accuracy"]]}
{"text": "We do not consider NA as a separate class during type prediction for WikiCoref and OntoNotes .", "entities": [[10, 12, "TaskName", "type prediction"], [13, 14, "DatasetName", "WikiCoref"], [15, 16, "DatasetName", "OntoNotes"]]}
{"text": "As shown , our model performs well on Lit - Bank , EmailCoref , and Ontonotes due to their favorable size in terms of training samples for the BERT - based type predictor .", "entities": [[15, 16, "DatasetName", "Ontonotes"], [28, 29, "MethodName", "BERT"]]}
{"text": "In line with our expectation , the largest improvement due to common types is seen for OntoNotes where the prob - lem reduces from an 18 - way classification to a 5way classification .", "entities": [[16, 17, "DatasetName", "OntoNotes"]]}
{"text": "Coreference Resolution : Each mention in the corpus occurs in the test - sets of the five - fold cross - validation type - prediction experiments exactly once .", "entities": [[0, 2, "TaskName", "Coreference Resolution"]]}
{"text": "We find that the improvements from type - information persist across LitBank , EmailCoref , and OntoNotes despite the use of predicted types , but , quite expectedly , remain smaller than the improvements from the gold annotated types .", "entities": [[11, 12, "DatasetName", "LitBank"], [16, 17, "DatasetName", "OntoNotes"]]}
{"text": "Scores on WikiCoref show no significant improvement over the baseline , which could be explained by the poor performance of the type prediction model on this dataset which reduces the potency of the feature for CR .", "entities": [[2, 3, "DatasetName", "WikiCoref"], [21, 23, "TaskName", "type prediction"]]}
{"text": "Table 6 shows the most frequently occurring entity - types for each of the genres in OntoNotes .", "entities": [[16, 17, "DatasetName", "OntoNotes"]]}
{"text": "In Table 4 , we compare the performance of our type prediction model on different types of pronouns , and noun phrases of varying length .", "entities": [[10, 12, "TaskName", "type prediction"]]}
{"text": "Therefore , the type prediction model has to solely rely on the context to make that decision .", "entities": [[3, 5, "TaskName", "type prediction"]]}
{"text": "A model that is trained with masked mentions would focus more on the context for type prediction and thus could lead to better performance on PRPs ( dem . ) .", "entities": [[15, 17, "TaskName", "type prediction"]]}
{"text": "The common list of types introduced in this work would allow for the creation of a larger training - set that includes mentions from multiple corpora ( including external NER datasets ) which could provide enough signal for the model to better learn the common types for PRPs ( dem . ) .", "entities": [[29, 30, "TaskName", "NER"]]}
{"text": "For the latter , the baseline considers PricewaterhouseCoopers Calgary ( PCC ) as part of a new coreference cluster , even though it refers to the organization of the email 's sender which was previously referred to as we in the email .", "entities": [[10, 11, "DatasetName", "PCC"]]}
{"text": "+ ET - pred ( orig ) , however , is unable to cluster PCC correctly which could be due to the fact that the type - prediction model incorrectly classifies the type of we as PER rather than ORG .", "entities": [[14, 15, "DatasetName", "PCC"]]}
{"text": "This could lead to the CR model considering PCC ( ORG ) as a new entity in the discourse rather than a postcedent of we .", "entities": [[8, 9, "DatasetName", "PCC"]]}
{"text": "In this work , we show the importance of using entity - type information in neural coreference resolution ( CR ) models with contextualized embeddings like BERT .", "entities": [[16, 18, "TaskName", "coreference resolution"], [26, 27, "MethodName", "BERT"]]}
{"text": "Since , these datasets vary in number and categories of the types they define , we also experiment with mapping the original corpus types to four common types ( PER , ORG , LOC , FAC ) based on previous NER research that can be learnt more easily through large NER datasets .", "entities": [[40, 41, "TaskName", "NER"], [50, 51, "TaskName", "NER"]]}
{"text": "Dropout 0.2 ( com ) experiments .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "The most drastic difference occurs for OntoNotes ( 19 - > 5 ) and Wi - kiCoref ( 8 - > 5 ) .", "entities": [[6, 7, "DatasetName", "OntoNotes"]]}
{"text": "OTHER type in WikiCoref is for freebase links that did not have an associated type stored in freebase , whereas NA is used for mentions which do not have a freebase link .", "entities": [[3, 4, "DatasetName", "WikiCoref"]]}
{"text": "For OntoNotes , NA refers to the mentions that did not get any type assigned to them even after the use of our cluster based type - propagation approach ( explained in Section 4 ) .", "entities": [[1, 2, "DatasetName", "OntoNotes"]]}
{"text": "A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis", "entities": [[11, 13, "TaskName", "Sentiment Analysis"]]}
{"text": "Aspect - level sentiment analysis aims to distinguish the sentiment polarity of each specific aspect term in a given sentence .", "entities": [[3, 5, "TaskName", "sentiment analysis"]]}
{"text": "Therefore , we propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional GRU .", "entities": [[16, 18, "MethodName", "bidirectional GRU"]]}
{"text": "Sentiment analysis , also known as opinion mining ( Liu , 2012 ; Pang et al , 2008 ) , is a vital task in Natural Language Processing ( NLP ) .", "entities": [[0, 2, "TaskName", "Sentiment analysis"], [6, 8, "TaskName", "opinion mining"]]}
{"text": "In this paper , we address the aspect - level sentiment analysis , which is a fine - grained task in the field of sentiment analysis .", "entities": [[10, 12, "TaskName", "sentiment analysis"], [24, 26, "TaskName", "sentiment analysis"]]}
{"text": "One important challenge in aspect - level sentiment analysis is how to model the semantic relationship between aspect terms and sentences .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}
{"text": "Compared with these methods , neural network architectures are capable of learning features without feature engineering , and have been widely used in a variety of NLP tasks such as machine translation , question answering ( Andreas et al , 2016 ) and text classification ( Lai et al , 2015 ) .", "entities": [[14, 16, "TaskName", "feature engineering"], [30, 32, "TaskName", "machine translation"], [33, 35, "TaskName", "question answering"], [43, 45, "TaskName", "text classification"]]}
{"text": "Recently , with the development of the neural networks , they are also applied to target - dependent sentiment analysis 1 , such as Target - Dependent LSTM ( TD - LSTM )", "entities": [[18, 20, "TaskName", "sentiment analysis"], [27, 28, "MethodName", "LSTM"], [31, 32, "MethodName", "LSTM"]]}
{"text": "( Tang et al , 2015 ) and Target - Connection LSTM ( TC - LSTM )", "entities": [[11, 12, "MethodName", "LSTM"], [15, 16, "MethodName", "LSTM"]]}
{"text": "Attention , which is widely applied to Computer Vision ( CV ) and NLP fields , is an effective mechanism and has been demonstrated in image recognition (", "entities": [[25, 27, "TaskName", "image recognition"]]}
{"text": "Mnih et al , 2014 ) , machine translation Luong et al , 2015 ) and reading comprehension ( Hermann et al , 2015 ; Cui et al , 2016 ) .", "entities": [[7, 9, "TaskName", "machine translation"], [16, 18, "TaskName", "reading comprehension"]]}
{"text": "Therefore , some researchers have designed attention networks to address the aspect - level sentiment analysis and have obtained comparable results , such as AE - LSTM , ATAE - LSTM and IAN ( Ma et al , 2017 ) .", "entities": [[14, 16, "TaskName", "sentiment analysis"], [24, 25, "MethodName", "AE"], [26, 27, "MethodName", "LSTM"], [30, 31, "MethodName", "LSTM"], [32, 33, "MethodName", "IAN"]]}
{"text": "However , these existing work ignores or does not explicitly model the position information of the aspect term in a sentence , which has been studied for improving performance in information retrieval ( IR ) .", "entities": [[30, 32, "TaskName", "information retrieval"]]}
{"text": "By analyzing this aspect - level sentiment analysis task and the corresponding dataset , we find that when an aspect term occurs in a sentence , its neighboring words in the sentence should be given more attention than other words with long distance .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}
{"text": "Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) .", "entities": [[0, 1, "DatasetName", "Inspired"], [30, 31, "MethodName", "GRU"]]}
{"text": "The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively .", "entities": [[7, 8, "MethodName", "GRU"]]}
{"text": "The main contributions of our work can be summarized as follows : ( 1 ) We attempt to explicitly investigate the effectiveness of the position information of aspect term for aspect - level sentiment analysis .", "entities": [[33, 35, "TaskName", "sentiment analysis"]]}
{"text": "( 2 ) We propose a position - aware bidirectional attention network ( PBAN ) based on Bi - GRU , which has been proved to be effective to improve the sentiment analysis performance .", "entities": [[19, 20, "MethodName", "GRU"], [31, 33, "TaskName", "sentiment analysis"]]}
{"text": "In this section , we describe the proposed model position - aware bidirectional attention network ( PBAN ) for aspect - level sentiment analysis and PBAN is shown in Figure 1 .", "entities": [[22, 24, "TaskName", "sentiment analysis"]]}
{"text": "Suppose that if a word in the aspect term occurs in the sentence , then its position index will be marked as \" 0 \" , and the position index of other words will be represented as the relative distance to the current aspect term .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "j s 0 , j s \u2264", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Word embedding Position embedding \u2026 \u2026 N w 1 w 1 p N p Bi - GRU 1 h 2 h \u2026 N h Bi - GRU 1 t h 2 t h \u2026 M t h M t 1 t Mean Pool Attention Mechanism 1 \uf067 R h 11 \uf061 12 \uf061 1N \uf061 1 M \uf061 2 M \uf061 MN \uf061", "entities": [[16, 17, "MethodName", "GRU"], [26, 27, "MethodName", "GRU"]]}
{"text": "\uf067 M \uf067 \u2026 \u2026 \u2026 \u2026 \u2a00 \u2026 \u2026 \u2a00 dot product dot product 1 s 2 s M s Term embedding \uf0c4 Figure 1 : The architecture of position - aware bidirectional attention network for aspect - level sentiment analysis ( PBAN ) .", "entities": [[40, 42, "TaskName", "sentiment analysis"]]}
{"text": "= [ 4 , 3 , 2 , 1 , 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ] .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "Bidirectional LSTMs have been successfully applied to various NLP tasks , and it models the context dependency with the forward LSTM and the backward LSTM .", "entities": [[20, 21, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"]]}
{"text": "The forward L - STM handles the sentence from left to right , and the backward LSTM processes it in the reverse order .", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "In this paper , we choose to use bidirectional GRU since it performs similarly to bidirectional LSTM but has fewer parameters and lower computational complexity .", "entities": [[8, 10, "MethodName", "bidirectional GRU"], [15, 17, "MethodName", "bidirectional LSTM"]]}
{"text": "w N ] and an aspect term contains M words [ t 1 , t 2 , ... , t M ] , then we get sentence embedding and aspect term embedding by looking up a word embedding matrix E R d\u00d7v respectively , where d denotes the dimension of the embedding , and v indicates the vocabulary size .", "entities": [[26, 28, "TaskName", "sentence embedding"]]}
{"text": "Then we input aspect term embeddings into the left Bi - GRU to get the hidden contextual representa - tion , which consists of", "entities": [[11, 12, "MethodName", "GRU"]]}
{"text": "For the right Bi - GRU structure , we take the concatenation of the position embedding and word embedding as the inputs , then we can obtain the final hidden contextual representation of the inputs , i.e. , h", "entities": [[5, 6, "MethodName", "GRU"]]}
{"text": "We firstly get the hidden contextual representation of the aspect term by the left Bi - GRU , and get the hidden contextual representation of inputs ( i.e. , the concatenation of word embedding and position embedding ) by the right Bi - GRU structure .", "entities": [[16, 17, "MethodName", "GRU"], [43, 44, "MethodName", "GRU"]]}
{"text": "Subsequently , \u03b1 ij is used to compute a weighted sum of the hidden representation s i , producing a semantic vector that represents the input sequence .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "Since we obtain the hidden contextual representation of the inputs by the right Bi - GRU , we utilize both the position and semantic information for calculating the attention weights of different words in aspect term .", "entities": [[15, 16, "MethodName", "GRU"]]}
{"text": "\u03b3", "entities": [[0, 1, "HyperparameterName", "\u03b3"]]}
{"text": "\u03b3", "entities": [[0, 1, "HyperparameterName", "\u03b3"]]}
{"text": "h is calculated by averagely pooling all Bi - GRU hidden states .", "entities": [[9, 10, "MethodName", "GRU"]]}
{"text": "Later , the sequence representation x is obtained by using a non - linear layer : x = tanh ( W R h R + b R ) ( 9 ) where W R and b R are weight matrix and bias respectively .", "entities": [[13, 15, "MethodName", "linear layer"]]}
{"text": "We feed x into a linear layer , the length of whose output equals to the number of class labels S .", "entities": [[5, 7, "MethodName", "linear layer"]]}
{"text": "Finally , we add a softmax layer to compute the probability distribution for judging the sentiment polarities as positive , negative or neutral : y = sof tmax ( W s x + b s ) ( 10 ) where W s and b s are the weight matrix and bias respectively for softmax layer .", "entities": [[5, 6, "MethodName", "softmax"], [26, 27, "DatasetName", "sof"], [53, 54, "MethodName", "softmax"]]}
{"text": "The PBAN model can be trained in an end - to - end way in a supervised learning framework , the aim of the training is to optimize all the parameters so as to minimize the objective function ( loss function ) as much as possible .", "entities": [[39, 40, "MetricName", "loss"]]}
{"text": "We regard the cross - entropy as the loss function , and the formula is as follows :", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "loss = \u2212 S", "entities": [[0, 1, "MetricName", "loss"]]}
{"text": "In order to evaluate the performance of our model , we compare our model with several baseline models , including LSTM , AE - LSTM , ATAE - LSTM , IAN ( Ma et al , 2017 ) and MemNet ( Tang et al , 2016 LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word .", "entities": [[20, 21, "MethodName", "LSTM"], [22, 23, "MethodName", "AE"], [24, 25, "MethodName", "LSTM"], [28, 29, "MethodName", "LSTM"], [30, 31, "MethodName", "IAN"], [46, 47, "MethodName", "LSTM"], [48, 49, "MethodName", "LSTM"]]}
{"text": "Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity .", "entities": [[20, 21, "MethodName", "softmax"]]}
{"text": "AE - LSTM :", "entities": [[0, 1, "MethodName", "AE"], [2, 3, "MethodName", "LSTM"]]}
{"text": "AE - LSTM first models the words in sentence via LSTM network and concatenate the aspect embedding to the hidden contextual representation for calculating the attention weights , which are employed to produce the final representation for the input sentence to judge the sentiment polarity .", "entities": [[0, 1, "MethodName", "AE"], [2, 3, "MethodName", "LSTM"], [10, 11, "MethodName", "LSTM"]]}
{"text": "ATAE - LSTM :", "entities": [[2, 3, "MethodName", "LSTM"]]}
{"text": "ATAE - LSTM extended AE - LSTM by appending the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .", "entities": [[2, 3, "MethodName", "LSTM"], [4, 5, "MethodName", "AE"], [6, 7, "MethodName", "LSTM"]]}
{"text": "The other design of ATAE - LSTM is the same as AE - LSTM .", "entities": [[6, 7, "MethodName", "LSTM"], [11, 12, "MethodName", "AE"], [13, 14, "MethodName", "LSTM"]]}
{"text": "IAN : IAN considers the separate modeling of aspect terms and sentences respectively .", "entities": [[0, 1, "MethodName", "IAN"], [2, 3, "MethodName", "IAN"]]}
{"text": "IAN is able to interactively learn attentions in the contexts and aspect terms , and generates the representations for aspect terms and contexts separately .", "entities": [[0, 1, "MethodName", "IAN"]]}
{"text": "The output of the last attention layer is fed to a softmax layer for predictions ( Tang et al , 2016 Table 2 shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively .", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "It is obvious that LSTM method gets the worst performance , because it treats aspect term and other words as the same , so that it can not take full advantage of the aspect term information and predicts the same polarity for different aspect terms in a sentence .", "entities": [[4, 5, "MethodName", "LSTM"]]}
{"text": "Furthermore , both AE - LSTM and ATAE - LSTM perform better than LSTM model , because they all consider the importance of the aspect term , and utilize the attention mechanism .", "entities": [[3, 4, "MethodName", "AE"], [5, 6, "MethodName", "LSTM"], [9, 10, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "Specifically , ATAE - LSTM outperforms AE - LSTM since it appends the aspect embedding to each word embedding and takes them as inputs , which helps the model obtain more semantic information related to aspect term .", "entities": [[4, 5, "MethodName", "LSTM"], [6, 7, "MethodName", "AE"], [8, 9, "MethodName", "LSTM"]]}
{"text": "IAN realizes the importance of interaction between aspect term and context , and models aspect term and context using two connected attention networks .", "entities": [[0, 1, "MethodName", "IAN"]]}
{"text": "Thus , IAN performs better than ATAE - LSTM , and achieves an improvement of 1.40 points and 3.40 points on Restaurant and Laptop datasets in Three - class respectively .", "entities": [[2, 3, "MethodName", "IAN"], [8, 9, "MethodName", "LSTM"]]}
{"text": "MemNet ( 9 ) utilizes a more complex structure that containing nine computational layers , and it achieves better results compared to IAN since MemNet reads the useful information from external memory repeatedly .", "entities": [[22, 23, "MethodName", "IAN"]]}
{"text": "Although both IAN and MemNet models performance better than other methods , they all perform less competitive than our PBAN both on Restaurant and Laptop datasets .", "entities": [[2, 3, "MethodName", "IAN"]]}
{"text": "For IAN model , it interactively learns the attentions between the aspect term and its corresponding sentence , but this attention mechanism is coarse - grained and it does not fully consider the influence of different words in aspect term on the sentence .", "entities": [[1, 2, "MethodName", "IAN"]]}
{"text": "In PBAN , the position information is regarded as the inputs of the Bi - GRU , so it can help calculate the weights of different words in aspect term and improve the final representation of the sentence .", "entities": [[15, 16, "MethodName", "GRU"]]}
{"text": "Generally speaking , by integrating the position information and the bidirectional attention mechanism , PBAN achieves the state - of - the - art performances , and it can effectively judge the sentiment polarity of different aspect term in its corresponding sentence so as to improve the classification accuracy .", "entities": [[48, 49, "MetricName", "accuracy"]]}
{"text": "Firstly , we design an ATAE - Bi - GRU model , whose structure is similar with ATAE - LSTM .", "entities": [[9, 10, "MethodName", "GRU"], [19, 20, "MethodName", "LSTM"]]}
{"text": "The only difference between these two models is that ATAE - Bi - GRU uses the Bi - GRU structure rather than LSTM , and other design is the same as ATAE - LSTM .", "entities": [[13, 14, "MethodName", "GRU"], [18, 19, "MethodName", "GRU"], [22, 23, "MethodName", "LSTM"], [33, 34, "MethodName", "LSTM"]]}
{"text": "Moreover , we also design a PAN model , whose structure is similar with the ATAE - Bi - GRU model .", "entities": [[19, 20, "MethodName", "GRU"]]}
{"text": "PAN takes the concatenation of the aspect term embedding and the word embedding as the inputs of the Bi - GRU structure to obtain the hidden contextual representation , and then PAN utilizes this representation and the position embedding of the aspect term to calculate the attention weights , so as to effectively judge the sentiment polarity of an aspect term .", "entities": [[20, 21, "MethodName", "GRU"]]}
{"text": "Because Bi - GRU structure has a big advantage over LSTM , it is obvious that ATAE - Bi - GRU model performs better than ATAE - LSTM model .", "entities": [[3, 4, "MethodName", "GRU"], [10, 11, "MethodName", "LSTM"], [20, 21, "MethodName", "GRU"], [27, 28, "MethodName", "LSTM"]]}
{"text": "For PAN model , it outperforms ATAE - LSTM and ATAE - Bi - GRU models , but it is worse than BAN model .", "entities": [[8, 9, "MethodName", "LSTM"], [14, 15, "MethodName", "GRU"]]}
{"text": "Compared with ATAE - Bi - GRU , the most difference is that PAN utilizes the position embedding to calculate the attention weights rather than the aspect term embedding like ATAE - Bi - GRU .", "entities": [[6, 7, "MethodName", "GRU"], [34, 35, "MethodName", "GRU"]]}
{"text": "Therefore , according to these three experimental results , we can prove the importance of the position information in aspect - level sentiment analysis task .", "entities": [[22, 24, "TaskName", "sentiment analysis"]]}
{"text": "As for BAN model , it outperforms IAN model while performs worse than PBAN model .", "entities": [[7, 8, "MethodName", "IAN"]]}
{"text": "negative compared with IAN model , BAN model can learn more semantic relationship between aspect term and sentence via bidirectional attention mechanism .", "entities": [[3, 4, "MethodName", "IAN"]]}
{"text": "In this section , we will briefly review some research on sentiment analysis in recent years .", "entities": [[11, 13, "TaskName", "sentiment analysis"]]}
{"text": "Since a simple and effective method to learn distributed representation was proposed ( Mikolov et al , 2013 ) , neural networks enhance target - dependent sentiment analysis significantly .", "entities": [[26, 28, "TaskName", "sentiment analysis"]]}
{"text": "Tang et al ( 2015 ) proposed TD - LSTM and TC - LSTM , where target information is automatically taken into account .", "entities": [[9, 10, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "These two models integrated the connections between target words and context words so as to significantly boost the classification accuracy .", "entities": [[19, 20, "MetricName", "accuracy"]]}
{"text": "With the successful application of the attention mechanism in machine translation and reading comprehension , it is also applied to aspect - level sentiment analysis in recent years .", "entities": [[9, 11, "TaskName", "machine translation"], [12, 14, "TaskName", "reading comprehension"], [23, 25, "TaskName", "sentiment analysis"]]}
{"text": "examined the latent relatedness of the aspect term and sentiment polarity for aspect - level sentiment analysis .", "entities": [[15, 17, "TaskName", "sentiment analysis"]]}
{"text": "They designed an attention - based LSTM to learn aspect term embedding , and let the aspect term embedding participate in calculating the attention weights .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "Ma et al ( 2017 ) proposed a new attention model IAN , which considered the separate modeling of aspect terms and could interactively learn attention in the contexts and aspect terms .", "entities": [[11, 12, "MethodName", "IAN"]]}
{"text": "In this paper , we have proposed a position - aware bidirectional network ( PBAN ) based on Bi - GRU for aspect - level sentiment analysis .", "entities": [[20, 21, "MethodName", "GRU"], [25, 27, "TaskName", "sentiment analysis"]]}
{"text": "Adversarial Multi - lingual Neural Relation Extraction", "entities": [[5, 7, "TaskName", "Relation Extraction"]]}
{"text": "Multi - lingual relation extraction aims to find unknown relational facts from text in various languages .", "entities": [[3, 5, "TaskName", "relation extraction"]]}
{"text": "To address these issues , we propose an adversarial multi - lingual neural relation extraction ( AMNRE ) model , which builds both consistent and individual representations for each sentence to consider the consistency and diversity among languages .", "entities": [[13, 15, "TaskName", "relation extraction"]]}
{"text": "Relation extraction ( RE ) is a crucial task in NLP , which aims to extract semantic relations between entity pairs from the sentences containing them .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}
{"text": "RE can potentially benefit many applications , such as knowledge base construction ( Zhong et al , 2015 ; Han et al , 2018 ) and question answering ( Xiang et al , 2017 ) .", "entities": [[26, 28, "TaskName", "question answering"]]}
{"text": "All these neural relation extraction ( NRE ) models merely focus on extracting relational facts from mono - lingual data , ignoring the rich information in multi - lingual data .", "entities": [[3, 5, "TaskName", "relation extraction"]]}
{"text": "propose a multi - lingual attention - based neural relation extraction ( MNRE ) model , which considers the consistency and complementarity in multi - lingual data .", "entities": [[9, 11, "TaskName", "relation extraction"]]}
{"text": "The attention - based neural relation extraction ( NRE ) model has become a foundation for some recent works ( Ji et al , 2017 ; Zeng et al , 2017 ; Liu et al , 2017b ; Wu et al , 2017 ; Feng et al , 2018 ; Zeng et al , 2018 ) .", "entities": [[5, 7, "TaskName", "relation extraction"]]}
{"text": "Faruqui and Kumar ( 2015 ) and Verga et al ( 2016 ) first attempt to adopt multi - lingual transfer learning for RE .", "entities": [[2, 3, "DatasetName", "Kumar"], [20, 22, "TaskName", "transfer learning"]]}
{"text": "In this paper , we propose a novel multi - lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces , which can achieve more effective representations for RE . et al ( 2015 ) propose adversarial training for image classification tasks .", "entities": [[43, 45, "TaskName", "image classification"]]}
{"text": "In domain adaptation , Ganin et al ( 2016 ) and Bousmalis et al ( 2016 ) adopt adverarial training strategies to transfer the features of one source domain to its corresponding target domain .", "entities": [[1, 3, "TaskName", "domain adaptation"]]}
{"text": "Inspired by Ganin et al ( 2016 ) , adversarial training has also been explored in some typical NLP tasks for multi - feature fusion .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Park and I m ( 2016 ) propose a multi - modal representation learning model based on adversarial training .", "entities": [[12, 14, "TaskName", "representation learning"]]}
{"text": "Then , Liu et al ( 2017a ) employ adversarial training to construct a multi - task learning model for text classification by extending the original binary adversarial training to the multiclass version .", "entities": [[14, 18, "TaskName", "multi - task learning"], [20, 22, "TaskName", "text classification"]]}
{"text": "And a similar adversarial framework is also adapted by to learn features from different datasets for chinese word segmentation .", "entities": [[16, 19, "TaskName", "chinese word segmentation"]]}
{"text": "The input layer transforms all input words in the sentence into corresponding input embeddings by concatenating their word embeddings and position embeddings .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "The word embeddings are pre - trained by Skip - Gram ( Mikolov et al , 2013 ) .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "w + k p \u00d72 , k w and k p are the dimensions of word embeddings and position embeddings respectively .", "entities": [[15, 17, "TaskName", "word embeddings"]]}
{"text": "} to its sentence embedding .", "entities": [[3, 5, "TaskName", "sentence embedding"]]}
{"text": "CNN slides a convolution kernel with the window size m to extract the k h - dimensional local features , hi", "entities": [[3, 4, "MethodName", "convolution"]]}
{"text": "( 1 ) A max - pooling is then adopted to obtain the final sentence embedding y as follows , [ y ] j = max { [ h1 ] j , . . .", "entities": [[14, 16, "TaskName", "sentence embedding"]]}
{"text": "RNN ( ) is the recurrent unit and we select gated recurrent unit ( GRU )", "entities": [[10, 13, "MethodName", "gated recurrent unit"], [14, 15, "MethodName", "GRU"]]}
{"text": "We concatenate both the forward and backward hidden states as the sentence embedding y , y", "entities": [[11, 13, "TaskName", "sentence embedding"]]}
{"text": "\u03b1", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 k j y k j .", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "In the consistent semantic space , we also assign a query vectorr to each relation r R and the attention score for each sentence is defined as follows , \u03b2", "entities": [[29, 30, "HyperparameterName", "\u03b2"]]}
{"text": "| S l | k=1 \u03b2", "entities": [[5, 6, "HyperparameterName", "\u03b2"]]}
{"text": "( 11 ) p ( r | s ) and p ( r | s j ) can be defined as follows , p ( r | sj ) = softmax", "entities": [[30, 31, "MethodName", "softmax"]]}
{"text": "[ Rjsj + dj ] , p ( r | s ) = softmax [ Rs + d ] , ( 12 ) where d j andd are bias vectors , R j is the specific relation matrix of the j - th language , andR is the consistent relation matrix .", "entities": [[13, 14, "MethodName", "softmax"]]}
{"text": "We define the objective function to train the relation extractor as follows , min \u03b8 Lnre ( \u03b8 )", "entities": [[14, 15, "HyperparameterName", "\u03b8"], [17, 18, "HyperparameterName", "\u03b8"]]}
{"text": "Inspired by Ganin et al ( 2016 ) , we adopt adversarial training into our framework to address this problem .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "The probability distributions over these sentences are formalized as follows , D ( s i j ) = softmax ( MLP ( s i j ) ) , ( 14 ) where MLP is a two - layer multilayer perceptron network .", "entities": [[18, 19, "MethodName", "softmax"], [20, 21, "DatasetName", "MLP"], [32, 33, "DatasetName", "MLP"]]}
{"text": "Contrary to the discriminator , the consistent sentence encoders are expected to produce sentence embeddings that can not be reliably predicted by the discriminator .", "entities": [[13, 15, "TaskName", "sentence embeddings"]]}
{"text": "min \u03b8 C E max \u03b8 D n j=1 |", "entities": [[1, 2, "HyperparameterName", "\u03b8"], [5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "The formula means that given a sentence of any language , the corresponding sentence encoder of its language generates the sentence embedding to confuse the discriminator .", "entities": [[20, 22, "TaskName", "sentence embedding"]]}
{"text": "Meanwhile , the discriminator tries its best to predict the language of the sentence according to the sentence embedding .", "entities": [[17, 19, "TaskName", "sentence embedding"]]}
{"text": "j , min \u03b8 D L D adv ( \u03b8 D )", "entities": [[3, 4, "HyperparameterName", "\u03b8"], [9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "Inspired by Bousmalis et al ( 2016 ) , we adopt orthogonality constraints to alleviate this issue .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "We minimize the following penalty function : min \u03b8 E L penalty ( \u03b8 E )", "entities": [[8, 9, "HyperparameterName", "\u03b8"], [13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b8 E is parameters of the all encoders .", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "All models are optimized using stochastic gradient descent ( SGD ) .", "entities": [[5, 8, "MethodName", "stochastic gradient descent"], [9, 10, "MethodName", "SGD"]]}
{"text": "We evaluate our models on a multi - lingual relation extraction dataset developed by .", "entities": [[9, 11, "TaskName", "relation extraction"]]}
{"text": "To give a complete view of the performance , we also report the area under the curve ( AUC ) .", "entities": [[18, 19, "MetricName", "AUC"]]}
{"text": "Following the settings of previous works , we use the pre - trained word embeddings learned by Skip - Gram as the initial word embeddings .", "entities": [[13, 15, "TaskName", "word embeddings"], [23, 25, "TaskName", "word embeddings"]]}
{"text": "We adopt the cosine similarity to measure the similarity between sentence embeddings encoded by consistent encoders .", "entities": [[10, 12, "TaskName", "sentence embeddings"]]}
{"text": "In this paper , we introduce a novel adversarial multi - lingual neural relation extraction model ( AMNRE ) .", "entities": [[13, 15, "TaskName", "relation extraction"]]}
{"text": "our AMNRE could effectively encode the consistency and diversity among languages , and achieves state - of - the - art performance in relation extraction .", "entities": [[23, 25, "TaskName", "relation extraction"]]}
{"text": "In fact , machine translation is a typical approach to align sentences in various languages .", "entities": [[3, 5, "TaskName", "machine translation"]]}
{"text": "In the future , we will combine machine translation with our model to further improve the extraction performance .", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context , author context , visual context , or cognitive features ( Davidov et al , 2010 ;", "entities": [[14, 16, "TaskName", "sarcasm detection"]]}
{"text": "Peled and Reichart ( 2017 ) employed similar design to generate a parallel dataset to use for generating interpretations of sarcastic messages using machine translation approaches .", "entities": [[23, 25, "TaskName", "machine translation"]]}
{"text": "We use the MPQA sentiment Lexicon ( Wilson et al , 2005 ) , 2004 ) .", "entities": [[3, 4, "DatasetName", "MPQA"]]}
{"text": "If the Negation strategy is se - lected , we identify the negated term in the H int and then search its aligned node from the S i m using the word - word alignment .", "entities": [[33, 35, "TaskName", "word alignment"]]}
{"text": "We train a binary classifier using SVM RBF Kernel with default parameters .", "entities": [[6, 7, "MethodName", "SVM"]]}
{"text": "The features are Twitter - trained word embeddings ( Ghosh et al , 2015 ) , modal verbs , pronouns , interrogative words , negations , and position of \" ? \" in a tweet .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context , author context , visual context , or cognitive features (", "entities": [[14, 16, "TaskName", "sarcasm detection"]]}
{"text": "The findings from our study could have multiple impacts on the sarcasm detection task .", "entities": [[11, 13, "TaskName", "sarcasm detection"]]}
{"text": "This paper partially based on the work supported by the DARPA - DEFT program .", "entities": [[10, 11, "DatasetName", "DARPA"]]}
{"text": "Intrinsic evaluations by schema matching and instance graph perplexity , prove the superior quality of our probabilistic graph schema library compared to linear representations .", "entities": [[8, 9, "MetricName", "perplexity"]]}
{"text": "For example , regarding the 2019 protest in Hong Kong International Airport , a typical question from analysts would be \" How long will the flights being canceled ? \"", "entities": [[11, 12, "DatasetName", "Airport"]]}
{"text": "We propose a Temporal Event Graph Model , an auto - regressive graph generation model , to reach this goal .", "entities": [[12, 14, "TaskName", "graph generation"]]}
{"text": "Argument role set of event e , defined by the IE ontology \u03a6E The type set of events \u03a6V The type set of entities \u03c6 ( ) A mapping function from a node to its type", "entities": [[11, 12, "MethodName", "ontology"]]}
{"text": "We use OneIE , a state - of - the - art Information Extraction system , to extract entities , relations and events , and then perform crossdocument entity ( Pan et al , 2015 ( Pan et al , , 2017 and event coreference resolution ( Lai et al , 2021 ) over the document cluster of each complex event .", "entities": [[43, 46, "TaskName", "event coreference resolution"]]}
{"text": "We further conduct event - event temporal relation extraction ( Ning et al , 2019 ; Wen et al , 2021b ) to determine the order of event pairs .", "entities": [[6, 9, "TaskName", "temporal relation extraction"]]}
{"text": "We consider the isolated events as irrelevant nodes in schema induction , so they are excluded from the instance graphs during graph construction .", "entities": [[21, 23, "TaskName", "graph construction"]]}
{"text": "We then add argument nodes based on the IE ontology .", "entities": [[9, 10, "MethodName", "ontology"]]}
{"text": "In the traditional graph generation setting , the order of node generation can be arbitrary .", "entities": [[3, 5, "TaskName", "graph generation"]]}
{"text": "We also add dummy START / END event nodes to indicate the starting / ending of the graph generation .", "entities": [[17, 19, "TaskName", "graph generation"]]}
{"text": "At the beginning of the generation process , the graph G 0 has a single start event", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "i = Pooling ( { e 0 , , e i\u22121 } ) .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "i , we add all of its arguments in A ( e i ) defined in the IE ontology as new entity nodes .", "entities": [[18, 19, "MethodName", "ontology"]]}
{"text": "An intuitive way is to encode different edge types with different convolutional filters , which is similar to RGCN ( Schlichtkrull et al , 2018 ) .", "entities": [[18, 19, "MethodName", "RGCN"]]}
{"text": "However , the number of RGCN parameters grows rapidly with the number of edge types and easily becomes unmanageable given the large number of relation types and argument roles in the IE ontology .", "entities": [[5, 6, "MethodName", "RGCN"], [32, 33, "MethodName", "ontology"]]}
{"text": "i , a , v j is : m i , j = ReLU ( W a ( ( e i \u2212 v j ) a ) ) , where denotes concatenation operation .", "entities": [[13, 14, "MethodName", "ReLU"]]}
{"text": "Similarly , the message between two entities v j and v k is : m j , k = ReLU ( W r ( ( v j \u2212 v k ) r ) ) .", "entities": [[17, 19, "HyperparameterName", "k ="], [19, 20, "MethodName", "ReLU"]]}
{"text": "Considering that the direction of the temporal edge is important , we parametrize the message over this edge by assigning two separate weight matrices to the outgoing and incoming vertices : m i , l = ReLU ( W bfr e", "entities": [[36, 37, "MethodName", "ReLU"]]}
{"text": "e i GRU e i", "entities": [[2, 3, "MethodName", "GRU"]]}
{"text": "\u03b1 i , j m i , j .", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "Inspired by copy mechanism ( Gu et al , 2016 ) , we classify each argument node v j to either a new entity with entity type \u03c6 ( v j ) , or an existing entity node in the previous graph G < i .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "= exp ( MLP r ( v j \u2212", "entities": [[3, 4, "DatasetName", "MLP"]]}
{"text": "r R\u222a [ O ] exp ( MLP r ( v j \u2212", "entities": [[7, 8, "DatasetName", "MLP"]]}
{"text": "We use two hidden layers with ReLU activation functions to implement the MLP .", "entities": [[6, 7, "MethodName", "ReLU"], [12, 13, "DatasetName", "MLP"]]}
{"text": "We adopt the DARPA KAIROS 6 ontology , a newly defined fine - grained ontology for Schema Learning , with 24 entity types , 46 relation types , 67 event types , and 85 argument roles .", "entities": [[3, 4, "DatasetName", "DARPA"], [6, 7, "MethodName", "ontology"], [14, 15, "MethodName", "ontology"]]}
{"text": "7 Our schema induction method does not rely on any specific ontology , only the IE system is trained on a given ontology to create the instance event graphs .", "entities": [[11, 12, "MethodName", "ontology"], [22, 23, "MethodName", "ontology"]]}
{"text": "General Schema Learning Corpus : The Schema Learning Corpus , released by LDC ( LDC2020E25 ) , includes 82 types of complex events , such as Disease Outbreak , Presentations and Shop Online .", "entities": [[0, 1, "DatasetName", "General"]]}
{"text": "We first collected Wikipedia articles that describe 4 types of complex events , i.e. , Car - bombing IED , Drone Strikes IED , Suicide IED and General IED .", "entities": [[27, 28, "DatasetName", "General"]]}
{"text": "The human schemas of the General dataset do not contain arguments and the relations between arguments , so we only compute this metric for the IED dataset .", "entities": [[5, 6, "DatasetName", "General"]]}
{"text": "Considering that there can be multiple ending events in one instance graph , we rank event type prediction scores and adopt MRR ( Mean Reciprocal Rank ) and HITS@1 as evaluation metrics .", "entities": [[16, 18, "TaskName", "type prediction"], [21, 22, "MetricName", "MRR"], [28, 29, "MetricName", "HITS@1"]]}
{"text": "To train event language model baseline , instead of using LSTM - based architecture following ( Pichotta and Mooney , 2016 ) , we adopt the state - of - the - art auto - regressive language XLNet .", "entities": [[10, 11, "MethodName", "LSTM"], [37, 38, "MethodName", "XLNet"]]}
{"text": "An average of 113 instance graphs is used for each complex event type in the IED scenario , and 383 instance graphs to learn the schema model in the General scenario .", "entities": [[29, 30, "DatasetName", "General"]]}
{"text": "We are the first to perform graph generation on event graphs .", "entities": [[6, 8, "TaskName", "graph generation"]]}
{"text": "This research is based upon work supported by U.S. DARPA KAIROS Program Nos .", "entities": [[9, 10, "DatasetName", "DARPA"]]}
{"text": "The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies , either expressed or implied , of DARPA , or the U.S. Government .", "entities": [[29, 30, "DatasetName", "DARPA"]]}
{"text": "Similarly , we expect diverse responses in tasks such as story generation ( Li et al , 2018 ) , question generation", "entities": [[10, 12, "TaskName", "story generation"], [20, 22, "TaskName", "question generation"]]}
{"text": "( Pan et al , 2019 ) and question answering ( Fan et al , 2019 ) .", "entities": [[8, 10, "TaskName", "question answering"]]}
{"text": "As a preliminary step , we introduce the decoding test : the tester is a neural generation model and the diversity parameter is a decoding parameter , such as softmax temperature ( Ackley et al , 1985 ) .", "entities": [[29, 30, "MethodName", "softmax"]]}
{"text": "We evaluate three families of popular diversity metrics with these tests : ( a ) n - gram - based metrics that estimate diversity based on surface patterns in a set of generated sentences , ( b ) neural metrics : we propose a reduction from evaluating sentence similarity to evaluating diversity , then evaluate diversity using state - of - the - art sentence similarity models , and ( c ) human evaluation : we explore multiple ways in which humans can be asked to estimate diversity , resulting in multiple Human Diversity Score ( HDS ) variations .", "entities": [[94, 95, "MetricName", "Score"]]}
{"text": "Applying our tests leads to several findings : ( i ) In the decoding test , n - gram - based metrics correlate well with decoding parameters , such as softmax temperature .", "entities": [[30, 31, "MethodName", "softmax"]]}
{"text": "Perplexity is the standard metric in language modeling , measuring the proximity of a language model ( LM ) , P LM , to the true distribution , P ref , by approximating the cross - entropy H ( P ref , P LM ) with held - out data from P ref .", "entities": [[0, 1, "MetricName", "Perplexity"]]}
{"text": "Thus , perplexity captures to some extent diversity .", "entities": [[2, 3, "MetricName", "perplexity"]]}
{"text": "For example , a dialog model that puts all probability mass on the output \" I do n't know \" for any given context will obtain infinite perplexity once it encounters any other response .", "entities": [[27, 28, "MetricName", "perplexity"]]}
{"text": "However , perplexity does not purely measure diversity , and high perplexity does not entail low diversity .", "entities": [[2, 3, "MetricName", "perplexity"], [11, 12, "MetricName", "perplexity"]]}
{"text": "For example , a LM with a uniform distribution over the vocabulary for each decoded token has high diversity , but its perplexity will be extremely high , due to its low quality .", "entities": [[22, 23, "MetricName", "perplexity"]]}
{"text": "Moreover , perplexity evaluates a LM , while the diversity of a NLG system is also strongly affected by the decoding procedure .", "entities": [[2, 3, "MetricName", "perplexity"]]}
{"text": "High average Self - BLEU indicates high similarity between generated sentences and low diversity .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "A human tester can observe c and d , and produce reviews accordingly ( such data can be easily mined from IMDB ) .", "entities": [[21, 22, "DatasetName", "IMDB"]]}
{"text": "Variations include ( a ) softmax temperature ( Ackley et al , 1985 ) , where a parameter \u03c4 controls the skewness of the softmax distribution at each step , ( b ) Nucleus ( Top - p ) sampling ( Holtzman et al , 2019 ) , where one samples at each step from the minimal set of most probable tokens whose cumulative probability is at least p , and ( c ) Top - k sampling , which samples from the top - k most probable tokens at each step .", "entities": [[5, 6, "MethodName", "softmax"], [24, 25, "MethodName", "softmax"]]}
{"text": "In the decoding test ( decTest ) , we define the tester to be a LM , such as GPT - 2 ( Radford et al , 2019 ) , and the diversity parameter d to be a decoding parameter such as temperature .", "entities": [[19, 20, "MethodName", "GPT"]]}
{"text": "In 6 we show that both n - gram - based similarity metrics and neural semantic similarity metrics provide useful diversity metrics .", "entities": [[15, 17, "TaskName", "semantic similarity"]]}
{"text": "Story completion ( storyGen ) ; We use the ROC Stories dataset ( Mostafazadeh et al , 2016 ) , in which the context c is the first four sentences of a story , and the response s is a single sentence that ends the story .", "entities": [[0, 2, "TaskName", "Story completion"]]}
{"text": "Dialog response generation ( respGen ) ; A comment - response pairs dataset extracted from the website reddit.com and pre - processed by Hashimoto et al ( 2019 ) .", "entities": [[1, 3, "TaskName", "response generation"]]}
{"text": "3 - words prompt completion ( promptGen ) ; Contexts C are 3 - words prompts , extracted from the Cornell Movie - Dialogs Corpus ( Danescu - Niculescu - Mizil and Lee , 2011 ) by taking the first three words from each original context .", "entities": [[20, 25, "DatasetName", "Cornell Movie - Dialogs Corpus"]]}
{"text": "We intentionally avoid NLG tasks where diversity is not necessarily desired , such as summarization and machine translation .", "entities": [[14, 15, "TaskName", "summarization"], [16, 18, "TaskName", "machine translation"]]}
{"text": ", 5 } , which we found to outperform any single choice of n. Neural metrics We exploit existing BERT - based models ( Devlin et al , 2019 ) fine - tuned for estimating similarity between two sentences ( applying the reduction from 5 ) .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "BERT - STS ; A BERT model fine - tuned on Semantic Textual Similarity ( Cer et al , 2017 ) : a collection of sentence pairs annotated with scores from 1 - 5 denoting their semantic similarity .", "entities": [[0, 1, "MethodName", "BERT"], [2, 3, "TaskName", "STS"], [5, 6, "MethodName", "BERT"], [11, 14, "TaskName", "Semantic Textual Similarity"], [36, 38, "TaskName", "semantic similarity"]]}
{"text": "We used RoBERTa - large , as suggested by the authors .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}
{"text": "4 Sentence - BERT ( sent - BERT ) ( Reimers and Gurevych , 2019 ) is a sentence - level embedding model based on BERT .", "entities": [[3, 4, "MethodName", "BERT"], [7, 8, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"]]}
{"text": "In decTest we measure the correlation between diversity metrics ( m div ) and the softmax temperature decoding parameter ( d ) .", "entities": [[15, 16, "MethodName", "softmax"]]}
{"text": "Data for promptGen was generated by GPT - 2 - large ( Radford et al , 2019 ) without fine - tuning .", "entities": [[6, 7, "MethodName", "GPT"]]}
{"text": "While softmax temperature enables skewing P LM to a more diverse P gen using \u03c4 >", "entities": [[1, 2, "MethodName", "softmax"]]}
{"text": "In conTest , we measure the correlation between diversity metrics ( m div ) and content diversity , represented by a binary parameter d { 0 , 1 } .", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "In addition to Spearman 's \u03c1 , we report the optimal single - threshold classifier accuracy ( OCA ) , i.e. , the best achievable accuracy in predicting the class of a response set ( high or low content diversity ) for any threshold \u03b7 on m div , such that if m div ( S c ) >", "entities": [[15, 16, "MetricName", "accuracy"], [25, 26, "MetricName", "accuracy"]]}
{"text": "Neural models perform better than n - gram - based metrics ( especially sent - BERT ) , but there is still a clear gap between automatic metrics and humans .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "Each response set was generated with a different value of decoding parameter for the three decoding methods : softmax temperature , Nucleus sampling , and Top - k.", "entities": [[18, 19, "MethodName", "softmax"]]}
{"text": "The second is accuracy , i.e. , whether the metric can predict which set has higher temperature ( e.g. , in automatic metrics this is whether the sign of the temperature difference and the sign of metric score difference agree ) .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "Accuracy ( acc ) of classifying which set has the higher temperature .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "acc"]]}
{"text": "Compared to Table 4 , The gap between the best performing neural metrics ( sent - BERT ) and absHDS was increased in favor to HDS ( 0.04 compared to 0.1 difference in Spearman 's \u03c1 ) .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "Reddit comment - response dataset used for respGen task contains 37 M /1 M /1 M train / validation / test comment - response pairs , extracted from the social website reddit.com scraped by pushshift.io followed by the pre - process described in ( Hashimoto et al , 2019 ) .", "entities": [[0, 1, "DatasetName", "Reddit"]]}
{"text": "We did not use this data for training since we used GPT - 2 without fine - tuning for promptGen . Auto - generated data For decTest , we used two pre - trained generative models for generating responses given the contexts : For storyGen and respGen tasks , we used MASS 11 He missed his meeting .", "entities": [[11, 12, "MethodName", "GPT"]]}
{"text": "Automatic Generation of Contrast Sets from Scene Graphs : Probing the Compositional Consistency of GQA", "entities": [[14, 15, "DatasetName", "GQA"]]}
{"text": "While most contrast sets were created manually , requiring intensive annotation effort , we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task .", "entities": [[31, 34, "DatasetName", "visual question answering"]]}
{"text": "Our method computes the answer of perturbed questions , thus vastly reducing annotation cost and enabling thorough evaluation of models ' performance on various semantic aspects ( e.g. , spatial or relational reasoning ) .", "entities": [[31, 33, "TaskName", "relational reasoning"]]}
{"text": "We demonstrate the effectiveness of our approach on the popular GQA dataset ( Hudson and Manning , 2019 ) and its semantic scene graph image representation .", "entities": [[10, 11, "DatasetName", "GQA"]]}
{"text": "Figure 1 : Illustration of our approach based on an example from the GQA dataset .", "entities": [[13, 14, "DatasetName", "GQA"]]}
{"text": "For each QA pair , the LXMERT predicted output is shown . the out - of - domain performance of these models is often severely deteriorated ( Jia and Liang , 2017 ; Ribeiro et", "entities": [[6, 7, "MethodName", "LXMERT"]]}
{"text": "Recently , Kaushik et al ( 2019 ) and introduced the contrast sets approach to probe out - of - domain generalization .", "entities": [[20, 22, "TaskName", "domain generalization"]]}
{"text": "In this work , we propose a method for automatic generation of large contrast sets for visual question answering ( VQA ) .", "entities": [[16, 19, "DatasetName", "visual question answering"], [20, 21, "TaskName", "VQA"]]}
{"text": "We experiment with the GQA dataset ( Hudson and Manning , 2019 ) .", "entities": [[4, 5, "DatasetName", "GQA"]]}
{"text": "GQA includes semantic scene graphs ( Krishna et al , 2017 ) representing the spatial relations between objects in the image , as exemplified in Fig .", "entities": [[0, 1, "DatasetName", "GQA"]]}
{"text": "We leverage the GQA scene graphs to create contrast sets , by automatically computing the answers to question perturbations , e.g. , verifying that there is no wall near the puddle in Fig .", "entities": [[3, 4, "DatasetName", "GQA"]]}
{"text": "Following , we evaluate two leading models , LXMERT ( Tan and Bansal , 2019 ) and MAC ( Hudson and Manning , 2019 ) on our contrast sets , and find a 13 - 17 % reduction in performance compared to the original validation set .", "entities": [[8, 9, "MethodName", "LXMERT"]]}
{"text": "We augment the GQA training set with automatically constructed training contrast sets ( adding 80 K samples to the existing 943 K in GQA ) , and observe that when trained with it , both LXMERT and MAC improve by about 14 % on the contrast sets , while maintaining their original validation performance .", "entities": [[3, 4, "DatasetName", "GQA"], [23, 24, "DatasetName", "GQA"], [35, 36, "MethodName", "LXMERT"]]}
{"text": "Our key contributions are : ( 1 ) We present an automatic method for creating contrast sets for VQA datasets with structured input representations ; ( 2 ) We automatically create contrast sets for GQA , and find that for two strong models , performance on the contrast sets is lower than on the original validation set ; and ( 3 ) We apply our method to augment the training data , improving both models ' performance on the contrast sets .", "entities": [[18, 19, "TaskName", "VQA"], [34, 35, "DatasetName", "GQA"]]}
{"text": "To construct automatic contrast sets for GQA we first identify a large subset of questions requiring specific reasoning skills ( 2.1 ) .", "entities": [[6, 7, "DatasetName", "GQA"]]}
{"text": "The questions in the GQA dataset present a diverse set of modelling challenges , as exemplified in Table 1 , including object identification and grounding , spatial reasoning and color identification .", "entities": [[4, 5, "DatasetName", "GQA"]]}
{"text": "In our example , this would entail grounding \" players \" in the question to the scene graph ( either via exact match or several other heuristics such as hard - coded lists of synonyms or co - hyponyms ) , locating its neighbors , and verifying that none of them are \" trees . \"", "entities": [[21, 23, "MetricName", "exact match"]]}
{"text": "does not already exist in GQA .", "entities": [[5, 6, "DatasetName", "GQA"]]}
{"text": "In question templates with two objects ( X and Y ) , we replace X with X ' , such that X ' is correlated with Y in other GQA scene graphs .", "entities": [[29, 30, "DatasetName", "GQA"]]}
{"text": "It can involve exact match , number match ( dogs in the question , and dog in the scene - graph ) , hyponyms ( animal in the question , and dog in the scene - graph ) , and synonyms ( motorbike in the question , and motorcycle in the scene - graph ) .", "entities": [[3, 5, "MetricName", "exact match"]]}
{"text": "3 in Appendix A.4 ) , oblivious to whether the question originated from GQA or from our automatic contrast set .", "entities": [[13, 14, "DatasetName", "GQA"]]}
{"text": "Our analysis shows that the human performance difference between the perturbed questions and the original questions can be attributed to the scene Figure 2 : GQA image ( left ) with example perturbations for different question templates ( right ) .", "entities": [[25, 26, "DatasetName", "GQA"]]}
{"text": "Training graph annotation errors in the GQA dataset : 3.5 % of the 4 % difference is caused by a discrepancy between image and scene graph ( objects appearing in the image and not in the graph , and vice versa ) .", "entities": [[6, 7, "DatasetName", "GQA"]]}
{"text": "We experiment with two top - performing GQA models , MAC ( Hudson and Manning , 2018 ) and LXMERT ( Tan and Bansal , 2019 ) , 3 to test their generalization on our automatic contrast sets , leading to various key observations .", "entities": [[7, 8, "DatasetName", "GQA"], [19, 20, "MethodName", "LXMERT"]]}
{"text": "We experiment with 1 , 3 , and 5 augmentations per question with the LXMERT model trained on the original GQA training set .", "entities": [[14, 15, "MethodName", "LXMERT"], [20, 21, "DatasetName", "GQA"]]}
{"text": "Our results suggest that both MAC and LXMERT under - perform when tested out of distribution .", "entities": [[7, 8, "MethodName", "LXMERT"]]}
{"text": "We proposed an automatic method for creating contrast sets for VQA datasets that use annotated scene graphs .", "entities": [[10, 11, "TaskName", "VQA"]]}
{"text": "We created contrast sets for the GQA dataset , which is designed to be compositional , balanced , and robust against statistical biases .", "entities": [[6, 7, "DatasetName", "GQA"]]}
{"text": "As our contrast sets can be generated cheaply , we further augmented the GQA training data with additional perturbed questions , and showed that this improves models ' performance on the contrast set .", "entities": [[13, 14, "DatasetName", "GQA"]]}
{"text": "Our proposed method can be extended to other VQA datasets .", "entities": [[8, 9, "TaskName", "VQA"]]}
{"text": "The images and original questions were sampled from the public GQA dataset ( Hudson and Manning , 2019 ) , in the English language .", "entities": [[10, 11, "DatasetName", "GQA"]]}
{"text": "The experiments have been performed with the public implementations of MAC ( Hudson and Manning , 2018 ) and LXMERT ( Tan and Bansal , 2019 ) , models : https : //github.com / airsplay / lxmert , https://github.com/stanfordnlp/ mac - network/.", "entities": [[19, 20, "MethodName", "LXMERT"], [36, 37, "MethodName", "lxmert"]]}
{"text": "The experiments were performed with a Linux virtual machine with a NVIDIA 's Tesla V100 GPU .", "entities": [[6, 7, "DatasetName", "Linux"]]}
{"text": "Table 5 reports the basic statistics of automatic contrast sets generation method when applied on the GQA validation dataset .", "entities": [[16, 17, "DatasetName", "GQA"], [17, 19, "DatasetName", "validation dataset"]]}
{"text": "We thank the authors of GQA for building the dataset , and the authors of LXMERT and MAC for sharing their code and making it usable .", "entities": [[5, 6, "DatasetName", "GQA"], [15, 16, "MethodName", "LXMERT"]]}
{"text": "In what follows , we explore a radical alternative , showing that if a suitable representation scheme is used , it is possible to incorporate all decisions related to PNs within Step 2 . Suppose each individual in the KB comes not just with a number of descriptive properties but with 0 or more PNs as well , where a PN is regarded as a property that is true of all individuals who bear this name . - ( being named )", "entities": [[51, 52, "DatasetName", "0"]]}
{"text": "Mr Joe Klein , Joe Klein , Joe , Klein Because longer versions of a person 's name are applicable to only some of the individuals to whom a shorter version is applicable , the values of the NAMES attribute often subsume each other : all people who are called Mr Joe Klein are also called Joe Klein , and so on .", "entities": [[41, 42, "DatasetName", "subsume"]]}
{"text": "BRIO : Bringing Order to Abstractive Summarization", "entities": [[6, 7, "TaskName", "Summarization"]]}
{"text": "Abstractive summarization models are commonly trained using maximum likelihood estimation , which assumes a deterministic ( onepoint ) target distribution in which an ideal model will assign all the probability mass to the reference summary .", "entities": [[1, 2, "TaskName", "summarization"]]}
{"text": "Our method achieves a new state - of - the - art result on the CNN / DailyMail ( 47.78 ROUGE - 1 ) and XSum ( 49.07 ROUGE - 1 ) datasets .", "entities": [[25, 26, "DatasetName", "XSum"]]}
{"text": "Neural methods for abstractive summarization ( Rush et al , 2015 ;", "entities": [[4, 5, "TaskName", "summarization"]]}
{"text": "Nallapati et al , 2016 ; Lewis et al , 2020 ; formulate summarization as a sequenceto - sequence ( Seq2Seq ) problem ( Sutskever et al , 2014 ) , learning to generate the summary in an autoregressive manner .", "entities": [[13, 14, "TaskName", "summarization"], [20, 21, "MethodName", "Seq2Seq"]]}
{"text": "To maintain reasonable performance even in the case of a sub - sequence with errors , we argue that the The candidate summaries are generated by a pre - trained model ( BART ) , and we select the best and the worst candidates ( w.r.t .", "entities": [[32, 33, "MethodName", "BART"]]}
{"text": "To understand whether existing models can accurately perform such relative comparisons , we conducted a preliminary study on pre - trained BART ( Lewis et al , 2020 ) , first generating two candidate summaries from the model and observing whether a higher probability is assigned to the candidate with a higher ROUGE ( Lin , 2004 ) score .", "entities": [[21, 22, "MethodName", "BART"]]}
{"text": "The contrastive loss encourages the order of model - predicted probabilities of candidate summaries to be coordinated with the actual quality metric M by which the summaries will be evaluated .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "The new SOTA performance on CNN / DailyMail ( Hermann et al , 2015 ) and XSum ( Narayan et al , 2018 ) datasets demonstrated the effectiveness of our method .", "entities": [[16, 17, "DatasetName", "XSum"]]}
{"text": "The goal of abstractive summarization is to create a function g that takes a source document D and generates an appropriate summary S S g ( D )", "entities": [[4, 5, "TaskName", "summarization"]]}
{"text": "( 1 ) Training Objective Neural abstractive summarization models aim to learn a neural model g that results in good summaries .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "It aims to maximize the likelihood of the reference summary S * , i.e. , \u03b8 * = argmax \u03b8 i log p g \u03b8 ( S * ( i ) |", "entities": [[15, 16, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"], [24, 25, "HyperparameterName", "\u03b8"]]}
{"text": "* < j ) log pg \u03b8 ( s | D , S", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "< j denotes the partial reference sequence { s * 0 , , s * j\u22121 } and s * 0 is a pre - defined start token .", "entities": [[10, 11, "DatasetName", "0"], [20, 21, "DatasetName", "0"]]}
{"text": "= 1 s = s * j 0 s = s * j ( 4 )", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "As a result , even if the generation model g achieves very high accuracy w.r.t .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "However , this intuition is not directly captured in the standard MLE objective used in training - a model obtaining zero MLE loss would assign zero probability to any candidate summary different from the reference .", "entities": [[22, 23, "MetricName", "loss"]]}
{"text": "= \u03b2 S = S *", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "We next describe precisely how we encourage coordination through contrastive learning .", "entities": [[9, 11, "MethodName", "contrastive learning"]]}
{"text": "Zhong et al , 2020 ) : Lctr = i j > i max ( 0 , f ( Sj ) \u2212 f ( Si ) + \u03bbij ) ( 8 ) where S i and S j are two different candidate summaries and ROUGE ( S i , S * ) >", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "= l t=1 log p g \u03b8 ( s t | D , S < t ; \u03b8 )", "entities": [[6, 7, "HyperparameterName", "\u03b8"], [17, 18, "HyperparameterName", "\u03b8"]]}
{"text": "This loss gives the abstractive model a dual purpose , first as a reference - free evaluation model , which can be used in a two - stage summarization pipeline , where it is used to score the candidates generated by a pre - trained generation model and select the final output from them .", "entities": [[1, 2, "MetricName", "loss"], [28, 29, "TaskName", "summarization"]]}
{"text": "However , since the autoregressive generation depends on both the token - level prediction accuracy and sequencelevel coordination , the model fine - tuned with the contrastive loss alone can no longer be used as a generation model .", "entities": [[14, 15, "MetricName", "accuracy"], [27, 28, "MetricName", "loss"]]}
{"text": "We note that the contrastive and the cross - entropy loss can effectively complement each other - since the contrastive loss is defined on the sequence level , the token - level cross - entropy loss serves as a normalization to ensure that the model could assign balanced probability mass across the whole sequence .", "entities": [[10, 11, "MetricName", "loss"], [20, 21, "MetricName", "loss"], [35, 36, "MetricName", "loss"]]}
{"text": "Training Methods of Seq2Seq Models In order to align the training objective and evaluation metric , structured losses have been used for the Seq2Seq model training .", "entities": [[3, 4, "MethodName", "Seq2Seq"], [23, 24, "MethodName", "Seq2Seq"]]}
{"text": "One main challenge of directly optimizing a Seq2Seq model with quality scores of the output is that the discrete sampling process makes the loss non - differentiable .", "entities": [[7, 8, "MethodName", "Seq2Seq"], [23, 24, "MetricName", "loss"]]}
{"text": "To circumvent this problem , reinforcement learning has been used to reformulate the conditional text generation tasks ( Ranzato et al , 2016 ;", "entities": [[13, 16, "TaskName", "conditional text generation"]]}
{"text": "Contrastive Learning Recently , contrastive learning ( Hadsell et al , 2006 ) has been introduced into several conditional text generation tasks , such as machine translation Pan et al , 2021 ) , text summarization ( Cao and Wang , 2021 ;", "entities": [[0, 2, "MethodName", "Contrastive Learning"], [4, 6, "MethodName", "contrastive learning"], [18, 21, "TaskName", "conditional text generation"], [25, 27, "TaskName", "machine translation"], [34, 36, "TaskName", "text summarization"]]}
{"text": "Among these application scenarios , most work deployed contrastive learning in the latent representation space , following the framework proposed in .", "entities": [[8, 10, "MethodName", "contrastive learning"]]}
{"text": "However , in this work we adopt contrastive learning over the discrete space of the generated texts .", "entities": [[7, 9, "MethodName", "contrastive learning"]]}
{"text": "Besides , instead of constructing the contrastive learning examples by rule - based methods ( e.g. perturbing the reference output ) , we use the generation models to construct the examples , which makes the contrastive learning task closer to the generation task .", "entities": [[6, 8, "MethodName", "contrastive learning"], [35, 37, "MethodName", "contrastive learning"]]}
{"text": "Sun and Li ( 2021 ) also adopted contrastive learning on the generated texts .", "entities": [[8, 10, "MethodName", "contrastive learning"]]}
{"text": "Some recent works Lee et al , 2021a ) have also explored discriminative reranking of candidates from neural natural language generation models , which adopt large pre - trained language models ( e.g. BERT ( Devlin et al , 2019 ) ) as the reranker .", "entities": [[33, 34, "MethodName", "BERT"]]}
{"text": "In this work , we factorize the Seq2Seq model ( e.g. , BART ) trained on the same dataset as the reranking model , which maximizes the parameter sharing across two stages .", "entities": [[7, 8, "MethodName", "Seq2Seq"], [12, 13, "MethodName", "BART"]]}
{"text": "Besides , our approach contributes an instance of leveraging large pre - trained Seq2Seq models as a quality estimation model ( Yuan et al , 2021 ) .", "entities": [[13, 14, "MethodName", "Seq2Seq"]]}
{"text": "XSum 5 ( Narayan et al , 2018 ) is a highly abstractive dataset of articles from the British Broadcasting Corporation ( BBC ) .", "entities": [[0, 1, "DatasetName", "XSum"]]}
{"text": "BART ( Lewis et al , 2020 ) and PEGASUS are both large pre - trained Seq2Seq LMs standard in the literature .", "entities": [[0, 1, "MethodName", "BART"], [16, 17, "MethodName", "Seq2Seq"]]}
{"text": "GSum ( Dou et al , 2021 ) is built on BART , and improves performance by using additional guidance from an extractive summarizer .", "entities": [[11, 12, "MethodName", "BART"]]}
{"text": "SimCLS introduces a two - stage framework where the pre - trained BART model is used to generate candidates and a pre - trained RoBERTa model is fine - tuned as an evaluation model to score the candidate summaries and select from them .", "entities": [[12, 13, "MethodName", "BART"], [24, 25, "MethodName", "RoBERTa"]]}
{"text": "It achieves state - of - the - art performance on both CNNDM and XSum .", "entities": [[14, 15, "DatasetName", "XSum"]]}
{"text": "GOLD ( Pang and He , 2021 ) uses offline reinforcement learning to train the BART model by treating the reference summaries as the demonstrations , a different formulation that can also improve the performance of the original BART .", "entities": [[15, 16, "MethodName", "BART"], [38, 39, "MethodName", "BART"]]}
{"text": "SeqCo ( Xu et al , 2021 ) and ConSum ( Sun and Li , 2021 ) are two recent methods that aim to leverage contrastive learning to improve the performance of the abstractive summarization model ( BART ) .", "entities": [[25, 27, "MethodName", "contrastive learning"], [34, 35, "TaskName", "summarization"], [37, 38, "MethodName", "BART"]]}
{"text": "In the following experiments , we use either BART or PEGASUS as a backbone .", "entities": [[8, 9, "MethodName", "BART"]]}
{"text": "We use BRIO - Ctr as an evaluation model that scores different candidate summaries generated by a Seq2Seq abstractive model and selects the final output from them , and BRIO - Mul as a standard Seq2Seq model that takes the source documents as input and generates the output in an autoregressive manner .", "entities": [[17, 18, "MethodName", "Seq2Seq"], [35, 36, "MethodName", "Seq2Seq"]]}
{"text": "For CNNDM and NYT we use BART as the backbone model while for XSum we use the pre - trained PEGASUS model as our base model since it achieves better performance than BART .", "entities": [[6, 7, "MethodName", "BART"], [13, 14, "DatasetName", "XSum"], [32, 33, "MethodName", "BART"]]}
{"text": "We have the following observations : ( 1 ) BRIO - Ctr outperforms SimCLS , its counterpart as an evaluation model in a two - stage summarization framework .", "entities": [[26, 27, "TaskName", "summarization"]]}
{"text": "Specifically , both BRIO - Ctr and SimCLS are used to score the candidate summaries generated by a Seq2Seq abstractive model ( BART ) .", "entities": [[18, 19, "MethodName", "Seq2Seq"], [22, 23, "MethodName", "BART"]]}
{"text": "We attribute BRIO - Ctr 's superior performance to its use of the same model architecture ( BART ) for both candidate generation and scoring , while SimCLS uses RoBERTa as the evaluation model .", "entities": [[17, 18, "MethodName", "BART"], [29, 30, "MethodName", "RoBERTa"]]}
{"text": "As a result , BRIO - Ctr maximizes the parameter sharing between the two stages , and preserves the power of the Seq2Seq model pre - trained on the same dataset .", "entities": [[22, 23, "MethodName", "Seq2Seq"]]}
{"text": "Notably , the previous state - of - the - art model , GSum , takes additional guidance as input and needs a separate encoder to encode the guidance information , while BRIO - Mul uses the same parameterization of BART .", "entities": [[40, 41, "MethodName", "BART"]]}
{"text": "Compared to other methods ( ConSum , SeqCo , GOLD ) that aim to improve upon BART , BRIO - Mul performs much better , showing the effectiveness of our training method .", "entities": [[16, 17, "MethodName", "BART"]]}
{"text": "( 3 ) Since on XSum we use PEGASUS instead of BART as the base model , the result shows that our method is not restricted to the specific choice of the base model .", "entities": [[5, 6, "DatasetName", "XSum"], [11, 12, "MethodName", "BART"]]}
{"text": "BRIO - Ctr is trained with the contrastive loss only , which no longer preserves its generation ability .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "R - 1/2 / L are the ROUGE - 1/2 / L F1 scores .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "However , the cross - entropy loss is still necessary to preserve the model 's ability as a generation model .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "We argue that this is because the token level accuracy is still important during the autoregressive generation process , where the individual tokens are predicted sequentially .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "In addition , we also found that the model tends to achieve the best performance ( w.r.t the ROUGE scores on the development set ) faster with a higher \u03b3 .", "entities": [[29, 30, "HyperparameterName", "\u03b3"]]}
{"text": "In Tab . 5 , we compare the performance of the pre - trained BART and our model ( BRIO - Mul ) with different beam widths used during inference .", "entities": [[14, 15, "MethodName", "BART"]]}
{"text": "We observe that the performance of BART goes down as the beam width increases .", "entities": [[6, 7, "MethodName", "BART"]]}
{"text": "To evaluate our method 's performance beyond ROUGE , we use a model - based semantic similarity metric , BERTScore ( Zhang * et al , 2020 ) , 7 as the evaluation metric M in Eq.7 to compare the performance of different candidate summaries .", "entities": [[15, 17, "TaskName", "semantic similarity"]]}
{"text": "Beams BART BRIO - Mul R - 1 R - 2 R - 1 R - The results in Tab . 6 show that ( 1 ) Our model can significantly improve the model performance when either ROUGE or BERTScore is used as the target evaluation metric for ordering candidate summaries .", "entities": [[1, 2, "MethodName", "BART"]]}
{"text": "Novel n - grams We compare the ratio of novel n - grams in reference , BRIO - Mul 's , and BART 's summaries .", "entities": [[22, 23, "MethodName", "BART"]]}
{"text": "As Tab . 7 shows , our model is more \" abstractive \" compared to BART , although reference summaries still contain more novel n - grams .", "entities": [[15, 16, "MethodName", "BART"]]}
{"text": "paring our model ( BRIO - Mul ) with the baseline model ( BART ) on different buckets of test examples grouped by the \" novelty \" of the reference summaries , 8 i.e. , Novelty ( D , S * )", "entities": [[13, 14, "MethodName", "BART"]]}
{"text": "1 ( g / GD )", "entities": [[4, 5, "DatasetName", "GD"]]}
{"text": "We calculate Spearman 's rank correlation for each sample , and use the average score as the overall correlation , We investigated two specific settings : 1 ) ranking candidate summaries generated by a different model ( PEGASUS ) ; 2 ) ranking candidate summaries generated by themselves ( BART & BRIO - Mul ) .", "entities": [[49, 50, "MethodName", "BART"]]}
{"text": "Then , since a more calibrated model 's confidence estimates better the accuracy of its predictions , the model 's estimated probability of one sequence should be more indicative of the quality of this sequence , which is essential for the beam search during inference .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "We investigate this relation from the opposite direction by evaluating whether our model ( BRIO - Mul ) , which is trained to have better sequencelevel performance , would also be more calibrated at the token - level compared with the baseline models that are trained using MLE and label smoothing .", "entities": [[49, 51, "MethodName", "label smoothing"]]}
{"text": "The results in Tab . 9 show that BRIO - Mul is better calibrated compared to BART , suggesting that our method helps to improve the token - level calibration by explicitly encouraging the model to have more accurate sequence - level probability estimations .", "entities": [[16, 17, "MethodName", "BART"]]}
{"text": "We found that ( 1 ) abstractive models are generally over - confident on their own predictions , ( 2 ) models are generally more calibrated on XSum than CNNDM .", "entities": [[27, 28, "DatasetName", "XSum"]]}
{"text": "This is likely due to the fact that XSum has shorter summaries therefore it is less likely to be affected by the exposure bias .", "entities": [[8, 9, "DatasetName", "XSum"]]}
{"text": "The training paradigm proposed in this paper may be extended to any Seq2Seq model .", "entities": [[12, 13, "MethodName", "Seq2Seq"]]}
{"text": "BART tammy abraham scored twice in the first half to give chelsea the lead .", "entities": [[0, 1, "MethodName", "BART"]]}
{"text": "BART movistar rider alejandro valverde won fleche wallonne on wednesday .", "entities": [[0, 1, "MethodName", "BART"]]}
{"text": "BART manuel pellegrini 's future at manchester city is under scrutiny .", "entities": [[0, 1, "MethodName", "BART"]]}
{"text": "10 presents an interesting pattern we observed when comparing the results of BRIO - Mul and BART , which demonstrates that our method helps the abstractive model to filter out noise patterns in the original data .", "entities": [[16, 17, "MethodName", "BART"]]}
{"text": "BART picked up this pattern , and generates this phrase in 96 output summaries .", "entities": [[0, 1, "MethodName", "BART"]]}
{"text": "In this work , we presented a new training paradigm that assigns candidate outputs probability mass according to their quality using contrastive learning .", "entities": [[21, 23, "MethodName", "contrastive learning"]]}
{"text": "While our method has achieved significant improvement on abstractive summarization , we note several directions for the future work to explore .", "entities": [[9, 10, "TaskName", "summarization"]]}
{"text": "First , since our method makes no assumptions specifically about the summarization task , it can be extended to other conditional text generation tasks such as machine translation .", "entities": [[11, 12, "TaskName", "summarization"], [20, 23, "TaskName", "conditional text generation"], [26, 28, "TaskName", "machine translation"]]}
{"text": "We also performed extensive search for the coefficient \u03b3 in Eq .", "entities": [[8, 9, "HyperparameterName", "\u03b3"]]}
{"text": "The checkpoint is \" google / pegasus - xsum \" \" containing around 568 M parameters .", "entities": [[8, 9, "DatasetName", "xsum"]]}
{"text": "Extremely Small BERT Models from Mixed - Vocabulary Training", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "Pretrained language models like BERT have achieved good results on NLP tasks , but are impractical on resource - limited devices due to memory footprint .", "entities": [[0, 3, "TaskName", "Pretrained language models"], [4, 5, "MethodName", "BERT"]]}
{"text": "Existing knowledge distillation methods used for model compression can not be directly applied to train student models with reduced vocabulary sizes .", "entities": [[1, 3, "MethodName", "knowledge distillation"], [6, 8, "TaskName", "model compression"]]}
{"text": "Our method compresses BERT LARGE to a task - agnostic model with smaller vocabulary and hidden dimensions , which is an order of magnitude smaller than other distilled BERT models and offers a better size - accuracy trade - off on language understanding benchmarks as well as a practical dialogue task .", "entities": [[3, 4, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"], [36, 37, "MetricName", "accuracy"]]}
{"text": "Recently , pre - trained context - aware language models like ELMo ( Peters et al , 2018 ) , GPT ( Radford et al , 2019 ) , BERT ( Devlin et al , 2018 ) and XLNet ( Yang et al , 2019 ) have outperformed traditional word embedding models like Word2Vec ( Mikolov et al , 2013 ) and GloVe ( Pennington et al , 2014 ) , and achieved strong results on a number of language understanding tasks .", "entities": [[11, 12, "MethodName", "ELMo"], [20, 21, "MethodName", "GPT"], [29, 30, "MethodName", "BERT"], [38, 39, "MethodName", "XLNet"], [62, 63, "MethodName", "GloVe"]]}
{"text": "Recent work has explored , inter alia , knowledge distillation ( Ba and Caruana , 2014 ; Hinton et al , 2015 ) to train small - footprint student models by implicit transfer of knowledge from a teacher model .", "entities": [[8, 10, "MethodName", "knowledge distillation"]]}
{"text": "This complicates task - agnostic distillation of BERT to Asterisk ( * ) denotes equal contribution .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "Research conducted when all authors were at Google .", "entities": [[7, 8, "DatasetName", "Google"]]}
{"text": "smaller - vocabulary student BERT models since the input vocabulary is also the output space for the masked language modeling ( MLM ) task used in BERT .", "entities": [[4, 5, "MethodName", "BERT"], [17, 20, "TaskName", "masked language modeling"], [21, 22, "DatasetName", "MLM"], [26, 27, "MethodName", "BERT"]]}
{"text": "This in turn limits these distillation methods ' ability to compress the input embedding matrix , that makes up a major proportion of model parameters e.g. the \u223c30 K input WordPiece embeddings of the BERT BASE model make up over 21 % of the model size .", "entities": [[30, 31, "MethodName", "WordPiece"], [34, 35, "MethodName", "BERT"], [35, 36, "MethodName", "BASE"]]}
{"text": "This proportion is even higher for most distilled BERT models , owing to these distilled models typically having fewer layers than their teacher BERT counterparts .", "entities": [[8, 9, "MethodName", "BERT"], [23, 24, "MethodName", "BERT"]]}
{"text": "We present a task and model - agnostic distillation approach for training small , reduced - vocabulary BERT models running into a few megabytes .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "We therefore align the student and teacher WordPiece embeddings by training the teacher on the MLM task with a mix of teacher - tokenized and student - tokenized words in a sequence , and then using these student embeddings to train smaller student models .", "entities": [[7, 8, "MethodName", "WordPiece"], [15, 16, "DatasetName", "MLM"]]}
{"text": "Work in NLP model compression falls broadly into four classes : matrix approximation , weight quantization , pruning / sharing , and knowledge distillation .", "entities": [[3, 5, "TaskName", "model compression"], [15, 16, "TaskName", "quantization"], [22, 24, "MethodName", "knowledge distillation"]]}
{"text": "Knowledge distillation focuses on implicit transfer of knowledge as soft teacher predictions ( Tang et al , 2019 ) , attention distributions ( Zagoruyko and Komodakis , 2016 ) and intermediate outputs ( Romero et al , 2014 ) .", "entities": [[0, 2, "MethodName", "Knowledge distillation"]]}
{"text": "WordPiece ( WP ) tokens ( Wu et al , 2016 ) are subword units obtained by applying greedy segmentation to a training corpus .", "entities": [[0, 1, "MethodName", "WordPiece"]]}
{"text": "Given such a corpus and a number of desired tokens D , a WordPiece vocabulary is generated by selecting D subword tokens such that the resulting corpus is minimal in the number of WordPiece when segmented according to the chosen WordPiece model .", "entities": [[13, 14, "MethodName", "WordPiece"], [33, 34, "MethodName", "WordPiece"], [40, 41, "MethodName", "WordPiece"]]}
{"text": "Most published BERT models use a vocabulary of 30522 Word - Pieces , obtained by running the above algorithm on the Wikipedia and BooksCorpus ( Zhu et al , 2015 ) corpora with a desired vocabulary size D of 30000 .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "For our student model , we chose a target vocabulary size D of 5000 WordPiece tokens .", "entities": [[14, 15, "MethodName", "WordPiece"]]}
{"text": "Using the same WordPiece vocabulary generation algorithm and corpus as above , we obtain a 4928 - WordPiece vocabulary for the student model .", "entities": [[3, 4, "MethodName", "WordPiece"], [17, 18, "MethodName", "WordPiece"]]}
{"text": "Additionally , the 30 K teacher BERT vocabulary includes 93.9 % of the WP tokens in this 5 K student vocabulary but does not subsume it .", "entities": [[6, 7, "MethodName", "BERT"], [24, 25, "DatasetName", "subsume"]]}
{"text": "For task - agnostic student models , we reuse BERT 's masked language modeling ( MLM ) task : words in context are randomly masked and predicted given the context via softmax over the model 's WP vocabulary .", "entities": [[9, 10, "MethodName", "BERT"], [11, 14, "TaskName", "masked language modeling"], [15, 16, "DatasetName", "MLM"], [19, 22, "DatasetName", "words in context"], [31, 32, "MethodName", "softmax"]]}
{"text": "We first train the student embeddings with the teacher model initialized from BERT LARGE .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "Note that since the student embeddings are set to a lower dimension than the teacher embeddings , as they are meant to be used in the smaller student model , we project the student embeddings up to the teacher embedding dimension using a trainable affine layer before these are input to the teacher BERT .", "entities": [[39, 41, "HyperparameterName", "embedding dimension"], [53, 54, "MethodName", "BERT"]]}
{"text": "Separate softmax layers P", "entities": [[1, 2, "MethodName", "softmax"]]}
{"text": "v i for token i. All teacher parameters ( \u03b8 t , eb t ) and student embeddings ( eb s ) are updated in this step .", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "= c i | \u03b8 t , eb s , eb t ) )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "All student model parameters ( \u03b8 s , eb s ) are updated .", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "= c i | \u03b8 s , eb s ) )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "For evaluation , we finetune the student model just as one would finetune the original BERT model i.e. , without using the teacher model or any taskspecific distillation .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "We fine - tune and evaluate the distilled student models on two classes of language understanding tasks : MNLI : Multi - Genre Natural Language Inference ( Williams et al , 2018 ) , a 3 - way sentence pair classification task with 393 K training instances .", "entities": [[18, 19, "DatasetName", "MNLI"], [23, 26, "TaskName", "Natural Language Inference"]]}
{"text": "SST - 2 : Stanford Sentiment Treebank ( Socher et al , 2013 ) , a 2 - way sentence classification task with 67 K training instances .", "entities": [[0, 1, "DatasetName", "SST"], [19, 21, "TaskName", "sentence classification"]]}
{"text": "Spoken Language Understanding : Since we are also keen on edge device applications , we also evaluate on spoken language understanding , a practical task in dialogue systems .", "entities": [[0, 3, "TaskName", "Spoken Language Understanding"], [18, 21, "TaskName", "spoken language understanding"]]}
{"text": "We use the SNIPS dataset ( Coucke et al , 2018 ) of \u223c14 K virtual assistant queries , each comprising one of 7 intents and values for one or more of the 39 pre - defined slots .", "entities": [[3, 4, "DatasetName", "SNIPS"]]}
{"text": "The intent detection and slot filling subtasks are modeled respectively as 7 - way sentence classification and sequence tagging with IOB slot labels .", "entities": [[1, 3, "TaskName", "intent detection"], [4, 6, "TaskName", "slot filling"], [14, 16, "TaskName", "sentence classification"]]}
{"text": "For GLUE , we train student models with 6 and 12 layers , 4 attention heads , and embedding / hidden dimensions fixed to 256 , each using a compact 5 K - WP vocabulary .", "entities": [[1, 2, "DatasetName", "GLUE"]]}
{"text": "We also evaluate baselines without knowledge distillation ( NoKD ) , parameterized identically to the distilled student models ( incl .", "entities": [[5, 7, "MethodName", "knowledge distillation"]]}
{"text": "the 5 K vocabulary ) , trained on the MLM teacher objective from scratch .", "entities": [[9, 10, "DatasetName", "MLM"]]}
{"text": "We also compare our models on GLUE with the following approaches : DistilBERT ( Sanh et al , 2019 ) structures for an optimized student model .", "entities": [[6, 7, "DatasetName", "GLUE"], [12, 13, "MethodName", "DistilBERT"]]}
{"text": "For SNIPS , we shift our focus to smaller , lowlatency models for on - device use cases .", "entities": [[1, 2, "DatasetName", "SNIPS"]]}
{"text": "The smaller models here may not be competitive on GLUE but are adequate for practical tasks such as spoken LU .", "entities": [[9, 10, "DatasetName", "GLUE"]]}
{"text": "We compare with two strong baselines : BERT BASE ( Chen et al , 2019a ) with intent and IOB slot tags predicted using the [ CLS ] and the first WP tokens of each word respectively , and StackProp ( Qin et al , 2019 ) , which uses a series of smaller recurrent and self - attentive encoders .", "entities": [[7, 8, "MethodName", "BERT"], [8, 9, "MethodName", "BASE"]]}
{"text": "For both stages , up to 20 input tokens were masked for MLM .", "entities": [[12, 13, "DatasetName", "MLM"]]}
{"text": "Table 1 shows results on downstream GLUE tasks and model sizes for our proposed models , BERT BASE / LARGE , and baselines .", "entities": [[6, 7, "DatasetName", "GLUE"], [16, 17, "MethodName", "BERT"], [17, 18, "MethodName", "BASE"]]}
{"text": "Compared with PKD / DistilBERT , our 6 - layer model outperforms PKD 3 while being > 7x smaller and our 12 - layer model is comparable to PKD 6 and DistilBERT 4 while being \u223c5 - 6x smaller .", "entities": [[4, 5, "MethodName", "DistilBERT"], [31, 32, "MethodName", "DistilBERT"]]}
{"text": "Interestingly , our models do particularly well on the MRPC task : the 6 - layer distilled model performs almost as well as PKD 6 while being over 10x smaller .", "entities": [[9, 10, "DatasetName", "MRPC"]]}
{"text": "This may be due to our smaller models being data - efficient on the smaller MRPC dataset .", "entities": [[15, 16, "DatasetName", "MRPC"]]}
{"text": "TinyBERT 's non - task - specific model results are reported on GLUE dev sets : these results are , therefore , not directly comparable with ours .", "entities": [[12, 13, "DatasetName", "GLUE"]]}
{"text": "MobileBERT performs strongly for the size while being task - agnostic .", "entities": [[0, 1, "MethodName", "MobileBERT"]]}
{"text": "TinyBERT sees major gains from task - specific data augmentation and distillation , and Mobile - BERT from student architecture search and bottleneck layers .", "entities": [[8, 10, "TaskName", "data augmentation"], [16, 17, "MethodName", "BERT"]]}
{"text": "Table 2 shows results on the SNIPS intent and slot tasks for our models and two state - of - theart baselines .", "entities": [[6, 7, "DatasetName", "SNIPS"]]}
{"text": "Impact of vocabulary size : We trained a model from scratch identical to BERT BASE except with our 5 K - WP student vocabulary .", "entities": [[13, 14, "MethodName", "BERT"], [14, 15, "MethodName", "BASE"]]}
{"text": "This suggests that a small WordPiece vocabulary may be almost as effective for sequence classification / tagging tasks , especially for smaller BERT models and up to moderately long inputs .", "entities": [[5, 6, "MethodName", "WordPiece"], [22, 23, "MethodName", "BERT"]]}
{"text": "Curiously , increasing the student vocabulary size to 7 K or 10 K did not lead to an increase in performance on GLUE .", "entities": [[22, 23, "DatasetName", "GLUE"]]}
{"text": "This model is 1.2 % smaller than our 4928 - WP distilled model , but drops 0.8 % / 0.7 % on SST - 2 / MNLI - m dev sets .", "entities": [[22, 23, "DatasetName", "SST"], [26, 29, "DatasetName", "MNLI - m"]]}
{"text": "This model , however , dropped 0.7 % / 0.5 % on SST - 2 / MNLI - m compared to our analogous 6\u00d7256 distilled model .", "entities": [[12, 13, "DatasetName", "SST"], [16, 19, "DatasetName", "MNLI - m"]]}
{"text": "We also tried pretraining BERT LARGE from scratch with the 5 K vocabulary and doing vanilla distillation for a 6\u00d7256 student : this model dropped 1.2 % / 0.7 % for SST - 2 / MNLI - m over our similar distilled model , indicating the efficacy of mixed - vocabulary training over vanilla distillation .", "entities": [[4, 5, "MethodName", "BERT"], [31, 32, "DatasetName", "SST"], [35, 38, "DatasetName", "MNLI - m"]]}
{"text": "We propose a novel approach to knowledge distillation for BERT , focusing on using a significantly smaller vocabulary for the student BERT models .", "entities": [[6, 8, "MethodName", "knowledge distillation"], [9, 10, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"]]}
{"text": "Our technique is unique in targeting the student vocabulary size , enabling easy combination with most BERT distillation methods .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "We also provide a detailed analysis of results including key insights that explain the improvement in MRR because of dialogue act information .", "entities": [[16, 17, "MetricName", "MRR"]]}
{"text": "Despite significant research in text generation , a pure generative model capable of generating syntactically and semantically correct text still remains a distant reality .", "entities": [[4, 6, "TaskName", "text generation"]]}
{"text": "There have been several efforts such as ( Vinyals and Le , 2015 ; Serban et al , 2016a ; Serban et al , 2016b ; Serban et al , 2017b ) for the task of dialogue generation , however these models still do not seem to work in practice ( Liu et al , 2016 ) .", "entities": [[36, 38, "TaskName", "dialogue generation"]]}
{"text": "Dialogue generation in a task - oriented oriented dialogue system , such as flight - booking and troubleshooting , is much easier than in a non - task oriented dialogue system .", "entities": [[0, 2, "TaskName", "Dialogue generation"]]}
{"text": "Hierarchical models have shown to perform better than non - hierarchical models for the task of dialogue generation , whereas Siamese models have been shown to outperform the encoder - decoder based models for the task of next utterance selection .", "entities": [[16, 18, "TaskName", "dialogue generation"]]}
{"text": "2 . We propose a novel model that combines the strength of Siamese network with strengths of hierarchical structure inherent in the conversations and dialogue act information .", "entities": [[12, 14, "MethodName", "Siamese network"]]}
{"text": "The model gives us the best of all , and outperforms the baseline models by a significant margin on the DailyDialog Dataset .", "entities": [[20, 21, "DatasetName", "DailyDialog"]]}
{"text": "4 . We modify the DailyDialog ( Li et al , 2017b ) dataset for the task of next utterance selection , and release it publicly along with the code - base of the proposed model 1 .", "entities": [[5, 6, "DatasetName", "DailyDialog"]]}
{"text": "d 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Several researchers ( Sordoni et al , 2015 ; Serban et al , 2016b ; Serban et al , 2017b ; Dehghani et al , 2017 ; Kumar et al , 2017 ) have shown that hierarchical models outperform standard non - hierarchical models .", "entities": [[27, 28, "DatasetName", "Kumar"]]}
{"text": "d K = \u03b1 *", "entities": [[1, 3, "HyperparameterName", "K ="], [3, 4, "HyperparameterName", "\u03b1"]]}
{"text": "Although there are several available datasets , such as SwDA ( Switchboard Dialogue Act Corpus ( Jurafsky , 1997 ) ) , MRDA ( Meeting Recorder Dialogue Act corpus ( Janin et al , 2003 ) ) , Ubuntu , OpenSubtitles ( Tiedemann , 2009 ) , etc . , they are not really suitable for our problem setting .", "entities": [[22, 23, "DatasetName", "MRDA"], [40, 41, "DatasetName", "OpenSubtitles"]]}
{"text": "Most of these datasets do not come with dialogue acts , and the ones which do ( i.e. SWDA and MRDA ) are small in size .", "entities": [[20, 21, "DatasetName", "MRDA"]]}
{"text": "Note that the SwDA and MRDA datasets contain 1003 and 51 conversations , respectively .", "entities": [[5, 6, "DatasetName", "MRDA"]]}
{"text": "To the best of our knowledge , a recently released dataset , DailyDialog ( Li et al , 2017b ) , is the only dataset that has utterances annotated with dialogue acts and is large enough for conversation modeling methods to work .", "entities": [[12, 13, "DatasetName", "DailyDialog"]]}
{"text": "We evaluate and report our results on the DailyDialog dataset .", "entities": [[8, 9, "DatasetName", "DailyDialog"]]}
{"text": "However , it is not always possible that such dialogue acts are available in practice , and it would be ideal to predict dialogue acts first ( Kumar et al , 2017 ) , and then use them for next utterance generation / retrieval ; having a model where both tasks , i.e. prediction and generation , are performed simultaneously may not be ideal for validating the hypothesis .", "entities": [[27, 28, "DatasetName", "Kumar"]]}
{"text": "Question A speaker intends to obtain information by asking a question Directive A speaker is requesting , accept / reject offer , or making a suggestion Comissive A speaker accept / reject a request or suggestion Table 1 : Dialogue Acts and their description available in the DailyDialog Dataset .", "entities": [[47, 48, "DatasetName", "DailyDialog"]]}
{"text": "The DailyDialog dataset in its original form is not directly useful for the task of next utterance selection , and hence requires preparation .", "entities": [[1, 2, "DatasetName", "DailyDialog"]]}
{"text": "Owing to the different conversational style of human and conversation agent , our objective is to build a model that is specific to the agent , i.e. bot .", "entities": [[10, 11, "DatasetName", "agent"], [24, 25, "DatasetName", "agent"]]}
{"text": "ED - It is a vanilla sequence to sequence model that uses an utterance encoder to obtain a representation of first K utterances which is then used in a decoder to generate next utterance .", "entities": [[6, 9, "MethodName", "sequence to sequence"]]}
{"text": "HRED - An extension of sequence to sequence model that uses a hierarchical encoder to obtain a representation of first K utterances , which is then used in decoder to generate next utterance .", "entities": [[5, 8, "MethodName", "sequence to sequence"]]}
{"text": "The word vectors were initialized with the 300 - dimensional Glove embeddings ( Pennington et al , 2014 ) , and were also updated during training .", "entities": [[10, 12, "MethodName", "Glove embeddings"]]}
{"text": "Dropout of 0.1 ( optimized over 0.0 to 0.7 in steps of 0.1 ) was applied to embeddings obtained from the output of conversation encoder .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "Since our problem formulation is retrieval based , we use standard IR metrics such as Mean Reciprocal Rank ( MRR ) and Recall@k as our evaluation metrics .", "entities": [[19, 20, "MetricName", "MRR"]]}
{"text": "MRR is calculated as the mean of the reciprocal rank of the true candidate response among other candidate responses .", "entities": [[0, 1, "MetricName", "MRR"]]}
{"text": "These results clearly indicate that the MRR of the true candidate response improves when dialogue acts of previous utterances are provided .", "entities": [[6, 7, "MetricName", "MRR"]]}
{"text": "In order to better exlpain this , we first compute the break - up of recall@1 according to the dialogue act classes .", "entities": [[15, 16, "MetricName", "recall@1"]]}
{"text": "Such a DA - class wise breakup of the recall@1 numbers helps us do an analysis with respect to individual DA - classes .", "entities": [[9, 10, "MetricName", "recall@1"]]}
{"text": "Several efforts have been made towards solving the problem of dialogue generation ( Vinyals and Le , 2015 ; Liu et al , 2016 ; Li et al , 2015 ) , however , due to the inherent difficulty of the problem , these efforts have only had limited success and are known to have issues like generating repetitive and generalized responses such as I do n't know or Ok .", "entities": [[10, 12, "TaskName", "dialogue generation"]]}
{"text": "A recent work by ( Zhao et al , 2017 ) has used dialogue acts for the task of dialogue generation .", "entities": [[19, 21, "TaskName", "dialogue generation"]]}
{"text": "In this paper , we present our systems for the MADAR Shared Task : Arabic Fine - Grained Dialect Identification .", "entities": [[18, 20, "TaskName", "Dialect Identification"]]}
{"text": "In S - 1 , our proposed systems are based on language modelling .", "entities": [[11, 13, "TaskName", "language modelling"]]}
{"text": "The dialects were distinguished by phoneme sequences , and the results of classifications based on each phonerecognizer were combined using a logistic regression classifier .", "entities": [[21, 23, "MethodName", "logistic regression"]]}
{"text": "Zaidan and Callison - Burch ( 2011 ) describe building a text corpus , based on reader commen - tary on newspaper websites , with significant dialect content ; the goal is to provide a corpus to improve machine translation for Arabic dialects .", "entities": [[38, 40, "TaskName", "machine translation"]]}
{"text": "Malmasi et al ( 2015 ) do Arabic dialect identification from text corpora , including the Multi - Dialect Parallel Corpus of Arabic ( Bouamor et al , 2014 ) and the Arabic Online Commentary database ( Zaidan and Callison - Burch , 2011 ) .", "entities": [[8, 10, "TaskName", "dialect identification"]]}
{"text": "We followed Salameh ) in using the kenlm language modelling tool ( Heafield et al , 2013 ) .", "entities": [[8, 10, "TaskName", "language modelling"]]}
{"text": "2 . Support vector machine , svm .", "entities": [[2, 5, "MethodName", "Support vector machine"], [6, 7, "MethodName", "svm"]]}
{"text": "We train Multilayer Perceptron ( MLP ) with one hidden ( dense ) layer with 400 units .", "entities": [[5, 6, "DatasetName", "MLP"]]}
{"text": "The output of the hidden layer is passed to a final fullyconnected softmax layer .", "entities": [[12, 13, "MethodName", "softmax"]]}
{"text": "The output of the softmax layer is a probability distribution over all 26 classes .", "entities": [[4, 5, "MethodName", "softmax"]]}
{"text": "As an activation function in the hidden layer of the MLP a Rectified Linear Unit ( ReLu ) is employed .", "entities": [[2, 4, "HyperparameterName", "activation function"], [10, 11, "DatasetName", "MLP"], [16, 17, "MethodName", "ReLu"]]}
{"text": "Each sequence of character n - grams is used as a separate input followed by a randomly initialized embedding layer and then two layers of Bidirectional LSTM ( BiLSTM ) ( Graves and Schmidhuber , 2005 ) with 64 units are employed ( see Figure 1 ) .", "entities": [[25, 27, "MethodName", "Bidirectional LSTM"], [28, 29, "MethodName", "BiLSTM"]]}
{"text": "The output vector of the BiLSTM layers is concatenated with the language model features and this concatenated vector is passed to the MLP layer with 400 units ( the same as described above ) .", "entities": [[5, 6, "MethodName", "BiLSTM"], [22, 23, "DatasetName", "MLP"]]}
{"text": "After the submissions had closed we experimented with eliminating the unavailable and non - Arabic tweets from True labels 127 0 0 12 3 4 1 7 0 0 30 1 0 0 8 0 1 0 0 0 0 3 1 0 1 1 0 160 1 1 0 1 0 0 8 0 0 1 7 1 1 4 1 4 2 2 2 1 0 1 1 1 0 3 149 4 16 1 0 0 1 10 1 1 1 2 1 8 0 0 0 0 0 0 2 0 0 0 9 0 2 118 2 1 0 6 2 8 9 7 0 2 18 3 0 0 2 0 2 7 1 0 1 0 0 0 21 4 134 1 1 0 0 19 1 4 1 3 4 2 0 0 0 0 3 1 0 0 1 0 2 0 0 5 2 127 25 1 3 1 1 3 0 1 0 4 3 3 1 0 9 2 5 2 0 0 1 1 1 2 1 29 128 1 2 0 1 6 0 1 0 0 8 2 3 0 5 2 3 2 1 0 8 1 0 7 2 1 1 129 2 3 18 0 0 1 9 3 0 1 2 1 3 5 0 0 2 1 2 5 0 6 9 3 1 1 131 2 1 8 0 4 2 0 0 0 2 0 6 1 4 1 11 0 6 4 19 3 37 2 0 0 2 100 2 1 1 5 1 9 0 0 0 0 3 0 1 0 2 2 16 0 1 16 4 4 2 17 2 3 108 4 1 2 10 0 0 0 0 0 0 8 1 0 1 0 1 1 0 4 2 2 3 1 1 1 0 150 1 8 4 5 0 1 3 1 4 2 1 1 2 1 0 7 1 1 2 0 0 0 0 2 1 4 136 3 0 2 0 1 2 31 1 1 1 3 0 1 1 1 1 6 4 0 0 2 5 3 3 14 1 123 1 6 0 1 3 0 10 5 5 1 3 1 7 2 1 26 2 1 2 7 5 1 1 6 0 2 121 2 0 0 1 0 2 9 1 0 1 0 0 2 3 8 10 1 1 1 0 1 1 3 0 4 2 147 1 12 0 0 0 0 1 0 2 0 1 0 0 0 0 3 8 0 1 0 0 0 0 1 0 0 167 1 1 0 6 4 5 0 1 1 1 7 2 2 0 1 0 0 2 1 1 0 0 0 1 9 1 158 4 0 7 1 1 0 1 0 2 3 1 3 1 3 1 1 5 1 3 15 1 8 1 10 0 32 83 1 16 4 4 0 1 0 1 7 0 3 0 0 0 0 2 0 1 1 36 1 2 1 0 0 0 139 0 1 0 3 1 1 2 0 0 3 0 2 2 3 6 5 0 13 1 18 1 1 2 9 7 0 115 2 7 0 1 0 3 3 1 12 3 2 2 5 3 2 7 10 0 5 12 2 1 3 2 0 7 109 4 1 1 0 1 1 1 2 1 6 2 1 3 1 1 7 0 9 1 3 4 3 1 0 6 4 140 0 2 0 2 6 0 1 1 2 3 0 3 1 2 0 2 0 2 0 0 0 2 1 3 1 0 146 0 22 1 3 0 2 3 0 0 2 9 2 0 4 0 2 2 4 0 0 1 1 4 1 1 4 151 3 1 training and testing and choosing Saudi Arabia ( which is the origin for the plurality of tweets at 36 % ) for users with no remaining tweets .", "entities": [[20, 21, "DatasetName", "0"], [21, 22, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [31, 32, "DatasetName", "0"], [32, 33, "DatasetName", "0"], [34, 35, "DatasetName", "0"], [36, 37, "DatasetName", "0"], [37, 38, "DatasetName", "0"], [38, 39, "DatasetName", "0"], [39, 40, "DatasetName", "0"], [42, 43, "DatasetName", "0"], [45, 46, "DatasetName", "0"], [49, 50, "DatasetName", "0"], [51, 52, "DatasetName", "0"], [52, 53, "DatasetName", "0"], [54, 55, "DatasetName", "0"], [55, 56, "DatasetName", "0"], [67, 68, "DatasetName", "0"], [71, 72, "DatasetName", "0"], [77, 78, "DatasetName", "0"], [78, 79, "DatasetName", "0"], [87, 88, "DatasetName", "0"], [88, 89, "DatasetName", "0"], [89, 90, "DatasetName", "0"], [90, 91, "DatasetName", "0"], [91, 92, "DatasetName", "0"], [92, 93, "DatasetName", "0"], [94, 95, "DatasetName", "0"], [95, 96, "DatasetName", "0"], [96, 97, "DatasetName", "0"], [98, 99, "DatasetName", "0"], [103, 104, "DatasetName", "0"], [109, 110, "DatasetName", "0"], [113, 114, "DatasetName", "0"], [114, 115, "DatasetName", "0"], [116, 117, "DatasetName", "0"], [120, 121, "DatasetName", "0"], [122, 123, "DatasetName", "0"], [123, 124, "DatasetName", "0"], [124, 125, "DatasetName", "0"], [130, 131, "DatasetName", "0"], [131, 132, "DatasetName", "0"], [139, 140, "DatasetName", "0"], [140, 141, "DatasetName", "0"], [141, 142, "DatasetName", "0"], [142, 143, "DatasetName", "0"], [145, 146, "DatasetName", "0"], [146, 147, "DatasetName", "0"], [148, 149, "DatasetName", "0"], [150, 151, "DatasetName", "0"], [151, 152, "DatasetName", "0"], [161, 162, "DatasetName", "0"], [163, 164, "DatasetName", "0"], [168, 169, "DatasetName", "0"], [173, 174, "DatasetName", "0"], [174, 175, "DatasetName", "0"], [184, 185, "DatasetName", "0"], [187, 188, "DatasetName", "0"], [189, 190, "DatasetName", "0"], [190, 191, "DatasetName", "0"], [194, 195, "DatasetName", "0"], [200, 201, "DatasetName", "0"], [203, 204, "DatasetName", "0"], [212, 213, "DatasetName", "0"], [213, 214, "DatasetName", "0"], [217, 218, "DatasetName", "0"], [223, 224, "DatasetName", "0"], [224, 225, "DatasetName", "0"], [229, 230, "DatasetName", "0"], [239, 240, "DatasetName", "0"], [242, 243, "DatasetName", "0"], [243, 244, "DatasetName", "0"], [244, 245, "DatasetName", "0"], [246, 247, "DatasetName", "0"], [252, 253, "DatasetName", "0"], [259, 260, "DatasetName", "0"], [260, 261, "DatasetName", "0"], [269, 270, "DatasetName", "0"], [270, 271, "DatasetName", "0"], [271, 272, "DatasetName", "0"], [272, 273, "DatasetName", "0"], [274, 275, "DatasetName", "0"], [276, 277, "DatasetName", "0"], [280, 281, "DatasetName", "0"], [294, 295, "DatasetName", "0"], [295, 296, "DatasetName", "0"], [296, 297, "DatasetName", "0"], [297, 298, "DatasetName", "0"], [298, 299, "DatasetName", "0"], [299, 300, "DatasetName", "0"], [302, 303, "DatasetName", "0"], [304, 305, "DatasetName", "0"], [307, 308, "DatasetName", "0"], [315, 316, "DatasetName", "0"], [321, 322, "DatasetName", "0"], [331, 332, "DatasetName", "0"], [336, 337, "DatasetName", "0"], [337, 338, "DatasetName", "0"], [338, 339, "DatasetName", "0"], [339, 340, "DatasetName", "0"], [345, 346, "DatasetName", "0"], [347, 348, "DatasetName", "0"], [355, 356, "DatasetName", "0"], [362, 363, "DatasetName", "0"], [363, 364, "DatasetName", "0"], [373, 374, "DatasetName", "0"], [376, 377, "DatasetName", "0"], [395, 396, "DatasetName", "0"], [399, 400, "DatasetName", "0"], [400, 401, "DatasetName", "0"], [402, 403, "DatasetName", "0"], [406, 407, "DatasetName", "0"], [408, 409, "DatasetName", "0"], [409, 410, "DatasetName", "0"], [417, 418, "DatasetName", "0"], [421, 422, "DatasetName", "0"], [427, 428, "DatasetName", "0"], [428, 429, "DatasetName", "0"], [429, 430, "DatasetName", "0"], [430, 431, "DatasetName", "0"], [432, 433, "DatasetName", "0"], [434, 435, "DatasetName", "0"], [436, 437, "DatasetName", "0"], [437, 438, "DatasetName", "0"], [438, 439, "DatasetName", "0"], [439, 440, "DatasetName", "0"], [442, 443, "DatasetName", "0"], [444, 445, "DatasetName", "0"], [445, 446, "DatasetName", "0"], [446, 447, "DatasetName", "0"], [447, 448, "DatasetName", "0"], [449, 450, "DatasetName", "0"], [450, 451, "DatasetName", "0"], [454, 455, "DatasetName", "0"], [458, 459, "DatasetName", "0"], [465, 466, "DatasetName", "0"], [467, 468, "DatasetName", "0"], [468, 469, "DatasetName", "0"], [472, 473, "DatasetName", "0"], [473, 474, "DatasetName", "0"], [474, 475, "DatasetName", "0"], [480, 481, "DatasetName", "0"], [484, 485, "DatasetName", "0"], [486, 487, "DatasetName", "0"], [503, 504, "DatasetName", "0"], [510, 511, "DatasetName", "0"], [512, 513, "DatasetName", "0"], [515, 516, "DatasetName", "0"], [517, 518, "DatasetName", "0"], [518, 519, "DatasetName", "0"], [519, 520, "DatasetName", "0"], [520, 521, "DatasetName", "0"], [522, 523, "DatasetName", "0"], [529, 530, "DatasetName", "0"], [530, 531, "DatasetName", "0"], [531, 532, "DatasetName", "0"], [533, 534, "DatasetName", "0"], [535, 536, "DatasetName", "0"], [540, 541, "DatasetName", "0"], [541, 542, "DatasetName", "0"], [543, 544, "DatasetName", "0"], [549, 550, "DatasetName", "0"], [558, 559, "DatasetName", "0"], [562, 563, "DatasetName", "0"], [564, 565, "DatasetName", "0"], [577, 578, "DatasetName", "0"], [584, 585, "DatasetName", "0"], [590, 591, "DatasetName", "0"], [603, 604, "DatasetName", "0"], [610, 611, "DatasetName", "0"], [614, 615, "DatasetName", "0"], [616, 617, "DatasetName", "0"], [619, 620, "DatasetName", "0"], [624, 625, "DatasetName", "0"], [628, 629, "DatasetName", "0"], [630, 631, "DatasetName", "0"], [632, 633, "DatasetName", "0"], [633, 634, "DatasetName", "0"], [634, 635, "DatasetName", "0"], [639, 640, "DatasetName", "0"], [641, 642, "DatasetName", "0"], [645, 646, "DatasetName", "0"], [648, 649, "DatasetName", "0"], [649, 650, "DatasetName", "0"], [653, 654, "DatasetName", "0"], [655, 656, "DatasetName", "0"], [659, 660, "DatasetName", "0"], [660, 661, "DatasetName", "0"]]}
{"text": "Fully Quantized Transformer for Machine Translation", "entities": [[2, 3, "MethodName", "Transformer"], [4, 6, "TaskName", "Machine Translation"]]}
{"text": "State - of - the - art neural machine translation methods employ massive amounts of parameters .", "entities": [[8, 10, "TaskName", "machine translation"]]}
{"text": "To this end , we propose FullyQT : an allinclusive quantization strategy for the Transformer .", "entities": [[10, 11, "TaskName", "quantization"], [14, 15, "MethodName", "Transformer"]]}
{"text": "To the best of our knowledge , we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer .", "entities": [[20, 21, "MetricName", "loss"], [28, 29, "MethodName", "Transformer"]]}
{"text": "Comparing ourselves to all previously proposed methods , we achieve state - of - the - art quantization results .", "entities": [[17, 18, "TaskName", "quantization"]]}
{"text": "The idea of using neural networks for machine translation was only recently proposed ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al , 2014 ; .", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "A few variations to this additive attention mechanism have been proposed , such as multiplicative and self - attention ( Luong et al , 2015 ; Cheng et al , 2016 ; Lin et al , 2017 ) .", "entities": [[5, 7, "MethodName", "additive attention"]]}
{"text": "The latter formed the basis of the Transformer network ( Vaswani et al , 2017 ) , which achieved state - of - the - art results in machine translation .", "entities": [[7, 8, "MethodName", "Transformer"], [28, 30, "TaskName", "machine translation"]]}
{"text": "This method called quantization has the advantage of providing good compression rates with minimal loss in accuracy .", "entities": [[3, 4, "TaskName", "quantization"], [14, 15, "MetricName", "loss"], [16, 17, "MetricName", "accuracy"]]}
{"text": "Properly quantizing the Transformer would allow computational speed gains at inference , as well as deployment on more constrained devices .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "In this work , we propose a quantization - aware training strategy for the entire Transformer architecture .", "entities": [[7, 8, "TaskName", "quantization"], [15, 16, "MethodName", "Transformer"]]}
{"text": "Our method is easy to implement and results are consistent with the full - precision Transformer .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "We test our approach on multiple translation tasks such as WMT14 EN - FR and WMT14 EN - DE and obtain state - of - the - art quantization results .", "entities": [[10, 11, "DatasetName", "WMT14"], [15, 16, "DatasetName", "WMT14"], [28, 29, "TaskName", "quantization"]]}
{"text": "In comparison with full - precision , our quantized models score equal or higher BLEU on most tasks .", "entities": [[14, 15, "MetricName", "BLEU"]]}
{"text": "We are , to the best of our knowledge , the first to show that the Transformer architecture can be fully quantized without impairing translation quality .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "We also perform an ablation study and show that quantizing specific components of the Transformer improves BLEU score .", "entities": [[14, 15, "MethodName", "Transformer"], [16, 18, "MetricName", "BLEU score"]]}
{"text": "In this section , we review a broad spectrum of quantization and pruning methods for neural network compression .", "entities": [[10, 11, "TaskName", "quantization"], [15, 18, "TaskName", "neural network compression"]]}
{"text": "These include , among many others , binary , ternary ( Lin et al , 2015 ; , uniform ( Jacob et al , 2017 ) and learned ( Zhang et al , 2018 ) quantization .", "entities": [[35, 36, "TaskName", "quantization"]]}
{"text": "To maintain performance though , specific architectures usually require custom tailored quantization schemes .", "entities": [[11, 12, "TaskName", "quantization"]]}
{"text": "Several recent work explore recurrent neural network ( Jordan , 1990 ) quantization .", "entities": [[12, 13, "TaskName", "quantization"]]}
{"text": "Ott et al ( 2016 ) propose an exponential quantization method for RNN weights .", "entities": [[9, 10, "TaskName", "quantization"]]}
{"text": "They find ternary and exponential quantization to work well on language modeling and speech recognition , while binary weights seemed ineffective .", "entities": [[5, 6, "TaskName", "quantization"], [13, 15, "TaskName", "speech recognition"]]}
{"text": "Meanwhile , propose modifications to the gates and interlinks of quantized LSTM and GRU cells , as well as a balanced quantization method for weights .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "GRU"], [21, 22, "TaskName", "quantization"]]}
{"text": "Most recently , Wang et al ( 2018 ) propose applying different quantization methods for different RNN components .", "entities": [[12, 13, "TaskName", "quantization"]]}
{"text": "al ( 2014 ) compare matrix factorization , binarization , k - means clustering , product quantization and residual quantization of CNNs .", "entities": [[8, 9, "TaskName", "binarization"], [10, 14, "MethodName", "k - means clustering"], [16, 17, "TaskName", "quantization"], [19, 20, "TaskName", "quantization"]]}
{"text": "Wu et al ( 2015 ) apply quantization to both kernels and fully connected layers of convolutional neural networks .", "entities": [[7, 8, "TaskName", "quantization"]]}
{"text": "al ( 2016 ) propose using binary weighted filters on AlexNet ( Krizhevsky et al , 2012 ) .", "entities": [[10, 11, "MethodName", "AlexNet"]]}
{"text": "Testing their method on ImageNet , they show classification accuracy to be on par with fullprecision .", "entities": [[4, 5, "DatasetName", "ImageNet"], [9, 10, "MetricName", "accuracy"]]}
{"text": "Quantization has been applied in tandem with other compression methods .", "entities": [[0, 1, "TaskName", "Quantization"]]}
{"text": "Han et al ( 2015 ) combine pruning , quantization , weight sharing and Huffman coding .", "entities": [[9, 10, "TaskName", "quantization"]]}
{"text": "In another line of work , Polino et al ( 2018 ) employ quantization with knowledge distillation ( Hinton et al , 2015 ) for higher compression rates .", "entities": [[13, 14, "TaskName", "quantization"], [15, 17, "MethodName", "knowledge distillation"]]}
{"text": "Moreover , Chen et al ( 2018 ) blend quantization with block based low - rank matrix approximation of embeddings .", "entities": [[9, 10, "TaskName", "quantization"]]}
{"text": "The pruning of neural networks for model compression has also been largely explored .", "entities": [[6, 8, "TaskName", "model compression"]]}
{"text": "They apply a penalty in the loss on the \u03b3 parameters of batch normalization layers .", "entities": [[6, 7, "MetricName", "loss"], [9, 10, "HyperparameterName", "\u03b3"], [12, 14, "MethodName", "batch normalization"]]}
{"text": "With a similar objective , Narang et al ( 2017b ) make better use of hardware by applying pruning and weight decay in blocks to minimize the number of loaded weight matrix chunks .", "entities": [[20, 22, "MethodName", "weight decay"]]}
{"text": "Similarly to quantization , pruning methods have also been adapted to specific architectures .", "entities": [[2, 3, "TaskName", "quantization"]]}
{"text": "In order to maintain dimension consistency , Wen et al ( 2017 ) propose to prune all basic LSTM structures concurrently .", "entities": [[18, 19, "MethodName", "LSTM"]]}
{"text": "Our quantization scheme was chosen to be uniform , meaning that the step size between two quantized values is constant .", "entities": [[1, 2, "TaskName", "quantization"], [12, 14, "HyperparameterName", "step size"]]}
{"text": "If the performance with uniform quantization is already on par with fullprecision , then more weighty methods are unnecessary .", "entities": [[5, 6, "TaskName", "quantization"]]}
{"text": "A brief overview of uniform quantization is given in this section .", "entities": [[5, 6, "TaskName", "quantization"]]}
{"text": "Given an element x of a tensor X , we apply the quantization function Q : Q ( x )", "entities": [[12, 13, "TaskName", "quantization"]]}
{"text": "\u2212 x min s * s + x min ( 1 ) s = xmax \u2212 x min 2 k \u2212 1 ( 2 ) where x min and x max defines the endpoints of the quantization interval .", "entities": [[36, 37, "TaskName", "quantization"]]}
{"text": "When quantization is applied to weights , these values are respectively min ( X ) and max ( X ) .", "entities": [[1, 2, "TaskName", "quantization"]]}
{"text": "However , when quantization is applied to activations , those values are running estimates .", "entities": [[3, 4, "TaskName", "quantization"]]}
{"text": "For all other operations , such as sums , the computational cost added by the quantization operation outweighs the benefit of performing the operation with reduced precision .", "entities": [[15, 16, "TaskName", "quantization"]]}
{"text": "More precisely , we quantize all weights of the Transformer , excluding biases .", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "The \u03b3 weights of Layer - Norms are also quantized .", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "In the Multi - Head Attention , we quantize the ( Q , K , V ) input , the softmax 's numerator , the softmax 's denominator , the softmax 's output and the Scaled Dot - Product Attention 's output .", "entities": [[2, 6, "MethodName", "Multi - Head Attention"], [20, 21, "MethodName", "softmax"], [25, 26, "MethodName", "softmax"], [30, 31, "MethodName", "softmax"], [35, 40, "MethodName", "Scaled Dot - Product Attention"]]}
{"text": "At inference , the softmax does not need to be computed in full - precision .", "entities": [[4, 5, "MethodName", "softmax"]]}
{"text": "Furthermore , the added flexibility can greatly alleviate the precision loss resulting from all values being mapped to a single low numerical precision domain .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "For activations , we use bucketing when quantizing : the sum of input embeddings with the positional encoding , the Q , K , V inputs , the Scaled Dot - Product Attention 's output , the feed - forward 's output , the LayerNorm 's numerator , quotient and output .", "entities": [[28, 33, "MethodName", "Scaled Dot - Product Attention"]]}
{"text": "The only zero values which we have to deal with are the padding , the Softmax numerator and output , the output of ReLU layers and dropouts .", "entities": [[15, 16, "MethodName", "Softmax"], [23, 24, "MethodName", "ReLU"]]}
{"text": "For ReLUs and the Softmax 's numerator and output , we fix their x min to 0 , which guarantees the perfect mapping of the value .", "entities": [[4, 5, "MethodName", "Softmax"], [16, 17, "DatasetName", "0"]]}
{"text": "Finally , quantization is applied before any dropout operation .", "entities": [[2, 3, "TaskName", "quantization"]]}
{"text": "Indeed , even though the zeros added to the output of the quantization layer might not be part of the domain , this only happens during training .", "entities": [[12, 13, "TaskName", "quantization"]]}
{"text": "Recently , simple quantization solutions have been applied to the Transformer .", "entities": [[3, 4, "TaskName", "quantization"], [10, 11, "MethodName", "Transformer"]]}
{"text": "Cheong and Daniel ( 2019 ) apply k - means quantization and binarization with two centroids over the weights of the network .", "entities": [[10, 11, "TaskName", "quantization"], [12, 13, "TaskName", "binarization"]]}
{"text": "Similarly , Fan ( 2019 ) compares binary , 4 and 8 - bit uniform quantization of the Transformer weights .", "entities": [[15, 16, "TaskName", "quantization"], [18, 19, "MethodName", "Transformer"]]}
{"text": "Achieving quantization of both weights and activations is much more beneficial .", "entities": [[1, 2, "TaskName", "quantization"]]}
{"text": "The first attempt at doing so for the Transformer applies 8 - bit quantization on weights and inputs of feed forward layers and binarizes the ( Q , K ) input of the Multi - Head Attention ( Tierno , 2019 ) .", "entities": [[8, 9, "MethodName", "Transformer"], [13, 14, "TaskName", "quantization"], [33, 37, "MethodName", "Multi - Head Attention"]]}
{"text": "The method resulted in a huge drop in translation accuracy .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "Achieving better performance , Bhandare et al ( 2019 ) quantize certain MatMul operations and use the KL divergence to estimate the most suited parameters for each quantization range .", "entities": [[27, 28, "TaskName", "quantization"]]}
{"text": "They restrain from quantizing all MatMuls , reporting poorer results in accuracy .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "Aside from translation , the concurrent work by Zafrir et al ( 2019 ) quantizes the embedding and fully connected layers of BERT ( Devlin et al , 2018 ) .", "entities": [[22, 23, "MethodName", "BERT"]]}
{"text": "The Softmax and LayerNorm operations are kept in full - precision .", "entities": [[1, 2, "MethodName", "Softmax"]]}
{"text": "On the GLUE benchmark , their loss in accuracy is minimal compared to the original model .", "entities": [[2, 3, "DatasetName", "GLUE"], [6, 7, "MetricName", "loss"], [8, 9, "MetricName", "accuracy"]]}
{"text": "All of these methods omit quantizing the whole Transformer architecture , resulting in suboptimal computational efficiency .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "In this section , we present the results of our full quantization scheme on various tasks .", "entities": [[11, 12, "TaskName", "quantization"]]}
{"text": "We first compare our method on a machine translation setup .", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "We also compare the impact of delaying quantization on translation quality .", "entities": [[7, 8, "TaskName", "quantization"]]}
{"text": "We apply our quantization strategy on both the base and big Transformer ( Vaswani et al , 2017 ) .", "entities": [[3, 4, "TaskName", "quantization"], [11, 12, "MethodName", "Transformer"]]}
{"text": "Our models were first evaluated on the WMT 2014 / 2017 English - to - German and WMT 2014", "entities": [[7, 9, "DatasetName", "WMT 2014"], [17, 19, "DatasetName", "WMT 2014"]]}
{"text": "We compare our results with the original Transformer and other 8 - bit quantization methods in Table 1 .", "entities": [[7, 8, "MethodName", "Transformer"], [13, 14, "TaskName", "quantization"]]}
{"text": "We also evaluate two other quantization approaches .", "entities": [[5, 6, "TaskName", "quantization"]]}
{"text": "The second approach applies our quantization strategy post - training ( see section 5.3 ) .", "entities": [[5, 6, "TaskName", "quantization"]]}
{"text": "In all cases except for post - quantization , BLEU was computed on the test set using the checkpoint which scored the highest accuracy on the validation set .", "entities": [[7, 8, "TaskName", "quantization"], [9, 10, "MetricName", "BLEU"], [23, 24, "MetricName", "accuracy"]]}
{"text": "Training with quantization was about twice as slow as with the baselines .", "entities": [[2, 3, "TaskName", "quantization"]]}
{"text": "All models were trained following the same setup as WMT14 EN - FR and WMT14 EN - DE .", "entities": [[9, 10, "DatasetName", "WMT14"], [14, 15, "DatasetName", "WMT14"]]}
{"text": "Since there is no test set for WMT14 ES - EN , we used the validation set as a test set and omitted computing any validation epochs during training .", "entities": [[7, 8, "DatasetName", "WMT14"]]}
{"text": "Most of the time , the highest BLEU was scored by the quantized model .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "We looked at training and validation curves to see if quantization had any effect , but saw no discernible difference .", "entities": [[10, 11, "TaskName", "quantization"]]}
{"text": "Although 6 - bit quantization seems to perform well , the compression advantage over 8 - bit is usually lost .", "entities": [[4, 5, "TaskName", "quantization"]]}
{"text": "Unless 6 - bit quantization results in better models , 8 - bit seems like the best choice for most hardware .", "entities": [[4, 5, "TaskName", "quantization"]]}
{"text": "To better understand which operations are more sensitive to quantization , we evaluate such effect on single operations of the Transformer .", "entities": [[9, 10, "TaskName", "quantization"], [20, 21, "MethodName", "Transformer"]]}
{"text": "By this , we mean quantizing the operation of a module for all Transformer layers .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "Table 4 shows results on the WMT14 EN - FR translation task for 8 - bit precision .", "entities": [[6, 7, "DatasetName", "WMT14"]]}
{"text": "BLEU was computed on the test set after 100k steps of training .", "entities": [[0, 1, "MetricName", "BLEU"]]}
{"text": "To successfully quantize this element without causing any loss in performance , we suspect quantizing other elements in the network helps .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "To further validate our quantization scheme , we evaluated four models trained with alterations to our design choices .", "entities": [[4, 5, "TaskName", "quantization"]]}
{"text": "Results on the WMT14 EN - FR task are presented in Table 5 .", "entities": [[3, 4, "DatasetName", "WMT14"]]}
{"text": "Our method 's goal is to increase computational efficiency when inferring with the Transformer .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "To this end , our quantization scheme only requires us to learn s and x min .", "entities": [[5, 6, "TaskName", "quantization"]]}
{"text": "Quantization could also be applied later during training .", "entities": [[0, 1, "TaskName", "Quantization"]]}
{"text": "All models were evaluated on the WMT14 EN - DE and WMT14 EN - FR translation tasks .", "entities": [[6, 7, "DatasetName", "WMT14"], [11, 12, "DatasetName", "WMT14"]]}
{"text": "BLEU was measured on the test set using the checkpoint which scored the highest accuracy on the validation set during training .", "entities": [[0, 1, "MetricName", "BLEU"], [14, 15, "MetricName", "accuracy"]]}
{"text": "Learning quantization parameters adds a significant computational cost during training .", "entities": [[1, 2, "TaskName", "quantization"]]}
{"text": "A major advantage to delaying quantization is to perform more training steps in the same given amount of time .", "entities": [[5, 6, "TaskName", "quantization"]]}
{"text": "Therefore , when training time is a constraint , a possible strategy is to train a model without quantization , perform more training steps and finally post - quantize the model .", "entities": [[18, 19, "TaskName", "quantization"]]}
{"text": "To evaluate if our quantization scheme generalizes well to other tasks , we evaluate it on two language modeling datasets : WikiText - 2 and WikiText - 103 .", "entities": [[4, 5, "TaskName", "quantization"]]}
{"text": "We trained four Transformer models , each with different precision .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "All models consist of two Transformer encoder layers , with the embedding and hidden size set to 200 .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "Multi - Head Attention has two heads with key and value size 64 .", "entities": [[0, 4, "MethodName", "Multi - Head Attention"]]}
{"text": "We experiment with node pruning our Transformer models .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "By useless , we mean nodes which do not cause any loss in translation quality when removed .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "The only nodes of the Transformer which can be removed without causing alterations to other components of the network are the nodes in between the two layers of each feed - forward network .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "In the case of the base Transformer , for a respective vocabulary of size 37000 and 32000 , 39.96 % and 41.65 % of the total weights are owned by the feed - foward networks .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "This number grows to 47.03 % and 48.18 % in the big Transformer .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "To evaluate which nodes can be safely pruned without affecting translation quality , we estimate x max for each node of the ReLU output over a few hundred steps .", "entities": [[22, 23, "MethodName", "ReLU"]]}
{"text": "These x max are computed before quantizing the ReLU output and do not replace the ones used by the quantization process .", "entities": [[8, 9, "MethodName", "ReLU"], [19, 20, "TaskName", "quantization"]]}
{"text": "Figure 3 in the appendix shows the histogram of these running estimates for one ReLU layer in the encoder and one in the decoder .", "entities": [[14, 15, "MethodName", "ReLU"]]}
{"text": "All other ReLU layers share the same pattern , where in the encoder there are always multiple x max close to 0 .", "entities": [[2, 3, "MethodName", "ReLU"], [21, 22, "DatasetName", "0"]]}
{"text": "Using this method , we can further compress the Transformer without affecting BLEU scores .", "entities": [[9, 10, "MethodName", "Transformer"], [12, 13, "MetricName", "BLEU"]]}
{"text": "For example , in the case of the big Transformer trained on WMT14 EN - FR , 169 nodes were pruned in the first ReLU of the encoder , while in the second , 1226 were pruned .", "entities": [[9, 10, "MethodName", "Transformer"], [12, 13, "DatasetName", "WMT14"], [24, 25, "MethodName", "ReLU"]]}
{"text": "BLEU varied by about 0.01\u22120.02 .", "entities": [[0, 1, "MetricName", "BLEU"]]}
{"text": "We proposed a full quantization strategy for the Transformer architecture .", "entities": [[4, 5, "TaskName", "quantization"], [8, 9, "MethodName", "Transformer"]]}
{"text": "With FullyQT , we achieve higher BLEU scores than all other quantization methods for the Transformer on multiple translation tasks and avoid any loss in BLEU compared to full - precision .", "entities": [[6, 7, "MetricName", "BLEU"], [11, 12, "TaskName", "quantization"], [15, 16, "MethodName", "Transformer"], [23, 24, "MetricName", "loss"], [25, 26, "MetricName", "BLEU"]]}
{"text": "Specifically , out of 35 experiments , 8 - bit quantization performed better than full - precision in 21 cases .", "entities": [[10, 11, "TaskName", "quantization"]]}
{"text": "If instead of minimizing inference time , one wants to maximize translation accuracy , then applying quantization to only certain components of the Transformer seems to be the best option .", "entities": [[12, 13, "MetricName", "accuracy"], [16, 17, "TaskName", "quantization"], [23, 24, "MethodName", "Transformer"]]}
{"text": "Indeed , our ablation study showed than BLEU score could increase even more when only specific elements of the Transformer were quantized .", "entities": [[7, 9, "MetricName", "BLEU score"], [19, 20, "MethodName", "Transformer"]]}
{"text": "We plan on extending our work to variations of the Transformer , as well as further exploring the compression of these networks .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "We make the following two assumptions about the way underspecified terms are interpreted in the course of semantic composition : ( 20 ) a. Ban on the duplication of underspecified terms : In a well - formed semantic representation of DTS , an underspecified @ - term with the same index can appear at most once .", "entities": [[17, 19, "TaskName", "semantic composition"]]}
{"text": "b. Normal form requirement on compositionally derived semantic terms : At each step of semantic composition , the semantic term assigned to the derived linguistic expression is in \u03b2 - normal form .", "entities": [[14, 16, "TaskName", "semantic composition"], [28, 29, "HyperparameterName", "\u03b2"]]}
{"text": "The effect in a nutshell is that , via type - lifting , we can ensure enough of the ' derivational structure ' of the sentence to be present in the ( beta - unreduced ) semantic translation to identify the ' possible binder ' of the pronoun before all the material is actually composed in the ( surface ) syntax .", "entities": [[32, 33, "HyperparameterName", "beta"]]}
{"text": "Since \u03b2 - reduction is prohibited before underspecification resolution , type checking for the underspecified term searches for an appropriate antecedent in the global context ( consisting of the previous linguistic discourse and extra - linguistic information ) .", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b2 ( t : sax * )", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b2 ( v : ( u : boy * ) admire ( # sax * )", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "To see why this requirement is needed , assume that no \u03b2 - reduction takes place in the course of the derivataion , and , ( as above ) that once the semantic representation for the whole sentence is obtained , there is no restriction on the order of \u03b2 - reduction and underspecification resolution for # terms .", "entities": [[11, 12, "HyperparameterName", "\u03b2"], [49, 50, "HyperparameterName", "\u03b2"]]}
{"text": "z and P can take place in any order , and the relative scope between the subject indefinites and the object universal depends on the order of application of \u03b2 - conversion and underspecification resolution for the two terms # boy and # girl .", "entities": [[29, 30, "HyperparameterName", "\u03b2"]]}
{"text": "TOD - BERT : Pre - trained Natural Language Understanding for Task - Oriented Dialogue", "entities": [[2, 3, "MethodName", "BERT"], [7, 10, "TaskName", "Natural Language Understanding"]]}
{"text": "To better model dialogue behavior during pre - training , we incorporate user and system tokens into the masked language modeling .", "entities": [[18, 21, "TaskName", "masked language modeling"]]}
{"text": "Our pre - trained task - oriented dialogue BERT ( TOD - BERT ) outperforms strong baselines like BERT on four downstream taskoriented dialogue applications , including intention recognition , dialogue state tracking , dialogue act prediction , and response selection .", "entities": [[8, 9, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [30, 33, "TaskName", "dialogue state tracking"]]}
{"text": "We also show that TOD - BERT has a stronger few - shot ability that can mitigate the data scarcity problem for task - oriented dialogue .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "By further fine - tuning these representations , breakthroughs have been continuously reported for various downstream tasks , especially natural language understanding .", "entities": [[19, 22, "TaskName", "natural language understanding"]]}
{"text": "Therefore , pre - training dialogue language models using chit - chat corpora from social media , such as Twitter or Reddit , has been recently investigated , especially for dialogue response generation ( Zhang et al , 2019 ) and retrieval ( Henderson et al , 2019b ) .", "entities": [[21, 22, "DatasetName", "Reddit"], [31, 33, "TaskName", "response generation"]]}
{"text": "We collect and combine nine human - human and multi - turn task - oriented dialogue corpora to train a task - oriented dialogue BERT ( TOD - BERT ) .", "entities": [[24, 25, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"]]}
{"text": "Like BERT ( Devlin et al , 2018 ) , TOD - BERT is formulated as a masked language model and uses a deep bidirectional Transformer ( Vaswani et al , 2017 ) encoder as its model architecture .", "entities": [[1, 2, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"], [25, 26, "MethodName", "Transformer"]]}
{"text": "Unlike BERT , TOD - BERT incorporates two special tokens for user and system to model the corresponding dialogue behavior .", "entities": [[1, 2, "MethodName", "BERT"], [5, 6, "MethodName", "BERT"]]}
{"text": "We select BERT because it is the most widely used model in NLP research recently , and our unified datasets can be easily applied to pre - train any existing language models .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "We test TOD - BERT on task - oriented dialogue systems on four core downstream tasks , including intention recognition , dialogue state tracking , dialogue act prediction , and response selection .", "entities": [[4, 5, "MethodName", "BERT"], [6, 11, "TaskName", "task - oriented dialogue systems"], [21, 24, "TaskName", "dialogue state tracking"]]}
{"text": "What we observe is : TOD - BERT outperforms BERT and other strong baselines such as GPT - 2 ( Radford et al , 2019 ) and DialoGPT ( Zhang et al , 2019 ) on all the selected downstream tasks , which further confirms its effectiveness for improving dialogue language understanding .", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"], [16, 17, "MethodName", "GPT"]]}
{"text": "We find that response contrastive learning is beneficial , but it is currently overlooked not well - investigated in dialogue pretraining research .", "entities": [[4, 6, "MethodName", "contrastive learning"]]}
{"text": "More importantly , TOD - BERT has a stronger few - shot ability than BERT on each task , suggesting that it can reduce the need for expensive human - annotated labels .", "entities": [[5, 6, "MethodName", "BERT"], [14, 15, "MethodName", "BERT"]]}
{"text": "TOD - BERT can be easily leveraged and adapted to a new taskoriented dialogue dataset .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "General Pre - trained Language Models , which are trained on massive general text such as Wikipedia and BookCorpus , can be roughly divided into two categories : uni - directional or bidirectional attention mechanisms .", "entities": [[0, 1, "DatasetName", "General"], [18, 19, "DatasetName", "BookCorpus"]]}
{"text": "GPT ( Radford et al , 2018 ) and GPT - 2 ( Radford et al , 2019 ) are representatives of uni - directional language models using a Transformer decoder , where the objective is to maximize left - to - right generation likelihood .", "entities": [[0, 1, "MethodName", "GPT"], [9, 10, "MethodName", "GPT"], [29, 31, "MethodName", "Transformer decoder"]]}
{"text": "On the other hand , BERT ( Devlin et al , 2018 ) , RoBERTa , and their variances are pre - trained using a Transformer encoder with bi - directional token prediction .", "entities": [[5, 6, "MethodName", "BERT"], [14, 15, "MethodName", "RoBERTa"], [25, 26, "MethodName", "Transformer"]]}
{"text": "These models are usually evaluated on classification tasks such as GLUE benchmark ( Wang et al , 2018 ) or span - based question answering tasks ( Ra - 1 github.com/jasonwu0731/ToD - BERT jpurkar et al , 2016 ) .", "entities": [[10, 11, "DatasetName", "GLUE"], [23, 25, "TaskName", "question answering"], [32, 33, "MethodName", "BERT"]]}
{"text": "For example , CTRL ( Keskar et al , 2019 ) is a conditional Transformer model , trained to condition on control codes that govern style , content , and task - specific behavior .", "entities": [[3, 4, "MethodName", "CTRL"], [14, 15, "MethodName", "Transformer"]]}
{"text": "Text - to - text Transformer ( T5 ) ( Raffel et al , 2019 ) unifies multiple text modeling tasks and achieves the promising results in various NLP benchmarks .", "entities": [[5, 6, "MethodName", "Transformer"], [7, 8, "MethodName", "T5"]]}
{"text": "Dialogue Pre - trained Language Models are mostly trained on open - domain conversational data from Reddit or Twitter for dialogue response generation .", "entities": [[16, 17, "DatasetName", "Reddit"], [21, 23, "TaskName", "response generation"]]}
{"text": "Transfertransfo ( Wolf et al , 2019 ) achieves good performance on ConvAI - 2 dialogue competition using GPT - 2 . DialoGPT ( Zhang et al , 2019 ) is an extension of GPT - 2 that is pre - trained on Reddit data for open - domain response generation .", "entities": [[18, 19, "MethodName", "GPT"], [34, 35, "MethodName", "GPT"], [43, 44, "DatasetName", "Reddit"], [49, 51, "TaskName", "response generation"]]}
{"text": "Con - veRT ( Henderson et al , 2019a ) pre - trained a dual transformer encoder for response selection task on large - scale Reddit ( input , response ) pairs .", "entities": [[25, 26, "DatasetName", "Reddit"]]}
{"text": "PLATO ( Bao et al , 2019 ) uses both Twitter and Reddit data to pre - trained a dialogue generation model with discrete latent variables .", "entities": [[12, 13, "DatasetName", "Reddit"], [19, 21, "TaskName", "dialogue generation"]]}
{"text": "All of them are designed to cope with the response generation task for opendomain chatbots .", "entities": [[9, 11, "TaskName", "response generation"]]}
{"text": "Budzianowski and Vuli\u0107 ( 2019 ) first apply the GPT - 2 model to train on response generation task , which takes system belief , database result , and last dialogue turn as input to predict next system responses .", "entities": [[9, 10, "MethodName", "GPT"], [16, 18, "TaskName", "response generation"]]}
{"text": "They first pre - train on Reddit corpora and then fine - tune on target dialogue domains , but their training and fine - tuning code is not released .", "entities": [[6, 7, "DatasetName", "Reddit"]]}
{"text": "Pre - training on a set of annotated NLG corpora can improve conditional generation quality using a GPT - 2 model .", "entities": [[17, 18, "MethodName", "GPT"]]}
{"text": "MetaLWOZ 37 , 884 432 , 036 11.4 47 Schema ( Rastogi et al , 2019 ) 22 , 825 463 , 284 20.3 17 Taskmaster ( Byrne et al , 2019 ) 13 , 215 303 , 066 22.9 6 MWOZ ( Budzianowski et al , 2018 ) 10 , 420 71 , 410 6.9 7 MSR - E2E 10 , 087 74 , 686 7.4 3 SMD ( Eric and Manning , 2017 ) 3 , 031 15 , 928 5.3 3 Frames ( Asri et al , 2017 ) 1 , 369 19 , 986 14.6 3 WOZ ( Mrk\u0161i\u0107 et al , 2016 ) 1 , 200 5 , 012 4.2 1 CamRest676 676 2 , 744 4.1 1", "entities": [[0, 1, "DatasetName", "MetaLWOZ"], [59, 60, "DatasetName", "E2E"], [68, 69, "DatasetName", "SMD"]]}
{"text": "Schema ( Rastogi et al , 2019 ) : Schema - guided dialogue has 22 , 825 dialogues and provides a challenging testbed for several tasks , in particular , dialogue state tracking .", "entities": [[30, 33, "TaskName", "dialogue state tracking"]]}
{"text": "The Schema dataset is used as the dialogue state tracking task for DSTC8 dialogue competition .", "entities": [[7, 10, "TaskName", "dialogue state tracking"]]}
{"text": "MWOZ ( Budzianowski et al , 2018 ) : Multi - Domain Wizard - of - Oz dataset contains 10 , 420 dialogues over seven domains , and it has multiple domains in a single dialogue .", "entities": [[12, 17, "DatasetName", "Wizard - of - Oz"]]}
{"text": "MSR - E2E : Microsoft end - toend dialogue challenge has 10 , 087 dialogues in three domains , movie - ticket booking , restaurant reservation , and taxi booking .", "entities": [[2, 3, "DatasetName", "E2E"]]}
{"text": "SMD ( Eric and Manning , 2017 ) :", "entities": [[0, 1, "DatasetName", "SMD"]]}
{"text": "Stanford multidomain dialogue is an in - car personal assistant dataset , comprising 3 , 301 dialogues and three domains : calendar scheduling , weather information retrieval , and point - of - interest navigation .", "entities": [[25, 27, "TaskName", "information retrieval"]]}
{"text": "These two corpora use the same data collection procedure and same ontology from DSTC2 ( Henderson et al , 2014 ) .", "entities": [[11, 12, "MethodName", "ontology"]]}
{"text": "They are one of the first task - oriented dialogue datasets that use Wizard of Oz style with text input instead of speech input , which improves the model 's capacity for the semantic understanding instead of its robustness to automatic speech recognition errors .", "entities": [[40, 43, "TaskName", "automatic speech recognition"]]}
{"text": "We train our TOD - BERT based on BERT architecture using two loss functions : masked language modeling ( MLM ) loss and response contrastive loss ( RCL ) .", "entities": [[5, 6, "MethodName", "BERT"], [8, 9, "MethodName", "BERT"], [12, 13, "MetricName", "loss"], [15, 18, "TaskName", "masked language modeling"], [19, 20, "DatasetName", "MLM"], [21, 22, "MetricName", "loss"], [25, 26, "MetricName", "loss"]]}
{"text": "Note that the datasets we used can be used to pre - train any existing language model architecture , and here we select BERT because it is the most widely used model in NLP research .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "We use the BERT - base uncased model , which is a transformer self - attention encoder ( Vaswani et al , 2017 ) with 12 layers and 12 attention heads with its hidden size d B = 768 .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Masked language modeling is a common pretraining strategy for BERT - like architectures , in which a random sample of tokens in the input sequence is selected and replaced with the special token [ MASK ] .", "entities": [[0, 3, "TaskName", "Masked language modeling"], [9, 10, "MethodName", "BERT"]]}
{"text": "The MLM loss function is the crossentropy loss on predicting the masked tokens .", "entities": [[1, 2, "DatasetName", "MLM"], [2, 3, "MetricName", "loss"], [7, 8, "MetricName", "loss"]]}
{"text": "TOD - BERT is initialized from BERT , a good starting parameter set , then is further pre - trained on those task - oriented corpora .", "entities": [[2, 3, "MethodName", "BERT"], [6, 7, "MethodName", "BERT"]]}
{"text": "The MLM loss function is L mlm", "entities": [[1, 2, "DatasetName", "MLM"], [2, 3, "MetricName", "loss"], [6, 7, "DatasetName", "mlm"]]}
{"text": "Response contrastive loss can also be used for dialogue language modeling since it does not require any additional human annotation .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "Unlike the original next sentence prediction ( NSP ) objective in BERT pre - training , which concatenates two segments A and B to predict whether they are consecutive text with binary classification , we apply a dual - encoder approach ( Henderson et al , 2019a ) and simulate multiple negative samples .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "We use TOD - BERT to encode all the contexts and their corresponding responses separately .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "i , i , M = Softmax ( CR T )", "entities": [[6, 7, "MethodName", "Softmax"]]}
{"text": "The Softmax function normalizes the vector per row .", "entities": [[1, 2, "MethodName", "Softmax"]]}
{"text": "In our setting , increasing batch size also means changing the positive and negative ratio in the contrastive learning .", "entities": [[5, 7, "HyperparameterName", "batch size"], [17, 19, "MethodName", "contrastive learning"]]}
{"text": "Batch size is a hyper - parameter that may be limited by hardware .", "entities": [[0, 2, "HyperparameterName", "Batch size"]]}
{"text": "Overall pre - training loss function is the weighted - sum of L mlm and L rcl , and in our experiments , we simply sum them up .", "entities": [[4, 5, "MetricName", "loss"], [13, 14, "DatasetName", "mlm"]]}
{"text": "We gradually reduce the learning rate without a warm - up period .", "entities": [[4, 6, "HyperparameterName", "learning rate"]]}
{"text": "GELU activation functions ( Hendrycks and Gimpel , 2016 ) is used .", "entities": [[0, 1, "MethodName", "GELU"]]}
{"text": "We care the most in this paper whether TOD - BERT , a pre - trained language model using aggregated taskoriented corpora , can show any advantage over BERT .", "entities": [[10, 11, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"]]}
{"text": "Also , we use the same architecture with a similar number of parameters for a fair comparison .", "entities": [[10, 13, "HyperparameterName", "number of parameters"]]}
{"text": "All the model parameters are updated with a gradient clipping to 1.0 using the same hyper - parameters during finetuning .", "entities": [[8, 10, "MethodName", "gradient clipping"]]}
{"text": "We select four crucial task - oriented downstream tasks to evaluate : intent recognition , dialogue state tracking , dialogue act prediction , and response selection .", "entities": [[12, 14, "TaskName", "intent recognition"], [15, 18, "TaskName", "dialogue state tracking"]]}
{"text": "Intent recognition task is a multi - class classification problem , where we input a sentence U and models predict one single intent class over I possible intents .", "entities": [[0, 2, "TaskName", "Intent recognition"], [5, 9, "TaskName", "multi - class classification"]]}
{"text": "P int = Softmax ( W 1 ( F ( U ) ) )", "entities": [[3, 4, "MethodName", "Softmax"]]}
{"text": "The model is trained with cross - entropy loss between the predicted distributions P int and the true intent labels .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "Dialogue state tracking can be treated as a multi - class classification problem using a predefined ontology .", "entities": [[0, 3, "TaskName", "Dialogue state tracking"], [8, 12, "TaskName", "multi - class classification"], [16, 17, "MethodName", "ontology"]]}
{"text": "G j is the slot projection layer of the j slot , and the number of layers | G | is equal to the number of ( domain , slot ) pairs .", "entities": [[14, 17, "HyperparameterName", "number of layers"]]}
{"text": "The model is trained with cross - entropy loss summed over all the pairs .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "Dialogue act prediction is a multi - label classification problem because a system response may contain multiple dialogue acts , e.g. , request and inform at the same time .", "entities": [[5, 9, "TaskName", "multi - label classification"]]}
{"text": "R N , ( 5 ) where W 2 R d B \u00d7N is a trainable linear mapping , N is the number of possible dialogue acts , and each value in A is between [ 0 , 1 ] after a Sigmoid layer .", "entities": [[36, 37, "DatasetName", "0"]]}
{"text": "MWOZ is the most common benchmark for task - oriented dialogues , especially for dialogue state tracking .", "entities": [[14, 17, "TaskName", "dialogue state tracking"]]}
{"text": "For each downstream task , we first conduct the experiments using the whole dataset , and then we simulate the few - shot setting to show the strength of our TOD - BERT .", "entities": [[32, 33, "MethodName", "BERT"]]}
{"text": "We run at least three times with different random seeds for each few - shot experiment to reduce data sampling variance , and we report its mean and standard deviation for these limited data scenarios .", "entities": [[9, 10, "DatasetName", "seeds"]]}
{"text": "We investigate two versions of TOD - BERT ; one is TOD - BERT - mlm that only uses MLM loss during pre - training , and the other is TOD - BERT - jnt , which is jointly trained with the MLM and RCL objectives .", "entities": [[7, 8, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"], [15, 16, "DatasetName", "mlm"], [19, 20, "DatasetName", "MLM"], [20, 21, "MetricName", "loss"], [32, 33, "MethodName", "BERT"], [42, 43, "DatasetName", "MLM"]]}
{"text": "We compare TOD - BERT with BERT and other baselines , including two other strong pre - training models GPT - 2 ( Radford et al , 2019 ) and DialoGPT ( Zhang et al , 2019 ) .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "BERT"], [19, 20, "MethodName", "GPT"]]}
{"text": "For a GPT - based model , we use mean pooling of its hidden states as its output representation , which we found it is better than using only the last token .", "entities": [[2, 3, "MethodName", "GPT"]]}
{"text": "TOD - BERT - jnt achieves the highest performance in this setting , suggesting its representation contains the most useful information .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "We evaluate accuracy on all the data , the in - domain intents only , and the out - of - scope intent only .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "TOD - BERTjnt achieves the highest in - scope and out - of - scope accuracy .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "Two evaluation metrics are commonly used in dialogue state tracking task : joint goal accuracy and slot accuracy .", "entities": [[7, 10, "TaskName", "dialogue state tracking"], [14, 15, "MetricName", "accuracy"], [17, 18, "MetricName", "accuracy"]]}
{"text": "The joint goal accuracy compares the predicted dialogue states to the ground truth at each dialogue turn .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "On the other hand , the slot accuracy individually compares each ( domain , slot , value ) triplet to its ground truth label .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Since the original ontology provided by Budzianowski et al ( 2018 ) is not complete ( some labeled values are not included in the ontology ) , we create a new ontology of all the possible annotated values .", "entities": [[3, 4, "MethodName", "ontology"], [24, 25, "MethodName", "ontology"], [31, 32, "MethodName", "ontology"]]}
{"text": "TOD - BERT outperforms BERT in all the setting , which further show the strength of task - oriented dialogue pre - training .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "BERT"]]}
{"text": "We conduct experiments on three different datasets and report micro - F1 and macro - F1 scores for the dialogue act prediction task , a multi - label classification problem .", "entities": [[9, 12, "MetricName", "micro - F1"], [13, 16, "MetricName", "macro - F1"], [25, 29, "TaskName", "multi - label classification"]]}
{"text": "The MLP model simply takes bag - of - word embeddings to make dialogue act prediction , and the RNN model is a bi - directional GRU network .", "entities": [[1, 2, "DatasetName", "MLP"], [9, 11, "TaskName", "word embeddings"], [26, 27, "MethodName", "GRU"]]}
{"text": "In Table 4 , one can observe that in full data scenario , TOD - BERT consistently works better than BERT and other baselines , no matter which datasets or which evaluation metrics .", "entities": [[15, 16, "MethodName", "BERT"], [20, 21, "MethodName", "BERT"]]}
{"text": "The k - of - 100 MWOZ DSTC2 GSIM 1 - to - 100 3 - to - 100 1 - to - 100 3 - to - 100 1 - to - 100 3 - to - 100 1 % Data BERT 7.8 % \u00b1 2.0 % 20.5 % \u00b1 4.4 % 3.7 % \u00b1 0.6 % 9.6 % \u00b1 1.3 % 4.0 % \u00b1 0.4 % 10.3 % \u00b1 1.1 % TOD - BERT - mlm 13.0 % \u00b1 1.1 % 34.6 % \u00b1 0.4 % 12.5 % \u00b1 6.7 % 24.9 % \u00b1 10.7 % 7.2 % \u00b1 4.0 % 15.4 % \u00b1 8.0 % TOD - BERT - jnt - - 37.5 % \u00b1 0.6 % 55.9 % \u00b1 0.4 % 12.5 % \u00b1 0.9 % 26.8 % \u00b1 0.8 % 10 % Data BERT 20.9 % \u00b1 2.6 % 45.4 % \u00b1 3.8 % 8.9 % \u00b1 2.3 % 21.4 % \u00b1 3.1 % 9.8 % \u00b1 0.1 % 24.4 % \u00b1 1.2 % TOD - BERT - mlm 22.3 % \u00b1 3.2 % 48.7 % \u00b1 4.0 % 19.0 % \u00b1 16.3 % 33.8 % \u00b1 20.4 % 11.2 % \u00b1 2.5 % 26.0 % \u00b1 2.7 % TOD - BERT - jnt - - 49.7 % \u00b1 0.3 % 66.6 % \u00b1 0.1 % 23.0 % \u00b1 1.0 % 42.6 % \u00b1 1.0 % Table 6 : Response selection evaluation results on three corpora for 1 % , 10 % and full data setting .", "entities": [[42, 43, "MethodName", "BERT"], [75, 76, "MethodName", "BERT"], [77, 78, "DatasetName", "mlm"], [110, 111, "MethodName", "BERT"], [138, 139, "MethodName", "BERT"], [171, 172, "MethodName", "BERT"], [173, 174, "DatasetName", "mlm"], [206, 207, "MethodName", "BERT"]]}
{"text": "During inference , we run five different random seeds to sample batches and report the average results .", "entities": [[8, 9, "DatasetName", "seeds"]]}
{"text": "The similar results are also consistently observed in DSTC2 and GSIM datasets , and the advantage of the TOD - BERT - jnt is more evident in the few - shot scenario .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "We do not report TOD - BERT - jnt for MWOZ few - shot setting because it is not fair to compare them with others as the full MWOZ training set is used for response contrastive learning during pre - training stage .", "entities": [[6, 7, "MethodName", "BERT"], [35, 37, "MethodName", "contrastive learning"]]}
{"text": "The response selection results are sensitive to the training batch size since the larger the batch size the harder the prediction .", "entities": [[9, 11, "HyperparameterName", "batch size"], [15, 17, "HyperparameterName", "batch size"]]}
{"text": "In Figure 2 , we visualize the embeddings of BERT , TOD - BERT - mlm , and TOD - BERT - jnt given the same input from the MWOZ test set .", "entities": [[9, 10, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"], [15, 16, "DatasetName", "mlm"], [20, 21, "MethodName", "BERT"]]}
{"text": "As one can observe , TOD - BERT - jnt has more clear group boundaries than TOD - BERT - mlm , and two of them are better than BERT .", "entities": [[7, 8, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [20, 21, "DatasetName", "mlm"], [29, 30, "MethodName", "BERT"]]}
{"text": "To analyze the results quantitatively , we run Kmeans , a common unsupervised clustering algorithms , on top of the output embeddings of BERT and TOD - BERT .", "entities": [[23, 24, "MethodName", "BERT"], [27, 28, "MethodName", "BERT"]]}
{"text": "We then compute the normalized mutual information ( NMI ) between the clustering result and the actual domain label for each utterance .", "entities": [[8, 9, "MetricName", "NMI"]]}
{"text": "Here is what we observe : TOD - BERT consistently achieves higher NMI scores than BERT .", "entities": [[8, 9, "MethodName", "BERT"], [12, 13, "MetricName", "NMI"], [15, 16, "MethodName", "BERT"]]}
{"text": "We propose task - oriented dialogue BERT ( TOD - BERT ) trained on nine human - human and multiturn task - oriented datasets across over 60 domains .", "entities": [[6, 7, "MethodName", "BERT"], [10, 11, "MethodName", "BERT"]]}
{"text": "TOD - BERT outperforms BERT on four dialogue downstream tasks , including intention classification , dialogue state tracking , dialogue act prediction , and response selection .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "BERT"], [15, 18, "TaskName", "dialogue state tracking"]]}
{"text": "TOD - BERT is easy - to - deploy and will be open - sourced , allowing the NLP research community to apply or fine - tune any task - oriented conversational problem .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "We further analyze the necessity of each component for the Turing - completeness of the network ; interestingly , we find that a particular type of residual connection is necessary .", "entities": [[26, 28, "MethodName", "residual connection"]]}
{"text": "We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks .", "entities": [[11, 13, "TaskName", "machine translation"]]}
{"text": "Transformer ( Vaswani et al , 2017 ) is a recent selfattention based sequence - to - sequence architecture which has led to state of the art results across various NLP tasks including machine translation ( Ott et al , 2018 ) , language modeling ( Radford et al , 2018 ) and question answering ( Devlin et al , 2019 ) .", "entities": [[0, 1, "MethodName", "Transformer"], [33, 35, "TaskName", "machine translation"], [53, 55, "TaskName", "question answering"]]}
{"text": "The role of various components of the Transformer in its efficacy is an important question for further improvements .", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "Since the Transformer does not process the input sequentially , it requires some form of positional information .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "At the same time , on machine translation , showed that the performance of Transformers with only positional masking ( Shen et al , 2018 ) is comparable to that with positional encodings .", "entities": [[6, 8, "TaskName", "machine translation"]]}
{"text": "We explore implications of our results on machine translation and synthetic tasks .", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "Chen et al ( 2018 ) showed that RNNs with ReLU activations are also Turing - complete .", "entities": [[10, 11, "MethodName", "ReLU"]]}
{"text": "Hron et al ( 2020 ) show that Transformers behave as Gaussian processes when the number of heads tend to infinity .", "entities": [[11, 13, "TaskName", "Gaussian processes"]]}
{"text": "Hahn ( 2020 ) showed some limitations of Transformer encoders in modeling regular and context - free languages .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "However , these are not applicable 2 to the complete Transformer architecture .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "= 0 d b and f b ( $ ) = 0 d b .", "entities": [[1, 2, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "t\u22121 + W x x t + b ) , where t \u2265 1 , function g ( ) is a multilayer feedforward network ( FFN ) with activation \u03c3 , bias vector b Q d h , matrices W h Q d h \u00d7d h and W", "entities": [[22, 24, "MethodName", "feedforward network"]]}
{"text": "x Q d h \u00d7d b , and h t Q d h is the hidden state with given initial hidden state h 0 ; d h is the hidden state dimension .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "Vanilla Transformer .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "We describe the original Transformer architecture with positional encoding ( Vaswani et al , 2017 ) as formalized by P\u00e9rez et al ( 2019 ) , with some modifications .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": ", x n ) of vectors , ( ii ) a seed vector y 0 .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "The transformer consists of composition of transformer encoder and transformer decoder .", "entities": [[9, 11, "MethodName", "transformer decoder"]]}
{"text": "This activation can be easily replaced by the standard ReLU activation via \u03c3 ( x ) =", "entities": [[9, 10, "MethodName", "ReLU"]]}
{"text": "ReLU ( x ) \u2212 ReLU ( x \u2212 1 ) .", "entities": [[0, 1, "MethodName", "ReLU"], [5, 6, "MethodName", "ReLU"]]}
{"text": "\u03b1 n )", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "( x ) i : = 0 otherwise .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "In practice , the softmax is often used but its output values are in general not rational .", "entities": [[4, 5, "MethodName", "softmax"]]}
{"text": "( iii ) For vanilla transformers , the scoring function f att used is a combination of multiplicative attention ( Vaswani et al , 2017 ) and a non - linear function :", "entities": [[17, 19, "MethodName", "multiplicative attention"]]}
{"text": "Transformer encoder .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "x n ) a sequence of vectors in Q d , and parameters \u03b8 .", "entities": [[13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "The parame - ters \u03b8 specify functions Q ( ) , K ( ) , V ( ) , and O ( ) , all of type Q d Q d .", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "The complete L - layer transformer encoder TEnc ( L ) ( X ; \u03b8 )", "entities": [[14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "TEnc ( L ) is obtained by composition of L singlelayer encoders : let X ( 0 ) :", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "= X , and for 0 \u2264 \u2264 L \u2212 1 , let X ( +1 )", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "= Enc ( X ( ) ; \u03b8 ) and finally , K e = K ( L )", "entities": [[7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "Transformer decoder .", "entities": [[0, 2, "MethodName", "Transformer decoder"]]}
{"text": "An L - layer Transformer decoder TDec L ( ( K e , V e ) , Y ; \u03b8 )", "entities": [[4, 6, "MethodName", "Transformer decoder"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "Note that while the output of a single - layer decoder is a sequence of vectors , the output of an L - layer Transformer decoder is a single vector .", "entities": [[24, 26, "MethodName", "Transformer decoder"]]}
{"text": "The complete Transformer .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "The output Trans ( X , y 0 )", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "= Y is computed by the recurrence\u1ef9 t+1 = TDec ( TEnc ( X ) , ( y 0 , y 1 , . . .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": ", for 0 \u2264 t \u2264 r \u2212 1 .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Directional Transformer .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "We denote the Transformer with only positional masking and no positional encodings as Directional Transformer and use them interchangeably .", "entities": [[3, 4, "MethodName", "Transformer"], [14, 15, "MethodName", "Transformer"]]}
{"text": "In this case , we use standard multiplicative attention as the scoring function in our construction , i.e , f att ( q , k i )", "entities": [[7, 9, "MethodName", "multiplicative attention"]]}
{"text": "Our definitions deviate slightly from practice , hard - attention being the main one since hardmax keeps the values rational whereas softmax takes the values to irrational space .", "entities": [[21, 22, "MethodName", "softmax"]]}
{"text": "Transformer Networks with positional encodings are not necessarily equivalent in terms of their computational expressiveness ( Yun et al , 2020 ) to those with only positional masking when considering the encoder only model ( as used in BERT and GPT - 2 ) .", "entities": [[0, 1, "MethodName", "Transformer"], [38, 39, "MethodName", "BERT"], [40, 41, "MethodName", "GPT"]]}
{"text": "We say that a Transformer simulates an RNN ( as defined in Sec . 3.1 ) if on every input s \u03a3 * , at each step t , the vector y t contains the hidden state h t as a subvector , i.e. y t =", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "Proof Sketch .", "entities": [[1, 2, "DatasetName", "Sketch"]]}
{"text": "The input s 0 , . . .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": ", s n \u03a3 * is provided to the transformer as the sequence of vectors x 0 , . . .", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "where x i = [ 0 d h , f b ( s i ) , 0 d h , i , 1 ] , which has as sub - vector the given base embedding f b ( s i ) and the positional encoding i , along with extra coordinates set to constant values and will be used later .", "entities": [[5, 6, "DatasetName", "0"], [16, 17, "DatasetName", "0"]]}
{"text": "The basic observation behind our construction of the simulating Transformer is that the transformer decoder can naturally implement the recurrence operations of the type used by RNNs .", "entities": [[9, 10, "MethodName", "Transformer"], [13, 15, "MethodName", "transformer decoder"]]}
{"text": "But the Transformer receives the whole input at the same time .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "Since the Transformer takes its input all at once , appending by $ 's is not possible ( in particular , we do not know how long the computation would take ) .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "After encountering a $ once , the Transformer will feed ( encoding of ) $ to O dec ( ) in subsequent steps until termination .", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "z i = x i , which can be easily achieved , e.g. by using the residual connection and setting the value vectors to 0 .", "entities": [[16, 18, "MethodName", "residual connection"], [24, 25, "DatasetName", "0"]]}
{"text": "i = [ 0 d b , 0 d b , 0 d b , \u22121 , i ] and", "entities": [[3, 4, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "[ 0 d b , f b ( s i ) , 0 d b , 0 , 0 ] .", "entities": [[1, 2, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [18, 19, "DatasetName", "0"]]}
{"text": "= [ h t , 0 d b , 0 d b , t + 1 , 1 ] .", "entities": [[5, 6, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "This is easy to arrange for t = 0 , and assuming it for t we prove it for t+1 .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "This is possible because in the key vector k i mentioned above , almost all coordinates other than the one representing the position i are set to 0 , allowing the mechanism to only focus on the positional information and not be distracted by the other contents of p t = y t : the scoring function has value f att ( p t , k i )", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "At this point , O dec ( ) has at its disposal the hidden state h t ( coming from y t via p t and the residual connection ) and the input symbol x t ( coming via the attention mechanism and the residual connection ) .", "entities": [[27, 29, "MethodName", "residual connection"], [44, 46, "MethodName", "residual connection"]]}
{"text": "Hence O ( ) can act just like the FFN ( Lemma C.4 ) underlying the RNN to compute h t+1 and", "entities": [[11, 12, "DatasetName", "Lemma"]]}
{"text": "Proof Sketch .", "entities": [[1, 2, "DatasetName", "Sketch"]]}
{"text": "The input s 0 , . . .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": ", s n is provided to the transformer as the sequence of vectors x 0 , . . .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "where x i = [ 0 d h , 0 d h , f b ( s i ) , s i , 0 , 0 m , 0 m , 0 m ] .", "entities": [[5, 6, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [23, 24, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [31, 32, "DatasetName", "0"]]}
{"text": "= [ 0 d h , 0 d h , 0 d b , s i , 0 , 0 m , 0 m , 0 m ] and", "entities": [[2, 3, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [22, 23, "DatasetName", "0"], [25, 26, "DatasetName", "0"]]}
{"text": "[ 0 d h , 0 d h , f b ( s i ) , 0 m , 0 , 0 m , s i , 0 m ] .", "entities": [[1, 2, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [21, 22, "DatasetName", "0"], [27, 28, "DatasetName", "0"]]}
{"text": "= [ h t\u22121 , 0 d h , 0 d b , 0 m , 1 2 t , 0 m , 0 m ,", "entities": [[5, 6, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [20, 21, "DatasetName", "0"], [23, 24, "DatasetName", "0"]]}
{"text": "We remark that in this construction , the scoring function is the standard multiplicative attention 3 .", "entities": [[13, 15, "MethodName", "multiplicative attention"]]}
{"text": "\u03bbt ( I ( s 0 = s t ) , I ( s 1 = s t ) , . . .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "See Lemma D.3 for more details .", "entities": [[1, 2, "DatasetName", "Lemma"]]}
{"text": "At this point , O dec ( ) has at its disposal the hidden state h t ( coming from z ( 1 ) t via p ( 2 ) t and the residual connection ) and the input symbol x t ( coming via the attention mechanism and the residual connection ) .", "entities": [[33, 35, "MethodName", "residual connection"], [50, 52, "MethodName", "residual connection"]]}
{"text": "Our proof for directional transformers entails that there is no loss of order information if positional information is only provided in the form of masking .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "Hence , either the attention head or the residual connection is sufficient to achieve Turing - completeness .", "entities": [[8, 10, "MethodName", "residual connection"]]}
{"text": "A similar argument can be made for the FFN in the encoder layer : either the residual connection or the FFN is sufficient for Turing - completeness .", "entities": [[16, 18, "MethodName", "residual connection"]]}
{"text": "The class of Transformers without residual connection around the decoderencoder attention block is not Turing - complete .", "entities": [[5, 7, "MethodName", "residual connection"]]}
{"text": "Proof Sketch .", "entities": [[1, 2, "DatasetName", "Sketch"]]}
{"text": "Without the residual connection , the decoder - encoder attention block produces a t = Att ( p t , K e , V e )", "entities": [[2, 4, "MethodName", "residual connection"]]}
{"text": "\u03b1 i", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 i 's such that", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 i", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "Note that , without residual connection a t can take on at most 2 n \u2212 1 values .", "entities": [[4, 6, "MethodName", "residual connection"]]}
{"text": "Here 's an example task : given a number \u2206 ( 0 , 1 ) , the network must produce numbers 0 , \u2206 , 2\u2206 , . .", "entities": [[11, 12, "DatasetName", "0"], [21, 22, "DatasetName", "0"]]}
{"text": "Such a limitation does not exist with a residual connection since the vector a t =", "entities": [[8, 10, "MethodName", "residual connection"]]}
{"text": "\u03b1 i", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "It is perhaps surprising that residual connection , originally proposed to assist in the learning ability of very deep networks , plays a vital role in the computational expressiveness of the network .", "entities": [[5, 7, "MethodName", "residual connection"]]}
{"text": "Although we showed that Transformers without decoder - encoder residual connection are not Turing complete , it does not imply that they are incapable of performing all the tasks .", "entities": [[9, 11, "MethodName", "residual connection"]]}
{"text": "We sample sentences of lengths between 5 - 12 words from Penn Treebank and create a train - test split of 40k - 1k with all sentences belonging to the same range of length .", "entities": [[11, 13, "DatasetName", "Penn Treebank"]]}
{"text": "In the counting task , we create a very simple dataset where the model is given one number between 0 and 100 as input and its goal is to predict the next five numbers .", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "For both these tasks , we compare vanilla Transformer with the one without decoder - encoder residual connection .", "entities": [[8, 9, "MethodName", "Transformer"], [16, 18, "MethodName", "residual connection"]]}
{"text": "As a baseline we also consider the model without decoder - decoder residual connection , since according to our results , that connection does not influence the computational power of the model .", "entities": [[12, 14, "MethodName", "residual connection"]]}
{"text": "We then assess the influence of the limitation on Machine Translation which requires a model to do a combination of both mapping and inferring from computations in previous timesteps .", "entities": [[9, 11, "TaskName", "Machine Translation"]]}
{"text": "We again compare vanilla Transformer with the ones without decoder - encoder and decoder - decoder residual connection .", "entities": [[4, 5, "MethodName", "Transformer"], [16, 18, "MethodName", "residual connection"]]}
{"text": "For the counting task , the one without decoder - encoder residual connection is incapable of performing it .", "entities": [[11, 13, "MethodName", "residual connection"]]}
{"text": "However , the other two including the one without decoder - decoder residual connection are able to accomplish the task by learning to make decisions based on their prior predictions .", "entities": [[12, 14, "MethodName", "residual connection"]]}
{"text": "While the drop from removing decoder - encoder residual connection is significant , it is still able to perform reasonably well since the task can be largely fulfilled by mapping different words from one sentence to another .", "entities": [[8, 10, "MethodName", "residual connection"]]}
{"text": "The decoder - encoder attention block plays a necessary role in conditioning the computation on the input sequence while the residual connection around it is necessary to keep track of previous computations .", "entities": [[20, 22, "MethodName", "residual connection"]]}
{"text": "The feedforward network in the decoder is the only component capable of performing computations based on the input and prior computations .", "entities": [[1, 3, "MethodName", "feedforward network"]]}
{"text": "Since RNNs can recognize all regular languages in finite precision ( Korsky and Berwick , 2019 ) , it follows from our construction that Transformer can also recognize a large class of regular languages in finite precision .", "entities": [[24, 25, "MethodName", "Transformer"]]}
{"text": "C.1 we discuss the effect of removing a residual connection on computational power of Transformers .", "entities": [[8, 10, "MethodName", "residual connection"]]}
{"text": "Indicator I ( P ) is 1 , if predicate P is true and is 0 otherwise .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "For a sequence X = ( x n , . . . , x n ) for some n \u2265 0 , we set X j : = ( x n , . . .", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "For a vector v , by 0 v we mean the all - 0 vector of the same dimension as v. Lett :", "entities": [[6, 7, "DatasetName", "0"], [13, 14, "DatasetName", "0"]]}
{"text": "In the main paper , for simplicity we stated the results for total recursive functions \u03c6 : { 0 , 1 } * { 0 , 1 } * , i.e. a function that is defined on every s { 0 , 1 } * and whose values can be computed by a Turing machine .", "entities": [[18, 19, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [40, 41, "DatasetName", "0"]]}
{"text": "Let \u03c6 : { 0 , 1 } * { 0 , 1 } * be partial recursive .", "entities": [[4, 5, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "A partial recursive function is one that need not be defined for every s { 0 , 1 } * , and there exists a Turing Machine M with the following property .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "The simulating RNN R ( M ) , gets as input encodings of s 1 s 2 ... s n in the first n steps , and from then on receives the vector 0 as input in each step .", "entities": [[33, 34, "DatasetName", "0"]]}
{"text": "In this case , R ( M ) enters a special accept state , and \u03a8 1 encodes \u03c6 ( s ) and \u03a8 2 = 0 .", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "Given any partial recursive function \u03c6 : { 0 , 1 } * { 0 , 1 } * computed by Turing machine M \u03c6 , there exists a simulating RNN R ( M \u03c6 ) .", "entities": [[8, 9, "DatasetName", "0"], [14, 15, "DatasetName", "0"]]}
{"text": ", x n ) of vectors in Q d , ( ii ) a seed vector y 0", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "The sequence X is obtained from the sequence ( s 0 , . . .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "for 0 \u2264 i \u2264 n.", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "The transformer consists of composition of transformer encoder and a transformer decoder .", "entities": [[10, 12, "MethodName", "transformer decoder"]]}
{"text": "The transformer encoder is obtained by composing one or more single - layer encoders and similarly the transformer decoder is obtained by composing one or more single - layer decoders .", "entities": [[17, 19, "MethodName", "transformer decoder"]]}
{"text": "if x < 0 , x if 0 \u2264 x \u2264 1 , 1 if x > 1 . ( 4 ) As mentioned in the main paper , we can easily work with the standard ReLU activation via \u03c3 ( x ) =", "entities": [[3, 4, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [36, 37, "MethodName", "ReLU"]]}
{"text": "ReLU ( x ) \u2212 ReLU ( x \u2212 1 ) .", "entities": [[0, 1, "MethodName", "ReLU"], [5, 6, "MethodName", "ReLU"]]}
{"text": "\u03b1 n )", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "( x ) i : = 0 otherwise .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "In practice , the softmax is often used but its output values are in general not rational .", "entities": [[4, 5, "MethodName", "softmax"]]}
{"text": "For the Turing - completeness proof of vanilla transformers , the scoring function f att used is a combination of multiplicative attention ( Vaswani et al , 2017 ) and a non - linear function :", "entities": [[20, 22, "MethodName", "multiplicative attention"]]}
{"text": "For directional trans - formers , the standard multiplicative attention is used , that is , f att ( q , k i )", "entities": [[8, 10, "MethodName", "multiplicative attention"]]}
{"text": "Transformer encoder .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "The parameters \u03b8 specify functions Q ( ) , K ( ) , V ( ) , and O ( ) , all of type Q d Q d .", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "The output of the L - layer Transformer encoder ( K e , V e )", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "= TEnc ( L ) ( X ) is fed to the Transformer decoder which we describe next .", "entities": [[12, 14, "MethodName", "Transformer decoder"]]}
{"text": "Transformer decoder .", "entities": [[0, 2, "MethodName", "Transformer decoder"]]}
{"text": "An L - layer Transformer decoder is obtained by repeated application of L single - layer decoders each with its own parameters and a transformation function F : Q d Q d applied to the last vector in the sequence of vectors output by the final decoder .", "entities": [[4, 6, "MethodName", "Transformer decoder"]]}
{"text": "We use z = TDec L ( ( K e , V e ) , Y ; \u03b8 ) to denote an L - layer Transformer decoder .", "entities": [[17, 18, "HyperparameterName", "\u03b8"], [25, 27, "MethodName", "Transformer decoder"]]}
{"text": "Note that while the output of a single - layer decoder is a sequence of vectors , the output of an L - layer Transformer decoder is a single vector .", "entities": [[24, 26, "MethodName", "Transformer decoder"]]}
{"text": "The complete Transformer .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "A Transformer network receives an input sequence X , a seed vector y 0 , and r N. For t \u2265 0 its output is a sequence", "entities": [[1, 2, "MethodName", "Transformer"], [13, 14, "DatasetName", "0"], [21, 22, "DatasetName", "0"]]}
{"text": "= TDec TEnc ( X ) , ( y 0 , y 1 , . . .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "We denote the complete Transformer by Trans ( X , y 0 )", "entities": [[4, 5, "MethodName", "Transformer"], [11, 12, "DatasetName", "0"]]}
{"text": "The Transformer \" halts \" when y T H , where H is a prespecified halting set .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "\u03b1", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 i s such that", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 i", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "Since the previous hidden state information was computed and stored in p t , without the residual connection , the information in a t depends solely on the output of the encoder .", "entities": [[16, 18, "MethodName", "residual connection"]]}
{"text": "\u03b1 i s depend on the query vector p t , it could still use it gain the necessary information from the vectors", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "v e i s. However , note that by definition of hard attention , the attention weights \u03b1 i in a t", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "Since the attention weights \u03b1 i are such that", "entities": [[4, 5, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 i", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "Hence , the network without decoder - encoder residual connection with n inputs can have at most 2 n \u22121 distinct a t values .", "entities": [[8, 10, "MethodName", "residual connection"]]}
{"text": "Note that , such a limitation will not exist with a residual connection since the vector a t = \u03a3", "entities": [[11, 13, "MethodName", "residual connection"]]}
{"text": "\u03b1 i", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "As an example to illustrate the limitation , consider the following simple problem , given a value \u2206 , where 0 \u2264 \u2206 \u2264 1 , the network must produce the values 0 , \u2206 , 2\u2206 , . . . , k\u2206 , where k is the maximum integer such that k\u2206 \u2264 1 .", "entities": [[20, 21, "DatasetName", "0"], [32, 33, "DatasetName", "0"]]}
{"text": "Since a t is fed to feedforward network which maps it to z t , the output of the decoder will remain the same at every timestep and it can not produce distinct values .", "entities": [[6, 8, "MethodName", "feedforward network"]]}
{"text": "If the residual connection is removed , the output of decoder - encoder attention block at each layer is a ( ) t = n", "entities": [[2, 4, "MethodName", "residual connection"]]}
{"text": "i=1 \u03b1", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "Since the output of the L layer decoder will be a feedforward network over a ( L ) t , the computation reduces to the single layer decoder case .", "entities": [[11, 13, "MethodName", "feedforward network"]]}
{"text": "This implies that the model without decoderencoder residual connection is limited in its capability to perform tasks which requires it to make inferences based on previously generated outputs .", "entities": [[7, 9, "MethodName", "residual connection"]]}
{"text": "The main task for the simulation is to design the input embedding ( building on the given base embedding f b ) , the feedforward network O ( ) and the matrices corresponding to functions Q ( ) , K ( ) , V ( ) .", "entities": [[24, 26, "MethodName", "feedforward network"]]}
{"text": "= [ 0 d h , f e ( s ) ; 0 d h , 0 , 0 ] .", "entities": [[2, 3, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [18, 19, "DatasetName", "0"]]}
{"text": "[ 0 d h , 0 d b , 0", "entities": [[1, 2, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "Together , these define the combined embedding f for a given input sequence s 0 s 1 s n \u03a3 * by f ( s i ) = f symb ( s i )", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "[ 0 d h , f b ( s i ) , 0 d h , i , 1 ] .", "entities": [[1, 2, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "During the computation of the Transformer , the underlying RNN will get the input st at step t for t = 0 , 1 , . . .", "entities": [[5, 6, "MethodName", "Transformer"], [21, 22, "DatasetName", "0"]]}
{"text": "This sequence leads to the RNN getting the embedding of the input sequence s 0 , . . .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "This is done by setting the matrix for V ( ) to the all - zeros matrix , and the feedforward networks to always output 0 .", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "The application of appropriately chosen linear transformations for the final K ( ) and V ( ) give the following lemma about the output of the encoder .", "entities": [[20, 21, "DatasetName", "lemma"]]}
{"text": "Lemma C.3 .", "entities": [[0, 1, "DatasetName", "Lemma"]]}
{"text": "[ 0 h , 0 s ; 0 h , \u22121 , i ] ,", "entities": [[1, 2, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [7, 8, "DatasetName", "0"]]}
{"text": "v i = [ 0 h , s i ; 0 h , 0 , 0 ] .", "entities": [[4, 5, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}
{"text": "= 0 identically , and use the residual connection so that p t = y t .", "entities": [[1, 2, "DatasetName", "0"], [7, 9, "MethodName", "residual connection"]]}
{"text": "For t \u2265 0 , at the t - th step we denote the input to the decoder as y t", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "Let h 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "= 0 h and\u1ef9 0", "entities": [[1, 2, "DatasetName", "0"], [4, 5, "DatasetName", "0"]]}
{"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "[ h t , 0 s ; 0 h , t + 1 , 1 ] .", "entities": [[4, 5, "DatasetName", "0"], [7, 8, "DatasetName", "0"]]}
{"text": "By construction , this is true for t = 0 : y 0", "entities": [[9, 10, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "= [ 0 h , 0 s ; 0 h , 1 , 1 ] .", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "By Lemma C.5 Att ( p t , K e , V e )", "entities": [[1, 2, "DatasetName", "Lemma"]]}
{"text": "= [ 0 h , v t+1 ; 0 h , 0 , 0 ] .", "entities": [[2, 3, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [13, 14, "DatasetName", "0"]]}
{"text": "( 10 ) Lemma C.5 basically shows how we retrieve the input s t+1 at the relevant step for further computation in the decoder .", "entities": [[3, 4, "DatasetName", "Lemma"]]}
{"text": "p t = [ h t , s t+1 , 0 h , t + 1 , 1 ] .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "In the final block of the decoder , the computation for RNN takes place : Lemma C.4 .", "entities": [[15, 16, "DatasetName", "Lemma"]]}
{"text": "= [ ( h t+1 \u2212 h t ) , \u2212s t+1 , 0 h , \u2212 ( t + 1 ) , \u22121 ] , where W h , W x and b denote the parameters of the RNN under consideration .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "[ h t+1 , 0 s ; 0 h , 0 , 0 ] .", "entities": [[4, 5, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "= [ h t+1 , 0 s ; 0 h , 0 , 0 ] , which means y", "entities": [[5, 6, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [13, 14, "DatasetName", "0"]]}
{"text": "= [ h t+1 , 0 s ; 0 h , t + 2 , 1 ] , proving our induction hypothesis .", "entities": [[5, 6, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "Proof of Lemma C.3 .", "entities": [[2, 3, "DatasetName", "Lemma"]]}
{"text": "= 0 , a i = 0", "entities": [[1, 2, "DatasetName", "0"], [6, 7, "DatasetName", "0"]]}
{"text": "+ x i , O ( a i ) = 0 , z i = 0 + a i = x i .", "entities": [[10, 11, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}
{"text": "x i ) = 0 can be achieved by setting the weight matrix as the all - 0 matrix .", "entities": [[4, 5, "DatasetName", "0"], [17, 18, "DatasetName", "0"]]}
{"text": "Recall that x i is defined as x", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "= [ 0 h , s i , 0 h , i , 1 ] .", "entities": [[2, 3, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "z i W v , where W T k = \uf8ee", "entities": [[8, 10, "HyperparameterName", "k ="]]}
{"text": "0 0 0 0 .", "entities": [[0, 1, "DatasetName", "0"], [1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"], [3, 4, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "0 0 0 0 0 0 1 0 0 \u22121 0 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb , and W k Q d\u00d7d , and similarly one can obtain", "entities": [[0, 1, "DatasetName", "0"], [1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"], [3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "Lemma C.5 .", "entities": [[0, 1, "DatasetName", "Lemma"]]}
{"text": "= [ 0 h , s t+1 , 0 h , 0 , 0 ] .", "entities": [[2, 3, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [13, 14, "DatasetName", "0"]]}
{"text": "Recall that p t = y t", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "[ h t , 0 , . . .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": ", 0 , t + 1 , 1 ] and k i", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "= [ 0 , 0 , . . .", "entities": [[2, 3, "DatasetName", "0"], [4, 5, "DatasetName", "0"]]}
{"text": ", 0 , \u22121 , i ] and hence p t ,", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Thus , for i [ n ] , the scoring functionf att ( p t , k i ) has the maximum value 0 at index i = t + 1 if", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "Proof of Lemma C.4 .", "entities": [[2, 3, "DatasetName", "Lemma"]]}
{"text": "Recall that a t =", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "[ h t , s t+1 , 0 h , t + 1 , 1 ]", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "W h W x 0 0 0 I 0 0 I 0 0 0 0 0 0", "entities": [[4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [14, 15, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [16, 17, "DatasetName", "0"]]}
{"text": "I \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb and b 1 = [ b h , 0 s , 0 h , 0 , 0 ] .", "entities": [[14, 15, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [20, 21, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "I 0 \u2212I 0 0 \u2212I 0 0 0 0 0 0 0 0 0 \u2212I \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb .", "entities": [[1, 2, "DatasetName", "0"], [3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [14, 15, "DatasetName", "0"]]}
{"text": "\u2212 h t , \u2212s t+1 , 0 h , \u2212 ( t + 1 ) , \u22121 ] , which is what we wanted to prove .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "There are a few changes in the architecture of the Transformer to obtain directional Transformer .", "entities": [[10, 11, "MethodName", "Transformer"], [14, 15, "MethodName", "Transformer"]]}
{"text": "The vector\u1ef9 is the output representation produced at the previous step and the first input vector to the decoder\u1ef9 0", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "+ z ( ) i , where Z ( 0 )", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "The Transformer network in this case will be more complex than the construction for the vanilla case .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "We will construct our Transformer to simulate an RNN of the form given in the definition with the recurrence where h i Q d h , s Q de", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "\uf8ef \uf8f0 0 0 0 0 0 0 0 0", "entities": [[2, 3, "DatasetName", "0"], [3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "I 0 0 0 0 0 0 0 0 0 0", "entities": [[1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"], [3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "I \u2212I 0 0 0 1 2 0 0 0 0 0 0 1 2 0 0 0 0 0 I 0 0 0 0 0 0 0 0 0 I 0 0 0 0 0 0 0 I Proof .", "entities": [[2, 3, "DatasetName", "0"], [3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [21, 22, "DatasetName", "0"], [22, 23, "DatasetName", "0"], [23, 24, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [29, 30, "DatasetName", "0"], [31, 32, "DatasetName", "0"], [32, 33, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [34, 35, "DatasetName", "0"], [35, 36, "DatasetName", "0"], [36, 37, "DatasetName", "0"], [37, 38, "DatasetName", "0"]]}
{"text": "Proof is very similar to proof of lemma C.4 .", "entities": [[7, 8, "DatasetName", "lemma"]]}
{"text": "\uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb and b 1 = 0 , then \u03c3 ( W 1 a ( 1 ) t + b 1 )", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "= [ 0 h , 0 h , s 0 : t , \u2206 t , 1 2 t+1 , \u03c9 t , \u03c9 t\u22121 ,", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "The models under consideration are the vanilla Transformer , the one without decoder - encoder residual connection and the one without decoderdecoder residual connection .", "entities": [[7, 8, "MethodName", "Transformer"], [15, 17, "MethodName", "residual connection"], [22, 24, "MethodName", "residual connection"]]}
{"text": "Our implementation of the Transformer is adapted from the implementation of ( Rush , 2018 ) .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "For the machine translation task , we use Open - NMT ( Klein et al , 2017 ) for our implementation .", "entities": [[2, 4, "TaskName", "machine translation"]]}
{"text": "The hyperparameters to train the vanilla Transformer were obtained from fairseq 's guidelines .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "We use noam optimizer in all our experiments .", "entities": [[3, 4, "HyperparameterName", "optimizer"]]}
{"text": "Given an input sequence s 0 s 1 s 2 s n \u03a3 * where s 0 = #", "entities": [[5, 6, "DatasetName", "0"], [16, 17, "DatasetName", "0"]]}
{"text": "For the computation of the Transformer , we also use a vector sequence in Q | \u03a3 | defined by where 0 \u2264 t \u2264 n. The vector \u03c9 t = ( \u03c9 t , 1 , . . .", "entities": [[5, 6, "MethodName", "Transformer"], [21, 22, "DatasetName", "0"]]}
{"text": ", \u03c9 t , | \u03a3 | ) contains the proportion of each input symbol till step t for 0 \u2264 t \u2264", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Note that \u03c9 t , 0 = 1 t+1 since the first coordinate corresponds to the proportion of the start symbol # which appears only once at t = 0 .", "entities": [[5, 6, "DatasetName", "0"], [29, 30, "DatasetName", "0"]]}
{"text": "Similarly , \u03c9 t , | \u03a3 | = 0 for 0 \u2264 t < n and \u03c9 t , | \u03a3 | = 1/ ( t + 1 ) for t \u2265 n , since the end symbol $ does n't appear till the end of the input and it appears only once at t =", "entities": [[9, 10, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "n. We define two more sequences of vectors in Here \u2206 t denotes the difference in the proportion of symbols between the t - th and ( t \u2212 1 ) - th steps , with the applicatin of sigmoid activation .", "entities": [[39, 41, "MethodName", "sigmoid activation"]]}
{"text": "In vector \u03b4 t , the last coordinate of \u2206 t has been replaced with 1/2 t+1 .", "entities": [[2, 3, "HyperparameterName", "\u03b4"]]}
{"text": "We set the last coordinate in \u03b4 t to an exponentially decreasing sequence so that after n steps we always have a nonzero score for the terminal symbol and it is taken as input in the underlying RNN .", "entities": [[6, 7, "HyperparameterName", "\u03b4"]]}
{"text": "Different and perhaps simpler choices for the last coordinate of \u03b4 t may be possible .", "entities": [[10, 11, "HyperparameterName", "\u03b4"]]}
{"text": "The input to the network DTrans M is the sequence ( s 0 , s 1 , . . .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": ", s n\u22121 , s n ) where s 0 = #", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "Our encoder is a simple single layer network such that TEnc ( x 0 , x 1 , . . .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "where K e = ( k e 0 , . . . , k e n ) and V e", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "= ( v e 0 , . .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "such that , Similar to our construction of the encoder for vanilla transformer ( Lemma C.3 ) , the above K e and V e can be obtained by making the output of Att ( )", "entities": [[14, 15, "DatasetName", "Lemma"]]}
{"text": "= 0 by choosing the V ( ) to always evaluate to 0 and similarly for O ( ) , and using residual connections .", "entities": [[1, 2, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "= \u1ef9 t , where 0 \u2264 t \u2264 r , where r is the step where the decoder halts .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Let h \u22121 = 0 h and h 0 = 0 h .", "entities": [[4, 5, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "We will prove by induction on t that for 0 \u2264 t \u2264 r we have This is true for t = 0 by the choice of seed vector : Assuming the truth of ( 14 ) for t , we show it for t + 1 . Layer 1 .", "entities": [[9, 10, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "Similar to the construction in Lemma C.3 , in the decoder - decoder attention block we set V ( 1 ) ( )", "entities": [[5, 6, "DatasetName", "Lemma"]]}
{"text": "= 0 d and use the residual connections to set p ( 1 ) t = y t .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": ", 0 )", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "where In Lemma D.2 we construct feed - forward network Layer 2 .", "entities": [[2, 3, "DatasetName", "Lemma"]]}
{"text": "In the first block of layer 2 , we set the value transformation function to identically zero similar to Lemma C.3 , i.e. V ( 2 ) ( ) = 0 which leads to the output of Att ( ) to be 0 and then using the residual connection we get p In the final block of the decoder in the second layer , the computation for RNN takes place .", "entities": [[19, 20, "DatasetName", "Lemma"], [30, 31, "DatasetName", "0"], [42, 43, "DatasetName", "0"], [47, 49, "MethodName", "residual connection"]]}
{"text": "In Lemma D.4 below we construct the feed - forward proving the induction hypothesis ( 14 ) for t + 1 , and completing the simulation of RNN .", "entities": [[1, 2, "DatasetName", "Lemma"]]}
{"text": "Lemma D.2 .", "entities": [[0, 1, "DatasetName", "Lemma"]]}
{"text": "which is what we wanted to prove . where t \u2265 0 and ' ' denotes an arbitrary value .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "For t \u2265 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "We now prove the lemma assuming the claim above .", "entities": [[4, 5, "DatasetName", "lemma"]]}
{"text": "Note that if s j = s t , then v e j = \u03b3 t .", "entities": [[14, 15, "HyperparameterName", "\u03b3"]]}
{"text": "Now we hav\u0113 completing the proof of the lemma modulo the proof of the claim , which we prove next .", "entities": [[8, 9, "DatasetName", "lemma"]]}
{"text": "For 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "t\u22121 has the form The last inequality used our assumption that s 0 = # and that # does not occur at any later time and therefore \u03c6 t\u22121 ,", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "We reproduce these for convenience : \u03b4t , 1 2 t+1 , 0 \u03c9 , 0 \u03c9 , \u03c9t ] , k e j = [ 0 h , 0 h , 0 s , s j , 0 , 0 \u03c9 , 0 \u03c9 , 0 \u03c9 ] .", "entities": [[12, 13, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [29, 30, "DatasetName", "0"], [32, 33, "DatasetName", "0"], [38, 39, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [43, 44, "DatasetName", "0"], [46, 47, "DatasetName", "0"]]}
{"text": "It now follows that for 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Thus , for 0 \u2264 t < n , in the vector p ( 2 ) t , k e 0 , . . .", "entities": [[3, 4, "DatasetName", "0"], [20, 21, "DatasetName", "0"]]}
{"text": "All other coordinates are 0 .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors", "entities": [[6, 9, "TaskName", "Multimodal Sentiment Analysis"]]}
{"text": "Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed .", "entities": [[0, 3, "TaskName", "Multimodal sentiment analysis"]]}
{"text": "We find that the main reason is that real - world applications can only access the text outputs by the automatic speech recognition ( ASR ) models , which may be with errors because of the limitation of model capacity .", "entities": [[20, 23, "TaskName", "automatic speech recognition"]]}
{"text": "Through further analysis of the ASR outputs , we find that in some cases the sentiment words , the key sentiment elements in the textual modality , are recognized as other words , which makes the sentiment of the text change and hurts the performance of multimodal sentiment analysis models directly .", "entities": [[46, 49, "TaskName", "multimodal sentiment analysis"]]}
{"text": "Specifically , we first use the sentiment word position detection module to obtain the most possible position of the sentiment word in the text and then utilize the multimodal sentiment word refinement module to dynamically refine the sentiment word embeddings .", "entities": [[38, 40, "TaskName", "word embeddings"]]}
{"text": "We conduct extensive experiments on the real - world datasets including MOSI - Speechbrain , MOSI - IBM , and MOSI - iFlytek and the results demonstrate the effectiveness of our model , which surpasses the current state - of - the - art models on three datasets .", "entities": [[11, 12, "DatasetName", "MOSI"], [15, 16, "DatasetName", "MOSI"], [20, 21, "DatasetName", "MOSI"], [22, 23, "DatasetName", "iFlytek"]]}
{"text": "Multimodal sentiment analysis ( MSA ) has been an emerging research field for its potential applications in human - computer interaction .", "entities": [[0, 3, "TaskName", "Multimodal sentiment analysis"]]}
{"text": "And other works consider the semantic gaps between multimodal data and adopt the adversarial learning ( Mai et al , 2020 ) and multi - task learning ( Hazarika et al , 2020 ) to map different modal features into a shared subspace .", "entities": [[23, 27, "TaskName", "multi - task learning"]]}
{"text": "To further analyze this problem , we build three real - world multimodal sentiment analysis datasets based on the existing dataset , CMU - MOSI ( Zadeh et al , 2016 ) .", "entities": [[12, 15, "TaskName", "multimodal sentiment analysis"], [24, 25, "DatasetName", "MOSI"]]}
{"text": "Specifically , we adopt three widely used ASR APIs including SpeechBrain , IBM , and iFlytek to process the original audios and obtain the recognized texts .", "entities": [[15, 16, "DatasetName", "iFlytek"]]}
{"text": "Then , we replace the gold texts in CMU - MOSI with the ASR results and get three realworld datasets , namely MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek .", "entities": [[10, 11, "DatasetName", "MOSI"], [22, 23, "DatasetName", "MOSI"], [26, 27, "DatasetName", "MOSI"], [31, 32, "DatasetName", "MOSI"], [33, 34, "DatasetName", "iFlytek"]]}
{"text": "We list the percentages of the sentiment word substitution error on the MOSI dataset for three ASR APIs in Figure 1 ( b ) .", "entities": [[12, 13, "DatasetName", "MOSI"]]}
{"text": "The percentage of the sentiment word substitution error on the MOSI - IBM is 17.6 % , which means about 17 of 100 utterances have this type of error .", "entities": [[10, 11, "DatasetName", "MOSI"]]}
{"text": "To further demonstrate the negative effect of the substitution error on the MSA models , we split the test data of MOSI - IBM into two groups by whether there is a substitution error .", "entities": [[21, 22, "DatasetName", "MOSI"]]}
{"text": "To tackle this problem , we propose the sentiment word aware multimodal refinement model , which can detect the positions of the sentiment words in the text and dynamically refine the word embeddings in the detected positions by incorporating multimodal clues .", "entities": [[31, 33, "TaskName", "word embeddings"]]}
{"text": "Specifically , we first use the sentiment word location module to detect the positions of sentiment words and meanwhile utilize the strong language model , BERT , to generate the candidate sentiment words .", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "Then we propose the multimodal sentiment word refinement module to refine the word embeddings based on the multimodal context information .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "We apply the multimodal gating network to filter out useless information from the input word embeddings in the filtering process and use the multimodal sentiment word attention network to leverage the useful information from candidate sentiment words as the supplement to the filtered word embeddings in the adding process .", "entities": [[14, 16, "TaskName", "word embeddings"], [43, 45, "TaskName", "word embeddings"]]}
{"text": "Finally , the refined sentiment word embeddings are used for multimodal feature fusion .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "We conduct extensive experiments on the MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek datasets to demonstrate the effectiveness of our proposed model .", "entities": [[6, 7, "DatasetName", "MOSI"], [10, 11, "DatasetName", "MOSI"], [15, 16, "DatasetName", "MOSI"], [17, 18, "DatasetName", "iFlytek"]]}
{"text": "The experimental results show that : ( 1 ) There is an obvious performance drop for the state - of - the - art MSA model , when the model is deployed in the real world taking the ASR outputs as the input of textual modality ; ( 2 ) Our proposed model outperforms all baselines , which can dynamically refine the sentiment word embeddings by leveraging multimodal information .", "entities": [[63, 65, "TaskName", "word embeddings"]]}
{"text": "The main contributions of this work are as follows : ( 1 ) We propose a novel sentiment word aware multimodal refinement model for multimodal sentiment analysis , which can dynamically reconstruct the sentiment semantics of the ASR texts with errors by utilizing the multimodal sentiment information resulting in more robust sentiment prediction ; ( 2 ) We validate the negative effect of the sentiment word substitution error on the state - of - the - art MSA model through the in - depth analysis ; ( 3 ) We evaluate our model on three real - world datasets , and the experimental results demonstrate that our model outperforms all baselines .", "entities": [[24, 27, "TaskName", "multimodal sentiment analysis"]]}
{"text": "Multimodal sentiment analysis has gained increasing attention from the community recently and some process has been made .", "entities": [[0, 3, "TaskName", "Multimodal sentiment analysis"]]}
{"text": "To effectively fuse them , they proposed the GME - LSTM ( A ) model , which consists of two modules , the gated multimodal embedding and the LSTM with the temporal attention .", "entities": [[10, 11, "MethodName", "LSTM"], [28, 29, "MethodName", "LSTM"], [31, 33, "MethodName", "temporal attention"]]}
{"text": "The experimental results on the multimodal emotion detection task show that training the models in an end - to - end manner can obtain better results than the pipeline models .", "entities": [[6, 7, "DatasetName", "emotion"]]}
{"text": "Motivated by it , they built the CH - SIMS dataset , which contains not only the multimodal sentiment labels but also unimodal sentiment labels .", "entities": [[7, 10, "DatasetName", "CH - SIMS"]]}
{"text": "And based on it , they proposed a multi - task learning framework to leverage two types of sentiment labels simultaneously .", "entities": [[8, 12, "TaskName", "multi - task learning"]]}
{"text": "To address it , Yu et al ( 2021 ) proposed the Self - MM model , which first generates the unimodal labels by utilizing the relationship between the unimodal and multimodal labels and then uses the multi - task learning to train the model .", "entities": [[37, 41, "TaskName", "multi - task learning"]]}
{"text": "However , even though lots of models are proposed and obtain promising results on the benchmark datasets , there are few works considering the noisy inputs when the MSA models are deployed in the real world .", "entities": [[14, 16, "DatasetName", "the benchmark"]]}
{"text": "We first use the sentiment word location module to detect the possible positions of sentiment words and then utilize the multimodal sentiment word refinement module to dynamically refine the word embeddings in the detected positions .", "entities": [[29, 31, "TaskName", "word embeddings"]]}
{"text": "Finally , the refined word embeddings are fed into the multimodal feature fusion module to predict the final sentiment labels .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "Specifically , we choose the BERT model ( Devlin et al , 2019 ) as our language model since the masked language modeling pretraining objective meets our needs perfectly .", "entities": [[5, 6, "MethodName", "BERT"], [20, 23, "TaskName", "masked language modeling"]]}
{"text": "And then we use the BERT model to predict the possible words in the position of the masked word .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "In practice , we use the gate mask p to record it , and p is 1 if k s is larger than k/2 and 0 otherwise .", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "In order to reduce the negative effects of the ASR errors , we propose the multimodal sentiment word refinement module , in which we refine the word embeddings of sentiment words from two aspects .", "entities": [[26, 28, "TaskName", "word embeddings"]]}
{"text": "One is that we uses the multimodal gating network to filter out the useless information from the input word embeddings .", "entities": [[18, 20, "TaskName", "word embeddings"]]}
{"text": "The other one is that we design the multimodal sentiment attention network to incorporate the useful information from candidate words generated by the BERT model .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "Given an utterance , which includes three modal unaligned features , word embeddings , acoustic features , and visual features , we denote them as", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "To obtain the context - aware representations , we apply the BERT model and LSTM networks to encode the features , producing h", "entities": [[11, 12, "MethodName", "BERT"], [14, 15, "MethodName", "LSTM"]]}
{"text": "Besides , we also use an LSTM network to fuse the acoustic and visual features for capturing high - level sentiment semantics and obtain h", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "network to filter the word embedding , which is implemented by a non - linear layer .", "entities": [[14, 16, "MethodName", "linear layer"]]}
{"text": "h l = BERT ( x l ) h v = LSTM v ( u v ) h a = LSTM a ( u a ) h", "entities": [[3, 4, "MethodName", "BERT"], [11, 12, "MethodName", "LSTM"], [20, 21, "MethodName", "LSTM"]]}
{"text": "va = LSTM va ( [ u v ; u a ] ) (", "entities": [[2, 3, "MethodName", "LSTM"]]}
{"text": "Furthermore , we propose a novel multimodal sentiment word attention network to leverage the sentiment - related information from the candidate words , more than half of which are sentiment words , generated by the BERT model to complement the word embeddings .", "entities": [[35, 36, "MethodName", "BERT"], [40, 42, "TaskName", "word embeddings"]]}
{"text": "We use a linear layer to implement the multimodal sentiment word attention network .", "entities": [[3, 5, "MethodName", "linear layer"]]}
{"text": "Then , we pass them to the linear layer and obtain the attention score g e t .", "entities": [[7, 9, "MethodName", "linear layer"]]}
{"text": "The attention scores are fed into a softmax function to obtain the attention weights .", "entities": [[7, 8, "MethodName", "softmax"]]}
{"text": "Finally , we apply the weights to the candidate word embeddings and get the sentiment embedding r e .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "Hence , we incorporate the embedding of the special word [ MASK ] , x mask , to let the BERT model handle this problem based on the context .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "We describe our multimodal feature fusion module in the section and it is noted that our proposed refinement approach only modifies the textual input token embeddings , which makes it easy to be adapted for other multimodal feature fusion models based on BERT , such as MISA ( Hazarika et al , 2020 ) .", "entities": [[42, 43, "MethodName", "BERT"]]}
{"text": "We first use the BERT model to encode the refined word embeddings z l = { x l 1 , x l 2 , ... , r l , .. , x l n l } and take the representation of [ CLS ] as the textual representation , which is denoted as v l .", "entities": [[4, 5, "MethodName", "BERT"], [10, 12, "TaskName", "word embeddings"]]}
{"text": "And then we use two LSTM networks to encode the visual and acoustic features and take the representations of the first words as the visual representation v v and acoustic representation v a .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "Finally , we fuse them using a non - linear layer to capture the interactions between them .", "entities": [[9, 11, "MethodName", "linear layer"]]}
{"text": "v l = BERT textual ( z l ) v v = LSTM visual ( x v ) v a = LSTM acoustic", "entities": [[3, 4, "MethodName", "BERT"], [12, 13, "MethodName", "LSTM"], [21, 22, "MethodName", "LSTM"]]}
{"text": "( x a ) v f = Relu ( W 4 ( [ v l ; v v ; v a ] ) + b 4 ) ( 5 ) where W 4 R d f v \u00d7 ( d l v + d a v + d v v ) , b 4 R d f", "entities": [[7, 8, "MethodName", "Relu"]]}
{"text": "We utilize a linear layer to predict the final sentiment regression labels .", "entities": [[3, 5, "MethodName", "linear layer"]]}
{"text": "Besides , to enhance the model to capture unimodal sentiment information , we use the Unimodal Label Generation Module ( ULGM ) ( Yu et al , 2021 ) to generate pseudo unimodal sentiment labels and adopt them to train our model in a multi - task learning manner .", "entities": [[44, 48, "TaskName", "multi - task learning"]]}
{"text": "We build three real - world datasets including MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek , on CMU - MOSI ( Zadeh et al , 2016 ) .", "entities": [[8, 9, "DatasetName", "MOSI"], [12, 13, "DatasetName", "MOSI"], [17, 18, "DatasetName", "MOSI"], [19, 20, "DatasetName", "iFlytek"], [24, 25, "DatasetName", "MOSI"]]}
{"text": "CMU - MOSI CMU multimodal opinion - level sentiment intensity ( CMU - MOSI ) consists of 93 videos collected from the YouTube website .", "entities": [[2, 3, "DatasetName", "MOSI"], [13, 14, "DatasetName", "MOSI"]]}
{"text": "However , the provided texts of the utterances in the MOSI dataset are manually transcribed from the corresponding videos by the expert transcribers , which is unrealistic for the real - world applications to obtain the texts in such a way .", "entities": [[10, 11, "DatasetName", "MOSI"]]}
{"text": "The utilized ASR model released by Ravanelli et al ( 2021 ) is built on the transformer encoder - decoder framework and trained on the Librispeech dataset ( Panayotov et al , 2015 ) .", "entities": [[25, 26, "DatasetName", "Librispeech"]]}
{"text": "The commercial APIs used by us are IBM 2 and iFlytek 3 speech - to - text APIs , which are wildly used by researchers and software developers .", "entities": [[10, 11, "DatasetName", "iFlytek"]]}
{"text": "Finally , we apply the three ASR models to transcribe the videos into texts and construct three new datasets , namely MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek .", "entities": [[21, 22, "DatasetName", "MOSI"], [25, 26, "DatasetName", "MOSI"], [30, 31, "DatasetName", "MOSI"], [32, 33, "DatasetName", "iFlytek"]]}
{"text": "We report the WER results of the adopted ASR models on MOSI in Appendix A. Datasets Models Evaluation Metrics Has0 - Acc \u2191 Has0 - F1 \u2191 Non0 - Acc \u2191 Non0 - F1 \u2191 MAE \u2193 Corr \u2191", "entities": [[11, 12, "DatasetName", "MOSI"], [21, 22, "MetricName", "Acc"], [25, 26, "MetricName", "F1"], [29, 30, "MetricName", "Acc"], [33, 34, "MetricName", "F1"], [35, 36, "MetricName", "MAE"]]}
{"text": "MOSI - Noted that , we do not adopt MOSEI , because it does not provide the original video clips for the extracted features and annotated sentiment labels , and we can not process the original audios .", "entities": [[0, 1, "DatasetName", "MOSI"]]}
{"text": "The random seeds we used are 1111 , 1112 , 1113 , 1114 , and 1115 .", "entities": [[2, 3, "DatasetName", "seeds"]]}
{"text": "MISA ( Hazarika et al , 2020 ) adopts multi - task learning to map different modal features into a shared subspace .", "entities": [[9, 13, "TaskName", "multi - task learning"]]}
{"text": "Self - MM ( Yu et al , 2021 ) first generates the pseudo unimodal sentiment labels and then adopts them to train the model in a multi - task learning manner .", "entities": [[27, 31, "TaskName", "multi - task learning"]]}
{"text": "In And we also list the results of the SOTA model , Self - MM , on the original MOSI dataset in the last row of the table for the performance comparison between Self - MM in the ideal world and real world .", "entities": [[19, 20, "DatasetName", "MOSI"]]}
{"text": "As we can see from the results , Self - MM obtains the best results on the MOSI - Gold dataset than the other datasets , which demonstrates that the ASR errors hurt the MSA models .", "entities": [[17, 18, "DatasetName", "MOSI"]]}
{"text": "We consider that the finetuning - based models can adapt the BERT encoder to the target task and learning more informative textual representations , which also makes them benefit more as the quality of texts increases .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "Comparing to the baselines especially Self - MM , our model achieves better performance in all evaluation metrics since our model can detect the substitution error of the sentiment words and then refine the word embeddings to reconstruct the sentiment semantics in the textual modality by filtering out useless information from the input words and incorporating useful information from the candidate words generated by the language model .", "entities": [[34, 36, "TaskName", "word embeddings"]]}
{"text": "We also observe that the improvement of our model compared with Self - MM on MOSI - iFlytek is smaller .", "entities": [[15, 16, "DatasetName", "MOSI"], [17, 18, "DatasetName", "iFlytek"]]}
{"text": "We consider that the main reason is fewer sentiment word substitution errors on MOSI - iFlytek .", "entities": [[13, 14, "DatasetName", "MOSI"], [15, 16, "DatasetName", "iFlytek"]]}
{"text": "Finally , the refined word embeddings are fed into the multimodal feature fusion module to predict the sentiment label .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "To address it , we propose the sentiment word aware multimodal refinement model , which can dynamically refine the word embeddings and reconstruct the corrupted sentiment semantics by incorporating the multimodal sentiment information .", "entities": [[19, 21, "TaskName", "word embeddings"]]}
{"text": "We evaluate our model on MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek and the results demonstrate the effectiveness of our approach .", "entities": [[5, 6, "DatasetName", "MOSI"], [9, 10, "DatasetName", "MOSI"], [14, 15, "DatasetName", "MOSI"], [16, 17, "DatasetName", "iFlytek"]]}
{"text": "- 2016 Task 5 : SVM and CRF for Aspect Detection and Unsupervised Aspect - Based Sentiment Analysis", "entities": [[5, 6, "MethodName", "SVM"], [7, 8, "MethodName", "CRF"], [13, 18, "TaskName", "Aspect - Based Sentiment Analysis"]]}
{"text": "Task 5 : Aspect - Based Sentiment Analysis , for the different subtasks proposed , as well as languages and dataset contexts .", "entities": [[3, 8, "TaskName", "Aspect - Based Sentiment Analysis"], [19, 21, "DatasetName", "and dataset"]]}
{"text": "In particular , we developed a system for category detection based on SVM .", "entities": [[12, 13, "MethodName", "SVM"]]}
{"text": "Finally for aspect - based sentiment analysis we carried out an unsupervised approach based on lexicons and syntactic dependencies , in English language for laptops and restaurants domains .", "entities": [[2, 7, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "Due to this , Sentiment Analysis ( SA ) techniques have attracted the interest of researches , trying to process all this amount of information by means of usually supervised methods based on classifiers .", "entities": [[4, 6, "TaskName", "Sentiment Analysis"]]}
{"text": "This is the reason why some studies emerged about the so - called aspect - based sentiment analysis ( Marcheggiani et al , 2014 ; Lu et al , 2011 ) .", "entities": [[13, 18, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "Finally , aspect - based sentiment analysis is required for one of the subtasks , associating a polarity , which can be positive , negative or neutral , to each of the categories found in the sentence or review .", "entities": [[2, 7, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "Partof - speech ( POS ) tagging and lemmatization are performed to ensure that all the inflected forms of a word are covered .", "entities": [[8, 9, "TaskName", "lemmatization"]]}
{"text": "In the case of English , Stanford Tagger is applied due to its better results , however it does not provide lemmatization .", "entities": [[21, 22, "TaskName", "lemmatization"]]}
{"text": "That is why using the resulting form and tag , lemma is extracted by means of Freeling Tagger ( Atserias et al , 2006 ; Padr\u00f3 and Stanilovsky , 2012 ) .", "entities": [[10, 11, "DatasetName", "lemma"]]}
{"text": "Sentiment Analysis ( ABSA )", "entities": [[0, 2, "TaskName", "Sentiment Analysis"]]}
{"text": "To do this , we used a linear SVM classifier combined with word lists .", "entities": [[8, 9, "MethodName", "SVM"]]}
{"text": "The library libsvm ( Chang and Lin , 2011 ) was used to implement the SVM classifier , using the following features for each sentence : Words : those words appearing in the sentence , which are nouns , verbs or adjectives are extracted .", "entities": [[15, 16, "MethodName", "SVM"]]}
{"text": "The inputs defined for the following algorithm are the list of categories obtained from SVM for each sentence ( CList ( s ) ) and the six word lists created previously .", "entities": [[14, 15, "MethodName", "SVM"]]}
{"text": "The output is the new list per sentence , containing the old categories from SVM and the new ones added .", "entities": [[14, 15, "MethodName", "SVM"]]}
{"text": "A training file is needed to build as input for the CRF , whose structure is as follows .", "entities": [[11, 12, "MethodName", "CRF"]]}
{"text": "In the first column , all the words for every sentence are written , then in the second column , the corresponding lemma .", "entities": [[22, 23, "DatasetName", "lemma"]]}
{"text": "An adjustment was made to the system already implemented for sentiment analysis in the whole sentence , which was presented in Semeval 2015 , task 10 : sentiment analysis in Twitter ( Fern\u00e1ndez - Gavilanes et al , 2015 ) , which was also unsupervised .", "entities": [[10, 12, "TaskName", "sentiment analysis"], [27, 29, "TaskName", "sentiment analysis"]]}
{"text": "Once we performed aspect category detection at sentence - level , we use this output as input for textlevel detection .", "entities": [[3, 6, "TaskName", "aspect category detection"]]}
{"text": "Moreover , we performed a simple adaptation from our original system , made for sentiment analysis in Twitter , presented to SemEval 2015 , so there is still a lot of improvement on this field .", "entities": [[14, 16, "TaskName", "sentiment analysis"]]}
{"text": "This paper describes the participation of the GTI group , AtlantTIC Research Center , University of Vigo , in the SemEval 2016 , Task 5 : Aspect - Based Sentiment Analysis .", "entities": [[26, 31, "TaskName", "Aspect - Based Sentiment Analysis"]]}
{"text": "We developed a supervised system based on SVM classifiers for category detection , and CRFs for opinion target detection .", "entities": [[7, 8, "MethodName", "SVM"]]}
{"text": "Then , for the aspect - based sentiment analysis we submitted a fully unsupervised system , based on syntactic dependencies and context - based polarity lexicons .", "entities": [[4, 9, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "Welcome to the first Workshop on Human - Computer Question Answering ( HCQA ) !", "entities": [[9, 11, "TaskName", "Question Answering"]]}
{"text": "Question answering is a central task in natural language processing ( NLP ) .", "entities": [[0, 2, "TaskName", "Question answering"]]}
{"text": "Unlike other NLP tasks , it also is easy for non - experts to understand when question answering systems perform well ( or fail ) .", "entities": [[16, 18, "TaskName", "question answering"]]}
{"text": "The goal of this workshop is to bring the community together to discuss the state of the art of question answering and interactively compete with top human trivia masters .", "entities": [[19, 21, "TaskName", "question answering"]]}
{"text": "This workshop highlights question answering on the real - world task of quiz bowl , a trivia game in which competitors are asked to identify entities such as battles , novels , and scientific terms .", "entities": [[3, 5, "TaskName", "question answering"]]}
{"text": "While computerized question answering systems have previously had success against humans , this workshop will be the first to pit different systems against each other and then have that winner face off against a top human team .", "entities": [[2, 4, "TaskName", "question answering"]]}
{"text": "Question answering is a task interesting to both academia and industry .", "entities": [[0, 2, "TaskName", "Question answering"]]}
{"text": "Peter Clark will talk about new types of question answering problems that he and his team are solving at the Allen Institute for AI .", "entities": [[8, 10, "TaskName", "question answering"]]}
{"text": "At the end of the workshop , we will have a dual computer - human tournament to test entrants ' question answering systems against each other and against the top human trivia masters .", "entities": [[20, 22, "TaskName", "question answering"]]}
{"text": "It measures feature importance by averaging the model 's output gradient interpolated along a straight - line path in the input data space .", "entities": [[2, 4, "TaskName", "feature importance"]]}
{"text": "We apply DIG using our proposed interpolation algorithms to generate attributions for three pre - trained language models - BERT ( Devlin et al , 2019 ) , DistilBERT ( Sanh et al , 2020 ) , and RoBERTa ( Liu et al , 2019 ) , each fine - tuned separately on three sentiment classification datasets - SST2 ( Socher et al , 2013 ) , IMDB ( Maas et al , 2011 ) , and Rotten Tomatoes ( Pang and Lee , 2005 ) .", "entities": [[19, 20, "MethodName", "BERT"], [28, 29, "MethodName", "DistilBERT"], [38, 39, "MethodName", "RoBERTa"], [58, 59, "DatasetName", "SST2"], [67, 68, "DatasetName", "IMDB"]]}
{"text": "The key distinction of our formulation from IG is that DIG is agnostic of any fixed step size parameter \u03b1 and thus allows non - linear interpolation paths in the embedding space .", "entities": [[16, 18, "HyperparameterName", "step size"], [19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "\u2248 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Method DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193", "entities": [[1, 2, "MethodName", "DistilBERT"], [2, 3, "MethodName", "RoBERTa"], [3, 4, "MethodName", "BERT"]]}
{"text": "Grad * Method DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193", "entities": [[3, 4, "MethodName", "DistilBERT"], [4, 5, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}
{"text": "We use pre - trained BERT ( Devlin et al , 2019 ) , DistilBERT ( Sanh et al , 2020 ) , and RoBERTa ( Liu et al , 2019 ) text classification models individually fine - tuned for SST2 , IMDB , and RT datasets .", "entities": [[5, 6, "MethodName", "BERT"], [14, 15, "MethodName", "DistilBERT"], [24, 25, "MethodName", "RoBERTa"], [32, 34, "TaskName", "text classification"], [40, 41, "DatasetName", "SST2"], [42, 43, "DatasetName", "IMDB"]]}
{"text": "Method DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193", "entities": [[1, 2, "MethodName", "DistilBERT"], [2, 3, "MethodName", "RoBERTa"], [3, 4, "MethodName", "BERT"]]}
{"text": "For the IMDB and RT datasets , we randomly sample a subset of 2 , 000 reviews from the public test sets to compare the different methods , due to computation costs .", "entities": [[2, 3, "DatasetName", "IMDB"]]}
{"text": "For the SST2 dataset , we use the complete set of 1 , 821 test sentences .", "entities": [[2, 3, "DatasetName", "SST2"]]}
{"text": "The results are shown in Tables 1 , 2 , and 3 for SST2 , IMDB , and Rotten Tomatoes respectively .", "entities": [[13, 14, "DatasetName", "SST2"], [15, 16, "DatasetName", "IMDB"]]}
{"text": "Specifically , we find that DIG - MAXCOUNT does n't outperform DIG - GREEDY by significantly large margins on any setting ( while the opposite is true for one setting - RoBERTa fine - tuned on IMDB dataset ) .", "entities": [[31, 32, "MethodName", "RoBERTa"], [36, 37, "DatasetName", "IMDB"]]}
{"text": "We perform the study on the DistilBERT model fine - tuned on SST2 dataset and the BERT model fine - tuned on Rotten Tomatoes dataset .", "entities": [[6, 7, "MethodName", "DistilBERT"], [12, 13, "DatasetName", "SST2"], [16, 17, "MethodName", "BERT"]]}
{"text": "We find that DIG has a significantly lower mean rank compared to IG ( p < .001 on both SST2 and Rotten Tomatoes 5 ) .", "entities": [[19, 20, "DatasetName", "SST2"]]}
{"text": "The log - odds metrics of IG , the two ablations , and our best variant of DIG for DistilBERT fine - tuned individually on all three datasets are reported in Table 4 .", "entities": [[19, 20, "MethodName", "DistilBERT"]]}
{"text": "DIG ( m , f = 0 ) refers to the standard DIG with no up - sampling .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "The results are shown for DIG - MAXCOUNT applied on DistilBERT model finetuned on SST2 dataset .", "entities": [[10, 11, "MethodName", "DistilBERT"], [14, 15, "DatasetName", "SST2"]]}
{"text": "Explanation algorithms that generate attributions can be broadly classified into two categories - model - agnostic algorithms , like LIME ( Ribeiro et al , 2016 ) , Input occlusion ( Li et al , 2016 ) , Integrated gradients 6 ( Sundararajan et al , 2017 ) , SHAP ( Lundberg and Lee , 2017 ) , etc . and model - dependent algorithms , like LRP ( Binder et al , 2016 ) , DeepLIFT ( Shrikumar et al , 2017 ) , CD ( Murdoch et al , 2018 ) , ACD ( Singh et al , 2019 ) , SOC ( Jin et al , 2020 ) , etc .", "entities": [[19, 20, "MethodName", "LIME"], [49, 50, "MethodName", "SHAP"], [103, 104, "DatasetName", "SOC"]]}
{"text": "Next , we briefly describe how IG is used to explain a model 's prediction which takes a sentence as input ( for example , the model can be a text classification network ) .", "entities": [[30, 32, "TaskName", "text classification"]]}
{"text": "w 0 .. w n ] be a sentence of length n and w i be the i th word embedding of the sentence .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Our DIG approach is a reformulation of the path method where the paths are not necessarily parameterized by \u03b1 , making it more applicable for discrete data domain .", "entities": [[18, 19, "HyperparameterName", "\u03b1"]]}
{"text": "In Figure 5 , we visualize the effect of changing top - k% on log - odds , comprehensiveness , and sufficiency metrics for DistilBERT model fine - tuned on the SST2 dataset .", "entities": [[24, 25, "MethodName", "DistilBERT"], [31, 32, "DatasetName", "SST2"]]}
{"text": "We compute the Pearson correlation between logodds and WAE for each dataset + LM pair .", "entities": [[3, 5, "MetricName", "Pearson correlation"]]}
{"text": "We observe that , there is a strong correlation on average for DistilBERT .", "entities": [[12, 13, "MethodName", "DistilBERT"]]}
{"text": "For BERT and RoBERTa we find a weak positive and negative correlation respectively .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "MethodName", "RoBERTa"]]}
{"text": "In this section , we present some interesting sentence visualizations based on explanations from DIG and IG for SST2 dataset in Figure 6 .", "entities": [[18, 19, "DatasetName", "SST2"]]}
{"text": "6 : Some example visualizations of attributions from DIG and IG for the DistilBERT model fine - tuned on SST2 dataset .", "entities": [[13, 14, "MethodName", "DistilBERT"], [19, 20, "DatasetName", "SST2"]]}
{"text": "Evaluating the state - of - the - art event detection systems on determining spatio - temporal distribution of the events on the ground is performed unfrequently .", "entities": [[9, 11, "TaskName", "event detection"]]}
{"text": "But , the ability to both ( 1 ) extract events \" in the wild \" from text and ( 2 ) properly evaluate event detection systems has potential to support a wide variety of tasks such as monitoring the activity of sociopolitical movements , examining media coverage and public support of these movements , and informing policy decisions .", "entities": [[24, 26, "TaskName", "event detection"]]}
{"text": "Therefore , we study performance of the best event detection systems on detecting Black Lives Matter ( BLM ) events from tweets and news articles .", "entities": [[8, 10, "TaskName", "event detection"]]}
{"text": "While this is crucial in order to factorize the individual , linguistic subtasks composing the event extraction process , it does not estimate the overall usability of machinecoded event data sets for micro - level modelling of social processes , particularly in the domain of socio - political and armed conflict , where spatial analysis has become standard .", "entities": [[15, 17, "TaskName", "event extraction"]]}
{"text": "The complex dynamics of the Black Lives Matter movement and its varied media coverage by news outlets and social media make it a particularly relevant use case for assessing the capability of automated , Event Extraction systems to model socio - political processes .", "entities": [[34, 36, "TaskName", "Event Extraction"]]}
{"text": "The Task 3 : \" Discovering Black Lives Matter Events \" 1 organized in the context of the Challenges and Applications of Automated Extraction of Socio - political Events from Text ( CASE ) 2021 workshop aims at doing so by challenging Event Extraction ( EE ) engines to extract a collection of protest events from two heterogeneous text collections ( i.e. , news and social media ) and then measuring a number of spatiotemporal correlation coefficients against a curated Gold Standard data set of protest incidents from the BLM movement .", "entities": [[42, 44, "TaskName", "Event Extraction"]]}
{"text": "Each team 's system is compared to simple baselines in order to properly evaluate their accuracy .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "We believe this effort will shed light on system performances beyond precision , recall , and F1 .", "entities": [[16, 17, "MetricName", "F1"]]}
{"text": "The goal of this task is to evaluate the performance of automatic event detection systems on modeling the spatial and temporal pattern of a social protest movement .", "entities": [[12, 14, "TaskName", "event detection"]]}
{"text": "If a valid article was not found matching this protest date and location , then we performed a Google search for the specific event .", "entities": [[18, 19, "DatasetName", "Google"]]}
{"text": "To compensate for the lack of coverage across all of June , we used the open source data set from the The Crowd Counting Consortium ( CCC ) 4 .", "entities": [[22, 24, "TaskName", "Crowd Counting"]]}
{"text": "This time series analysis is sufficient to estimate how well the automatic systems capture the time trends of the protest movement .", "entities": [[1, 4, "TaskName", "time series analysis"]]}
{"text": "However , it does not compute accuracy of system data in estimating the spatial variation of the target process .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Moreover , we used Root Mean Squared Error ( RMSE ) to measure the absolute value of the error on estimating cell / event counts from the Gold Standard .", "entities": [[7, 8, "MetricName", "Error"], [9, 10, "MetricName", "RMSE"]]}
{"text": "The semantic distance is computed using the cosine between the projections of the sentence embeddings of the texts of the events records .", "entities": [[13, 15, "TaskName", "sentence embeddings"]]}
{"text": "Twitter data has been cleaned of hashtags , URLs , and accounts names , as these have a negative impact on the semantic similarity measure .", "entities": [[22, 24, "TaskName", "semantic similarity"]]}
{"text": "Thus , the Python library spaCy ( Honnibal et al , 2020 ) for retrieving NER / GPE entities , given its much smaller computational cost .", "entities": [[15, 16, "TaskName", "NER"]]}
{"text": "Event documents are identified using the winning solution submitted to CASE 2021 Task 1 - Subtask 1 : event document classification ( Hettiarachchi et al , 2021 ) .", "entities": [[19, 21, "TaskName", "document classification"]]}
{"text": "However , since the majority of the tweets are not geotagged and to extract the location details mentioned in the text , they used a NER approach too .", "entities": [[25, 26, "TaskName", "NER"]]}
{"text": "For NER , a transformer model is fine - tuned for token classification using the data set released with the WNUT 2017 Shared Task on Novel and Emerging Entity Recognition ( Derczynski et al , 2017 ) .", "entities": [[1, 2, "TaskName", "NER"], [11, 13, "TaskName", "token classification"], [20, 22, "DatasetName", "WNUT 2017"], [31, 32, "DatasetName", "Derczynski"]]}
{"text": "This model is a pretrained XLM - RoBERTa model , fine - tuned on the multi - language article data from Task 1 Subtask 1 and sentence data from Subtask 2 , with a classification head that predicts if the input text is a protest or not .", "entities": [[5, 6, "MethodName", "XLM"], [7, 8, "MethodName", "RoBERTa"]]}
{"text": "NoConflict Team NoConflict used their model of protest event sentence classification from the winning submission of the English version of Task 1 Subtask 2 .", "entities": [[9, 11, "TaskName", "sentence classification"]]}
{"text": "Their model is based on a RoBERTa ( Liu et al , 2019 ) backbone with a second pretraining ( Gururangan et al , 2020 ) stage done on the POLUSA ( Gebhard and Hamborg , 2020 ) data set before finetuned on Subtask 2 data .", "entities": [[6, 7, "MethodName", "RoBERTa"], [30, 31, "DatasetName", "POLUSA"]]}
{"text": "For each remaining article , they run a transformer - based ( Vaswani et al , 2017 ) named entity recognition from spaCy ( Honnibal et al , 2020 ) to identify the location and date of the events .", "entities": [[18, 21, "TaskName", "named entity recognition"]]}
{"text": "The event sentence classification system details can be found in Hu and Stoehr ( 2021 ) .", "entities": [[2, 4, "TaskName", "sentence classification"]]}
{"text": "For each potential event tweet , they identify the location and time based on the metadata of the tweet itself and the main tweet if it is a retweet .", "entities": [[28, 29, "DatasetName", "retweet"]]}
{"text": "NoConflict ( NYT ) had the highest Pearson r and lowest RMSE across all systems , as well as the highest Spearman \u03c1 ( with the Merged data ) .", "entities": [[11, 12, "MetricName", "RMSE"]]}
{"text": "In Figure 2 we plot the time series of total daily protest cells for the best performing instance of each system on New York Times ( left ) and Twitter ( right ) data , respectively .", "entities": [[6, 8, "TaskName", "time series"]]}
{"text": "The goal of the \" Discovering Black Lives Matter Events \" Shared Task was to explore novel performance evaluations of pretrained event detection systems .", "entities": [[21, 23, "TaskName", "event detection"]]}
{"text": "NexusDdpl turned out to be quite high both in terms of event detection accuracy , as well as geo - coding correlation .", "entities": [[11, 13, "TaskName", "event detection"], [13, 14, "MetricName", "accuracy"]]}
{"text": "Solving the Trigger Curse in Few - shot Event Detection via Causal Intervention", "entities": [[8, 10, "TaskName", "Event Detection"]]}
{"text": "Event detection has long been troubled by the trigger curse : overfitting the trigger will harm the generalization ability while underfitting it will hurt the detection performance .", "entities": [[0, 2, "TaskName", "Event detection"]]}
{"text": "In this paper , we identify and solve the trigger curse problem in few - shot event detection ( FSED ) from a causal view .", "entities": [[16, 18, "TaskName", "event detection"]]}
{"text": "Experiments show that our method significantly improves the FSED on ACE05 , MAVEN and KBP17 datasets .", "entities": [[12, 13, "DatasetName", "MAVEN"]]}
{"text": "Event detection ( ED ) aims to identify and classify event triggers in a sentence , e.g. , detecting an Attack event triggered by fire in \" They killed by hostile fire in Iraqi \" .", "entities": [[0, 2, "TaskName", "Event detection"]]}
{"text": "By contrast , fewshot event detection ( FSED ) aims to build effective event detectors that are able to detect new events from instances ( query ) with a few labeled instances ( support set ) .", "entities": [[4, 6, "TaskName", "event detection"]]}
{"text": "Deng et al , 2020 ; Cong et al , 2021 ) .", "entities": [[0, 3, "DatasetName", "Deng et al"]]}
{"text": "1 or 0 tion models , especially in few - shot scenario ( Bronstein et al , 2015 ; Liu et al , 2017 ; Chen et al , 2018 ; Liu et al , 2019 ; Ji et al , 2019 ) .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Due to the trigger curse , event detection models nearly degenerate to a trigger matcher , ignore the majority of contextual information and mainly rely on whether the candidate word matches the dominant triggers .", "entities": [[6, 8, "TaskName", "event detection"]]}
{"text": "This results in the decisive role of triggers in event extraction , and therefore conventional event extraction approaches commonly follow the triggercentric procedure ( i.e. , identifying triggers first and then using triggers as an indicator to find arguments in contexts ) .", "entities": [[9, 11, "TaskName", "event extraction"], [15, 17, "TaskName", "event extraction"]]}
{"text": "Furthermore , the case grammar theory in linguistics ( Fillmore , 1967 ) also formulate the language using such trigger / predicate - centric assumption , and have been widely exploited in many NLP tasks like semantic role labeling ( Gildea and Jurafsky , 2002 ) and abstract meaning representation ( Banarescu et al , 2013 ) .", "entities": [[36, 39, "TaskName", "semantic role labeling"]]}
{"text": "We conducted experiments on ACE05 1 , MAVEN 2 and KBP17 3 datasets .", "entities": [[7, 8, "DatasetName", "MAVEN"]]}
{"text": "\u221d \u03c6 ( s , q ; \u03b8 ) is the matching model between q and s parametrized by \u03b8 .", "entities": [[7, 8, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "} \u222a { t 0 } , where t i is the i - th predicted token and t 0 is the original trigger of the support set instance , then P ( t | e ) is estimated by averaging logit obtained from the MLM : P ( ti | e )", "entities": [[4, 5, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [45, 46, "DatasetName", "MLM"]]}
{"text": "\u03bb i = 0 ( 1 \u2212 \u03bb ) exp ( li ) j exp ( lj )", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "i = 0 ( 2 ) where l i is the logit for the i th token .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "To reduce the noise introduced by MLM , we assign an additional hyperparameter \u03bb ( 0 , 1 ) to t 0 .", "entities": [[6, 7, "DatasetName", "MLM"], [15, 16, "DatasetName", "0"], [21, 22, "DatasetName", "0"]]}
{"text": "Optimizing via Representation Learning .", "entities": [[2, 4, "TaskName", "Representation Learning"]]}
{"text": "Given the interventional distribution , FSED model can be learned by minimizing the loss function on it : L ( \u03b8 )", "entities": [[13, 14, "MetricName", "loss"], [20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "= \u2212 q Q f ( P ( Y | do ( C ) , e , q ; \u03b8 ) )", "entities": [[19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "( Y | s , q ; \u03b8 ) P ( s | C , t ) P ( t | e ) )", "entities": [[7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "However , the optimization of L ( \u03b8 ) needs to calculate every P ( Y | s , q ; \u03b8 ) , which is quite time - consuming .", "entities": [[7, 8, "HyperparameterName", "\u03b8"], [21, 22, "HyperparameterName", "\u03b8"]]}
{"text": "= \u2212 q Q g ( R ( q ; \u03b8 ) , t T s S P", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "R ( s ; \u03b8 ) )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "g ( , ) is a distance metric measuring the similarity between two representations .", "entities": [[6, 8, "HyperparameterName", "distance metric"]]}
{"text": "Such loss function is widely used in many metric - based methods ( e.g. , Prototypical Networks and Relation Networks ) .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "In the Appendix , we prove L SG ( \u03b8 ) is equivalent to L ( \u03b8 ) .", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [16, 17, "HyperparameterName", "\u03b8"]]}
{"text": "BERT base ( uncased ) is used as the encoder for all models and MLM for trigger collection .", "entities": [[0, 1, "MethodName", "BERT"], [14, 15, "DatasetName", "MLM"]]}
{"text": "Causal Inference .", "entities": [[0, 2, "MethodName", "Causal Inference"]]}
{"text": "Causal inference aims to make reliable predictions using the causal effect between variables ( Pearl , 2009 ) .", "entities": [[0, 2, "MethodName", "Causal inference"]]}
{"text": "Few - shot Event Detection .", "entities": [[3, 5, "TaskName", "Event Detection"]]}
{"text": "Few - shot event detection has been studied in many different settings .", "entities": [[3, 5, "TaskName", "event detection"]]}
{"text": "Deng et al ( 2020 ) decompose FSED into two subtasks : trigger identification and few - shot classification .", "entities": [[0, 3, "DatasetName", "Deng et al"]]}
{"text": "i j = 0 . Q", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "n i \u2265 0 means the number of triggers of concerned event in Q i .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "Following previous event detection works ( Chen et al , 2015 ) , the predicted trigger is correct if its event type and offsets match those of a gold trigger .", "entities": [[2, 4, "TaskName", "event detection"]]}
{"text": "We evaluate all methods using macro - F1 and micro - F1 scores , and micro - F1 is taken as the primary measure .", "entities": [[5, 8, "MetricName", "macro - F1"], [9, 12, "MetricName", "micro - F1"], [15, 18, "MetricName", "micro - F1"]]}
{"text": "Encoder We use BERT ( Devlin et al , 2019 ) to encoder the support set and the query .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Given a sentece X = { x 1 , x 2 , . . . , x n } , BERT encodes the sequence and output the represent of each token in X : R = { r 1 , r 2 , . . .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "R i | r R i r , i = 0 , 1 where", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "i , j , k = g ( p k ,", "entities": [[4, 6, "HyperparameterName", "k ="]]}
{"text": "= Softmax ( s i , j , 0 , s i , j , 1 ) ( 7 )", "entities": [[1, 2, "MethodName", "Softmax"], [8, 9, "DatasetName", "0"]]}
{"text": "During training , we use the Cross - Entropy loss on each token of query .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "p k \u2212 q j i | ) where means concatenation vectors and F is two - layer feed - forward neural networks with a ReLU function on the first layer .", "entities": [[25, 26, "MethodName", "ReLU"]]}
{"text": "We prove L SG ( \u03b8 ) is equivalent to L ( \u03b8 ) , which indicates that minimizing L SG ( \u03b8 ) is equivalent to minimizing L ( \u03b8 ) .", "entities": [[5, 6, "HyperparameterName", "\u03b8"], [12, 13, "HyperparameterName", "\u03b8"], [22, 23, "HyperparameterName", "\u03b8"], [30, 31, "HyperparameterName", "\u03b8"]]}
{"text": "At first , we define a function \u03c6 ( s , q ) \u221d P ( Y | s , q ; \u03b8 ) and then we need to prove that g ( t T s S P ( t | e ) p ( s | C , t ) r s , q )", "entities": [[22, 23, "HyperparameterName", "\u03b8"]]}
{"text": "All of our experiments are implemented on one Nvidia TITAN RTX .", "entities": [[9, 10, "DatasetName", "TITAN"]]}
{"text": "For pretraining , we train a supervised event detection model using the training set .", "entities": [[7, 9, "TaskName", "event detection"]]}
{"text": "For finetuning , we use the support set to finetune the parameters of the event detection model and then detect the event in query .", "entities": [[14, 16, "TaskName", "event detection"]]}
{"text": "In this paper , we introduce a recurrent neural network based approach for the multi - modal sentiment and emotion analysis .", "entities": [[19, 20, "DatasetName", "emotion"]]}
{"text": "Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state - of - the - art systems .", "entities": [[13, 14, "DatasetName", "emotion"]]}
{"text": "The motivation of multi - modal sentiment and emotion analysis lies in fact to leverage the varieties of ( often distinct ) information from multiple sources for building more efficient systems .", "entities": [[8, 9, "DatasetName", "emotion"]]}
{"text": "Traditionally , ' text ' has been the key factor in any Natural Language Processing ( NLP ) tasks , including sentiment and emotion analysis .", "entities": [[23, 24, "DatasetName", "emotion"]]}
{"text": "In contrast , multi - modal sentiment and emotion analysis take inputs from more than one sources e.g. text , visual , acoustic for the analysis .", "entities": [[8, 9, "DatasetName", "emotion"]]}
{"text": "In our current work , we propose an end - to - end Context - aware Interactive Attention ( CIA ) based recurrent neural network for sentiment and emotion analysis .", "entities": [[28, 29, "DatasetName", "emotion"]]}
{"text": "The main contributions of our current research are as follows : ( 1 ) We propose an Inter - modal Interactive Module ( IIM ) that aims to learn the interaction among the diverse and distinct features of the input modalities , i.e. , text , acoustic and visual ; ( 2 ) We employ a Context - aware Attention Module ( CAM ) that identifies and assigns the weights to the neighboring utterances based on their contributing features .", "entities": [[62, 63, "MethodName", "CAM"]]}
{"text": "It exploits the interactive representations of pairwise modalities to learn the attention weights , and ( 3 ) We present new state - of - the - arts for five benchmark datasets for both sentiment and emotion predictions .", "entities": [[36, 37, "DatasetName", "emotion"]]}
{"text": "al , 2018a ; Mihalcea , 2012 ; Lee et al , 2018 ; Tsai et al , 2018 ) suggest that multi - modal sentiment and emotion analysis are relatively new areas as compared to uni - modal analysis .", "entities": [[27, 28, "DatasetName", "emotion"]]}
{"text": "Feature selection ( fusion ) is a challenging and important task for any multi - modal analysis .", "entities": [[0, 2, "MethodName", "Feature selection"]]}
{"text": "al ( 2016 ) proposed a multi - kernel learning based feature selection method for multimodal sentiment and emotion recognition .", "entities": [[11, 13, "MethodName", "feature selection"], [18, 20, "TaskName", "emotion recognition"]]}
{"text": "A convolutional deep belief network ( CDBN ) is proposed in ( Ranganathan et al , 2016 ) to learn salient multi - modal features of low - intensity expressions of emotions , whereas Lee et al ( 2018 ) introduced a convolutional attention network to learn multimodal feature representation between speech and text data for multi - modal emotion recognition .", "entities": [[2, 5, "MethodName", "deep belief network"], [59, 61, "TaskName", "emotion recognition"]]}
{"text": "A feature level fusion vector was built , and then a Support Vector Machine ( SVM ) classifier was used to detect the emotional duality and mixed emotional experience in ( Patwardhan , 2017 ) .", "entities": [[11, 14, "MethodName", "Support Vector Machine"], [15, 16, "MethodName", "SVM"]]}
{"text": "Tzirakis et al ( 2017 ) introduced a Long Short Term Memory ( LSTM )", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "based end - to - end multi - modal emotion recognition system in which convolutional neural network ( CNN ) and a deep residual network are used to capture the emotional content for various styles of speaking , robust features .", "entities": [[9, 11, "TaskName", "emotion recognition"], [23, 25, "MethodName", "residual network"]]}
{"text": "al ( 2017a ) presented a literature survey on various affect dimensions e.g. , sentiment analysis , emotion analysis , etc . , for the multi - modal analysis .", "entities": [[14, 16, "TaskName", "sentiment analysis"], [17, 18, "DatasetName", "emotion"]]}
{"text": "al ( 2017b ) introduced an Long Short Term Memory ( LSTM ) based framework for sentiment classification which uses contextual information to capture interrelationships between the utterances .", "entities": [[11, 12, "MethodName", "LSTM"]]}
{"text": "Recently , Zadeh et al ( 2018c ) introduced the largest multimodal dataset namely CMU - MOSEI for sentiment and emotion analysis .", "entities": [[14, 17, "DatasetName", "CMU - MOSEI"], [20, 21, "DatasetName", "emotion"]]}
{"text": "various state - ofthe - art systems for both sentiment and emotion analysis .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "Very recently , Akhtar et al ( 2019 ) in - troduced an attention based multi - task learning framework for sentiment and emotion classification on the CMU - MOSEI dataset .", "entities": [[15, 19, "TaskName", "multi - task learning"], [23, 25, "TaskName", "emotion classification"], [27, 30, "DatasetName", "CMU - MOSEI"]]}
{"text": "In comparison to the existing systems , our proposed approach aims to exploits the interaction between the input modalities through an autoencoder based inter - modal interactive module .", "entities": [[21, 22, "MethodName", "autoencoder"]]}
{"text": "We propose an end - to - end Contextaware Interactive Attention ( CIA ) based recurrent neural network for sentiment and emotion analysis .", "entities": [[21, 22, "DatasetName", "emotion"]]}
{"text": "Next , we extract the sequential pattern of the utterances through a Bi - directional Gated Recurrent Unit ( Bi - GRU )", "entities": [[15, 18, "MethodName", "Gated Recurrent Unit"], [21, 22, "MethodName", "GRU"]]}
{"text": "In our network , we , additionally , learn the interaction among the modalities through a feedforward network .", "entities": [[16, 18, "MethodName", "feedforward network"]]}
{"text": "At first , all the three modalities are passed through a separate Bi - GRU .", "entities": [[14, 15, "MethodName", "GRU"]]}
{"text": "x i y j biGRU ( C x i y j ) C", "entities": [[4, 5, "MethodName", "biGRU"]]}
{"text": "x i biGRU ( x i ) for i , j 1 , ... , K do \u2200x , y [ T , V , A ] , and x =", "entities": [[2, 3, "MethodName", "biGRU"]]}
{"text": "i , y j CAM ( M", "entities": [[4, 5, "MethodName", "CAM"]]}
{"text": "return A wise concatenation is performed over the output of Bi - GRU and passed through a fully - connected layer to extract the bi - modal interaction ( BI ) .", "entities": [[12, 13, "MethodName", "GRU"]]}
{"text": "Further , we employ a Context - aware Attention Module ( CAM ) to exploit the correspondence among the neighboring utterances .", "entities": [[11, 12, "MethodName", "CAM"]]}
{"text": "The inputs to the CAM are the two representations for each pair of modalities , e.g. , mean representation M T A and bi - modal interaction BI T A for the text - acoustic pair .", "entities": [[4, 5, "MethodName", "CAM"]]}
{"text": "The aim is to compute the interactive attention weights utilizing a softmax activation for each utterance in the video .", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "We summarize the process of CAM in Algorithm3 .", "entities": [[5, 6, "MethodName", "CAM"]]}
{"text": "For example , in multi - modal sentiment analysis all the three modalities , i.e. , text , acoustic , and visual , aim to predict the expressed polarity of an utterance .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}
{"text": "For the evaluation of our proposed approach , we employ five multi - modal benchmark datasets 1 covering two affect analysis tasks , i.e. , sentiment and emotion .", "entities": [[27, 28, "DatasetName", "emotion"]]}
{"text": "Table 1 : Results of sentiment and emotion analysis for the proposed approach .", "entities": [[7, 8, "DatasetName", "emotion"]]}
{"text": "Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also in line with the other existing works ( Zadeh et al , 2018c", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "The above datasets offer different dimension of sentiment analysis .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}
{"text": "Two - class ( pos and neg ) classification : MO - SEI , MOSI , ICT - MMMO , and MOUD .", "entities": [[14, 15, "DatasetName", "MOSI"]]}
{"text": "Seven - class ( strong pos , moderate pos , weak pos , neu , weak neg , moderate neg , and strong neg ) classification : MOSEI and MOSI .", "entities": [[29, 30, "DatasetName", "MOSI"]]}
{"text": "Intensity prediction : MOSEI and MOSI .", "entities": [[5, 6, "DatasetName", "MOSI"]]}
{"text": "As the evaluation metric , we employ accuracy ( weighted accu - racy", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "We evaluate our proposed CIA model on five benchmark datasets i.e. , MOUD , MOSI , YouTube , ICT - MMMO , and MOSEI .", "entities": [[14, 15, "DatasetName", "MOSI"]]}
{"text": "We use tanh as the activation function for the inter - modal interactive module ( IIM ) , while we employ ReLu for the context - aware attention module .", "entities": [[5, 7, "HyperparameterName", "activation function"], [21, 22, "MethodName", "ReLu"]]}
{"text": "For each dataset , we use Adam as optimizer .", "entities": [[6, 7, "MethodName", "Adam"], [8, 9, "HyperparameterName", "optimizer"]]}
{"text": "In this paper , we address three multi - modal affective analysis problems , namely i.e. , sentiment classification ( S C ) , sentiment intensity ( S I ) and emotion classification ( E C ) .", "entities": [[31, 33, "TaskName", "emotion classification"]]}
{"text": "We use softmax as a classifier for sentiment classification , while optimizing the categorical cross - entropy as a loss function .", "entities": [[2, 3, "MethodName", "softmax"], [19, 20, "MetricName", "loss"]]}
{"text": "In comparison , we use sigmoid for prediction and binary cross - entropy as the loss function for the emotion classification .", "entities": [[15, 16, "MetricName", "loss"], [19, 21, "TaskName", "emotion classification"]]}
{"text": "As the emotions in the dataset are multi - labeled , we apply a threshold over the predicted sigmoid outputs for each emotion and consider all the emotions as present whose respective values are above the threshold .", "entities": [[22, 23, "DatasetName", "emotion"]]}
{"text": "Furthermore , for the other datasets , i.e. , MOSI , ICT - MMMO , YouTube , and MOUD , we also observe a similar phenomenon as well ( c.f . Table 1 ) .", "entities": [[9, 10, "DatasetName", "MOSI"]]}
{"text": "In particular , we compare with the following systems : Bag of Feature - Multimodal Sentiment Analysis ( BoF - MSA )", "entities": [[14, 17, "TaskName", "Multimodal Sentiment Analysis"]]}
{"text": "( Zadeh et al , 2018c ) , Tensor Fusion Network ( TFN ) , Random Forest ( RF ) ( Breiman , 2001 ) , Support Vector Machine ( Zadeh et al , 2016 ) , Multi - Attention Recurrent Network ( MARN ) ( Zadeh et al , 2018a ) , Dynamic Fusion Graph ( DFG ) ( Zadeh et al , 2018c ) , Multi Modal Multi Utterance - Bimodal Attention ( MMMU - BA ) ( Ghosal et al , 2018 ) , Bi - directional Contextual LSTM ( BC - LSTM )", "entities": [[26, 29, "MethodName", "Support Vector Machine"], [77, 78, "DatasetName", "BA"], [91, 92, "MethodName", "LSTM"], [95, 96, "MethodName", "LSTM"]]}
{"text": "We show the comparative results in Table 5a and Table 5b for emotion and sentiment analysis , respectively .", "entities": [[12, 13, "DatasetName", "emotion"], [14, 16, "TaskName", "sentiment analysis"]]}
{"text": "score and weighted accuracy , respectively , than the state - of - the - art DFG ( Zadeh", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "Furthermore , we also see improvements for most of the individual emotion classes as well .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "In sentiment analysis ( c.f . Table 5b ) , for all the five datasets and different experimental setups , the proposed CIA framework obtains the improved accuracies for the classification tasks .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}
{"text": "For intensity prediction , our proposed framework yields lesser mean - absolute - error with high Pearson correlation scores .", "entities": [[16, 18, "MetricName", "Pearson correlation"]]}
{"text": "In Table 6 , we list the utterances of a CMU - MOSEI video along with their correct and predicted labels for both the proposed and baseline systems .", "entities": [[10, 13, "DatasetName", "CMU - MOSEI"]]}
{"text": "We also analyze the context - aware attention module ( CAM ) with the help of heatmaps of the attention weights .", "entities": [[10, 11, "MethodName", "CAM"]]}
{"text": "In Figure 3a , each cell ( i , j ) of the heatmap signifies the weights of utterance ' j ' for the classification of utterance ' i ' .", "entities": [[13, 14, "MethodName", "heatmap"]]}
{"text": "We argue that the proposed CAM module captures the diversity in the input modalities of the contextual utterances for the correct prediction .", "entities": [[5, 6, "MethodName", "CAM"]]}
{"text": "For emotion prediction , the CIA model captures all the emotions correctly , while the CIA - IIM framework fails to predict the correct emotions of the utterances , u 2 and u 3 .", "entities": [[1, 2, "DatasetName", "emotion"]]}
{"text": "For the same video , we also show the attention heatmaps for emotion in Figure 3 .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "For the utterance u 2 , our proposed model ( CIA ) captures the emotion class ' sad ' as the CAM module assigns higher attention weights on the utterances u 2 and u 3 in Figure 3d , u 4 in Figure 3e , and u 2 in Figure 3f .", "entities": [[14, 15, "DatasetName", "emotion"], [21, 22, "MethodName", "CAM"]]}
{"text": "Since the system finds the contributing neighbours as utterances u 2 , u 3 and u 4 for various combinations , we argue that it utilizes the information of these utterances - which all express the ' sad ' emotion - for the correct prediction of utterance u 2 as ' sad ' .", "entities": [[39, 40, "DatasetName", "emotion"]]}
{"text": "In this paper , we have proposed a Context - aware Interactive Attention framework that aims to capture the interaction between the input modalities for the multi - modal sentiment and emotion prediction .", "entities": [[31, 32, "DatasetName", "emotion"]]}
{"text": "Experiments suggest the effectiveness of the proposed model over various existing systems , for both sentiment and emotion analysis , as we obtained new state - of - the - art for all five datasets .", "entities": [[17, 18, "DatasetName", "emotion"]]}
{"text": "In current work , we undertook the problem of sentiment and emotion analysis for a single - party utterances .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "Improving Graph - based Sentence Ordering with Iteratively Predicted Pairwise Orderings", "entities": [[4, 6, "TaskName", "Sentence Ordering"]]}
{"text": "Dominant sentence ordering models can be classified into pairwise ordering models and set - to - sequence models .", "entities": [[1, 3, "TaskName", "sentence ordering"]]}
{"text": "In this paper , we propose a novel sentence ordering framework which introduces two classifiers to make better use of pairwise orderings for graph - based sentence ordering ( Yin et al , 2019 ( Yin et al , , 2021 .", "entities": [[8, 10, "TaskName", "sentence ordering"], [26, 28, "TaskName", "sentence ordering"]]}
{"text": "At last , we adapt a GRN - based sentence ordering model (", "entities": [[9, 11, "TaskName", "sentence ordering"]]}
{"text": "Particularly , when equipped with BERT ( Devlin et al , 2019 ) and FHDecoder ( Yin et al , 2020 ) , our model achieves state - of - the - art performance .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "As an important subtask , sentence ordering aims at recovering unordered sentences back to naturally coherent paragraphs .", "entities": [[5, 7, "TaskName", "sentence ordering"]]}
{"text": "It is required to deal with logic and syntactic consistency , and has increasingly attracted attention due to its wide applications on several tasks such as text generation ( Konstas and Lapata , 2012 ; Holtzman et al , 2018 )", "entities": [[26, 28, "TaskName", "text generation"]]}
{"text": "Recently , inspired by the great success of deep learning in other NLP tasks , researchers have resorted to neural sentence ordering models , which can be classified into : pairwise ordering models Agrawal et", "entities": [[20, 22, "TaskName", "sentence ordering"]]}
{"text": "al , 2016 ; Li and Jurafsky , 2017 ; Moon et al , 2019 ; Kumar et al , 2020 ; Prabhumoye et al , 2020 ; Zhu et al , 2021 ) and set - to - sequence models ( Gong et al , 2016 ; Nguyen and Joty , 2017 ; Logeswaran et al , 2018 ; Mohiuddin et al , 2018 ; Cui et", "entities": [[16, 17, "DatasetName", "Kumar"]]}
{"text": "By contrast , the latter is mainly based on an encoder - decoder framework , where an encoder is first used to learn contexualized sentence representations by considering other sentences , and then a decoder , such as pointer network ( Vinyals et al , 2015a ) , outputs ordered sentences .", "entities": [[38, 40, "MethodName", "pointer network"]]}
{"text": "et al ( 2020 ) propose FHDecoder that is equipped with three pairwise ordering prediction modules to enhance the pointer network decoder .", "entities": [[19, 21, "MethodName", "pointer network"]]}
{"text": "Along this line , Cui et al ( 2020 ) introduce BERT to exploit the deep semantic connection and relative orderings between sentences and achieve SOTA performance when equipped with FHDecoder .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "Therefore , we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited .", "entities": [[12, 14, "TaskName", "sentence ordering"]]}
{"text": "In this paper , we propose a novel iterative pairwise ordering prediction framework which introduces two classifiers to make better use of pairwise orderings for graph - based sentence ordering ( Yin et al , 2019 ( Yin et al , , 2021 .", "entities": [[28, 30, "TaskName", "sentence ordering"]]}
{"text": "Based on the final weighted graph representation , we adapt SE - GRN to construct a graph - based sentence ordering model , of which the decoder is also a pointer network .", "entities": [[19, 21, "TaskName", "sentence ordering"], [30, 32, "MethodName", "pointer network"]]}
{"text": "To the best of our knowledge , our work is the first to exploit pairwise orderings to enhance the graph encoding for graph - based set - to - squence sentence ordering .", "entities": [[30, 32, "TaskName", "sentence ordering"]]}
{"text": "Early studies mainly focused on exploring humandesigned features for sentence ordering ( Lapata , 2003 ; Barzilay and Lee , 2004 ; Lapata , 2005 , 2008 ; Elsner and Charniak , 2011 ; Guinaudeau and Strube , 2013 ) .", "entities": [[9, 11, "TaskName", "sentence ordering"]]}
{"text": "Recently , neural network based sentence ordering models have become dominant , consisting of the following two kinds of models : 1 ) Pairwise models .", "entities": [[5, 7, "TaskName", "sentence ordering"]]}
{"text": "al , 2016 ; Li and Jurafsky , 2017 ; Kumar et al , 2020 ; Prabhumoye et al , 2020 ; Zhu et al , 2021 ) .", "entities": [[10, 11, "DatasetName", "Kumar"]]}
{"text": "For example , first framed sentence ordering as a ranking task conditioned on pairwise scores .", "entities": [[5, 7, "TaskName", "sentence ordering"]]}
{"text": "Inspired by the successful applications of graph neural network ( GNN ) in many NLP tasks Xue et al , 2019 ; , Yin", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Very recently , we notice that Chowdhury et al ( 2021 ) proposes a BART - based sentence ordering model .", "entities": [[14, 15, "MethodName", "BART"], [17, 19, "TaskName", "sentence ordering"]]}
{"text": "Please note that our porposed framework is compatible with BART ( Lewis et al , 2020 ) .", "entities": [[9, 10, "MethodName", "BART"]]}
{"text": "example , we can easily adapt the BART encoder as our sentence encoder .", "entities": [[7, 8, "MethodName", "BART"]]}
{"text": "al ( 2020 ) introduced three pairwise ordering predicting modules ( FHDecoder ) to enhance the pointer network decoder of ATTOrder - Net .", "entities": [[16, 18, "MethodName", "pointer network"]]}
{"text": "Recently , Cui et al ( 2020 ) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "However , significantly different from them , we borrow the idea from the mask - predict framework ( Gu et al , 2018 ; Ghazvininejad et al , 2019 ; Deng et al , 2020 ) to progressively incorporate pairwise ordering information into SE - Graph , which is the basis of our graph - based sentence ordering model .", "entities": [[30, 33, "DatasetName", "Deng et al"], [56, 58, "TaskName", "sentence ordering"]]}
{"text": "To the best of our knowledge , our work is the first attempt to explore iteratively refined GNN for sentence ordering .", "entities": [[19, 21, "TaskName", "sentence ordering"]]}
{"text": "As shown in Figure 1 , SE - GRN is composed of a Bi - LSTM sentence encoder , GRN paragraph encoder , and a pointer network ( Vinyals et al , 2015b ) decoder .", "entities": [[15, 16, "MethodName", "LSTM"], [25, 27, "MethodName", "pointer network"]]}
{"text": "Node representations of each sentence and each entity are first initialized with the concatenation of bidirectional last states of the Bi - LSTM sentence encoder and the corresponding GloVe word embedding , respectively .", "entities": [[22, 23, "MethodName", "LSTM"], [28, 29, "MethodName", "GloVe"]]}
{"text": "Afterwards , \u03ba ( l - 1 ) i is updated by concatenating its original representation \u03ba ( 0 ) i , the messages from neighbours ( m ( l ) i andm ( l ) i ) and the global state g ( l - 1 ) via GRU : \u03be ( l ) i", "entities": [[18, 19, "DatasetName", "0"], [49, 50, "MethodName", "GRU"]]}
{"text": "= [ \u03ba ( 0 ) i ; m ( l ) i ; m ( l ) i ; g ( l - 1 ) ] , \u03ba ( l ) i = GRU ( \u03be ( l ) i , \u03ba ( l - 1 ) i ) .", "entities": [[4, 5, "DatasetName", "0"], [34, 35, "MethodName", "GRU"]]}
{"text": "v i N jw ( ( l - 1 ) j , \u03ba ( l - 1 ) i , rij ) \u03ba ( l - 1 ) i , m ( l ) j = v j N jw ( ( l - 1 ) j , ( l - 1 ) j ) ( l - 1 ) j , \u03be ( l ) j = [ embj ; m ( l ) j ; m ( l ) j ; g ( l - 1 ) ] , ( l ) j = GRU ( \u03be ( l ) j , ( l - 1 ) j ) .", "entities": [[96, 97, "MethodName", "GRU"]]}
{"text": "( 3 ) Finally , the messages from both sentence and entity states are used to update global state g ( l - 1 ) via g ( l ) = GRU ( 1 | V |", "entities": [[31, 32, "MethodName", "GRU"]]}
{"text": "= softmax ( q T tanh ( W h d t + U K ( L ) o t\u22121 ) ) , h d t = LSTM ( h d t\u22121 , \u03ba ( 0 ) o t\u22121 ) .", "entities": [[1, 2, "MethodName", "softmax"], [26, 27, "MethodName", "LSTM"], [34, 35, "DatasetName", "0"]]}
{"text": "Then , we adapt the conventional GRN to establish a neural sentence ordering model based on the final IRSE - Graph .", "entities": [[11, 13, "TaskName", "sentence ordering"]]}
{"text": "Inspired by Gui et al ( 2020 ) , we successively introduce two classifiers - initial classifier and iterative classifier to construct IRSE - Graph .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "and [ \u03ba i ; \u03ba i ] , which are fed into an MLP classifier to obtain two probabilities .", "entities": [[14, 15, "DatasetName", "MLP"]]}
{"text": "In Step 2 , on the top of learned sentence representations , we stack an MLP classifier to predict pairwise orderings for sentence node pairs in VP ( k ) ( Lines 16 - 19 ) .", "entities": [[15, 16, "DatasetName", "MLP"]]}
{"text": "The procedure of constructing IRSE - Graph Input : the initial IRSE - Graph : G= ( V , E , W ) with all w i , i = 0 ; two thresholds : \u03b4min , \u03b4max Output : the final IRSE - Graph : G = ( V , E , W ) 1 : VP ( 0 ) 2 : { \u03bai } I i=1 GRN ( G ) 3 : for any linked sentence node pair ( vi , v i ) & & i < i do 4 : w i , i InitialClassifer ( [ \u03bai ; \u03ba i ] ) 5 : w i , i InitialClassifer ( [", "entities": [[30, 31, "DatasetName", "0"], [59, 60, "DatasetName", "0"]]}
{"text": "if \u03b4min \u2264 w i , i \u2264 \u03b4max then 8 : VP ( 0 ) VP ( 0 ) \u222a { ( vi ,", "entities": [[14, 15, "DatasetName", "0"], [18, 19, "DatasetName", "0"]]}
{"text": "w i , i 0.5 10 : end if 11 : end for 12 : k 0 13 : repeat 14 : iteration ( Lines 20 - 23 ) .", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "To enable iterative classifier generalizable to any IRSE - Graph with partial predicted pairwise orderings , we first set al ss - edge weights to 1 or 0 according to their ground - truth pairwise orderings , and then train the classifier to correctly predict pari - wise orderings for other pairs .", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "i , i = 1 and w i , i = 0 , vice versa .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "For example , in the left part of Figure 3 , the ground - truth sentence sequence is s 1 , s 2 , s 3 , s 4 , and thus we assign the ss - edge weights of linked sentence node pairs ( v 1 , v 2 ) , ( v 3 , v 2 ) , ( v 3 , v 4 ) , ( v 2 , v 4 ) as follows : w 1 , 2 = 1 , w 2 , 3 = 1 , w 2 , 4 = 1 , w 3 , 4 = 1 , and w 2 , 1 = 0 , w 3 , 2 = 0 , w 4 , 2 = 0 , w 4 , 3 = 0 .", "entities": [[112, 113, "DatasetName", "0"], [119, 120, "DatasetName", "0"], [126, 127, "DatasetName", "0"], [133, 134, "DatasetName", "0"]]}
{"text": "al , , 2021 , we construct a graph - based sentence ordering model .", "entities": [[11, 13, "TaskName", "sentence ordering"]]}
{"text": "Note that the above two classifiers and our sentence ordering model are all based on IRSE - Graph rather than the conventional SE - Graph , which makes the standard GRN unable to be applied directly .", "entities": [[8, 10, "TaskName", "sentence ordering"]]}
{"text": "Cui et al , 2018 ; Yin et al , 2021 ) , we carry out experiments on five benchmark datasets : SIND , ROCStory .", "entities": [[22, 23, "DatasetName", "SIND"]]}
{"text": "SIND is a visual storytelling dataset and ROCStory ( Mostafazadeh et al , 2016 ) is about commonsense stories .", "entities": [[0, 1, "DatasetName", "SIND"], [3, 5, "TaskName", "visual storytelling"]]}
{"text": "NIPS Abstract , AAN Abstract , arXiv Abstract .", "entities": [[6, 7, "DatasetName", "arXiv"]]}
{"text": "These three datasets consist of abstracts from research papers , which are collected from NIPS , ACL anthology and arXiv , respectively ( Radev et al , 2016 ; ( Yin et al , 2021 ) for our model and its variants .", "entities": [[19, 20, "DatasetName", "arXiv"]]}
{"text": "Specifically , we apply 100 - dimensional GloVe word embeddings , and set the sizes of Bi - LSTM hidden states , sentence node states , and entity node states as 512 , 512 and 150 , respectively .", "entities": [[7, 8, "MethodName", "GloVe"], [8, 10, "TaskName", "word embeddings"], [18, 19, "MethodName", "LSTM"]]}
{"text": "When constructing our model based on BERT , we use the same settings as ( Cui et al , 2020 ) .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Besides , we report the performance of following sentence ordering models : 1 ) Pairwise models : Pairwise Model , RankTxNet ( Kumar et al , 2020 ) , andB - TSort ( Prabhumoye et al , 2020 ) , ConsGraph ( Zhu et al , 2021 ) ; 2 ) Set - to - sequence models : HAN ( Wang and Wan , 2019 ) , LSTM+PtrNet ( Gong et al , 2016 ) ,", "entities": [[8, 10, "TaskName", "sentence ordering"], [22, 23, "DatasetName", "Kumar"]]}
{"text": "In addition to FHDecoder , we construct the sentence encoder based on BERT , where the mean - pooling outputs of all learned word representations are used to initialize sentence nodes .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "Table 1 reports the overall experimental results of sentence ordering .", "entities": [[8, 10, "TaskName", "sentence ordering"]]}
{"text": "When incorporating BERT and FHDecoder into IRSE - GRN , our model achieves SOTA performance on most of datasets .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "Second , IRSE - GRN+FHDecoder exhibits better performance than IRSE - GRN and all non - BERT baselines , which are shown above the upper dotted line of Table 1 , across datasets in different domains .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "Third , when constructing our model based on BERT , IRSE - GRN+BERT+FHDecoder also outperforms all BERT - based baselines , such as Cons - Graph , BERSON , achieving SOTA performance .", "entities": [[8, 9, "MethodName", "BERT"], [16, 17, "MethodName", "BERT"]]}
{"text": "Finally , we note that IRSE - GRN+BERT+FH - Decoder gains relatively marginal improvement on SIND and ROCStory , and performs worse than BERSON in PMR on SIND .", "entities": [[15, 16, "DatasetName", "SIND"], [27, 28, "DatasetName", "SIND"]]}
{"text": "Specifically , average edge numbers of SIND and ROCStory are 2.85 and 5.66 respectively , far fewer than 16.60 , 10.86 and 16.73 on NIPS Abstract , ANN Abstract and arXiv Abstract .", "entities": [[6, 7, "DatasetName", "SIND"], [30, 31, "DatasetName", "arXiv"]]}
{"text": "As displayed in Table 3 , IRSE - GRN surpasses all non - BERT baselines , and IRSE - GRN+BERT+ FHDecoder wins against BERTSON .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "We conduct several experiments to investigate the impacts of our proposed components on ROCstory dataset and arXiv dataset which are the two largest datasets .", "entities": [[16, 17, "DatasetName", "arXiv"]]}
{"text": "Following previous studies ( Barzilay and Lapata , 2005 ; Nayeem and Chali , 2017 ) spect the validity of our proposed framework via multi - document summarization .", "entities": [[24, 28, "TaskName", "multi - document summarization"]]}
{"text": "Concretely , we train different neural sentence ordering models on a large - scale summarization corpus ( Fabbri et al , 2019 ) , and then individually use them to reorder the small - scale summarization data of DUC2004 ( Task2 ) .", "entities": [[6, 8, "TaskName", "sentence ordering"], [14, 15, "TaskName", "summarization"], [35, 36, "TaskName", "summarization"]]}
{"text": "In this work , we propose a novel sentence ordering framework that makes better use of pairwise orderings for graph - based sentence ordering .", "entities": [[8, 10, "TaskName", "sentence ordering"], [22, 24, "TaskName", "sentence ordering"]]}
{"text": "Then , based on this refined graph , we construct a graph - based sentence ordering model .", "entities": [[14, 16, "TaskName", "sentence ordering"]]}
{"text": "Moreover , when equipped with BERT and FHDecoder , our enhanced model achieves SOTA performance across datasets .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "In the future , we plan to explore more effective GNN for sentence ordering .", "entities": [[12, 14, "TaskName", "sentence ordering"]]}
{"text": "In agreement with Rakuten catalog analysts we set the most popular spelling of an author name as the one found on Wikipedia 4 or DBpedia ( Lehmann et al , 2015 ) .", "entities": [[24, 25, "DatasetName", "DBpedia"]]}
{"text": "The canonical form is the one that is matched with Wikipedia or DBpedia ; else the one provided by the greatest number of sources .", "entities": [[12, 13, "DatasetName", "DBpedia"]]}
{"text": "2 . Matching of Rakuten authors : we build entities using fuzzy search on the author name field on DBpedia and consider the DBpedia value to be canonical .", "entities": [[19, 20, "DatasetName", "DBpedia"], [23, 24, "DatasetName", "DBpedia"]]}
{"text": "3 . Name variants : DBpedia , BnF , and JRCnames ( Steinberger et al , 2011 ; Maud et", "entities": [[5, 6, "DatasetName", "DBpedia"]]}
{"text": "As an example , by using the wikiPageRedirects field in DBpedia we can build a large entity for the canonical name \" Anton Tchekhov \" , containing \" Anton Tchechov \" , \" Ant\u00f2n P\u00e0vlovi\u010d Ch\u00e9chov \" , \" Checkhov \" , \" Anton Chekov \" , and many more .", "entities": [[10, 11, "DatasetName", "DBpedia"]]}
{"text": "Each network is a recurrent neural network ( RNN ) composed of a character - level embedding layer with 256 units , a bidirectional long shortterm memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) with 2 \u00d7 128 units , and a dense layer with 256 units .", "entities": [[28, 29, "MethodName", "LSTM"]]}
{"text": "Each network takes a name as input and outputs a representation - the two representations are then compared using cosine similarity with a target value equal to 1 for name variants of the same entity , and to 0 otherwise .", "entities": [[38, 39, "DatasetName", "0"]]}
{"text": "At test time , we search for the canonical name whose representation is closest to that of the query , using only the high - quality name entities from DBpedia , BnF , and JRC - names .", "entities": [[29, 30, "DatasetName", "DBpedia"]]}
{"text": "The dataset of name entities is again employed to train a sequence - to - sequence ( seq2seq ) model ( Sutskever et al , 2014 ) to produce the canonical form of a name from one of its variants .", "entities": [[17, 18, "MethodName", "seq2seq"]]}
{"text": "The seq2seq model is an encoder - decoder using RNNs , with a character embedding layer , as in the case of the Siamese network .", "entities": [[1, 2, "MethodName", "seq2seq"], [23, 25, "MethodName", "Siamese network"]]}
{"text": "The encoder is a bi - directional LSTM with 2 \u00d7 256 units , while the decoder is a plain LSTM with 512 units connected to a softmax layer that computes a probability distribution over the characters .", "entities": [[7, 8, "MethodName", "LSTM"], [20, 21, "MethodName", "LSTM"], [27, 28, "MethodName", "softmax"]]}
{"text": "For this purpose we train a logistic regression to estimate the probability that a proposal is the canonical form for an author 's name .", "entities": [[6, 8, "MethodName", "logistic regression"]]}
{"text": "Specifically , we represent a proposal with a set of 12 features : 11 indicating whether it is found in the bibliographic sources , generated from the seq2seq model , matched with the Siamese network or equal to the input name , and one last feature corresponding to the cosine distance between the representation of the proposal and that of the input name .", "entities": [[27, 28, "MethodName", "seq2seq"], [33, 35, "MethodName", "Siamese network"]]}
{"text": "Siamese approximate name matching We evaluate the Siamese network on a held out test set , and compare it to an n - gram distance , by checking that the nearest neighbor of a name variant is the canonical name of the entity to which it belongs .", "entities": [[7, 9, "MethodName", "Siamese network"]]}
{"text": "Siamese networks are more effective than simpler rule - based approaches and more specifically they perform better than the n - gram baseline on the following cases : Vittorio Hugo Victor Hugo : capturing name variants in different languages ; Bill Shakespeare William Shakespeare : capturing common nicknames Name correction with seq2seq networks Similarly to the previous approach , the seq2seq network is evaluated on a held out test set by checking that one of the generated name variants is the canonical name of the entity to which it belongs .", "entities": [[51, 52, "MethodName", "seq2seq"], [60, 61, "MethodName", "seq2seq"]]}
{"text": "As expected , name normalization using seq2seq network gives poorer performances than approximate matching within a dataset of known authors , but constitutes a complementary approach that is useful in case of formatting issues or incomplete names .", "entities": [[6, 7, "MethodName", "seq2seq"]]}
{"text": "Some examples where seq2seq performs better than the other methods are as follows : V. Hugo Victor Hugo : first name prediction for authors we do n't have in the canonical database ; Vicor Hugo Victor Hugo : misspelling correction for authors we do n't have in the canonical database .", "entities": [[3, 4, "MethodName", "seq2seq"]]}
{"text": "The three most important contributions are the match with the Siamese network , the match via ISBN in Babelio , and the similarity with the input catalog name , confirming the relevance of a multi - approach design choice .", "entities": [[10, 12, "MethodName", "Siamese network"]]}
{"text": "Named entity linking ( Shen et al , 2015 ) , where one aims at determining the identity of entities ( such as a person 's name ) mentioned in text , is another related problem .", "entities": [[1, 3, "TaskName", "entity linking"]]}
{"text": "The crucial difference with the disambiguation of book authors is that entity linking systems leverage the context of the named entity mention to link unambiguously to an entity in a pre - populated knowledge base .", "entities": [[11, 13, "TaskName", "entity linking"]]}
{"text": "To the best of our knowledge , the problem of normalization of book authors name has not been tackled in the previous literature , except for a work on named entity linking for French writers ( Frontini et al , 2015 ) .", "entities": [[30, 32, "TaskName", "entity linking"]]}
{"text": "Adversarial examples have successfully tricked deep neural networks for image classification : two images that look exactly the same to a human receive \u21e4 *", "entities": [[9, 11, "TaskName", "image classification"]]}
{"text": "Figure 1 : An adversarial example generated by TFAD - JUSTED for BERT fine - tuned on the Rotten Tomatoes sentiment analysis dataset .", "entities": [[12, 13, "MethodName", "BERT"], [20, 22, "TaskName", "sentiment analysis"]]}
{"text": "When used for adversarial training , TEXTFOOLER 's examples decreased model accuracy , but TFADJUSTED 's examples did not .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "Demonstration that reported differences in attack success between TEXTFOOLER and GENET - ICATTACK are the result of more lenient constraint enforcement .", "entities": [[10, 11, "MethodName", "GENET"]]}
{"text": "GENETICATTACK uses a genetic algorithm to attack an LSTM trained on the IMDB 6 document - level sentiment classification dataset .", "entities": [[8, 9, "MethodName", "LSTM"], [12, 13, "DatasetName", "IMDB"]]}
{"text": "TEXTFOOLER uses a greedy approach to attack an LSTM , CNN , and BERT trained on five classification datasets .", "entities": [[8, 9, "MethodName", "LSTM"], [13, 14, "MethodName", "BERT"]]}
{"text": "7 They successfully attack two of the most effective models for text classification : LSTM and BERT .", "entities": [[11, 13, "TaskName", "text classification"], [14, 15, "MethodName", "LSTM"], [16, 17, "MethodName", "BERT"]]}
{"text": "To generate examples for evaluation , we attacked BERT using TEXTFOOLER and attacked an LSTM using GENETICATTACK .", "entities": [[8, 9, "MethodName", "BERT"], [14, 15, "MethodName", "LSTM"]]}
{"text": "We evaluate both methods on the IMDB dataset .", "entities": [[6, 7, "DatasetName", "IMDB"]]}
{"text": "In addition , we evaluate TEXTFOOLER on the Yelp polarity document - level sentiment classification dataset and the Movie Review ( MR ) sentence - level sentiment classification dataset ( Pang and Lee , 2005 ; Zhang et al , 2015 ) .", "entities": [[21, 22, "DatasetName", "MR"]]}
{"text": "( Liang et al , 2017 ; Samanta and Mehta , 2017 ) 3 3 3 3 General Paraphrase .", "entities": [[17, 18, "DatasetName", "General"]]}
{"text": "Automatic evaluation of semantic similarity is a well - studied NLP task .", "entities": [[3, 5, "TaskName", "semantic similarity"]]}
{"text": "The STS Benchmark is used as a common measurement ( Cer et al , 2017 ) .", "entities": [[1, 3, "DatasetName", "STS Benchmark"]]}
{"text": "Michel et al ( 2019 ) explored the use of common evaluation metrics for machine translation as a proxy for semantic similarity in the attack setting .", "entities": [[14, 16, "TaskName", "machine translation"], [20, 22, "TaskName", "semantic similarity"]]}
{"text": "While n - gram overlap based approaches are computationally cheap and work well in the machine translation setting , they do not correlate with human judgment as well as sentence encoders .", "entities": [[15, 17, "TaskName", "machine translation"]]}
{"text": "Some attacks have used sentence encoders to encode two sentences into a pair of fixed - length vectors , then used the cosine distance between the vectors as a proxy for semantic similarity .", "entities": [[31, 33, "TaskName", "semantic similarity"]]}
{"text": "Another option is BERT fine - tuned for semantic similarity , which achieved a score of 0.865 ( Devlin et al , 2018 ) .", "entities": [[3, 4, "MethodName", "BERT"], [8, 10, "TaskName", "semantic similarity"]]}
{"text": "To quantify semantic similarity of x and x adv , we asked users whether they agreed that the changes between the two passages preserved meaning on a scale of 1 ( Strongly Disagree ) to 5 ( Strongly Agree ) .", "entities": [[2, 4, "TaskName", "semantic similarity"]]}
{"text": "LanguageTool uses rules to detect grammatical errors , statistics to detect uncommon sequences of words , and language model perplexity to detect commonly confused words .", "entities": [[19, 20, "MetricName", "perplexity"]]}
{"text": "A perturbed example x adv is not suspicious if the percentage of judges who identify x adv as computer - altered is at most \u270f ns , where 0 \uf8ff \u270f ns \uf8ff 1 .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "For example , Warstadt et al ( 2018 ) trained sentence encoders on a real / fake task as a proxy for evaluation of linguistic acceptability .", "entities": [[24, 26, "TaskName", "linguistic acceptability"]]}
{"text": "As this is a time - consuming task for long documents , we only evaluated adversarial examples generated by TEXTFOOLER on the sentence - level MR dataset .", "entities": [[25, 26, "DatasetName", "MR"]]}
{"text": "To enforce semantic preservation , we tuned two thresholds which filter out invalid word substitutions : ( a ) minimum cosine similarity between counter - fitted word embeddings and ( b ) minimum cosine similarity between sentence embeddings .", "entities": [[26, 28, "TaskName", "word embeddings"], [36, 38, "TaskName", "sentence embeddings"]]}
{"text": "We used the IMDB , Yelp , and MR datasets for classifcation as in Section 4 .", "entities": [[3, 4, "DatasetName", "IMDB"], [8, 9, "DatasetName", "MR"]]}
{"text": "We added the SNLI and MNLI entailment datasets ( Bowman et al , 2015 ; Williams et al , 2018 ) for the portions not requring human evaluation .", "entities": [[3, 4, "DatasetName", "SNLI"], [5, 6, "DatasetName", "MNLI"]]}
{"text": "Quality Examples Using the 9 , 595 samples in the MR training set as seed inputs , TEXTFOOLER generated 7 , 382 adversarial examples , while TFADJUSTED generated just 825 .", "entities": [[10, 11, "DatasetName", "MR"]]}
{"text": "We append each set of adversarial examples to a copy of the original MR training set and fine - tuned a pre - trained BERT model for 10 epochs .", "entities": [[13, 14, "DatasetName", "MR"], [24, 25, "MethodName", "BERT"]]}
{"text": "While neither training method strongly impacts accuracy , the augmentation using TFADJUSTED has a better impact than that of TEXTFOOLER .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "We then re - ran the two attacks using 1000 examples from the MR test set as seeds .", "entities": [[13, 14, "DatasetName", "MR"], [17, 18, "DatasetName", "seeds"]]}
{"text": "Again averaging over 5 random seeds , we found no significant change in robustness .", "entities": [[5, 6, "DatasetName", "seeds"]]}
{"text": "That is , models trained on the original MR dataset were approximately as robust as those trained on the datasets augmented with TEXTFOOLER and TFADJUSTED examples .", "entities": [[8, 9, "DatasetName", "MR"]]}
{"text": "Attacks were run on 1000 samples against BERT fine - tuned on the MR dataset .", "entities": [[7, 8, "MethodName", "BERT"], [13, 14, "DatasetName", "MR"]]}
{"text": "As models improve at measuring semantic similarity , we will be able to more rigorously ensure that adversarial perturbations preserve semantics .", "entities": [[5, 7, "TaskName", "semantic similarity"]]}
{"text": "It remains to be seen how robust BERT is when subject to paraphrase attacks that rigorously preserve semantics and grammaticality .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "Michel et al ( 2019 ) defined a framework for evaluating attacks on machine translation models , focusing on meaning preservation constraints , but restricted their definitions to sequence - to - sequence models .", "entities": [[13, 15, "TaskName", "machine translation"]]}
{"text": "Probabilistic Case - based Reasoning for Open - World Knowledge Graph Completion", "entities": [[9, 12, "TaskName", "Knowledge Graph Completion"]]}
{"text": "If such a system can achieve high accuracy , it is appealing owing to its simplicity , interpretability , and scalability .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "An advantage of non - parametric models is that it can adapt to growing data by adjusting its number of parameters .", "entities": [[18, 21, "HyperparameterName", "number of parameters"]]}
{"text": "Our approach is non - parametric because - ( a ) Instead of storing reasoning rules in parameters ( Das et al , 2018 ; Minervini et al , 2020 ) , it derives them dynamically from k - similar entities ( like a non - parametric k - nn classifier ( Cover and Hart , 1967 ) ) .", "entities": [[47, 50, "MethodName", "k - nn"]]}
{"text": "Each entity is represented as a sparse vector of its outgoing edge types , i.e. e i { 0 , 1 } | R | .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "Agglomerative clustering has been shown to be effective in many knowledge - base related tasks such as entity resolution ( Lee et al , 2012 ; Vashishth et al , 2018 ) and in general has shown to outperform flat clustering methods such as K - means ( Green et al , 2012 ;", "entities": [[17, 19, "TaskName", "entity resolution"]]}
{"text": "However , retraining to obtain entity embeddings on industrial scale KGs might be impractical ( e.g. consider Facebook social graph where new users are joining continuously ) .", "entities": [[5, 7, "TaskName", "entity embeddings"]]}
{"text": "Recall , that we represent entities as a sparse vector of its edge types and hence this step is trivial for our approach .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "NELL - 995 , FB122 ( Guo et al , 2016 ) , WN18RR ( Dettmers et al , 2018 ) .", "entities": [[0, 1, "DatasetName", "NELL"], [4, 5, "DatasetName", "FB122"], [13, 14, "DatasetName", "WN18RR"]]}
{"text": "FB122 is a subset of the dataset derived from Freebase , FB15 K ( Bordes et al , 2013 ) , containing 122 relations regarding people , locations , and sports .", "entities": [[0, 1, "DatasetName", "FB122"]]}
{"text": "NELL - 995 ( Xiong et al , 2017 ) a subset of the NELL derived from the 995th iteration of the system .", "entities": [[0, 1, "DatasetName", "NELL"], [14, 15, "DatasetName", "NELL"]]}
{"text": "WN18RR was created by Dettmers et al ( 2018 ) from WN18 by removing inverse relation test - leakage .", "entities": [[0, 1, "DatasetName", "WN18RR"], [11, 12, "DatasetName", "WN18"]]}
{"text": "Following previous work , we evaluate our method using HITS@N and mean reciprocal rank ( MRR ) , which are standard metrics for evaluating a ranked list .", "entities": [[15, 16, "MetricName", "MRR"]]}
{"text": "Knowledge Base Completion .", "entities": [[0, 3, "TaskName", "Knowledge Base Completion"]]}
{"text": "( Dettmers et al , 2018 ) , RotatE", "entities": [[8, 9, "MethodName", "RotatE"]]}
{"text": "Open - world Knowledge Base Completion .", "entities": [[3, 6, "TaskName", "Knowledge Base Completion"]]}
{"text": "We extend the most competitive embedding based model - RotatE", "entities": [[9, 10, "MethodName", "RotatE"]]}
{"text": "This initialization minimizes the RotatE objective for the new embedding ensuring that it is \" well - placed \" according to the model in the previous time step .", "entities": [[4, 5, "MethodName", "RotatE"]]}
{"text": "Next , the model is further trained on the new batch of triples so that the new entity embeddings get trained .", "entities": [[17, 19, "TaskName", "entity embeddings"]]}
{"text": "We also try a setting where we try freezing the initially trained entity embeddings and only training the new entity and relation embeddings .", "entities": [[12, 14, "TaskName", "entity embeddings"]]}
{"text": "We would like to highlight the difference between the performance of our model and that of Das et al ( 2020 ) on the test - II evaluation of FB122 where triples can be answered by learning logical rules .", "entities": [[29, 30, "DatasetName", "FB122"]]}
{"text": "We also perform comparably to most embedding based models and achieve state - of - the - art results on the overall test sets of FB122 and", "entities": [[25, 26, "DatasetName", "FB122"]]}
{"text": "NELL - 995 .", "entities": [[0, 1, "DatasetName", "NELL"]]}
{"text": "Effect of path length on WN18RR :", "entities": [[5, 6, "DatasetName", "WN18RR"]]}
{"text": "On the dev set of WN18RR , out of 2985 queries where our method does not rank the answer in the top - 10 , 2030 queries require a minimum path length greater than 3 . Path - based reasoning models have no power to answer these queries .", "entities": [[5, 6, "DatasetName", "WN18RR"]]}
{"text": "We report results on the RotatE model with randomly initialized embeddings for new entities ( RotatE ) and the model with systematic initialization of new entity embeddings ( RotatE+ ) .", "entities": [[5, 6, "MethodName", "RotatE"], [15, 16, "MethodName", "RotatE"], [25, 27, "TaskName", "entity embeddings"]]}
{"text": "We also report results with freezing the already seen entity representations and only learning representations for new entities ( RotatE - Freeze ) .", "entities": [[19, 20, "MethodName", "RotatE"]]}
{"text": "For both datasets , the offline - best results were obtained by RotatE ( 47.1 for FB122 test - I , 48 for WN18RR ) .", "entities": [[12, 13, "MethodName", "RotatE"], [16, 17, "DatasetName", "FB122"], [23, 24, "DatasetName", "WN18RR"]]}
{"text": "On FB122 , we observe that the model prefers to learn new information more by sacrificing previously learned facts ( 2nd subfigure in figure 3 ) ( ii )", "entities": [[1, 2, "DatasetName", "FB122"]]}
{"text": "( iii ) On the full evaluation , RotatE+ performs better than RotatE showing that bad initialization deteriorates performance over time , however , there is still a large gap between the best performance ( iv ) Our approach almost matches our performance in oracle setting indicating the effectiveness of the online clustering and fast parameter approximation .", "entities": [[12, 13, "MethodName", "RotatE"]]}
{"text": "Inductive representation learning on KGs .", "entities": [[1, 3, "TaskName", "representation learning"]]}
{"text": "Rule induction in knowledge graphs .", "entities": [[3, 5, "TaskName", "knowledge graphs"]]}
{"text": "Classic work in inductive logic programming ( ILP ) ( Muggleton et al , 1992 ; Quinlan , 1990 ) induce rules from grounded facts .", "entities": [[3, 6, "TaskName", "inductive logic programming"]]}
{"text": "( politician - us - member - of - political - group , person - belongs - to - organization \u22121 , agent - belongs - to - organization ) ( agent - collaborates - with - agent , agent - belongs - to - organization ) ( Getoor and Taskar , 2007 ;", "entities": [[22, 23, "DatasetName", "agent"], [31, 32, "DatasetName", "agent"], [37, 38, "DatasetName", "agent"], [39, 40, "DatasetName", "agent"]]}
{"text": "Embedding - based approach for link prediction .", "entities": [[5, 7, "TaskName", "link prediction"]]}
{"text": "Our simple approach which needs no iterative opti - mization outperforms most of them and performs comparably to the latest RotatE model .", "entities": [[20, 21, "MethodName", "RotatE"]]}
{"text": "Moreover we outperform RotatE in the online experiments .", "entities": [[3, 4, "MethodName", "RotatE"]]}
{"text": "This work is funded in part by the Center for Data Science and the Center for Intelligent Information Retrieval , and in part by the National Science Foundation under Grants No . IIS - 1514053 and No . 1763618 , and in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction .", "entities": [[17, 19, "TaskName", "Information Retrieval"]]}
{"text": "Computing Infrastructure : All our experiments were run on a Xeon E5 - 2680 v4 @ 2.40GHz CPU with 128 GB RAM .", "entities": [[21, 22, "MethodName", "RAM"]]}
{"text": "The NELL - 995 does not come with a validation set , and therefore we selected 3000 edges randomly from the full NELL KB .", "entities": [[1, 2, "DatasetName", "NELL"], [22, 23, "DatasetName", "NELL"]]}
{"text": "As a result , many of the query relations were different from what was present in the splits of NELL - 995 and hence is not a good representative .", "entities": [[19, 20, "DatasetName", "NELL"]]}
{"text": "The fixed number of parameters in our model are essentially the sparse non - learned entity vectors ( which can be easily stored in COO format without taking much space ) .", "entities": [[2, 5, "HyperparameterName", "number of parameters"]]}
{"text": "Other than that , our model is non - parametric with the number of parameters tied to the data .", "entities": [[12, 15, "HyperparameterName", "number of parameters"]]}
{"text": "For experiments on WN18RR : Inference time : 18.9 queries / s ( total of 6268 queries ) Train time : around 20 mins .", "entities": [[3, 4, "DatasetName", "WN18RR"]]}
{"text": "Best Hyper - parameters : - For experiments on FB122 : Inference time :", "entities": [[9, 10, "DatasetName", "FB122"]]}
{"text": "Incorporating related text information has proven successful in stock market prediction .", "entities": [[8, 11, "TaskName", "stock market prediction"]]}
{"text": "In this work , we propose a BERT - based Hierarchical Aggregation Model to summarize a large amount of finance news to predict forex movement .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "Then we extract the most crucial news in each group by the SOTA extractive summarization method .", "entities": [[13, 15, "TaskName", "extractive summarization"]]}
{"text": "At the group level , we concatenate news headlines in the same group and regard news extraction in each group as an extractive summarization task .", "entities": [[22, 24, "TaskName", "extractive summarization"]]}
{"text": "We modify the SOTA extractive summarization model proposed in ( Liu , 2019 ) to select the most important news .", "entities": [[4, 6, "TaskName", "extractive summarization"]]}
{"text": "BERT ( Devlin et al , 2018 ) is a potent pretrained contextualized sentence representation and has proven obvious improvement for many NLP tasks ( Sun et al , 2019 ; Xu et", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "Liu ( 2019 ) proposes a modified BERT for extractive summarization and achieve the state - of - the - art result in extractive document summarization task .", "entities": [[7, 8, "MethodName", "BERT"], [9, 11, "TaskName", "extractive summarization"], [23, 26, "TaskName", "extractive document summarization"]]}
{"text": "Si et al ( 2014 ) utilize the sentiment analysis to help the prediction .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}
{"text": "al ( 2018 ) adopt the summarization of news body instead of headline to predict .", "entities": [[6, 7, "TaskName", "summarization"]]}
{"text": "Yet some others choose multi - news :", "entities": [[4, 7, "DatasetName", "multi - news"]]}
{"text": "Compared to stock prediction , works about forex prediction is much scarce , and most of these works ( Carapu\u00e7o et al , 2018 ; Bakhach et al , 2016 ;", "entities": [[2, 4, "TaskName", "stock prediction"]]}
{"text": "Shen and Liang ( 2016 ) employ stacked autoencoder to get the trade data representation and adopt support vector regression to predict .", "entities": [[8, 9, "MethodName", "autoencoder"]]}
{"text": "de Almeida et al ( 2018 ) combine SVM with genetic algorithms to optimize investments in Forex markets based on history price .", "entities": [[8, 9, "MethodName", "SVM"], [10, 12, "MethodName", "genetic algorithms"]]}
{"text": "Following this work , Seifollahi and Shajari ( 2019 ) add word sense disambiguation in the sentiment analysis of news headlines .", "entities": [[11, 14, "TaskName", "word sense disambiguation"], [16, 18, "TaskName", "sentiment analysis"]]}
{"text": "In this work , we propose a selection and aggregation neural framework to process the larger amount of finance news and employ the powerful pre - trained BERT as text encoder , which can learn the deep semantic information effectively .", "entities": [[27, 28, "MethodName", "BERT"]]}
{"text": "And f { 1 , 0 } is the forex movement label telling whether the forex trade price is up or down after a certain time ( we call it prediction delay ) .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "In the Intra - group extraction step , news in the same group is connected as a continuous paragraph , and we conduct extractive summarization on this paragraph to select the most important news .", "entities": [[23, 25, "TaskName", "extractive summarization"]]}
{"text": "Specifically , we employ BERT as the encoder to get the contextualized paragraph representation and compute the importance score for each news .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "Inspired by the BERT - based extractive summarization model proposed in ( Liu , 2019 ) , we modify this method to select the most crucial news in each group .", "entities": [[0, 1, "DatasetName", "Inspired"], [3, 4, "MethodName", "BERT"], [6, 8, "TaskName", "extractive summarization"]]}
{"text": "The form of group news input for BERT encoder is illustrated in Figure 3 .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "For the segment embedding , we use the loop of [ E A , E B ] to extend the raw segment embedding of BERT to multi - sentences .", "entities": [[24, 25, "MethodName", "BERT"]]}
{"text": "After the BERT encoding , all the [ CLS ] tokens cls are regarded as the semantic representations of the corresponding news .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "The importance score for each news is calculated base on these [ CLS ] tokens : score i = sigmoid ( W 0", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "* cls i + b 0 ) ( 1 ) t", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "( 2 ) s i = softmax ( t i ) ( 3 )", "entities": [[6, 7, "MethodName", "softmax"]]}
{"text": "W 0 and b 0 are the trainable parameters .", "entities": [[1, 2, "DatasetName", "0"], [4, 5, "DatasetName", "0"]]}
{"text": "= Relu ( R t *", "entities": [[1, 2, "MethodName", "Relu"]]}
{"text": "= softmax ( W p * R + b p ) ( 9 ) means element - wise multiplication .", "entities": [[1, 2, "MethodName", "softmax"]]}
{"text": "The news categories 1 are { Business Sectors , Business General , Business Assets , Business Commodities , Business Organizations , Politics&International Affairs , Arts&Culture&Entertainment&Sports , Science & Technology , Other } .", "entities": [[10, 11, "DatasetName", "General"]]}
{"text": "The min - max scale is applied for each currency pair 's samples to scale the raw numbers in y to [ 0 , 1 ] according to the maximum and minimum value of each feature .", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "| x , y , \u03b8 )", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b8 is the model parameters .", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "We choose the pytorch - pretrained - BERT 3 as BERT implement and choose the bert - baseuncased version in which there are 12 layers , 768 hidden states and 12 attention heads in the transformer .", "entities": [[7, 8, "MethodName", "BERT"], [10, 11, "MethodName", "BERT"]]}
{"text": "We truncate the BERT input to 256 tokens and fine - tune the BERT parameters during training .", "entities": [[3, 4, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"]]}
{"text": "The evaluation metrics are macro - F1 and Matthews Correlation Coefficient ( MCC ) .", "entities": [[4, 7, "MetricName", "macro - F1"]]}
{"text": "Since there are few existing works , we modify two advanced models from stock prediction field which adopt multi - news as input for this task .", "entities": [[13, 15, "TaskName", "stock prediction"], [18, 21, "DatasetName", "multi - news"]]}
{"text": "SVM :", "entities": [[0, 1, "MethodName", "SVM"]]}
{"text": "This method chooses the support vector machine to predict the result based on the feature vectors extracted by the method introduced in ( Seifollahi and Shajari , 2019 ) .", "entities": [[4, 7, "MethodName", "support vector machine"]]}
{"text": "It includes a hybrid attention mechanism and Gated Recurrent Unit to combine multi - day 's stock news to predict movement .", "entities": [[7, 10, "MethodName", "Gated Recurrent Unit"]]}
{"text": "We use every 5 minutes instead of each day as time unit for this method and the StockNet method because there is too much news for forex trading and the experiments show that the latest news has the most influence .", "entities": [[17, 18, "DatasetName", "StockNet"]]}
{"text": "StockNet :", "entities": [[0, 1, "DatasetName", "StockNet"]]}
{"text": "Instead , it gets the representation for each news independently using BERT .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "LSTM+Attention : This method uses the bidirectional LSTM and self - attention to replace the BERT as text encoder .", "entities": [[6, 8, "MethodName", "bidirectional LSTM"], [15, 16, "MethodName", "BERT"]]}
{"text": "The number of LSTM hidden states is 256 , and the hidden - layer is 3 .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "The LSTM+Attention method performs worse than the BERT - based method , which proves that BERT has stronger power of sentence encoding .", "entities": [[7, 8, "MethodName", "BERT"], [15, 16, "MethodName", "BERT"]]}
{"text": "In this work , we propose a BERT - based Hierarchical Aggregation Model to summarize a large amount of finance news for forex movement prediction .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "Polyglot Semantic Parsing in APIs", "entities": [[1, 3, "TaskName", "Semantic Parsing"]]}
{"text": "Traditional approaches to semantic parsing ( SP ) work by training individual models for each available parallel dataset of text - meaning pairs .", "entities": [[3, 5, "TaskName", "semantic parsing"]]}
{"text": "In this paper , we explore the idea of polyglot semantic translation , or learning semantic parsing models that are trained on multiple datasets and natural languages .", "entities": [[15, 17, "TaskName", "semantic parsing"]]}
{"text": "While capturing the semantics of docstrings is in general a difficult task , learning the translation from descriptions to formal code representations ( e.g. , formal representations of functions ) is proposed as a reasonable first step towards learning more general natural language understanding models in the software domain .", "entities": [[41, 44, "TaskName", "natural language understanding"]]}
{"text": "In followup work ( Richardson and Kuhn , 2017a ) , they proposed using the resulting models to do automated question - answering ( QA ) and code retrieval on target APIs , and experimented with an additional set of software datasets built from 27 open - source Python projects .", "entities": [[1, 2, "DatasetName", "followup"]]}
{"text": "To deal with these issues , we aim to learn more general text - to - code translation models that are trained on multiple datasets simultaneously .", "entities": [[16, 18, "TaskName", "code translation"]]}
{"text": "Most of this recent work focuses on processing large amounts of API data in bulk ( Gu et al , 2016 ; Miceli Barone and Sennrich , 2017 ) , either for learning longer executable programs from text ( Yin and Neubig , 2017 ; Rabinovich et al , 2017 ) , or solving the inverse problem of code to text generation ( Iyer et al , 2016 ; .", "entities": [[60, 62, "TaskName", "text generation"]]}
{"text": "For example , the two edges from the initial state 0 in Figure 2 are labeled as 2C and 2Clojure , which identify the C and Clojure programming languages respectively .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "The paths starting from the initial state 0 , in contrast , correspond to all valid component representations in all languages .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "Using the trick above , our setup facilitates both monolingual decoding , i.e. , generating components specific to a particular output language ( e.g. , the C language via the path shown in bold ) , and polyglot decoding , i.e. , generating any output language by starting at the initial state 0 ( e.g. , C and Clojure ) .", "entities": [[52, 53, "DatasetName", "0"]]}
{"text": "The first is an encoder network , which uses a bi - directional recurrent neural network architecture with LSTM units ( Hochreiter and Schmidhuber , 1997 ) to compute a sequence of forward annotations or hidden states ( \u2212 h 1 , ... , \u2212 h", "entities": [[18, 19, "MethodName", "LSTM"]]}
{"text": "| z | i=1 log p \u0398", "entities": [[6, 7, "HyperparameterName", "\u0398"]]}
{"text": "< i , x ) \u223c softmax ( f ( \u0398 , z < i , x ) )", "entities": [[6, 7, "MethodName", "softmax"], [10, 11, "HyperparameterName", "\u0398"]]}
{"text": "We implement the function f in the following way : f ( \u0398 ,", "entities": [[12, 13, "HyperparameterName", "\u0398"]]}
{"text": "i = MLP ( c i , g i ) ( 6 )", "entities": [[2, 3, "DatasetName", "MLP"]]}
{"text": "g i = LSTM dec", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "( g i\u22121 , E out z i\u22121 , c i ) ( 7 ) where MLP is a multi - layer perceptron model with a single hidden layer , E out R | \u03a3 dec | \u00d7e is a randomly initialized embedding matrix ,", "entities": [[16, 17, "DatasetName", "MLP"]]}
{"text": "\u03b1 i , j , or c i = | x", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "j=1 \u03b1 i , j h j , which is jointly learned using an additional single layered multi - layer perceptron defined in the following way : \u03b1", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [27, 28, "HyperparameterName", "\u03b1"]]}
{"text": "i , j \u221d exp ( e i , j ) ; e i , j = MLP ( g i\u22121 , h j ) ( 8 ) Lexical Bias and Copying", "entities": [[17, 18, "DatasetName", "MLP"]]}
{"text": "Adj [ u ] softmax ( f ( \u0398 , p , x ) )", "entities": [[4, 5, "MethodName", "softmax"], [8, 9, "HyperparameterName", "\u0398"]]}
{"text": "A distribution is then computed over these actions using a softmax function and particular actions are chosen accordingly during training and decoding .", "entities": [[10, 11, "MethodName", "softmax"]]}
{"text": "In general terms , the decoder described above works like an ordinary neural decoder with the difference that each decision ( i.e. , new target - side word translation ) is constrained ( in line 7 ) by the transitions allowed in the underlying graph in order to ensure wellformedness of each component output .", "entities": [[27, 29, "TaskName", "word translation"]]}
{"text": "Standardly , we optimize these models using stochastic gradient descent with the objective of finding parameters\u0398 that minimize the negative conditional log - likelihood of the training dataset .", "entities": [[7, 10, "MethodName", "stochastic gradient descent"], [21, 24, "MetricName", "log - likelihood"]]}
{"text": "We use a BPE subword encoding ( Sennrich et al , 2015 ) of both input and output words to make the representations more similar and transliterated all datasets ( excluding Japanese datasets ) to an 8 - bit latin encoding .", "entities": [[3, 4, "MethodName", "BPE"]]}
{"text": "For GeoQuery graph construction , we built a single graph for all languages by extracting general rule templates from all representations in the dataset , and exploited additional information and patterns using the Geobase database and the semantic grammars used in ( Wong and Mooney , 2006b ) .", "entities": [[2, 4, "TaskName", "graph construction"]]}
{"text": "For the technical datasets , the goal is to see if our model generates correct signature representations from unobserved descriptions using exact match .", "entities": [[21, 23, "MetricName", "exact match"]]}
{"text": "For Sportscaster , we evaluate by exact match to a gold representation .", "entities": [[6, 8, "MetricName", "exact match"]]}
{"text": "This result has also been found in relation to neural AMR semantic parsing , where similar issues of sparsity are encountered ( Peng et al , 2017 ) .", "entities": [[11, 13, "TaskName", "semantic parsing"]]}
{"text": "Even by doubling the amount of training data by training on all datasets ( results not shown ) , this did not improve the accuracy , suggesting that much more data is needed ( more discussion below ) .", "entities": [[24, 25, "MetricName", "accuracy"]]}
{"text": "Logical Form Translation : answer ( elevation 1 ( highest ( place ( loc 2 ( stateid ( ' alabama ' ) ) ) ) ) )", "entities": [[2, 3, "TaskName", "Translation"]]}
{"text": "Semantic Parsing Results SP results are summarized in Table 2 .", "entities": [[0, 2, "TaskName", "Semantic Parsing"]]}
{"text": "In general , we believe that focusing on polyglot and mixed language decoding is not only of interest to applications ( e.g , mixed language API QA ) but also allows for new forms of SP evaluation that are more revealing than only translation accuracy .", "entities": [[44, 45, "MetricName", "accuracy"]]}
{"text": "When comparing the accuracy of the best monolingual Geo model and the worst performing neural polyglot model , one could mistakingly think that these models have equal abilities , though the polyglot model is much more robust and general .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "Since the time of the Index Thomisticus by father Roberto Busa ( Busa , 1974 ( Busa , - 1980 , which is usually mentioned among the first electronic ( nowadays called \" digital \" ) annotated corpora available , NLP tools for automatic morphological analysis and lemmatisation of a richly inflected language like Latin were needed .", "entities": [[44, 46, "TaskName", "morphological analysis"]]}
{"text": "In particular , in the area of Digital Humanities there is growing interest in Named Entity Recognition ( NER ) , especially for purposes of geographicalbased analysis of texts .", "entities": [[14, 17, "TaskName", "Named Entity Recognition"], [18, 19, "TaskName", "NER"]]}
{"text": "NER is a sub - branch of Information Extraction , whose inception goes back to the Sixth Message Understanding Conference ( MUC - 6 ) ( Grishman and Sundheim , 1996 ) .", "entities": [[0, 1, "TaskName", "NER"]]}
{"text": "NER aims at recognising and labelling ( multi ) words , as names of people , things , places , etc .", "entities": [[0, 1, "TaskName", "NER"]]}
{"text": "Since MUC - 6 , NER has largely expanded , with several applications also on ancient languages ( see , for example , Depauw and Van Beek , 2009 ) .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "Although Lemlat provides quite a large coverage of the Latin lexicon , its performance is limited by the absence of an Onomasticon in its lexical basis , which would be helpful for tasks like NER .", "entities": [[34, 35, "TaskName", "NER"]]}
{"text": "Given that in Latin proper names undergo morphological inflection , in this paper we describe our work of enhancing Lemlat with an Onomasticon .", "entities": [[7, 9, "TaskName", "morphological inflection"]]}
{"text": "It counts 40 , 014 lexical entries and 43 , 432 lemmas , as more than one lemma can be included into the same lexical entry .", "entities": [[17, 18, "DatasetName", "lemma"]]}
{"text": "Given an input wordform that is recognised by Lemlat , the tool produces in output the corresponding lemma ( s ) and a number of tags conveying ( a ) the inflectional paradigm of the lemma ( s ) ( e.g. first declension noun ) and ( b ) the morphological features of the input wordform ( e.g. singular nominative ) , as well as the identification number ( ID ) of the lemma ( s ) in the lexical basis of Lemlat .", "entities": [[17, 18, "DatasetName", "lemma"], [35, 36, "DatasetName", "lemma"], [73, 74, "DatasetName", "lemma"]]}
{"text": "For instance , receiving in input the wordform abamitae ( \" great - aunt \" ) , Lemlat outputs the corresponding lemma ( abamita , ID : A0019 ) , the tags for its inflectional paradigm ( N1 : first declension noun ) and those for the morphological features of the input wordform ( feminine singular genitive and dative ; feminine plural nominative and vocative ) .", "entities": [[21, 22, "DatasetName", "lemma"]]}
{"text": "The LES is defined as the invariable part of the inflected form ( e.g. abamit for abamit - ae ) .", "entities": [[18, 19, "MethodName", "ae"]]}
{"text": "In other words , the LES is the sequence ( or one of the sequences ) of characters that remains the same in the inflectional paradigm of a lemma ( hence , the LES does not necessarily correspond to the word stem ) .", "entities": [[28, 29, "DatasetName", "lemma"]]}
{"text": "Lemlat includes a LES archive , in which LES are assigned an ID and a number of inflectional features among which are a tag for the gender of the lemma ( for nouns only ) and a code ( called CODLES ) for its inflectional category .", "entities": [[29, 30, "DatasetName", "lemma"]]}
{"text": "In Busa ( 1988 ) , three kinds of metadata are assigned to each lemma : ( a ) a code for the section of the dictionary in which the lemma occurs ( e.g. ON : the lemma occurs in the Onomasticon ) , ( b ) a code for the inflectional paradigm the lemma belongs to and its gender ( e.g. BM : second declension masculine nouns ) and ( c ) the number of lines of the lexical entry for the lemma in Forcellini .", "entities": [[14, 15, "DatasetName", "lemma"], [30, 31, "DatasetName", "lemma"], [37, 38, "DatasetName", "lemma"], [54, 55, "DatasetName", "lemma"], [83, 84, "DatasetName", "lemma"]]}
{"text": "In order to perform this task as automatically as possible , we built a number of rules to extract the relevant information for each lemma in the list , namely its LES , CODLES and gender .", "entities": [[24, 25, "DatasetName", "lemma"]]}
{"text": "By exploiting the morphological tagging of Busa ( 1988 ) , which groups sets of lemmas showing common inflectional features , our rules treat automatically such inflectionally regular groups .", "entities": [[3, 5, "TaskName", "morphological tagging"]]}
{"text": "The first type ( 60 rules ) builds the LES by removing one or more characters from the right side of the lemma .", "entities": [[22, 23, "DatasetName", "lemma"]]}
{"text": "Such a removal is constrained by the code for the inflectional paradigm of the lemma , which is then used to create both the CODLES and the tag for the gender .", "entities": [[14, 15, "DatasetName", "lemma"]]}
{"text": "For instance , the lemma marcus ( \" Mark \" ) is assigned the inflectional paradigm BM in Busa ( 1988 ) .", "entities": [[4, 5, "DatasetName", "lemma"]]}
{"text": "One rule states that the LES for BM lemmas ending in - us is built by removing the last two characters from the lemma ( marcus > marc )", "entities": [[23, 24, "DatasetName", "lemma"]]}
{"text": "The second type of rules ( 19 ) adds one or more characters on the right side of the lemma to build the LES .", "entities": [[19, 20, "DatasetName", "lemma"]]}
{"text": "Again , this is done according both to the inflectional paradigm and to the ending of the lemma in Busa ( 1988 ) .", "entities": [[17, 18, "DatasetName", "lemma"]]}
{"text": "One example is the lemma bappo ( \" Bappo \" ) , whose LES is bappon , as third declension imparisyllable nouns are analysed by Lemlat by using the basis for their singular genitive ( bappon - is ) .", "entities": [[4, 5, "DatasetName", "lemma"]]}
{"text": "The third type of rules ( 19 ) replaces one or more characters on the right side of the lemma with others .", "entities": [[19, 20, "DatasetName", "lemma"]]}
{"text": "The lemma charybdis , - is ( \" Charybdis \" ) is a third declension parisyllable feminine noun ending in - is .", "entities": [[1, 2, "DatasetName", "lemma"]]}
{"text": "For instance , apollonides ( \" Apollonides \" ) shows both a singular genitive of the second declension ( in - i ) and one of the first declension ( in - ae ) .", "entities": [[32, 33, "MethodName", "ae"]]}
{"text": "These are wordforms that are hard - coded in the LES archive and are assigned the same ID of the LES used to build their base lemma .", "entities": [[26, 27, "DatasetName", "lemma"]]}
{"text": "For instance , the nominative singular of the lemma jesus ( \" Jesus \" ) is attested also as hiesus , ihesus and zesus .", "entities": [[8, 9, "DatasetName", "lemma"], [12, 13, "TaskName", "Jesus"]]}
{"text": "Beside the LES jes ( used for the base lemma jesus ) , in the LES archive also the wordforms hiesus , ihesus and zesus are recorded and assigned the same ID of the LES jes .", "entities": [[9, 10, "DatasetName", "lemma"]]}
{"text": "First , we focused on the accuracy of the rules for automatic enhancement .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "For instance , the lemma augustus occurs both in the original Lemlat ( a first class adjective , \" solemn \" ) and in the Onomasticon ( a proper name , \" Augustus \" ) .", "entities": [[4, 5, "DatasetName", "lemma"]]}
{"text": "A prevailing paradigm in neural text generation is one - shot generation , where text is produced in a single step .", "entities": [[5, 7, "TaskName", "text generation"]]}
{"text": "We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text .", "entities": [[7, 9, "TaskName", "text generation"]]}
{"text": "To this end , we propose a novel text editing task , and introduce WikiDocEdits , a dataset of singlesentence edits extracted from Wikipedia revision histories .", "entities": [[14, 15, "DatasetName", "WikiDocEdits"]]}
{"text": "Recent large generative language models such as GPT - 2 ( Radford et al , 2019 ) , and GPT - 3 ( Brown et al , 2020 ) , demonstrate an impressive ability to generate fluent text , but their outputs are difficult to control beyond a prompt , and they manifest a tendency to hallucinate facts ( Wiseman et al , 2017 ) .", "entities": [[7, 8, "MethodName", "GPT"], [19, 20, "MethodName", "GPT"]]}
{"text": "While this work focuses on text , we also note that these arguments extend to other settings where a system must generate a complex , structured object for a user , such as image or code generation .", "entities": [[35, 37, "TaskName", "code generation"]]}
{"text": "We show that a transformer - based editing model trained on our data outperforms \" parrot \" and GPT - 2 baselines , and obtains competitive results compared to gold - standard edits in human evaluations .", "entities": [[15, 16, "MethodName", "parrot"], [18, 19, "MethodName", "GPT"]]}
{"text": "Moreover , let D be an edited version of D. Then our task is , given a dataset of edits D = { ( D 0 , q 0 , G 0 , D 0 ) , ... , ( D N , q N , G N , D N ) } , learn to produce document D , given D , q , and G. Note that while previous work on text editing usually only considers D as input , we include both a form of control q and grounding G. The command is needed because otherwise the type of edit to be made is undefined , while the grounding provides external knowledge needed to make an edit .", "entities": [[25, 26, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [31, 32, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "To accompany our text editing task we present a novel dataset of nearly 12 million sentence - level edits , WikiDocEdits .", "entities": [[20, 21, "DatasetName", "WikiDocEdits"]]}
{"text": "See T5 Encoder < bos > 0 \u2032 1 \u2032 T5 Decoder 0 \u2032 1 \u2032 2 \u2032 \u2026 \u2026", "entities": [[1, 2, "MethodName", "T5"], [6, 7, "DatasetName", "0"], [10, 11, "MethodName", "T5"], [12, 13, "DatasetName", "0"]]}
{"text": "We formalize our model , which we refer to as Interactive Editor , as a standard auto - regressive sequence to sequence model .", "entities": [[19, 22, "MethodName", "sequence to sequence"]]}
{"text": "= { s 0 , ... , s i\u22121 } are the tokens preceding s i in s .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "We use the same encoder - decoder architecture as T5 ( Raffel et al , 2020 ) and initialize our model with pretrained language model weights .", "entities": [[9, 10, "MethodName", "T5"]]}
{"text": "In order to adapt T5 for our task , we represent all our inputs as sequences of tokens .", "entities": [[4, 5, "MethodName", "T5"]]}
{"text": "We also use the standard cross - entropy loss to train .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "We train our model on a subset of \u223c1 , 020 K edits from WikiDocEdits .", "entities": [[14, 15, "DatasetName", "WikiDocEdits"]]}
{"text": "We use the T5 - base implementation from Huggingface ( Wolf et al , 2020 ) , and finetune all weights in the model .", "entities": [[3, 4, "MethodName", "T5"]]}
{"text": "While this should not a priori transfer to evaluating different tasks , our task in fact bears a high similarity to machine translation because of how the output is constrained by the inputs .", "entities": [[21, 23, "TaskName", "machine translation"]]}
{"text": "Thus , in our setting , BLEU makes sense as a metric since in principle a good model output should not deviate too far from the reference .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "One issue with BLEU is that the source and target sentences in our task are already very similar , so a model that simply parrots back the source sentence could achieve an unduly high score .", "entities": [[3, 4, "MetricName", "BLEU"]]}
{"text": "Finally , we compute sentence - level accuracy , which reports the proportion of edits for which the model output exactly matched the reference .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "First , we consider the parrot baseline that simply outputs the source sentence as is .", "entities": [[5, 6, "MethodName", "parrot"]]}
{"text": "We use a pretrained GPT - 2 model ( Radford et al , 2019 ) that generates a sentence given the left context .", "entities": [[4, 5, "MethodName", "GPT"]]}
{"text": "Even on a harsh metric like accuracy our model achieves a nontrivial score , although we suspect most of the edits that the model gets exactly right are fluency edits .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "The columns report the same metrics as in our main table of results , with the exception of S - BLEU , which reports the BLEU score between the source sentence and target , and the last column , which reports the number of test edits that were classified into each category .", "entities": [[20, 21, "MetricName", "BLEU"], [25, 27, "MetricName", "BLEU score"]]}
{"text": "The S - BLEU scores confirm this since the source sentences in the fluency examples are much more similar to the target sentences than for the content edits .", "entities": [[3, 4, "MetricName", "BLEU"]]}
{"text": "In fact , when looking at the absolute improvement of the BLEU over the S - BLEU scores , the model performs equally well on both types of edits .", "entities": [[11, 12, "MetricName", "BLEU"], [16, 17, "MetricName", "BLEU"]]}
{"text": "0 corresponds to strong disagreement , and 5 to strong agreement .", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "In this sense , we can also draw the parallel to dialog response generation Dinan et al , 2018 ) , task - oriented dialog , or open domain question answering ( Min et al , 2019 ; Chen et al , 2017 ) , that also involve user responses or queries , although these tasks are not concerned with text generation in the context of document creation .", "entities": [[12, 14, "TaskName", "response generation"], [29, 31, "TaskName", "question answering"], [60, 62, "TaskName", "text generation"]]}
{"text": "While earlier work in Story Generation focused more on plan - based architectures ( Lebowitz , 1985 ) , more recent work moved towards end - to - end approaches allowing generation to be unconstrained and creative .", "entities": [[4, 6, "TaskName", "Story Generation"]]}
{"text": "As narratives are often aimed at particular goals expressed in terms of outlines and plans , much of the literature in Story Generation is framed as a form of controllable generation , using storylines ( Peng et al , 2018 ) , events ( Martin et al , 2017 , plot words or word skeletons ( Xu et al , 2018 ;", "entities": [[21, 23, "TaskName", "Story Generation"]]}
{"text": "Our work takes a significantly different approach , as we treat document or story generation as an iterative process that allows a human to generate a full document from scratch , but also allows constraints to be more dynamic ( e.g. , add nationality in Table 9 only if the system missed that the first time ) .", "entities": [[13, 15, "TaskName", "story generation"]]}
{"text": "More related to our own setting , Faruqui et al ( 2018 ) propose WikiAtomicEdits , a dataset of edits crawled from Wikipedia .", "entities": [[14, 15, "DatasetName", "WikiAtomicEdits"]]}
{"text": "al ( 2018 ) use WikiAtomicEdits and propose the task of learning to represent edits , which Marrese - Taylor et al ( 2020 ) expand using a variational approach .", "entities": [[5, 6, "DatasetName", "WikiAtomicEdits"]]}
{"text": "In this work we argued that text generation should be interactive , and , as a means towards that end , we proposed a general text editing task , where a system must edit a document in response to a user command .", "entities": [[6, 8, "TaskName", "text generation"]]}
{"text": "This allows for fuzzy matching between BERT embeddings instead of requiring exact word matches .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "We can define the BERT recall , R BERT , for a sentence edit The BERT embeddings used to compute R BERT were produced using a pretrained BERT base model .", "entities": [[4, 5, "MethodName", "BERT"], [8, 9, "MethodName", "BERT"], [15, 16, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"], [27, 28, "MethodName", "BERT"]]}
{"text": "s \u2212 s , with respect to some text corpus C as w s \\s idf ( w ) max w C BERT ( w ) T BERT ( w )", "entities": [[22, 23, "MethodName", "BERT"], [27, 28, "MethodName", "BERT"]]}
{"text": "We used an uncased BERT base model to compute the embeddings .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "BERT Recall Similarly to the coverage analysis in appendix D , we can use R BERT , with the grounding as C , to assess how well each word inserted by the model is supported by the grounding .", "entities": [[0, 1, "MethodName", "BERT"], [1, 2, "MetricName", "Recall"], [15, 16, "MethodName", "BERT"]]}
{"text": "The only difference is that the model output now replaces the reference target s in the formula for R BERT .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "Table 14 gives the summary statistics for R BERT across our test set , computed on the outputs of our full model , and the ablated model without grounding .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "This baseline achieves a high R BERT score , likely because of spurious matches with the grounding .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Grounding Usage While R BERT attempts to measure how faithful the model is to the grounding ( i.e. is the information inserted by the model found in the grounding ? ) , we can also attempt to measure how much the grounding is used ( i.e. how much of the information inserted by the model is only found in the grounding ? ) .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "While this is n't obvious to compute similarities between BERT embeddings , we can use exact word matches instead .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "Neural machine translation ( NMT ) has achieved impressive performance recently by using large - scale parallel corpora .", "entities": [[1, 3, "TaskName", "machine translation"]]}
{"text": "Inspired by the finding that monolingual data can greatly improve the NMT performance , we propose a multi - task neural model that jointly learns to perform bi - directional translation and agglutinative language stemming .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Neural machine translation ( NMT ) has achieved impressive performance on many high - resource machine translation tasks ( Bahdanau et al , 2015 ;", "entities": [[1, 3, "TaskName", "machine translation"], [15, 17, "TaskName", "machine translation"]]}
{"text": "Zhang and Zong ( 2016 ) use the source - side monolingual data and employ the multi - task learning framework for translation and source sentence reordering .", "entities": [[16, 20, "TaskName", "multi - task learning"]]}
{"text": "modify the decoder to enable multi - task learning for translation and language modeling .", "entities": [[5, 9, "TaskName", "multi - task learning"]]}
{"text": "Stemming is a morphological analysis method , which is widely used for information retrieval tasks ( Kishida , 2005 ) .", "entities": [[3, 5, "TaskName", "morphological analysis"], [12, 14, "TaskName", "information retrieval"]]}
{"text": "Multi - task learning ( MTL ) aims to improve the generalization performance of a main task by using the other related tasks , which has been successfully applied to various research fields ranging from language ( Liu et al , 2015 ;", "entities": [[0, 4, "TaskName", "Multi - task learning"]]}
{"text": "Hashimoto et al ( 2017 ) present a joint MTL model to handle the tasks of part - of - speech ( POS ) tagging , dependency parsing , semantic relatedness , and textual entailment for English .", "entities": [[16, 19, "DatasetName", "part - of"], [26, 28, "TaskName", "dependency parsing"]]}
{"text": "Kiperwasser and Ballesteros ( 2018 ) utilize the POS tagging and dependency parsing for English - German machine translation .", "entities": [[11, 13, "TaskName", "dependency parsing"], [17, 19, "TaskName", "machine translation"]]}
{"text": "Recently , several works have combined the MTL method with sequence - to - sequence NMT model for machine translation tasks .", "entities": [[18, 20, "TaskName", "machine translation"]]}
{"text": "Inspired by their work , we employ the standard NMT model with one encoder and one decoder for parameter sharing and model generalization .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Several works on morphologically - rich NMT have focused on using morphological analysis to pre - process the training data ( Luong et al , 2016 ;", "entities": [[11, 13, "TaskName", "morphological analysis"]]}
{"text": "We propose a multi - task neural model for machine translation from and into a low - resource and morphologically - rich agglutinative language .", "entities": [[9, 11, "TaskName", "machine translation"]]}
{"text": "In this work , we follow the NMT model proposed by Vaswani et al ( 2017 ) , which is implemented as Transformer .", "entities": [[22, 23, "MethodName", "Transformer"]]}
{"text": "Firstly , the Transformer model maps the source sequence = ( 1 , \u2026 , ) and the target sentence = ( 1 , \u2026 , ) into a word embedding matrix , respectively .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "Each layer has three sub - layers : the multi - head self - attention , the multi - head attention , and the fully connected feed - forward network .", "entities": [[17, 21, "MethodName", "multi - head attention"]]}
{"text": "The multi - head attention attends to the outputs of the encoder and decoder to generate a context vector .", "entities": [[1, 5, "MethodName", "multi - head attention"]]}
{"text": "The feed - forward network followed by a linear layer maps the context vector into a vector with the original space dimension .", "entities": [[8, 10, "MethodName", "linear layer"]]}
{"text": "Finally , the softmax function is applied on the vector to predict the target word sequence .", "entities": [[3, 4, "MethodName", "softmax"]]}
{"text": "The statistics of the training , validation , and test datasets on Turkish - English and Uyghur - Chinese machine translation tasks are shown in Table 1 .", "entities": [[19, 21, "TaskName", "machine translation"]]}
{"text": "For the Turkish - English machine translation , following ( Sennrich et al , 2015a ) , we use the WIT corpus ( Cettolo et al , 2012 ) and the SETimes corpus ( Tyers and Alperen , 2010 ) as the training dataset , merge the dev2010 and tst2010 as the validation dataset , and use tst2011 , tst2012 , tst2013 , tst2014 from the IWSLT as the test datasets .", "entities": [[5, 7, "TaskName", "machine translation"], [20, 21, "DatasetName", "WIT"], [52, 54, "DatasetName", "validation dataset"]]}
{"text": "For the Uyghur - Chinese machine translation , we use the news data from the China Workshop on Machine Translation in 2017 ( CWMT2017 ) as the training dataset and validation dataset , use the news data from CWMT2015 as the test dataset .", "entities": [[5, 7, "TaskName", "machine translation"], [18, 20, "TaskName", "Machine Translation"], [30, 32, "DatasetName", "validation dataset"]]}
{"text": "We utilize the jieba toolkit 4 to segment the Chinese sentences , we utilize the Zemberek toolkit 5 with morphological disambiguation ( Sak et al , 2007 ) and the morphological analysis tool ( Tursun et al , 2016 ) to annotate the morpheme structure of the words in Turkish and Uyghur , respectively .", "entities": [[19, 21, "TaskName", "morphological disambiguation"], [30, 32, "TaskName", "morphological analysis"]]}
{"text": "Then the byte pair encoding ( BPE ) technique ( Sennrich et al , 2015b ) is applied on the stem unit \" hece \" to segment it into \" he@@ \" and \" ce@@ \" .", "entities": [[2, 5, "MethodName", "byte pair encoding"], [6, 7, "MethodName", "BPE"]]}
{"text": "Training Sentence Samples En - Tr Translation < MT > We go through initiation rit@@ es . Ba\u015fla@@ ma rit\u00fcel@@ lerini ya\u015f@@ \u0131yoruz .", "entities": [[6, 7, "TaskName", "Translation"]]}
{"text": "Tr - En Translation < MT > Ba\u015fla@@ ma rit\u00fcel@@ lerini ya\u015f@@ \u0131yoruz .", "entities": [[3, 4, "TaskName", "Translation"]]}
{"text": "In this paper , we utilize the above morphological segmentation method for our experiments by applying BPE on the stem units with 15 K merge operations for the Turkish words and 10 K merge operations for the Uyghur words .", "entities": [[16, 17, "MethodName", "BPE"]]}
{"text": "Moreover , we employ BPE to segment the words in English and Chinese by learning separate vocabulary with 32 K merge operations .", "entities": [[4, 5, "MethodName", "BPE"]]}
{"text": "Table 2 shows the training sentence samples for multi - task neural model on Turkish - English machine translation task .", "entities": [[17, 19, "TaskName", "machine translation"]]}
{"text": "In addition , to certify the effectiveness of the morphological segmentation method , we employ the pure BPE to segment the words in Turkish and Uyghur by learning a separate vocabulary with 36 K and 38 K merge operations , respectively .", "entities": [[17, 18, "MethodName", "BPE"]]}
{"text": "We employ the Transformer model implemented in the Sockeye toolkit .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "The maximum sentence length is set to 100 tokens with 0.1 label smoothing .", "entities": [[11, 13, "MethodName", "label smoothing"]]}
{"text": "We apply layer normalization and add dropout to the embedding and transformer layers with 0.1 probability .", "entities": [[2, 4, "MethodName", "layer normalization"]]}
{"text": "More details about the models are shown below : General NMT Model : The standard NMT model trained on the experimental data segmented by BPE .", "entities": [[9, 10, "DatasetName", "General"], [24, 25, "MethodName", "BPE"]]}
{"text": "Following Niu et al ( 2018b ) , we train a single NMT model to perform bi - directional machine translation .", "entities": [[19, 21, "TaskName", "machine translation"]]}
{"text": "We can observe that the baseline NMT model is comparable to the general NMT model , and it achieves the highest BLEU scores on almost all the test datasets in both directions , which indicates that the NMT baseline based on our proposed segmentation method is competitive .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "The table shows that the multi - task neural model outperforms both the baseline NMT model and bi - directional NMT model , and it achieves the highest BLEU scores on almost all the test datasets in both directions , which suggests that the multi - task neural model is capable of improving the bi - directional translation quality on agglutinative languages .", "entities": [[28, 29, "MetricName", "BLEU"]]}
{"text": "We can see that the perplexity values are consistently lower on the multi - task neural model , and it converges rapidly .", "entities": [[5, 6, "MetricName", "perplexity"]]}
{"text": "However , for the English - Turkish machine translation task , which can be seen as agglutinative language generation task , using the mixed data of talks and news achieves further improvements of the BLEU scores on almost all the test datasets .", "entities": [[7, 9, "TaskName", "machine translation"], [34, 35, "MetricName", "BLEU"]]}
{"text": "We also evaluate the translation performance of the general NMT model , baseline NMT model , and multi - task neural model with external news data on the machine translation task between Uyghur and Chinese .", "entities": [[28, 30, "TaskName", "machine translation"]]}
{"text": "The results indicate that the multi - task neural model achieves the highest BLEU scores on the test dataset by utilizing external monolingual data for the stemming task on Uyghur sentences .", "entities": [[13, 14, "MetricName", "BLEU"]]}
{"text": "Extensive experimental results show that the proposed model is beneficial for the agglutinative language machine translation , and only a small amount of the agglutinative data can improve the translation performance in both directions .", "entities": [[14, 16, "TaskName", "machine translation"]]}
{"text": "In the area of dialogue systems , there are few such tools and frameworks and they mostly remain focused on simple tasks that can be encoded in a state - based dialogue model ( see , e.g. , Williams et al , 2016 and the Dialogue State Tracking Challenge 1 ) .", "entities": [[45, 49, "DatasetName", "Dialogue State Tracking Challenge"]]}
{"text": "Dialogue management involves understanding the intention of the user 's contributions to the dialogue , and deciding what to do or say next .", "entities": [[0, 2, "TaskName", "Dialogue management"]]}
{"text": "In the early 2000s , Allen and colleagues described a preliminary plan - based CPS model of dialogue based on an analysis of an agent 's collaborative behavior at various levels : \uf0b7 An individual problem - solving level , where each agent manages its own problemsolving state , plans and executes individual actions , etc . ; \uf0b7 A collaborative problem - solving level , which models and manages the joint or collaborative problem - solving state ( shared goals , resources , situations ) ; \uf0b7", "entities": [[24, 25, "DatasetName", "agent"], [42, 43, "DatasetName", "agent"]]}
{"text": "Then , we will describe our dialogue manager , with a focus on its interface with the domain - specific problem solving agent .", "entities": [[22, 23, "DatasetName", "agent"]]}
{"text": "A collaborative conversational agent must understand a user 's utterances , that is , obtain a representation of the meaning of the utterance , recognize its intention , and then reason with this intention to decide what to do and/or say next .", "entities": [[3, 4, "DatasetName", "agent"]]}
{"text": "This follows the common separation of a conversational agent 's functionality into interpretation , behavior and generation , but where the separation lines are is critical for realizing the idea of isolating domainindependent from domain - specific processing .", "entities": [[8, 9, "DatasetName", "agent"]]}
{"text": "We take the output of NL Understanding ( assumed here to have broad lexical , syntactic and semantic coverage ) to be a domain - independent semantic representation of the user 's utterance ( a communicative act ) , expressed in terms of a domainindependent ontology .", "entities": [[45, 46, "MethodName", "ontology"]]}
{"text": "Intention recognition is performed by the CPS agent , which takes into account the discourse context and converts communicative acts into abstract communicative intentions .", "entities": [[7, 8, "DatasetName", "agent"]]}
{"text": "These communicative intentions need to be further evaluated with respect to the actual problem - solving state , so they are not fully interpreted until they reach the problem solving agent .", "entities": [[30, 31, "DatasetName", "agent"]]}
{"text": "This agent is responsible for the domain - specific behaviorhereafter we will refer to it as the Behavioral Agent ( BA ) and for operationalizing the communicative intentions into actions ( which may involve planning , acting on the world , updating its knowledge of the situation , etc . ) .", "entities": [[1, 2, "DatasetName", "agent"], [18, 19, "DatasetName", "Agent"], [20, 21, "DatasetName", "BA"]]}
{"text": "An autonomous BA should be able to plan and act on its own , but neither the BA nor the user can singlehandedly decide on the status of collaborative goals without a commitment from the other party .", "entities": [[2, 3, "DatasetName", "BA"], [17, 18, "DatasetName", "BA"]]}
{"text": "The BA expresses its attitude towards shared goals by sending to the CPS agent its own communicative intentions , which the CPS agent will use to update the collaborative state and generate communicative acts for NL generation ( such as accepting or rejecting a goal , or proposing a new one ) .", "entities": [[1, 2, "DatasetName", "BA"], [13, 14, "DatasetName", "agent"], [22, 23, "DatasetName", "agent"]]}
{"text": "Customization : Figure 1 includes , on the left side , a number of resources needed by our ideal dialogue system : ( 1 ) a broad lexicon for NL understanding ; ( 2 ) a general - purpose ( upper - level ) ontology ; and , optionally , ( 3 ) a domain ontology .", "entities": [[44, 45, "MethodName", "ontology"], [55, 56, "MethodName", "ontology"]]}
{"text": "Even a state - of - the - art broad coverage parser , with an extensive domain - independent high - level ontology and lexicon , will not contain all the word senses and concepts needed for every application domain .", "entities": [[22, 23, "MethodName", "ontology"]]}
{"text": "Additionally , the general ontology concepts need to be mapped onto the domain ontology used by the back - end problem solvers .", "entities": [[4, 5, "MethodName", "ontology"], [13, 14, "MethodName", "ontology"]]}
{"text": "Nevertheless , we believe these customization tasks are easier to accomplish and require less linguistic expertise than building a dialogue manager for every application , let al ne building domain - specific natural language understanding components .", "entities": [[32, 35, "TaskName", "natural language understanding"]]}
{"text": "Unlike prior work on CPSbased dialogue management , we focus on the interface between the CPS agent ( CPSA ) and the BA .", "entities": [[5, 7, "TaskName", "dialogue management"], [16, 17, "DatasetName", "agent"], [22, 23, "DatasetName", "BA"]]}
{"text": "The CPS Model defines an objective as an intention that is driving the agent 's current behavior ( Allen et al , 2002 ) .", "entities": [[13, 14, "DatasetName", "agent"]]}
{"text": "An objective can be proposed by either agent , provided they are ready to commit to it .", "entities": [[7, 8, "DatasetName", "agent"]]}
{"text": "Once an objective has been jointly committed to , either agent can propose to drop their commitment to it , via a CPS act called ABANDON .", "entities": [[10, 11, "DatasetName", "agent"]]}
{"text": "A proposal to bring an inactive objective back into an active state an agent results in a SELECT act .", "entities": [[13, 14, "DatasetName", "agent"]]}
{"text": "Finally , an agent can propose that an objective should be considered completed , via a RELEASE act .", "entities": [[3, 4, "DatasetName", "agent"]]}
{"text": "Note that all of these four acts can be proposed , indicating the agent 's intentional stance towards their commitment to that objective .", "entities": [[13, 14, "DatasetName", "agent"]]}
{"text": "If , on the other hand , the BA wants to propose that an objective be jointly pursued , say that it wants to start working on O1 by a subgoal O2 of placing a block on the table , it can do so via a PROPOSE act , whose content is the intention to commit to that objective : where C2 is indexed into the CONTEXT of the act for a representation of the event of placing a block on the table .", "entities": [[8, 9, "DatasetName", "BA"]]}
{"text": "Upon receiving this act , the CPSA will update the collaborative state to reflect the BA 's intention to commit to O2 , and formulate a communicative act for NLG to realize the proposal in a system utterance .", "entities": [[15, 16, "DatasetName", "BA"]]}
{"text": "The CPSA is responsible for gathering the agreements of both the user and the BA .", "entities": [[14, 15, "DatasetName", "BA"]]}
{"text": "When the CPSA recognizes that the user is proposing an objective , it will first send an EVALUATE act to the BA , whose content is the proposed objective , e.g. , : ( EVALUATE : content ( ADOPT : i d O1 : what C1 : as ( GOAL ) )", "entities": [[21, 22, "DatasetName", "BA"]]}
{"text": "This act creates an obligation on the part of the BA to evaluate whether it is able to commit to it in the current situation , and , if so , respond by signaling agreement ( ACCEPTABLE ) , rejection ( REJECTED ) , or , when it can not even interpret what the objective is , a failure ( FAILURE ) .", "entities": [[10, 11, "DatasetName", "BA"]]}
{"text": "For example , the BA 's agreement , that is , its intention to commit to the objective proposed by the user , would be communicated via : ( ACCEPTABLE : content ( ADOPT : i d O1 : what C1 : as ( GOAL ) )", "entities": [[4, 5, "DatasetName", "BA"]]}
{"text": "Since the user has already signaled their intention to commit to the objective by proposing it , on receiving from the BA that the objective is ACCEPTABLE , the CPSA knows that there is mutual agreement , decides that that the objective is now adopted , and sends back to the BA the following CPS act : to signal that now there is a joint commitment to O1 .", "entities": [[21, 22, "DatasetName", "BA"], [51, 52, "DatasetName", "BA"]]}
{"text": "This creates an obligation on the part of the BA to pursue O1 in whatever manner it deems appropriate .", "entities": [[9, 10, "DatasetName", "BA"]]}
{"text": "When we have a system - proposed objective , such as O2 above , if the user expresses their acceptance ( \" Yes \" , \" Sure \" , \" I can handle that \" , etc . ) , the CPSA will recognize this as completing the agreement , and then it would adopt the objective and send the COMMIT act to the BA .", "entities": [[64, 65, "DatasetName", "BA"]]}
{"text": "One is MODIFICATION , used when one of the agents is expressing an intention of changing in some manner a prior objective ( for example , if one of the agents had suggested placing a blue block on the table , the other agent might suggest placing a red block instead ) .", "entities": [[43, 44, "DatasetName", "agent"]]}
{"text": "It is possible , however , that the BA may be able to use its more detailed knowledge of the situation to make that determination .", "entities": [[8, 9, "DatasetName", "BA"]]}
{"text": "Thus , upon receiving an objective marked as an elaboration of another one , if the BA deems it acceptable , it has the obligation to clarify the relation as well .", "entities": [[16, 17, "DatasetName", "BA"]]}
{"text": "If the BA rejects it , the user will likely not be satisfied with a simple \" No \" .", "entities": [[2, 3, "DatasetName", "BA"]]}
{"text": "Similarly , if the BA fails to understand the objective ( or if it encounters any other type of failure , e.g. , while trying to perform some action ) , the system should be able to explain what happened .", "entities": [[4, 5, "DatasetName", "BA"]]}
{"text": "As for how to repair the situation , this can be an alternative objective , that the BA is ready to commit to , which could be either a modification of the reject - ed one , or , perhaps , an objective which , if realized , would make the rejected objective acceptable .", "entities": [[17, 18, "DatasetName", "BA"]]}
{"text": "For example , if the user wanted to build an all - blue 5 - block tower , but the BA has only 4 blue blocks , it would reject the goal ( INSUFFICIENT - RESOURCES ) , but it could suggest as an alternative that a 4 - block blue tower would be an achievable alternative .", "entities": [[20, 21, "DatasetName", "BA"]]}
{"text": "One agent may inform the other of a fact that they believe the other should know .", "entities": [[1, 2, "DatasetName", "agent"]]}
{"text": "When agent A informs agent B of a fact P , this indicates A 's immediate intention that B knows P. Similarly , if A asks B whether P is true ( an ask - if speech act ) or what object satisfies P ( an ask - wh speech act ) , A 's immediate intention is that B informs A of those particular facts ( Allen and Perrault , 1980 ) .", "entities": [[1, 2, "DatasetName", "agent"], [4, 5, "DatasetName", "agent"]]}
{"text": "In our model , an assertion of a fact results in the following CPS act : where C3 is an identifier pointing to a representation of the content of the assertion in the CONTEXT of the CPS act .", "entities": [[17, 18, "DatasetName", "C3"]]}
{"text": "The BA needs to decide , if it accepts A3 , how this addition will change its understanding of the situation and affect O1 or any other ( adopted ) objective .", "entities": [[1, 2, "DatasetName", "BA"]]}
{"text": "For example , if one of these CPS acts is initiated by the user , the act must be evaluated by the BA .", "entities": [[22, 23, "DatasetName", "BA"]]}
{"text": "If originating from the BA , the act must be proposed first , and realized through a communicative act .", "entities": [[4, 5, "DatasetName", "BA"]]}
{"text": "Insofar as the BA is capable of foreseeing these effects , it ought to inform the CPSA so the collaborative state can be updated .", "entities": [[3, 4, "DatasetName", "BA"]]}
{"text": "To facilitate an orderly conversation , it restricts both the timing and the magnitude of the BA 's ability to affect the collaborative state .", "entities": [[16, 17, "DatasetName", "BA"]]}
{"text": "This act can be sent to the BA whenever there are no pending updates to the collaborative state , and no outstanding communicative acts to process or to wait on .", "entities": [[7, 8, "DatasetName", "BA"]]}
{"text": "In effect , by sending this act , the CPSA transfers the task initiative to the BA , which gives it the chance to , ultimately , influence discourse initiative as well .", "entities": [[16, 17, "DatasetName", "BA"]]}
{"text": "The BA has the obligation to respond with a single update to the collaborative state , presumably the one with the highest priority .", "entities": [[1, 2, "DatasetName", "BA"]]}
{"text": "The BA 's reply to a WHAT - NEXT depends on its own private problem - solving state .", "entities": [[1, 2, "DatasetName", "BA"]]}
{"text": "One other possibility is that the BA is currently not doing any reasoning , but simply acting on the active objective , or has accomplished it .", "entities": [[6, 7, "DatasetName", "BA"]]}
{"text": "2 . WORKING - ON - IT , which indicates that the BA is actively pursuing A1 , but it will take more time .", "entities": [[12, 13, "DatasetName", "BA"]]}
{"text": "3 . WAITING - FOR - USER , which indicates that the BA can not make progress on A1 because it is waiting for the user to act on it ( or another objective that A1 depends on ) .", "entities": [[12, 13, "DatasetName", "BA"]]}
{"text": "This CPS act also allows the BA to communicate partial execution status ( that it has executed some actions , though it has not accomplished the objective yet ) , but we leave those details out of this discussion .", "entities": [[6, 7, "DatasetName", "BA"]]}
{"text": "The TRIPS system comes with a broad coverage parser with an extensive grammar and an effective 100 , 000 + word semantic vocabulary defined in terms of a 4000 concept domain - independent ontology .", "entities": [[33, 34, "MethodName", "ontology"]]}
{"text": "It operates in concert with a suite of statistical preprocessing components , performing tasks such as part - ofspeech tagging , named entity recognition , and identification of likely constituent boundaries .", "entities": [[21, 24, "TaskName", "named entity recognition"]]}
{"text": "We packaged the TRIPS NLU components ( including the lexicon and ontology ) with our CPS agent , thereby creating a dialogue system shell , which we call Cogent .", "entities": [[11, 12, "MethodName", "ontology"], [16, 17, "DatasetName", "agent"]]}
{"text": "This system does not include a BA or an NLG component ( Cogent 's components are surrounded with a dashed line in Figure 1 ) .", "entities": [[6, 7, "DatasetName", "BA"]]}
{"text": "Of note , by default all CPS acts have their contents expressed in the TRIPS ontology .", "entities": [[15, 16, "MethodName", "ontology"]]}
{"text": "We are also providing a tool for mapping concepts in the TRIPS ontology to domain ontologies .", "entities": [[12, 13, "MethodName", "ontology"]]}
{"text": "We have adapted the TRIPS interpretation manager to use these mappings to produce content in the domain ontology , to make it easier for the Behavioral Agents to interpret the CONTEXT associated with each CPS act .", "entities": [[17, 18, "MethodName", "ontology"]]}
{"text": "The details of the ontology mapping tool and the mappings it creates are , however , beyond the scope of this paper .", "entities": [[4, 5, "MethodName", "ontology"]]}
{"text": "We describe briefly six system prototypes that have been built using Cogent as the base frame - work ; thus , they all use the same CPS agent described above .", "entities": [[27, 28, "DatasetName", "agent"]]}
{"text": "All these systems have been developed as part of DARPA 's Communicating with Computers ( CwC ) program 3 .", "entities": [[9, 10, "DatasetName", "DARPA"]]}
{"text": "To manage this wide range of problemsolving behaviors , BoB 's BA integrates a variety of agents with specific expertise .", "entities": [[11, 12, "DatasetName", "BA"]]}
{"text": "ing domain - specific named entity recognizers ) and some additional ontology concepts and mappings ; we provided those customizations .", "entities": [[11, 12, "MethodName", "ontology"]]}
{"text": "We packaged this dialogue manager with a suite of broad coverage natural language understanding components ( from the TRIPS system ) and created a new , domain - independent CPS - based dialogue system shell .", "entities": [[11, 14, "TaskName", "natural language understanding"]]}
{"text": "This research was supported by the DARPA Communicating with Computers program , under ARO contract W911NF - 15 - 1 - 0542 .", "entities": [[6, 7, "DatasetName", "DARPA"]]}
{"text": "On Negative Interference in Multilingual Models : Findings and A Meta - Learning Treatment", "entities": [[10, 13, "TaskName", "Meta - Learning"]]}
{"text": "Motivated by these observations , we also present a meta - learning algorithm that obtains better cross - lingual transferability and alleviates negative interference , by adding languagespecific layers as meta - parameters and training them in a manner that explicitly improves shared layers ' generalization on all languages .", "entities": [[9, 12, "TaskName", "meta - learning"]]}
{"text": "Multilingual language models such as mBERT ( Devlin et al , 2018 ) and XLM ( Lample and Conneau , 2019 ) have been proven effective for cross - lingual transfer learning by pretraining a single shared Transformer model ( Vaswani et al , 2017 ) jointly on multiple languages .", "entities": [[5, 6, "MethodName", "mBERT"], [14, 15, "MethodName", "XLM"], [27, 31, "TaskName", "cross - lingual transfer"], [37, 38, "MethodName", "Transformer"]]}
{"text": "Similarly , recent work ( Johnson et al , 2017 ; Tan et al , 2019 ; Aharoni et al , 2019 ; in multilingual neural machine translation ( NMT ) also observed performance degradation on high - resource language pairs .", "entities": [[26, 28, "TaskName", "machine translation"]]}
{"text": "In multi - task learning ( Ruder , 2017 ) , this phenomenon is known as negative interference or negative transfer ( Wang et al , 2019 ) , where training multiple tasks jointly hinders the performance on individual tasks .", "entities": [[1, 5, "TaskName", "multi - task learning"]]}
{"text": "Multilingual transfer learning aims at utilizing knowledge transfer across languages to boost performance on low - resource languages .", "entities": [[1, 3, "TaskName", "transfer learning"]]}
{"text": "State - of - theart multilingual language models are trained on multiple languages jointly to enable cross - lingual transfer through parameter sharing .", "entities": [[16, 20, "TaskName", "cross - lingual transfer"]]}
{"text": "For example , in Figure 1 , we compare the performance on a named entity recognition ( NER ) task of monolingually - trained models vs. bilingual models ( trained on lg and English ) vs. state - of - the - art XLM .", "entities": [[13, 16, "TaskName", "named entity recognition"], [17, 18, "TaskName", "NER"], [43, 44, "MethodName", "XLM"]]}
{"text": "Without loss of generality , we focus on analyzing bilingual models to minimize confounding factors .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "Besides , since multilingual models are trained to enable cross - lingual transfer , we also report their performance on the zero - shot cross - lingual transfer setting , where the model is only finetuned on the source language , say lg 1 , and tested on the target language lg 2 .", "entities": [[9, 13, "TaskName", "cross - lingual transfer"], [21, 28, "TaskName", "zero - shot cross - lingual transfer"]]}
{"text": "Recent work ( Yu et al , 2020 ) shows that gradient conflict between dissimilar tasks , defined as a negative cosine similarity between gradients , is predictive of negative interference in multi - task learning .", "entities": [[32, 36, "TaskName", "multi - task learning"]]}
{"text": "To achieve this , we prune multilingual models for each language using relaxed L 0 norm regularization ( Louizos et al , 2017 ) , and compare parameter similarities between languages .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "( x i ; \u03b8 ) , y i )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "We focus on standard multilingual masked language modeling ( MLM ) used in mBERT and XLM .", "entities": [[5, 8, "TaskName", "masked language modeling"], [9, 10, "DatasetName", "MLM"], [13, 14, "MethodName", "mBERT"], [15, 16, "MethodName", "XLM"]]}
{"text": "For pretraining , we mainly follow the setup and implementation of XLM ( Lample and Conneau , 2019 ) .", "entities": [[11, 12, "MethodName", "XLM"]]}
{"text": "For each model , we use BPE ( Sennrich et al , 2016 ) to learn 32k subword vocabulary shared between languages .", "entities": [[6, 7, "MethodName", "BPE"]]}
{"text": "Each model is a standard Transformer ( Vaswani et al , 2017 ) with 8 layers , 12 heads , 512 embedding size and 2048 hidden dimension for the feedforward layer .", "entities": [[5, 6, "MethodName", "Transformer"], [24, 25, "DatasetName", "2048"]]}
{"text": "For evaluation , we consider four downstream tasks : named entity recognition ( NER ) , part - ofspeech tagging ( POS ) , question answering ( QA ) , and natural language inference ( NLI ) .", "entities": [[9, 12, "TaskName", "named entity recognition"], [13, 14, "TaskName", "NER"], [24, 26, "TaskName", "question answering"], [31, 34, "TaskName", "natural language inference"]]}
{"text": "We use the WikiAnn ( Pan et al , 2017 ) dataset , which is a sequence labelling task built automatically from Wikipedia .", "entities": [[3, 4, "DatasetName", "WikiAnn"]]}
{"text": "A linear layer with softmax classifier is added on top of pretrained models to predict the label for each word based on its first subword .", "entities": [[1, 3, "MethodName", "linear layer"], [4, 5, "MethodName", "softmax"]]}
{"text": "We report the F1 score .", "entities": [[3, 5, "MetricName", "F1 score"]]}
{"text": "Similar to NER , POS is also a sequence labelling task but with a focus on synthetic knowledge .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "In particular , we use the Universal Dependencies treebanks ( Nivre et al , 2018 ) .", "entities": [[6, 8, "DatasetName", "Universal Dependencies"]]}
{"text": "Task - specific layers are the same and we report F1 , as in NER .", "entities": [[10, 11, "MetricName", "F1"], [14, 15, "TaskName", "NER"]]}
{"text": "We choose to use the TyDiQA - GoldP dataset ( Clark et al , 2020 ) that covers typologically diverse languages .", "entities": [[5, 8, "DatasetName", "TyDiQA - GoldP"]]}
{"text": "Similar to popular QA dataset such as SQuAD ( Rajpurkar et al , 2018 ) , this is a span prediction task where task - specific linear classifiers are used to predict start / end positions of the answer .", "entities": [[7, 8, "DatasetName", "SQuAD"]]}
{"text": "Standard metrics of F1 and Exact Match ( EM ) are reported .", "entities": [[3, 4, "MetricName", "F1"], [5, 7, "MetricName", "Exact Match"], [8, 9, "MetricName", "EM"]]}
{"text": "NLI XNLI ( Conneau et al , 2018 ) is probably the most popular cross - lingual benchmark .", "entities": [[1, 2, "DatasetName", "XNLI"]]}
{"text": "In fact , monolingual models even perform better than XLM on four out of six languages including hi and te , despite that XLM is much larger in model sizes and trained with much more resources .", "entities": [[9, 10, "MethodName", "XLM"], [23, 24, "MethodName", "XLM"]]}
{"text": "Besides , it also demonstrates that parameters in multi - head attention layers obtain higher similarities than those in feedforward layers , suggesting that attention mechanism might be more languageuniversal .", "entities": [[8, 12, "MethodName", "multi - head attention"]]}
{"text": "For each type of component , we create two separate copies in each Transformer layer , one designated for each language , while the rest of the network remains unchanged .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "We also find that language - specific feedforward layers obtain larger performance gains compared to attention layers , consistent with our prior analysis .", "entities": [[15, 17, "HyperparameterName", "attention layers"]]}
{"text": "Inspired by recent work in meta learning ( Flennerhag et al , 2019 ) that utilizes meta parameters to improve gradient geometry of the base network , we propose a novel meta - learning formulation of multilingual models that exploits language - specific parameters to i m - prove generalization of shared parameters .", "entities": [[0, 1, "DatasetName", "Inspired"], [31, 34, "TaskName", "meta - learning"]]}
{"text": "For a model with some predefined languagespecific parameters \u03c6 = { \u03c6 i } L i=1 , where \u03c6 i is designated for the i - th language , and shared parameters \u03b8 , our solution is to treat \u03c6 as meta parameters and \u03b8 as base parameters .", "entities": [[32, 33, "HyperparameterName", "\u03b8"], [44, 45, "HyperparameterName", "\u03b8"]]}
{"text": "Ideally , we want \u03c6 to store non - transferable language - specific knowledge to resolve conflicts and improve generalization of \u03b8 in all languages ( a.k.a . mitigate negative interference and improve cross - lingual transferability ) .", "entities": [[21, 22, "HyperparameterName", "\u03b8"]]}
{"text": "Therefore , we train \u03c6 based on the following principle : if \u03b8 follows the gradients on training data for a given \u03c6 , the resulting \u03b8 should obtain a good validation performance on all languages .", "entities": [[12, 13, "HyperparameterName", "\u03b8"], [26, 27, "HyperparameterName", "\u03b8"]]}
{"text": "L i val ( \u03b8 * , \u03c6 i ) s.t .", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "To be specific , at each training step t on the ith language during pretraining , we first adapt a gradient step on \u03b8 to obtain a new \u03b8 and update \u03c6 i Update language - specific parameters as : \u03c6 ( t+1 )", "entities": [[23, 24, "HyperparameterName", "\u03b8"], [28, 29, "HyperparameterName", "\u03b8"]]}
{"text": "i \u2212 \u03b2\u2207 \u03b8 ( t )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "L i train ( \u03b8 ( t ) , \u03c6 ( t ) i ) , \u03c6 ( t ) j ) )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "L train ( \u03b8 ( \u03c6 ( t+1 )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "\u2212 \u03b2\u2207 \u03b8 ( t )", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "Notice that \u03b8 is a function of \u03c6 ( t ) i and thus this optimization requires computing the gradient of gradient .", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "Finally , in the second phase , we update \u03b8 based on the new \u03c6 ( t+1 ) : \u03b8 ( t+1 )", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "= \u03b8 ( t ) \u2212 \u03b2\u2207 \u03b8 ( t )", "entities": [[1, 2, "HyperparameterName", "\u03b8"], [7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "L train ( \u03b8 ( t ) , \u03c6 ( t+1 ) )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "For example , we plot training loss in the early stage in Figure 4c , which shows that ordinary adapters converge slower than JointPair due to overfitting of language - specific adapters while meta adapters converge much faster .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "Unsupervised multilingual language models such as mBERT ( Devlin et al , 2018 ) and XLM ( Lample and Conneau , 2019 ; work surprisingly well on many NLP tasks without parallel training signals ( Pires et al , 2019 ; Wu and Dredze , 2019 ) .", "entities": [[6, 7, "MethodName", "mBERT"], [15, 16, "MethodName", "XLM"]]}
{"text": "Our work is also related to transfer learning ( Pan and Yang , 2010 ) and multi - task learning ( Ruder , 2017 ) .", "entities": [[6, 8, "TaskName", "transfer learning"], [16, 20, "TaskName", "multi - task learning"]]}
{"text": "While these methods focus on testing latent representations , we directly compare similarity of neural network structures through network pruning .", "entities": [[18, 20, "TaskName", "network pruning"]]}
{"text": "Notice that XNLI only has training data in available in English so we only evaluate zero - shot crosslingual performance on it .", "entities": [[2, 3, "DatasetName", "XNLI"]]}
{"text": "Following ( Hu et al , 2020 ) , we finetune the model for 10 epochs for NER and POS , 2 epochs for QA and 200 epochs for XNLI .", "entities": [[17, 18, "TaskName", "NER"], [29, 30, "DatasetName", "XNLI"]]}
{"text": "where W z i R d\u00d7b and g is some activation function such as relu .", "entities": [[10, 12, "HyperparameterName", "activation function"], [14, 15, "MethodName", "relu"]]}
{"text": "It is then projected back to the original input dimension d with a residual connection :", "entities": [[13, 15, "MethodName", "residual connection"]]}
{"text": "L i train ( \u03b8 + , \u03c6 ( t ) i )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "L i train ( \u03b8 \u2212 , \u03c6 ( t ) i )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "We show the full results on the TyDiQA - GoldP dataset in Table 7 .", "entities": [[7, 10, "DatasetName", "TyDiQA - GoldP"]]}
{"text": "- 2016 Task 1 : A Flexible and Extendable System for Semantic Text Similarity using Types , Surprise and Phrase Linking", "entities": [[12, 14, "TaskName", "Text Similarity"]]}
{"text": "We present in this paper a system for measuring Semantic Text Similarity ( STS ) in English .", "entities": [[10, 12, "TaskName", "Text Similarity"], [13, 14, "TaskName", "STS"]]}
{"text": "Additionally , the use of Surprise , Types and phrase linking is not limited to STS and can be used across various Natural Language Processing tasks , while our method of combining scores provides a flexible way of combining variously generated Similarity Scores .", "entities": [[15, 16, "TaskName", "STS"]]}
{"text": "The goal of Semantic Text Similarity ( STS ) is to find the degree of overlap in the meaning of two pieces of text .", "entities": [[4, 6, "TaskName", "Text Similarity"], [7, 8, "TaskName", "STS"]]}
{"text": "STS has a wide variety of applications , including text summarisation ( Aliguliyev , 2009 ) , machine translation ( Kauchak and Barzilay , 2006 ) , and search optimisation ( Sriram et al , 2010 ) .", "entities": [[0, 1, "TaskName", "STS"], [17, 19, "TaskName", "machine translation"]]}
{"text": "The STS task , which has been set by the SemEval conference for the past number of years ( Agirre et al , 2014 ; Agirre et al , 2015 ) , requires that submitted systems assign a score between 0 ( the sentences are on different topics ) and 5 ( the sentences mean exactly the same thing ) that reflects how similar two sentences are .", "entities": [[1, 2, "TaskName", "STS"], [40, 41, "DatasetName", "0"]]}
{"text": "Most systems that tackled SemEval 's STS task in previous years have involved three main approaches : The first is text alignment , based on the content words ' meaning ( Sultan et al , 2015 ;", "entities": [[6, 7, "TaskName", "STS"]]}
{"text": "Word embeddings provide a method of mapping words or phrases to vectors , whose cosine distance represents semantic similarity .", "entities": [[0, 2, "TaskName", "Word embeddings"], [17, 19, "TaskName", "semantic similarity"]]}
{"text": "They have proved to be powerful in many NLP tasks , and have been used by top ranking systems at SemEval STS ( Sultan et al , 2015 ; H\u00e4nig et al , 2015 ) .", "entities": [[21, 22, "TaskName", "STS"]]}
{"text": "We use word2vec 1 , with the model trained by Google on the Google News dataset , through its Python interface Gensim 2 .", "entities": [[10, 11, "DatasetName", "Google"], [13, 14, "DatasetName", "Google"]]}
{"text": "This problem could very easily extend to the problem of Word Sense Disambiguation , which we avoid by use of a heuristic .", "entities": [[10, 13, "TaskName", "Word Sense Disambiguation"]]}
{"text": "In this particular case , the semantic similarity of the sentences is dependent on the head of the phrase that the word \" Prime \" is contained in ( i.e. \" Minister \" and \" Number \" ) .", "entities": [[6, 8, "TaskName", "semantic similarity"]]}
{"text": "We address this by finding phrases that consist of adjectives , adverbs and nouns , and varying the importance of the semantic similarity between words that are not the head of that phrase .", "entities": [[21, 23, "TaskName", "semantic similarity"]]}
{"text": "To do this , each of our Methods is ranked using three metrics with respect to the training set : The first is by use of the Pearson Correlation ( a criterion we call \" Method \" ) , the second is by the sum of the absolute error between Similarity Scores ( a criterion we call \" Error \" ) .", "entities": [[27, 29, "MetricName", "Pearson Correlation"], [58, 59, "MetricName", "Error"]]}
{"text": "The final score is scaled between 0 and 5 and averaged across the 10 candidates returned by OneLook .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "As a consequence , we found that results were clustered around 0 and 5 .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "A breakdown of each of the run 's performance against the 2015 STS data set is provided in Table 3 .", "entities": [[12, 13, "TaskName", "STS"]]}
{"text": "Our system also achieves comparable results ( 0.7793 ) to that presented by Sultan et al ( 2015 ) ( 0.779 ) on the 2014 STS dataset .", "entities": [[25, 26, "TaskName", "STS"]]}
{"text": "In this paper we have described the system we used for participation in the SemEval STS Monolingual Task which made use of Types , Phrase Linking , and a method of establishing common noun importance .", "entities": [[15, 16, "TaskName", "STS"]]}
{"text": "We also intend to use the STS data for learning the weights of different Types for use in other NLP applications .", "entities": [[6, 7, "TaskName", "STS"]]}
{"text": "Our immediate objectives will be in better defining types , re - categorising common noun Types based on clearer instructions to manual annotators , including finer definitions of Types for proper nouns using named entity recognition , and exploring methods of defining Types for verbs , adverbs and adjectives .", "entities": [[33, 36, "TaskName", "named entity recognition"]]}
{"text": "We also intend to explore the use of Types in Question Classification and Question Answering .", "entities": [[11, 12, "TaskName", "Classification"], [13, 15, "TaskName", "Question Answering"]]}
{"text": "UnibucKernel : Geolocating Swiss German Jodels Using Ensemble Learning", "entities": [[7, 9, "TaskName", "Ensemble Learning"]]}
{"text": "The dialect identification task is about accurately predicting the latitude and longitude of test samples .", "entities": [[1, 3, "TaskName", "dialect identification"]]}
{"text": "To minimize the prediction error , we approach the problem from a few different perspectives and consider various types of features , from lowlevel character n - grams to high - level BERT embeddings .", "entities": [[32, 33, "MethodName", "BERT"]]}
{"text": "We propose a single ensemble model joining the power of several individual models through meta - learning based on Extreme Gradient Boosting ( XGBoost ) ( Chen and Guestrin , 2016 ) .", "entities": [[14, 17, "TaskName", "meta - learning"]]}
{"text": "Previous usage in dialect identification has proved the efficiency of this technique in the task of interest ( Butnaru and Ionescu , 2018b ; G\u0203man and Ionescu , 2020 ; Ionescu and Butnaru , 2017 ; .", "entities": [[3, 5, "TaskName", "dialect identification"]]}
{"text": "The ability of capturing morphological relationships at the character level and using them as features for CNNs is also known to give promising results in dialect identification Tudoreanu , 2019 ) .", "entities": [[25, 27, "TaskName", "dialect identification"]]}
{"text": "Different from works using solely character - level CNNs for dialect identification Tudoreanu , 2019 ) , we believe that the addition of words might bring the benefit of learning dialectspecific multi - word expressions that are hard to capture at the character level ( Dhingra et al , 2016 ;", "entities": [[10, 12, "TaskName", "dialect identification"]]}
{"text": "Bidirectional Encoder Representations from Transformers ( BERT ) ( Devlin et al , 2019 ) is a top performing technique used in recent years for solving mainstream NLP problems .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Thus , it seems fit to also include the outputs of a fine - tuned German version of BERT in our XGBoost meta - learner .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "We present related work on dialect identification and geolocation of short texts in Section 2 .", "entities": [[5, 7, "TaskName", "dialect identification"]]}
{"text": "Quercini et al , 2010 ) , from ruled - based methods ( Bilhaut et al , 2003 ) to various machine learning techniques that rely on named entity recognition ( Ding et al , 2000 ; Gelernter and Mushegian , 2011 ; Qin et al , 2010 ) .", "entities": [[27, 30, "TaskName", "named entity recognition"]]}
{"text": "The latter methods usually employ clustering techniques based on topic models .", "entities": [[9, 11, "TaskName", "topic models"]]}
{"text": "Some techniques are based on deep neural networks such as the popular BERT architecture ( G\u0203man and Ionescu , 2020 ; Scherrer and Ljube\u0161i\u0107 , 2020 ) , bidirectional Long Short - Term Memory ( LSTM ) networks applied on FastText embeddings ( Mishra , 2020 ) or character - level CNNs ( G\u0203man and Ionescu , 2020 ) .", "entities": [[12, 13, "MethodName", "BERT"], [29, 34, "MethodName", "Long Short - Term Memory"], [35, 36, "MethodName", "LSTM"], [40, 41, "MethodName", "FastText"]]}
{"text": "Other techniques are based on shallow or handcrafted features such as a gridbased prediction using an n - gram language model ( Jauhiainen et al , 2020 ) or a clustering technique that shifts the problem into a discrete space , then uses an SVM for the classification of posts into regions .", "entities": [[44, 45, "MethodName", "SVM"]]}
{"text": "Our best submission ( G\u0203man and Ionescu , 2020 ) in last year 's campaign was an ensemble based on XGBoost as meta - learner over the predictions of three different models : \u03bd - SVR with string kernels , a character - level CNN and an LSTM based on BERT embeddings .", "entities": [[47, 48, "MethodName", "LSTM"], [50, 51, "MethodName", "BERT"]]}
{"text": "Instead of using an LSTM with BERT embeddings , we finetune the cased version of German BERT and use it directly for double regression in a setup that is more suitable to the data set size , compared to our previous endeavour .", "entities": [[4, 5, "MethodName", "LSTM"], [6, 7, "MethodName", "BERT"], [16, 17, "MethodName", "BERT"]]}
{"text": "Being relatively simple to use and implement , this technique has many applications according to the literature ( Cozma et al , 2018 ; Gim\u00e9nez - P\u00e9rez et al , 2017 ; Masala et al , 2017 ; Ionescu et al , 2014Popescu and Ionescu , 2013 ) , with emphasis on dialect identification and the good results obtained for this task in previous VarDial evaluation campaigns ( Butnaru and Ionescu , 2018b ; Ionescu and Butnaru , 2017 ; .", "entities": [[52, 54, "TaskName", "dialect identification"]]}
{"text": "For two strings x i and x j over a set of characters S , the presence bits string kernel is defined as follows : where n is the length of n - grams and # ( x , g ) is a function that returns 1 when the number of occurrences of n - gram g in x is greater than 1 , and 0 otherwise .", "entities": [[65, 66, "DatasetName", "0"]]}
{"text": "SVR ( Drucker et al , 1997 ) is a modified Support Vector Machines ( SVM ) ( Cortes and Vapnik , 1995 ) model that is repurposed for regression .", "entities": [[15, 16, "MethodName", "SVM"]]}
{"text": "Similar to SVM , SVR uses the notion of support vectors and margin in order to find an optimal estimator .", "entities": [[2, 3, "MethodName", "SVM"]]}
{"text": "Another reason to employ \u03bd - SVR in our regression task is that it was found to surpass other regression methods for other use cases , such as complex word identification ( Butnaru and Ionescu , 2018a ) .", "entities": [[28, 31, "TaskName", "complex word identification"]]}
{"text": "Word embeddings are vectorial word representations that associate similar vectors to semanti - cally related words , allowing us to express semantic relations mathematically in the generated embedding space .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "Considering the sometimes orthogonal benefits of character and word embeddings , an intuitive idea has emerged , namely that of combining the character and word representations , which should complement each other in various aspects and provide better meaningful cues in the learning process of hybrid neural architectures ( Liang et al , 2017 ) .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "The hybrid architecture concatenates two CNNs , out of which one is equipped with a character embedding layer and the other has an analogous word embeddings layer .", "entities": [[24, 26, "TaskName", "word embeddings"]]}
{"text": "The activation maps resulting after the last conv blocks of the char and the word CNNs are concatenated and the hybrid network continues with four fully - connected ( fc ) layers with ReLU activations .", "entities": [[33, 34, "MethodName", "ReLU"]]}
{"text": "Unlike other contemporary attempts at using transformers in language modeling ( Radford et al , 2018 ) , BERT ( Devlin et al , 2019 ) builds deep language representations in a self - supervised fashion and incorporates context from both directions .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "The masked language modeling technique enables BERT to pretrain these deep bidirectional representations , that can be further fine - tuned and adapted for a variety of downstream tasks , without significant architectural updates .", "entities": [[1, 4, "TaskName", "masked language modeling"], [6, 7, "MethodName", "BERT"]]}
{"text": "We also make use of this property in the current work , employing the Hugging Face ( Wolf et al , 2020 ) version of the cased German BERT model 1 .", "entities": [[28, 29, "MethodName", "BERT"]]}
{"text": "We fine - tune this pre - trained German BERT model for the geolocation of Swiss German short texts , in a regression setup .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "Shrinkage acts as a learning rate , reducing the influence of each individual tree .", "entities": [[4, 6, "HyperparameterName", "learning rate"]]}
{"text": "These results are consistent with those reported by Ionescu and Butnaru ( 2017 ) and G\u0203man and Ionescu ( 2020 ) , suggesting that the 3 - 5 n - gram range is optimal for German dialect identification .", "entities": [[36, 38, "TaskName", "dialect identification"]]}
{"text": "Similarly , for the proportion of support vectors \u03bd , we consider 10 values uniformly covering the interval ( 0 , 1 ] with a step of 0.1 .", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "In the optimization phase , common evaluation metrics such as the mean squared error ( MSE ) and the mean absolute error ( MAE ) are typically used to measure performance .", "entities": [[15, 16, "MetricName", "MSE"], [23, 24, "MetricName", "MAE"]]}
{"text": "In addition to these two metrics , we consider another error function as candidate for the loss to be minimized , namely the Huber loss .", "entities": [[16, 17, "MetricName", "loss"], [24, 25, "MetricName", "loss"]]}
{"text": "Although minimizing the Huber loss tends to give optimal values for the classical evaluation metrics , we finally use MSE as loss , given that it seems to converge to better results in terms of the median distance , which is the official evaluation metric in the SMG shared task .", "entities": [[4, 5, "MetricName", "loss"], [19, 20, "MetricName", "MSE"], [21, 22, "MetricName", "loss"]]}
{"text": "We train the hybrid architecture on mini - batches of 96 samples for 1000 epochs with early stopping .", "entities": [[16, 18, "MethodName", "early stopping"]]}
{"text": "Fine - Tuned German BERT .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "We fine - tune three BERT models in slightly different setups .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "The pretrained base model used as starting point is the cased German BERT ( Wolf et al , 2020 ) .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "We monitor the median distance for early stopping and observe that the best performance upon convergence is obtained by the third model , with a median distance of 30.17 km on the validation set , followed by the results of the first ( 30.63 km ) and second ( 33.86 km ) models , respectively .", "entities": [[6, 8, "MethodName", "early stopping"]]}
{"text": "In the ensemble , we include only the top scoring BERT model .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "Thus , we have the hybrid CNN relying on both words and characters as features , the shallow \u03bd - SVR based on string kernels and three fine - tuned German BERT models looking at higher - level features and understanding dependencies in a bidirectional manner .", "entities": [[31, 32, "MethodName", "BERT"]]}
{"text": "Among the independent models , the hybrid CNN obtains slightly better results in terms of the median distance ( 30.05 km ) , whereas the second attempt at fine - tuning BERT gives the worst distances , namely 33.86 km for the median distance and 38.85 km for the mean distance .", "entities": [[31, 32, "MethodName", "BERT"]]}
{"text": "In this paper , we proposed an ensemble learning model for the geolocation of Swiss German social media posts .", "entities": [[7, 9, "TaskName", "ensemble learning"]]}
{"text": "The ensemble is based on an XGBoost meta - learner applied on top of three individual models : a hybrid CNN , an approach based on string kernels and a fine - tuned German BERT model .", "entities": [[34, 35, "MethodName", "BERT"]]}
{"text": "BERT Prescriptions to Avoid Unwanted Headaches : A Comparison of Transformer Architectures for Adverse Drug Event Detection", "entities": [[0, 1, "MethodName", "BERT"], [10, 11, "MethodName", "Transformer"], [15, 17, "TaskName", "Event Detection"]]}
{"text": "Pretrained transformer - based models , such as BERT and its variants , have become a common choice to obtain state - of - the - art performances in NLP tasks .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "In the identification of Adverse Drug Events ( ADE ) from social media texts , for example , BERT architectures rank first in the leaderboard .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "This raising interest is attested , for example , by the ACL workshop series on Social Media Health Mining ( SMM4H ) , in which shared tasks on ADE detection have been regularly organized since 2016 ( Paul et al , 2016 ; Sarker and Gonzalez - Hernandez , 2017 ;", "entities": [[20, 21, "DatasetName", "SMM4H"]]}
{"text": "Studies like ; ; Daniulaityte et al ( 2016 ) were among the first to propose machine learning systems for the detection of ADE in social media texts , using traditional feature engineering and word embeddings - based approaches .", "entities": [[31, 33, "TaskName", "feature engineering"], [34, 36, "TaskName", "word embeddings"]]}
{"text": "With the introduction of the SMM4H shared task , methods based on neural networks became a more and more common choice for tackling the task ( Wu et al , 2018 ; Nikhil and Mundra , 2018 ) , and finally , it was the turn of Transformer - based models such as BERT ( Devlin et al , 2019 ) and BioBERT ( Lee et al , 2020 ) , which are the building blocks of most of the top performing systems in the recent competitions Mahata et al , 2019 ; Miftahutdinov et al , 2019 ) .", "entities": [[5, 6, "DatasetName", "SMM4H"], [47, 48, "MethodName", "Transformer"], [53, 54, "MethodName", "BERT"]]}
{"text": "At the same time , the task has been independently tackled also by researchers in Named Entity Recognition , since ADE detection represents a classical case of a challenging task where the entities can be composed by discontinuous spans of text ( Stanovsky et al , 2017 ; Dai et al , 2020 ; Wunnava et al , 2020 ) .", "entities": [[15, 18, "TaskName", "Named Entity Recognition"]]}
{"text": "The \" golden child \" of this revolution is BERT ( Devlin et al , 2019 ) , which was the first system to apply the bidirectional training of a Transformer to a language modeling task .", "entities": [[9, 10, "MethodName", "BERT"], [30, 31, "MethodName", "Transformer"]]}
{"text": "More specifically , BERT is trained with a Masked Language Modeling objective : random words in the input sentences are replaced by a [ MASK ] token and the model attempts to predict the masked token based on the surrounding context .", "entities": [[3, 4, "MethodName", "BERT"], [8, 11, "TaskName", "Masked Language Modeling"]]}
{"text": "Following BERT 's success , several similar architectures have been introduced in biomedical NLP , proposing different forms of in - domain training or using different corpora ( Beltagy et al , 2019 ; Alsentzer et al , 2019 ; Lee et al , 2020 ;", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "Some of them already proved to be efficient for ADE detection : for example , the top system of the SMM4H shared task 2019 is based on an ensemble of BioBERTs ( Weissenbacher et al , 2019 ) .", "entities": [[20, 21, "DatasetName", "SMM4H"]]}
{"text": "During the training of Span - BERT , random contiguous spans of tokens are masked , rather than individual words , forcing the model to predict the full span from the tokens at its boundaries .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "SMM4H is the training dataset for Task 2 of the SMM4H shared task 2019 ( Weissenbacher et", "entities": [[0, 1, "DatasetName", "SMM4H"], [10, 11, "DatasetName", "SMM4H"]]}
{"text": "The datasets correspond to different text genres : the tweets of SMM4H are mostly short messages , containing informal language , while the texts of CADEC are longer and structured descriptions .", "entities": [[11, 12, "DatasetName", "SMM4H"]]}
{"text": "Apart from the original BERT , we experimented with SpanBERT , for its peculiar pretraining procedure which focuses on predicting and encoding spans instead of single words , and with four BERT variants with in - domain knowledge , which differ from each other both for the corpus they were trained on and for the kind of pretraining .", "entities": [[4, 5, "MethodName", "BERT"], [31, 32, "MethodName", "BERT"]]}
{"text": "BERT Standard model , pretrained on general purpose texts ( Wikipedia and BookCorpus ) .", "entities": [[0, 1, "MethodName", "BERT"], [12, 13, "DatasetName", "BookCorpus"]]}
{"text": "This model is pretrained using the same corpus as the original BERT , so it comes with no in - domain knowledge .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "But the pretraining procedure makes its embeddings more appropriate for NER - like tasks .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "as it introduces an additional loss called Span Boundary Objective ( SBO ) , alongside the traditional Masked Language Modelling ( MLM ) used for BERT .", "entities": [[5, 6, "MetricName", "loss"], [18, 20, "TaskName", "Language Modelling"], [21, 22, "DatasetName", "MLM"], [25, 26, "MethodName", "BERT"]]}
{"text": "The MLM loss measures if it is possible to reconstruct each original word", "entities": [[1, 2, "DatasetName", "MLM"], [2, 3, "MetricName", "loss"]]}
{"text": "We chose BioBERT v1.1 ( + PubMed ) , which outperformed other BioBERT v1.0 versions ( including the ones trained on full texts ) in NER tasks involving Diseases and Drugs .", "entities": [[25, 26, "TaskName", "NER"]]}
{"text": "BioClinicalBERT ( Alsentzer et al , 2019 ) , pretrained from a BioBERT checkpoint , on clinical texts from the MIMIC - III database .", "entities": [[20, 23, "DatasetName", "MIMIC - III"]]}
{"text": "SciBERT ( Beltagy et al , 2019 ) , pretrained from scratch , on papers retrieved from Semantic Scholar ( 82 % of medical domain ) .", "entities": [[17, 19, "DatasetName", "Semantic Scholar"], [23, 25, "DatasetName", "medical domain"]]}
{"text": "Notice that two in - domain architectures were pretrained from scratch ( SciBERT and PubMed - BERT ) , meaning that they have a unique vocabulary tailored on their pretraining corpus , and include specific embeddings for in - domain words .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "BioBERT and BioClinicalBERT were instead pretrained starting from a BERT and BioBERT checkpoint , respectively .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "This means that the vocabularies are built from general - domain texts ( similarly to BERT ) and the embeddings are initialized likewise .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "For all of the BERT variants , we take into account two versions .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "The first one simply uses the model to generate a sequence of embeddings ( one for each sub - word token ) , which are then passed to a Linear Layer + Softmax to project them to the output space ( one value for each output label ) and turn them into a probability distribution over the labels .", "entities": [[29, 31, "MethodName", "Linear Layer"], [32, 33, "MethodName", "Softmax"]]}
{"text": "The second version combines the Transformerbased model with a Conditional Random Field ( CRF ) classifier ( Lafferty et al , 2001 ; Papay et al , 2020 ) .", "entities": [[9, 12, "MethodName", "Conditional Random Field"], [13, 14, "MethodName", "CRF"]]}
{"text": "The outputs generated by the first version become the input of a CRF module , producing another sequence of subword - level IOB labels .", "entities": [[12, 13, "MethodName", "CRF"]]}
{"text": "This step aims at denoising the output labels produced by the previous components .", "entities": [[4, 5, "TaskName", "denoising"]]}
{"text": "Learning rate , dropout rate and maximum epoch were chosen evaluating the models on the validation set .", "entities": [[0, 2, "HyperparameterName", "Learning rate"]]}
{"text": "This procedure was repeated five times with different random seeds , and finally we averaged the results over the five runs .", "entities": [[9, 10, "DatasetName", "seeds"]]}
{"text": "SpanBERT and PubMedBERT emerge as the top performing models , with close F1 - scores , and in particular , the SpanBERT models achieve the top score on both datasets , proving that modeling spans gives an important advantage for the identification of ADEs .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "For both models , the addition of CRF generally determines a slight improvement on CADEC , while it is detrimental on SMM4H. On SMM4H , the F1 - scores of BioBERT , SciBERT and Bio - ClinicalBERT consistently improve over the standard BERT , but they are outperformed by its CRFaugmented version , while on CADEC they perform closely to the standard model .", "entities": [[7, 8, "MethodName", "CRF"], [23, 24, "DatasetName", "SMM4H"], [26, 27, "MetricName", "F1"], [34, 35, "DatasetName", "Bio"], [42, 43, "MethodName", "BERT"]]}
{"text": "The models are also being compared with TM - RLeiden : we can notice that both versions of SpanBERT and PubMedBERT outperform it on CADEC ( the differences are also statistically significant for the McNemar test at p < 0.001 ) , while only the basic versions of the same models retain an advantage on it on SMM4H ( also in this case , the difference is significant at p < 0.001 ) .", "entities": [[57, 58, "DatasetName", "SMM4H"]]}
{"text": "The Samples belong to SMM4H ( 1 - 3 , 6 ) and CADEC ( 4 - 5 ) .", "entities": [[4, 5, "DatasetName", "SMM4H"]]}
{"text": "Still , for some of the models - including SpanBERT , PubMedBERT and TMRLeiden - this difference reaches a marginal significance ( p < 0.05 ) exclusively on the SMM4H dataset , where correctly identified spans have more difficult words .", "entities": [[29, 30, "DatasetName", "SMM4H"]]}
{"text": "We selected the samples on which one of the architectures performed significantly better than the other one in terms of F1 - Score , and analyzed them manually .", "entities": [[20, 23, "MetricName", "F1 - Score"]]}
{"text": "Table 5 is a summary of the information about the version of all Transformer - based models used and their pretraining methods .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "- 2016 Task 6 : An Ensemble Model for Stance Detection in Twitter", "entities": [[9, 11, "TaskName", "Stance Detection"]]}
{"text": "We present the IUCL system , based on supervised learning , for the shared task on stance detection .", "entities": [[16, 18, "TaskName", "stance detection"]]}
{"text": "We also use gradient boosting decision trees and SVM and merge all classifiers into an ensemble method .", "entities": [[8, 9, "MethodName", "SVM"]]}
{"text": "Stance detection is a difficult task since it often requires reasoning in order to determine whether an utterance is in favor of or against a specific issue .", "entities": [[0, 2, "TaskName", "Stance detection"]]}
{"text": "In the shared task ( see Mohammad et al ( 2016 ) for details about the shared task ) , we interpret it as a variant of sentiment analysis and adopt an approach that combines shallow lexical features with an ensemble of different supervised machine learning classifiers .", "entities": [[27, 29, "TaskName", "sentiment analysis"]]}
{"text": "We extract frequency counts of each token in the entire corpus and in each stance ( Favor , Against , None ) per target for use in the feature selection process .", "entities": [[28, 30, "MethodName", "feature selection"]]}
{"text": "One of the major decisions in developing a machine learning system for stance detection lies in the choice of features and of feature representations .", "entities": [[12, 14, "TaskName", "stance detection"]]}
{"text": "Detecting stance in political tweets can be regarded as a form of sentiment analysis for short text , and we assume that different stances of tweets are partially expressed by the choice of words .", "entities": [[12, 14, "TaskName", "sentiment analysis"]]}
{"text": "Another possibility would be to follow approaches in sentiment analysis and use sentiment lexicons .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}
{"text": "Since unigrams include a high number of irrelevant features and also constitute a rather impoverished representation , we use feature selection as well as word vectors in our experiments .", "entities": [[19, 21, "MethodName", "feature selection"]]}
{"text": "We use information gain ( IG ) for feature selection on unigrams .", "entities": [[8, 10, "MethodName", "feature selection"]]}
{"text": "The three classifiers are GBDT , random forest , and SVM ; the ensemble uses their output ( predicted label and its probability ) .", "entities": [[10, 11, "MethodName", "SVM"]]}
{"text": "To alleviate these problems , we perform feature selection using information gain ( IG ) .", "entities": [[7, 9, "MethodName", "feature selection"]]}
{"text": "We choose IG because it has been shown to be robust across different sentiment analysis data sets and across different skewing ratios , compared to other feature selection methods ( Liu et al , 2014 ) .", "entities": [[13, 15, "TaskName", "sentiment analysis"], [26, 28, "MethodName", "feature selection"]]}
{"text": "We have experimented with two different word vector models , word2vec ( Mikolov et al , 2013 ) and GloVe ( Pennington et al , 2014 ) .", "entities": [[19, 20, "MethodName", "GloVe"]]}
{"text": "We have used the pre - trained word2vec obtained from the Google News dataset , which contains a 300 - dimensional vector representation for 3 million words and phrases 1 , and the pre - trained GloVe , which is obtained from 2 billion tweets and has a 250dimensional vector representation for 1.2 million words and phrases 2 .", "entities": [[11, 12, "DatasetName", "Google"], [36, 37, "MethodName", "GloVe"]]}
{"text": "We can see that GloVe performs consistently better than word2vec except for Feminist where word2vec is 0.6 % better than GloVe .", "entities": [[4, 5, "MethodName", "GloVe"], [20, 21, "MethodName", "GloVe"]]}
{"text": "We assume that this performance gap is mainly caused by the domain difference from which the word vectors are obtained : We used GloVe pretrained on tweets and word2vec pre - trained on news .", "entities": [[23, 24, "MethodName", "GloVe"]]}
{"text": "In other words , GloVe provides a broader coverage for this data set .", "entities": [[4, 5, "MethodName", "GloVe"]]}
{"text": "This score measures for each word ( its lemma ) the association with positive stance , sums up all words in the tweet , and normalizes the score by the tweet length .", "entities": [[8, 9, "DatasetName", "lemma"]]}
{"text": "Since there is little research on determining the best fitting bias for stance detection , we explore three different classifiers for the stance classification , support vector machines ( SVM ) , random forest , and gradient boosting decision trees ( GBDT ) .", "entities": [[12, 14, "TaskName", "stance detection"], [22, 24, "TaskName", "stance classification"], [29, 30, "MethodName", "SVM"]]}
{"text": "We choose SVM because it is the most widely used machine learning model for text classification and sentiment analysis ( e.g. , ( Pil\u00e1szy , 2005 ) ) .", "entities": [[2, 3, "MethodName", "SVM"], [14, 16, "TaskName", "text classification"], [17, 19, "TaskName", "sentiment analysis"]]}
{"text": "SVM and random forest are trained on different numbers of selected unigrams for each target : 1 , 700 for Abortion , 1 , 535 for Atheism , 1 , 381 for Climate , 1 , 749 for Feminist , and 1 , 704 for Hillary .", "entities": [[0, 1, "MethodName", "SVM"]]}
{"text": "GBDT is trained on the word vectors : 300 dimensions for word2vec and 250 dimensions for GloVe .", "entities": [[16, 17, "MethodName", "GloVe"]]}
{"text": "SVM Our initial experiments using cross validation on training data showed that linear kernel performed better than non - linear ones , and that the LinearSVC implementation ( one - vs - rest strategy for multi - class ) outperformed SVC ( one - vs - one strategy ) .", "entities": [[0, 1, "MethodName", "SVM"]]}
{"text": "Our initial experiments showed that GBDT handles word vector features better than SVM and random forest .", "entities": [[12, 13, "MethodName", "SVM"]]}
{"text": "These results show that the GBDT approach using GloVe reaches the highest result ( 64.64 ) among the individual classifiers .", "entities": [[8, 9, "MethodName", "GloVe"]]}
{"text": "The random forest classifier , which constitutes our official submission is about 1 percentage point lower ( 63.60 ) , and the SVM classifier is about 1.5 percentage points below that ( 61.93 ) .", "entities": [[22, 23, "MethodName", "SVM"]]}
{"text": "For this reason , we modified the scorer so that it would calculate accuracy , precision , and recall for individual stances per target separately .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "The official metric is the macro - averaged F - measure on Favor and Against while accuracy is equivalent to the micro - averaged F - measure based on all classes .", "entities": [[8, 11, "MetricName", "F - measure"], [16, 17, "MetricName", "accuracy"], [24, 27, "MetricName", "F - measure"]]}
{"text": "For the ensembles , the version without global features reaches higher accuracies for Abortion , Climate , and Hillary , the version with global features has a higher accuracy for Feminist , and they tie for Atheism .", "entities": [[28, 29, "MetricName", "accuracy"]]}
{"text": "In this shared task , we regard stance detection as a special case of sentiment analysis , using supervised classifiers and bag of unigrams and word vectors as features .", "entities": [[7, 9, "TaskName", "stance detection"], [14, 16, "TaskName", "sentiment analysis"]]}
{"text": "We also experimented with other single models ( SVM and GBDT ) and with an ensemble model built on a memory - based classifier .", "entities": [[8, 9, "MethodName", "SVM"]]}
{"text": "The GBDT model using GloVe word vectors reaches a higher score of 64.64 , which may be a result of the word vectors ' capability to capture similarities among words , which helps in dealing with out - of - vocabulary words .", "entities": [[4, 5, "MethodName", "GloVe"]]}
{"text": "Finally , it is worth pointing out that our approach to stance detection utilizes very surface oriented features .", "entities": [[11, 13, "TaskName", "stance detection"]]}
{"text": "AStarTwice at SemEval - 2021 Task 5 : Toxic Span Detection using RoBERTa - CRF , Domain Specific Pre - Training and Self - Training", "entities": [[12, 13, "MethodName", "RoBERTa"], [14, 15, "MethodName", "CRF"]]}
{"text": "Our solution is built upon RoBERTa language model and Conditional Random Fields ( CRF ) .", "entities": [[5, 6, "MethodName", "RoBERTa"], [13, 14, "MethodName", "CRF"]]}
{"text": "We pre - trained RoBERTa on Civil Comments dataset , enabling it to create better contextual representation for this task .", "entities": [[4, 5, "MethodName", "RoBERTa"]]}
{"text": "In addition to these , we also identified some pre - processing steps that significantly improved our F1 score .", "entities": [[17, 19, "MetricName", "F1 score"]]}
{"text": "With rising abusive language and hate on such platforms , it is more important than ever to maintain online conversations constructive and inclusive .", "entities": [[2, 4, "TaskName", "abusive language"]]}
{"text": "We formulated the task as a sequence tagging problem and used RoBERTa ( Liu et al , 2019b ) , a pre - trained Transformer - based ( Vaswani et al , 2017 ) language model as our base model .", "entities": [[11, 12, "MethodName", "RoBERTa"], [24, 25, "MethodName", "Transformer"]]}
{"text": "We further pre - trained RoBERTa on the Civil Comments Dataset as a masked language model ( Devlin et al , 2018 ) to create a domain - specific model .", "entities": [[5, 6, "MethodName", "RoBERTa"]]}
{"text": "We employed a Conditional Random Field ( CRF ) layer ( Lafferty et al , 2001 ) for predicting the most probabilistic sequence of labels for each input sequence .", "entities": [[3, 6, "MethodName", "Conditional Random Field"], [7, 8, "MethodName", "CRF"]]}
{"text": "Similar to other domain - specific models ( Beltagy et al , 2019 ; Lee et al , 2020 ; Paraschiv et al , 2020 ) , we pretrained the RoBERTa - base model on the Civil comments dataset using Masked Language Modelling ( MLM ) ( Devlin et al , 2018 ) to provide the necessary domain knowledge and created our model RoBERTa ( p ) .", "entities": [[30, 31, "MethodName", "RoBERTa"], [41, 43, "TaskName", "Language Modelling"], [44, 45, "DatasetName", "MLM"], [63, 64, "MethodName", "RoBERTa"]]}
{"text": "Pre - Processing : We applied a few preprocessing steps before fine - tuning RoBERTa on the input text samples .", "entities": [[14, 15, "MethodName", "RoBERTa"]]}
{"text": "Model : We provided the text samples as input to our pre - trained RoBERTa ( p ) model to get 768dimensional contextual embeddings for each token .", "entities": [[14, 15, "MethodName", "RoBERTa"]]}
{"text": "These contextual embeddings were passed through two dense layers of 512 and 128 dimensions , followed by a Conditional Random Fields ( CRF )", "entities": [[22, 23, "MethodName", "CRF"]]}
{"text": "The CRF layer models the correlation between the labels predicted for the individual tokens .", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "In the training dataset , sample length varies from 1 to 421 tokens , with an average length of 47 tokens when tokenized using the RoBERTa - base tokenizer .", "entities": [[25, 26, "MethodName", "RoBERTa"]]}
{"text": "The overall score is obtained by taking the mean of the F1 score of all samples in the test set .", "entities": [[11, 13, "MetricName", "F1 score"]]}
{"text": "Table 1 shows that our RoBERTa ( p ) model outperforms the original RoBERTa model .", "entities": [[5, 6, "MethodName", "RoBERTa"], [13, 14, "MethodName", "RoBERTa"]]}
{"text": "Adding the CRF layer further improves the F1 score by eliminating the problem of independent label prediction .", "entities": [[2, 3, "MethodName", "CRF"], [7, 9, "MetricName", "F1 score"]]}
{"text": "It is evident from table 1 that the BIO tagging scheme performs better than the IO tagging scheme when working with CRF , suggesting it can better understand the span nature of the output .", "entities": [[21, 22, "MethodName", "CRF"]]}
{"text": "To investigate our model 's most problematic cases , we analysed the samples for which our model gave a zero F1 score .", "entities": [[20, 22, "MetricName", "F1 score"]]}
{"text": "We built our solution on the RoBERTa language model and Conditional Random Fields ( CRF ) .", "entities": [[6, 7, "MethodName", "RoBERTa"], [14, 15, "MethodName", "CRF"]]}
{"text": "Though RoBERTa alone can achieve great results , we highlighted the benefits of using external datasets and the performance improvements it can help us achieve .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}
{"text": "We pre - trained RoBERTa on the Civil Comments dataset to impart domain - specific knowledge to it .", "entities": [[4, 5, "MethodName", "RoBERTa"]]}
{"text": "In addition to these , we also discovered some pre - processing steps that significantly improved our F1 score .", "entities": [[17, 19, "MetricName", "F1 score"]]}
{"text": "Experimenting with different tagging schemes , we found out that the BIO scheme works the best with CRF .", "entities": [[17, 18, "MethodName", "CRF"]]}
{"text": "In future , we plan to experiment with other language models such as T5 ( Raffel et al , 2019 ) , XL - Net ( Yang et al , 2019 ) and DeBERTa ( He et al , 2020 ) .", "entities": [[13, 14, "MethodName", "T5"], [33, 34, "MethodName", "DeBERTa"]]}
{"text": "Pre - training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine - tuning .", "entities": [[17, 19, "MethodName", "movement pruning"]]}
{"text": "Approaches such as magnitude pruning ( Han et al , 2015 ) , L0 regularization ( Louizos et al , 2018 ) , lottery ticket hypothesis ( Frankle and Carbin , 2018 ) , diff pruning ( Guo et al , 2020 ) , and movement pruning ( Sanh et al , 2020 ) have demonstrated remarkable reductions in model size .", "entities": [[45, 47, "MethodName", "movement pruning"]]}
{"text": "On the other hand distillation methods have been more effective at producing faster models as has been shown by DistilBERT ( Sanh et al , 2019 ) , TinyBERT ( Jiao et al , 2019 ) or MobileBERT ( Sun et al , 2020 ) .", "entities": [[19, 20, "MethodName", "DistilBERT"], [37, 38, "MethodName", "MobileBERT"]]}
{"text": "We integrate this approach with Movement pruning ( Sanh et al , 2020 ) , a simple method for pruning pre - trained models during fine - tuning .", "entities": [[5, 7, "MethodName", "Movement pruning"]]}
{"text": "Experiments consider a large variety of different benchmark datasets comparing accuracy and efficiency .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "Experiments on summarization also show a 1.39x speedup for an average of 2 points drop on all ROUGE metrics on CNN / DailyMail , and for a reduction of decoder weights of 3.5x . 1 Available at https://github.com/ huggingface / nn_pruning", "entities": [[2, 3, "TaskName", "summarization"]]}
{"text": "Knowledge distillation , introduced by Hinton et al ( 2015 ) , is a popular compression technique .", "entities": [[0, 2, "MethodName", "Knowledge distillation"]]}
{"text": "Distillation has been used to obtain significantly smaller BERT models achieving competitive performances .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "al ( 2019 ) distills BERT into shallower students during the pre - training stage and optionally during the finetuning stage .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "MobileBERT ( Sun et al , 2020 ) and TinyBERT ( Jiao et al , 2019 ) are obtained thanks to a layer - wise distillation strategy .", "entities": [[0, 1, "MethodName", "MobileBERT"]]}
{"text": "While Mao et al ( 2020 ) combine SVD - based matrix factorization with unstructured pruning , use structured pruning in order to reduce the rank .", "entities": [[8, 9, "DatasetName", "SVD"]]}
{"text": "Related to our approach , Kim and Awadalla ( 2020 ) and McCarley ( 2019 ) both apply structured pruning on the heads of the multi - head attention ( MHA ) and on the inner - layer nodes of the feed - forward network ( FFN ) .", "entities": [[25, 29, "MethodName", "multi - head attention"]]}
{"text": "Starting with a transformer model with parameters \u03b8 , our goal is to produce a set of parameters \u03b8 that are both fine - tuned for a specific end - task and smaller in such a way that inference can be efficiently computed on parallel hardware .", "entities": [[7, 8, "HyperparameterName", "\u03b8"], [18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "The two largest lines in the transformer parameter budget are the feed - forward network sublayer ( FFN ) and the multi - head attention sub - layer ( MHA ) .", "entities": [[21, 25, "MethodName", "multi - head attention"]]}
{"text": "Movement pruning ( Sanh et al , 2020 ) is a scorebased pruning approach that encourages the model to optimize these score parameters .", "entities": [[0, 2, "MethodName", "Movement pruning"]]}
{"text": "Specifically , we focus on the soft - movement variant of movement pruning that sets M ( S )", "entities": [[11, 13, "MethodName", "movement pruning"]]}
{"text": "In this work , we extend movement pruning to work on blocks of local parameters .", "entities": [[6, 8, "MethodName", "movement pruning"]]}
{"text": "We split the movement pruning regularization term into : \u03bb att \u03c3 ( S att )", "entities": [[3, 5, "MethodName", "movement pruning"]]}
{"text": "We conduct experiments on five ( English ) tasks commonly used to evaluate pre - trained language models : question answering ( SQuAD v1.1 Rajpurkar et al , 2016 ) and ( SQuAD v2 Rajpurkar et al , 2018 ) , natural language inference ( MNLI Williams et al , 2018 ) , sentence similarity ( QQP Chen et al , 2018 ) , sentiment classification ( SST - 2 Socher et al , 2013 ) and abstractive summarization ( CNN / DailyMail Hermann et al , 2015 ) .", "entities": [[19, 21, "TaskName", "question answering"], [22, 23, "DatasetName", "SQuAD"], [32, 33, "DatasetName", "SQuAD"], [41, 44, "TaskName", "natural language inference"], [45, 46, "DatasetName", "MNLI"], [56, 57, "DatasetName", "QQP"], [67, 68, "DatasetName", "SST"], [78, 79, "TaskName", "summarization"]]}
{"text": "SQuAD is formulated as a span - extraction task , MNLI and QQP are sentence pairs classification tasks , SST - 2 is a sentence classification task and CNN / DailyMail ( \" CNN \" ) is formulated as a conditional generation task .", "entities": [[0, 1, "DatasetName", "SQuAD"], [10, 11, "DatasetName", "MNLI"], [12, 13, "DatasetName", "QQP"], [19, 20, "DatasetName", "SST"], [24, 26, "TaskName", "sentence classification"]]}
{"text": "We use BERT ( Devlin et al , 2019 )", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "Movement pruning is a fully unstructured approach and gives an upper bound on the sparsity trade - offs we hope to achieve , even if it provides little speed benefit .", "entities": [[0, 2, "MethodName", "Movement pruning"]]}
{"text": "DistilBERT ( Sanh et al , 2019 ) is obtained by distilling through pre - training a pre - trained BERT into a smaller model .", "entities": [[0, 1, "MethodName", "DistilBERT"], [20, 21, "MethodName", "BERT"]]}
{"text": "TinyBERT ( Jiao et al , 2019 ) distills a finetuned model while using data augmentation .", "entities": [[14, 16, "TaskName", "data augmentation"]]}
{"text": "We also provide the number of parameters in the linear layers of the Transformer layers for each of our models and for the reference ones : as the linear layers represent most of the FLOPS , this is a good proxy for the computation required and to some extent for the compute time , when the model characteristics are equivalent .", "entities": [[4, 7, "HyperparameterName", "number of parameters"], [13, 14, "MethodName", "Transformer"], [34, 35, "MetricName", "FLOPS"]]}
{"text": "For SQuAD v1.1 , we are using 20 epochs instead of typically 2 for BERT models .", "entities": [[1, 2, "DatasetName", "SQuAD"], [14, 15, "MethodName", "BERT"]]}
{"text": "The pruning approaches are shown in Table 1 . Block pruning use square block sizes throughout all the linear layers , as an extension of the original movement pruning for which the block size is 1 .", "entities": [[27, 29, "MethodName", "movement pruning"]]}
{"text": "Hybrid pruning jointly removes hidden dimensions in feed - forward layers W 1 and W 2 , using movement pruning to create the dimension mask .", "entities": [[18, 20, "MethodName", "movement pruning"]]}
{"text": "For the attention layers , pruning only some rows or columns in W q , W k , W v and W o can not be practically exploited .", "entities": [[2, 4, "HyperparameterName", "attention layers"]]}
{"text": "To do so , we choose a block size on attention that equals the head size while still using the same soft movement pruning strategy .", "entities": [[22, 24, "MethodName", "movement pruning"]]}
{"text": "Results on SQuAD are shown in Figure 2 , which compares our approach for speed and density to baseline BERT - Base tuned models such as TinyBERT - 6 and DistilBERT ( MobileBERT is discussed below ) .", "entities": [[2, 3, "DatasetName", "SQuAD"], [19, 20, "MethodName", "BERT"], [30, 31, "MethodName", "DistilBERT"], [32, 33, "MethodName", "MobileBERT"]]}
{"text": "The main result is that the Hybrid Pruning model is as fast as the baseline and approaches the same accuracy while at the same time producing significantly smaller models in terms of density .", "entities": [[19, 20, "MetricName", "accuracy"]]}
{"text": "The figures also include two intrinsic baselines : our reimplementation of Movement pruning and pure Block pruning .", "entities": [[11, 13, "MethodName", "Movement pruning"]]}
{"text": "We find that our implementation of Movement pruning is highly effective at producing sparse models ( even leading to a small increase in accuracy ) but does not produce significant speedups .", "entities": [[6, 8, "MethodName", "Movement pruning"], [23, 24, "MetricName", "accuracy"]]}
{"text": "This indicates that our setup for measur - BERT performance on other tasks .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "Comparison with MobileBERT All methods can be improved further using a larger teacher model .", "entities": [[2, 3, "MethodName", "MobileBERT"]]}
{"text": "It should be noted that Mo - bileBERT makes use of additional optimizations not present in the original BERT - large we are using : LayerNorms are replaced by purely linear NoNorms , and GeLUs are replaced by ReLUs .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "For these experiments , we use a BERT - large teacher to perform meaningful comparisons , using our best method Hybrid Filled .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "This trend reverses after this point : a larger teacher is detrimental to accuracy when the student is very heavily pruned .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "Encoder - Decoder Finally , we apply these methods to two encoder - decoder architectures , BARTbase and BART - large for the task of summarization .", "entities": [[18, 19, "MethodName", "BART"], [25, 26, "TaskName", "summarization"]]}
{"text": "Voita et al ( 2019 ) observed that for machine translation models , encoder heads were much easier to prune than decoder ones .", "entities": [[9, 11, "TaskName", "machine translation"]]}
{"text": "We see that Hybrid pruning leads to large decoder compression ratios ( 3.4 on BART - base and 3.5 BART - large ) with only a small drop in ROUGE score .", "entities": [[14, 15, "MethodName", "BART"], [19, 20, "MethodName", "BART"]]}
{"text": "Large Model Pruning To test that this approach scales to large models , we apply Hybrid pruning on BERT - large on SQuAD v1.1 .", "entities": [[18, 19, "MethodName", "BERT"], [22, 23, "DatasetName", "SQuAD"]]}
{"text": "This pruned model is actually faster than a BERT - base model ( Table 5 ) .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "We can compare Hybrid Pruning of SQuAD v2 BERT - large models with the results of the structured pruning method described in McCarley ( 2019 ) .", "entities": [[6, 7, "DatasetName", "SQuAD"], [8, 9, "MethodName", "BERT"]]}
{"text": "When pruning a larger model , the final model is actually better than a smaller one with the same absolute number of parameters .", "entities": [[20, 23, "HyperparameterName", "number of parameters"]]}
{"text": "We also note that , with the original Movement Pruning method , we see some speedup due to full dimension pruning .", "entities": [[8, 10, "MethodName", "Movement Pruning"]]}
{"text": "However we see that using blocks leads to a significant speed improvement compared to Movement Pruning .", "entities": [[14, 16, "MethodName", "Movement Pruning"]]}
{"text": "Quantization Quantization is often of critical importance for practical applications .", "entities": [[0, 1, "TaskName", "Quantization"], [1, 2, "TaskName", "Quantization"]]}
{"text": "We , therefore , wanted to check that our networks could be subjected to quantization without significant loss of accuracy , especially when considering the issues that could arise with the high level of sparsity of some FFNs .", "entities": [[14, 15, "TaskName", "quantization"], [17, 18, "MetricName", "loss"], [19, 20, "MetricName", "accuracy"]]}
{"text": "Table 6 shows the results of full 8 - bit quantization tests on our models .", "entities": [[10, 11, "TaskName", "quantization"]]}
{"text": "These indicate that the method is compatible with quantization , and the models using quantization on top of our pruning method achieve very high gains in terms of size ( as well as speed ) .", "entities": [[8, 9, "TaskName", "quantization"], [14, 15, "TaskName", "quantization"]]}
{"text": "We report experimental results with the addition of a teacher distillation step as previous work showed this boosts movement pruning at little cost .", "entities": [[18, 20, "MethodName", "movement pruning"]]}
{"text": "In this section , we conduct an ablation study to evaluate the impact of distillation using a BERT - base teacher .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "The distillation effect is larger for smaller datasets such as SST - 2 , which are prone to over - fitting .", "entities": [[10, 11, "DatasetName", "SST"]]}
{"text": "The method does not resort to techniques such as data augmentation or architecture search , and it works on a diverse set of tasks and base models .", "entities": [[9, 11, "TaskName", "data augmentation"]]}
{"text": "As better and larger models are published at an increasing pace , we can rely on a simple and robust method to accelerate them on specific tasks without sacrificing accuracy and distribute these models easily while keeping most of the original model accuracy .", "entities": [[29, 30, "MetricName", "accuracy"], [42, 43, "MetricName", "accuracy"]]}
{"text": "For example , the SQuAD V1 checkpoints referenced in this paper are listed with the hyperparameters and related information .", "entities": [[4, 5, "DatasetName", "SQuAD"]]}
{"text": "Some of the models we produced during this research can be used directly from the Hugging Face model hub .", "entities": [[16, 18, "TaskName", "Face model"]]}
{"text": "We are using Block Movement pruning for each model , with different block patterns , pruning only the attention layers .", "entities": [[4, 6, "MethodName", "Movement pruning"], [18, 20, "HyperparameterName", "attention layers"]]}
{"text": "On this metric , we decided to compare our models to the best models available i.e. the distilled models ( MobileBERT , TinyBERT ) , even though the method is different , as they are the strongest \" speed / accuracy \" baseline available .", "entities": [[20, 21, "MethodName", "MobileBERT"], [40, 41, "MetricName", "accuracy"]]}
{"text": "In Table 9 we compare with TinyBERT ( Jiao et al , 2019 ) and MobileBERT ( Sun et al , 2020 We compare as well to Hybrid pruning , with and without a teacher , with the unstructured methods from Sanh et al ( 2020 ) ( the original Movement Pruning method we are using ) and Gordon et al ( 2020 )", "entities": [[15, 16, "MethodName", "MobileBERT"], [50, 52, "MethodName", "Movement Pruning"]]}
{"text": "The dataset , VIST - Edit 1 , includes 14 , 905 humanedited versions of 2 , 981 machine - generated visual stories .", "entities": [[3, 6, "DatasetName", "VIST - Edit"]]}
{"text": "The stories were generated by two state - of - the - art visual storytelling models , each aligned to 5 human - edited versions .", "entities": [[13, 15, "TaskName", "visual storytelling"]]}
{"text": "We establish baselines for the task , showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models .", "entities": [[25, 27, "TaskName", "visual storytelling"]]}
{"text": "Per the evaluation of the first Visual Storytelling Challenge ( Mitchell et al , 2018 ) , the ability of an algorithm to tell a sound story is still far from that of a human .", "entities": [[6, 8, "TaskName", "Visual Storytelling"]]}
{"text": "We introduce the first dataset for human edits of machine - generated visual stories , VIST - Edit , and explore how these collected edits may be used for the task of visual story post - editing ( see Figure 1 ) .", "entities": [[15, 18, "DatasetName", "VIST - Edit"]]}
{"text": "The original visual storytelling ( VIST ) task , as introduced by Huang et al ( 2016 ) , takes a sequence of five photos as input and generates a short story describing the photo sequence .", "entities": [[2, 4, "TaskName", "visual storytelling"], [5, 6, "DatasetName", "VIST"]]}
{"text": "Huang et al also released the VIST dataset , containing 20 , 211 photo sequences , aligned to human - written stories .", "entities": [[6, 7, "DatasetName", "VIST"]]}
{"text": "On the other hand , the automatic postediting task revises the story generated from visual storytelling models , given both a machinegenerated story and a photo sequence .", "entities": [[14, 16, "TaskName", "visual storytelling"]]}
{"text": "Automatic post - editing treats the VIST system as a black box that is fixed and not modifiable .", "entities": [[0, 4, "TaskName", "Automatic post - editing"], [6, 7, "DatasetName", "VIST"]]}
{"text": "Its goal is to correct systematic errors of the VIST system and leverage the user edit data to improve story quality .", "entities": [[9, 10, "DatasetName", "VIST"]]}
{"text": "The visual story post - editing task is related to ( i ) automatic post - editing and ( ii ) stylized visual captioning .", "entities": [[13, 17, "TaskName", "automatic post - editing"]]}
{"text": "Automatic post - editing ( APE ) revises the text generated typically from a machine translation ( MT ) system , given both the source sentences and translated sentences .", "entities": [[0, 4, "TaskName", "Automatic post - editing"], [5, 6, "DatasetName", "APE"], [14, 16, "TaskName", "machine translation"]]}
{"text": "Like the proposed VIST post - editing task , APE aims to correct the systematic errors of MT , reducing translator workloads and increasing productivity ( Astudillo et al , 2018 ) .", "entities": [[3, 4, "DatasetName", "VIST"], [9, 10, "DatasetName", "APE"]]}
{"text": "Recently , neural models have been applied to APE in a sentence - to - sentence manner ( Libovick\u1ef3 et al , 2016 ; Junczys - Dowmunt and Grundkiewicz , 2016 ) , differing from previous phrase - based models that translate and reorder phrase segments for each sentence , such as ( Simard et al , 2007 ;", "entities": [[8, 9, "DatasetName", "APE"]]}
{"text": "The collected data could be useful for story APE tasks .", "entities": [[8, 9, "DatasetName", "APE"]]}
{"text": "Visual story post - editing could also be considered relevant to style transfer on image captions .", "entities": [[11, 13, "TaskName", "style transfer"]]}
{"text": "In this paper , we adopt the APE approach to treat preand post - edited stories as parallel data instead of the style transfer approach that omits this parallel relationship during model training .", "entities": [[7, 8, "DatasetName", "APE"], [22, 24, "TaskName", "style transfer"]]}
{"text": "Obtaining Machine - Generated Visual Stories This VIST - Edit dataset contains visual stories gen - erated by two state - of - the - art models , GLAC and AREL .", "entities": [[7, 10, "DatasetName", "VIST - Edit"]]}
{"text": "GLAC ( Global - Local Attention Cascading Networks ) ( Kim et al , 2018 ) achieved the highest human evaluation score in the first VIST Challenge ( Mitchell et al , 2018 ) .", "entities": [[2, 6, "MethodName", "Global - Local Attention"], [25, 26, "DatasetName", "VIST"]]}
{"text": "We obtain the pre - trained GLAC model provided by the authors via Github and run it on the entire VIST test set and obtain 2 , 019 stories .", "entities": [[20, 21, "DatasetName", "VIST"]]}
{"text": "AREL ( Adversarial REward Learning ) was the earliest available implementation online , and achieved the highest METEOR score on public test set in the VIST Challenge .", "entities": [[17, 18, "DatasetName", "METEOR"], [25, 26, "DatasetName", "VIST"]]}
{"text": "We also acquire a small set of human edits for 962 AREL 's stories generated using VIST test set , collected by Hsu et al ( 2019 ) .", "entities": [[16, 17, "DatasetName", "VIST"]]}
{"text": "We calculate the average number of Part - Of - Speech ( POS ) tags for tokens in each story using the python NLTK ( Bird et al , 2009 ) package , as shown in Table 1 .", "entities": [[6, 9, "DatasetName", "Part - Of"]]}
{"text": "For reference , the average T T R noun of a human - written story ( the entire VIST dataset ) is 0.86 .", "entities": [[18, 19, "DatasetName", "VIST"]]}
{"text": "AREL 's post - editing models are trained on the augmented AREL training set and evaluated on the AREL test set of VIST - Edit , and GLAC 's models are tested using GLAC sets , too .", "entities": [[22, 25, "DatasetName", "VIST - Edit"]]}
{"text": "Two neural approaches , Long short - term memory ( LSTM ) and Transformer , are used as baselines , where we experiment using ( i ) text only ( T ) and ( ii ) both text and images ( T+I ) as inputs .", "entities": [[4, 9, "MethodName", "Long short - term memory"], [10, 11, "MethodName", "LSTM"], [13, 14, "MethodName", "Transformer"]]}
{"text": "LSTM", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "An LSTM seq2seq model is used ( Sutskever et al , 2014 ) .", "entities": [[1, 2, "MethodName", "LSTM"], [2, 3, "MethodName", "seq2seq"]]}
{"text": "For the text - image setting , we first extract the image features using the pre - trained ResNet - 152 model and represent each image as a 2048 - dimensional vector .", "entities": [[18, 19, "MethodName", "ResNet"], [28, 29, "DatasetName", "2048"]]}
{"text": "The input sequence with both image information and text information is then encoded by LSTM , identical as in the text - only setting .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "We also use the Transformer architecture ( Vaswani et al , 2017 ) ) , Written - by - a - Human ( \" This story sounds like it was written by a human . \" ) , Visually - Grounded , and Detailed .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "LSTM ( T ) improves all aspects for stories by AREL , and improves \" Focus \" and \" Human - like \" aspects for stories by GLAC .", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "The input matrix R ( len+5 ) \u00d7dim is then passed into the Transformer as in the text - only setting .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "Data Augmentation In order to obtain sufficient training samples for neural models , we pair lessedited stories with more - edited stories of the same photo sequence to augment the data .", "entities": [[0, 2, "TaskName", "Data Augmentation"]]}
{"text": "In VIST - Edit , five human - edited stories are collected for each photo sequence .", "entities": [[1, 4, "DatasetName", "VIST - Edit"]]}
{"text": "This data augmentation strategy gives us in total fifteen ( 5 2 +5 = 15 ) training samples given five human - edited stories .", "entities": [[1, 3, "TaskName", "data augmentation"]]}
{"text": "Human Evaluation Following the evaluation procedure of the first VIST Challenge ( Mitchell et al , 2018 ) , for each visual story , we recruit five human judges on MTurk to rate it on six aspects ( at $ 0.1 / HIT . )", "entities": [[9, 10, "DatasetName", "VIST"]]}
{"text": "The LSTM using text - only input outperforms all other baselines .", "entities": [[1, 2, "MethodName", "LSTM"]]}
{"text": "These results demonstrate that a relatively small set of human edits can be used to boost the story quality of an existing large VIST model .", "entities": [[23, 24, "DatasetName", "VIST"]]}
{"text": "The inefficacy of image features and Transformer model might be caused by the small size of VIST - Edit .", "entities": [[6, 7, "MethodName", "Transformer"], [16, 19, "DatasetName", "VIST - Edit"]]}
{"text": "We then switch to use the human - written stories ( VIST test set ) as references , but again , all the automatic evaluation metrics generate lower scores even when the editing was done by human ( Table 4 . )", "entities": [[11, 12, "DatasetName", "VIST"]]}
{"text": "In row { of Table 5 , the reported correlation \u03c1 of METEOR is consistent with the findings in Huang et al ( 2016 ) , which suggests that METEOR could be useful when comparing among stories generated by the same visual storytelling model .", "entities": [[12, 13, "DatasetName", "METEOR"], [29, 30, "DatasetName", "METEOR"], [41, 43, "TaskName", "visual storytelling"]]}
{"text": "VIST - Edit , the first dataset for human edits of machine - generated visual stories , is introduced .", "entities": [[0, 3, "DatasetName", "VIST - Edit"]]}
{"text": "Evaluation of acoustic word embeddings", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "Recently , researchers in speech recognition have started to reconsider using whole words as the basic modeling unit , instead of phonetic units .", "entities": [[4, 6, "TaskName", "speech recognition"]]}
{"text": "This paper focuses on the evaluation of acoustic word embeddings .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "We propose two approaches to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations in order to evaluate whether they capture discriminative phonetic information .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "Recent studies have started to reconsider the use of whole words as the basic modeling unit in speech recognition and query applications , instead of phonetic units .", "entities": [[17, 19, "TaskName", "speech recognition"]]}
{"text": "This paper focuses on the evaluation of these acoustic word embeddings .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "We propose two approaches to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "The approach we used to build acoustic word embeddings is inspired from the one proposed in ( Bengio and Heigold , 2014 ) .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "The deep neural architecture depicted in figure 1 is used to train the acoustic word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "It is composed of convolution and pooling layers , followed by fully connected layers which feed the final softmax layer .", "entities": [[4, 5, "MethodName", "convolution"], [18, 19, "MethodName", "softmax"]]}
{"text": "The embedding layer is the fully connected layer just below the softmax one , named s in the figure 1 .", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "It is trained using the triplet ranking loss function in order to project orthographic word representations to the same space as the acoustic embeddings s.", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "This evalua - tion task can be performed on many ways , for example through the use of a dynamic time warping ( DTW ) to quantify the similarity between two segments when using frame level embeddings ( Thiolliere et al , 2015 ) , or by using the euclidean distance or the cosine similarity between embeddings representing the segments .", "entities": [[19, 22, "MethodName", "dynamic time warping"], [23, 24, "MethodName", "DTW"]]}
{"text": "In this study , we propose two approaches to evaluate acoustic word embeddings w + .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "We suggest to build different evaluation sets in order to assess the acoustic word embeddings ( w + ) performances on orthographic and phonetic similarity and homophones detection tasks .", "entities": [[13, 15, "TaskName", "word embeddings"]]}
{"text": "In our evaluation , we would like to measure the loss of orthographic information carried by w + and the potential gain of acoustic information due to this projection , in comparison to the information carried by o + .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "Next , we compute two similarity scores that correspond to the orthographic and phonetic similarity scores sim score attributed for each pair of words , which are defined as : sim score = 10 \u2212 min ( 10 , SER/10 ) ( 2 ) where min ( ) is a function used to have an edition distance between 0 and 10 .", "entities": [[58, 59, "DatasetName", "0"]]}
{"text": "This approach is used in ( Gao et al , 2014 ; Ji et al , 2015 ; Levy et al , 2015 ; Ghannay et al , 2016 ) to evaluate the linguistic word embeddings on similarity tasks , in which the similarity scores are attributed by human annotators .", "entities": [[34, 36, "TaskName", "word embeddings"]]}
{"text": "3 Experiments on acoustic word embeddings", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "The evaluation sets described in section 2.2 are generated from these two vocabularies : in the 52k vocabulary , all the acoustic word embeddings w + are related to words which have been observed during the training of the CNN .", "entities": [[22, 24, "TaskName", "word embeddings"]]}
{"text": "This means that at least two acoustic signal embeddings have been computed from the audio for each one of these words ; in the 160k vocabulary , about 110k acoustic word embeddings were computed for words never observed in the audio data .", "entities": [[30, 32, "TaskName", "word embeddings"]]}
{"text": "The quantitative evaluation of the acoustic word embeddings w + is performed on orthographic similarity , phonetic similarity , and homophones detection tasks .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "They show that the acoustic word embeddings w + are more relevant for the phonetic similarity task , while o + are obviously the best ones on the orthographic similarity task .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "So , in addition to making possible a measure of similarity distance between the acoustic signal ( represented by s ) and a word ( represented by w + ) , acoustic word embeddings are better than orthographic ones to measure the phonetic proximity between two words .", "entities": [[32, 34, "TaskName", "word embeddings"]]}
{"text": "This confirms that acoustic word embeddings have captured additional information about word pronunciation than the one carried by orthographic word embeddings .", "entities": [[4, 6, "TaskName", "word embeddings"], [19, 21, "TaskName", "word embeddings"]]}
{"text": "To give more insight into the difference of the quality of the orthographic word embeddings o + and the acoustic ones w + , we propose an empirical comparison by showing the nearest neighbours of a given set of words .", "entities": [[13, 15, "TaskName", "word embeddings"]]}
{"text": "In this paper , we have investigated the intrinsic evaluation of acoustic word embeddings .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "We have proposed two approaches to evaluate the performances of these acoustic word embeddings and compare them to their orthographic embeddings : orthographic and phonetic performance by ranking pairs and measuring the Spearman 's rank correlation coefficient ( Spearman 's \u03c1 ) , and by measuring the precision in a homophone detection task .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "Experiments show that the acoustic word embeddings are better than orthographic ones to measure the phonetic proximity between two words .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "This confirms that acoustic word embeddings have captured additional information about word pronunciation .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "Hyperbolic Capsule Networks for Multi - Label Classification", "entities": [[4, 8, "TaskName", "Multi - Label Classification"]]}
{"text": "Thus , we propose Hyperbolic Capsule Networks ( HYPERCAPS ) for Multi - Label Classification ( MLC ) , which have two merits .", "entities": [[11, 15, "TaskName", "Multi - Label Classification"]]}
{"text": "The main difference between Multi - Class Classification ( MCC ) and Multi - Label Classification ( MLC ) is that datasets in MCC have only serval mutually exclusive classes , while datasets in MLC contain much more correlated labels .", "entities": [[4, 8, "TaskName", "Multi - Class Classification"], [12, 16, "TaskName", "Multi - Label Classification"]]}
{"text": "Recent works for text classification , such as CNN - KIM ( Kim , 2014 ) and FASTTEXT ( Joulin et al , 2017 ) , focus on encoding a document into a fixed - length vector as the distributed document representation ( Le and Mikolov , 2014 ) .", "entities": [[3, 5, "TaskName", "text classification"], [17, 18, "MethodName", "FASTTEXT"]]}
{"text": "Inspired by the hyperbolic representation learning methods which demonstrate that the hyper - bolic space has more representation capacity than the Euclidean space ( Nickel and Kiela , 2017 ; Ganea et al , 2018a ) , Hyperbolic Capsule Networks ( HYPERCAPS ) is proposed .", "entities": [[0, 1, "DatasetName", "Inspired"], [4, 6, "TaskName", "representation learning"]]}
{"text": "The exponential map exp p : T p B n B n for w T p B n \\ { 0 } is consequently defined as exp p ( w ) = p ( tanh ( \u03bb p 2 w ) w w ) .", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "( 3 ) M\u00f6bius scalar multiplication for k R and p B n \\ { 0 } is defined as k \u2297", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "( 4 ) And k \u2297 p = 0 when p = 0 B n .", "entities": [[8, 9, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "The definition of M\u00f6bius matrix - vector multiplication for M R m\u00d7n and p B n when M p = 0 is as follows M \u2297", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "( 5 ) And M \u2297 p = 0 when M p = 0 . HDR is developed based on these operations .", "entities": [[8, 9, "DatasetName", "0"], [13, 14, "DatasetName", "0"]]}
{"text": "Neural networks are generally used as effective feature extractors for text classification .", "entities": [[10, 12, "TaskName", "text classification"]]}
{"text": ", x T ] , pre - trained wdimensional word embeddings ( e.g. GLOVE ( Pennington et al , 2014 ) ) are used to compose word vector representations E = [ e 1 , . . .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "N - gram kernels K R k\u00d7w with different window size k are applied on the local region of the word representations E t : t+k\u22121 R k\u00d7w to construct the local features as l t = \u03d5 ( K E t : t+k\u22121 ) , ( 6 ) where denotes the element - wise multiplication and \u03d5 is a non - linearity ( e.g. ReLU ) .", "entities": [[65, 66, "MethodName", "ReLU"]]}
{"text": "With totally d channels , the local hyperbolic capsules at position t can be constructed as l t = exp 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "Bidirectional GRU ( Chung et al , 2014 ) is adopted to incorporate forward and backward global contextual information and construct the global hyperbolic capsules .", "entities": [[0, 2, "MethodName", "Bidirectional GRU"]]}
{"text": "Forward and backward hidden states at time - step t are obtained by \u2212 h t = GRU ( \u2212\u2212 h t\u22121 , e t ) , \u2212 h t = GRU ( \u2212\u2212 h t+1 , e t ) .", "entities": [[17, 18, "MethodName", "GRU"], [31, 32, "MethodName", "GRU"]]}
{"text": "( 8 ) Each of the total 2 T hidden states can be taken as a global hyperbolic capsule using the exponential map , i.e. \u2212 g t = exp 0", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "The coupling coefficient c ij is iteratively updated during the HDR procedure and computed by the routing softmax c ij = exp ( b ij ) k exp ( b ik ) , ( 12 ) where the logits b ij are the log prior probabilities between capsule i and j , which are initialized as 0 .", "entities": [[17, 18, "MethodName", "softmax"], [56, 57, "DatasetName", "0"]]}
{"text": "ij 0 c = \u03c3 ( W c 1 T e", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "for all capsule i in layer and capsule j in layer + 1 : c ij softmax ( b ij ) Eq . 12 5 : for all capsule j in layer ( + 1 ) : v j M i c ij \u2297\u00fb j | i 6 : for all capsule i in layer and capsule j in layer +", "entities": [[16, 17, "MethodName", "softmax"]]}
{"text": "j=1 y j log ( c j ) + ( 1 \u2212 y j ) log ( 1 \u2212 c j ) , ( 17 ) where c j [ 0 , 1 ] is the j - th element in c and y j { 0 , 1 } denotes the ground truth about label j. The adaptive routing layer selects the candidate labels during test .", "entities": [[30, 31, "DatasetName", "0"], [46, 47, "DatasetName", "0"]]}
{"text": "Let N + denote the true label set and N \u2212 denote the set of randomly selected negative labels , the loss function is derived as L f", "entities": [[21, 22, "MetricName", "loss"]]}
{"text": "= \u2212 j N + log ( a j ) + j N \u2212 log ( 1 \u2212 a j ) , ( 18 ) where a j = \u03c3 ( d B ( v j , 0 ) ) is activations of the j - th label - aware capsules , which is proportional to the distance from the origin of the Poincar\u00e9 ball .", "entities": [[37, 38, "DatasetName", "0"]]}
{"text": "Datasets Experiments are carried out on four publicly available MLC datasets , including the small - scale AAPD ( Yang et al , 2018b ) and RCV1 ( Lewis et al , 2004 ) , the large - scale ZHIHU 1 and EUR - LEX57 K ( Chalkidis et al , 2019 ) .", "entities": [[26, 27, "DatasetName", "RCV1"]]}
{"text": "( j + 1 ) min ( k , y 0 ) j=1 1 / log ( j + 1 ) , ( 20 ) where y j { 0 , 1 } denotes the the ground truth about label j , rank k ( a ) denotes the indices of the candidate label - aware hyperbolic capsules with k largest activations in descending order , and y 0", "entities": [[10, 11, "DatasetName", "0"], [29, 30, "DatasetName", "0"], [68, 69, "DatasetName", "0"]]}
{"text": "Baselines To demonstrate the effectiveness of HYPERCAPS on the benchmark datasets , six comparative text classification methods are chosen as the baselines .", "entities": [[8, 10, "DatasetName", "the benchmark"], [14, 16, "TaskName", "text classification"]]}
{"text": "FASTTEXT ( Joulin et al , 2017 ) is a representative encoding - based method which use average pooling to construct document representations and MLP to make the predictions .", "entities": [[0, 1, "MethodName", "FASTTEXT"], [17, 19, "MethodName", "average pooling"], [24, 25, "DatasetName", "MLP"]]}
{"text": "SLEEC ( Bhatia et al , 2015 ) is a typical label - embedding method for MLC , which uses k - nearest neighbors search to predict the labels .", "entities": [[20, 24, "MethodName", "k - nearest neighbors"]]}
{"text": "SGM ( Yang et al , 2018b ) applies the seq2seq model with attention mechanism , which takes the global contextual information .", "entities": [[10, 11, "MethodName", "seq2seq"]]}
{"text": "al , 2019 ) uses a combination of CNN and LSTM with a dynamic gate that controls the information from these two parts .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "NLP - CAP ( Zhao et al , 2019 ) is a capsule - based approach for MLC , which reformulates the routing algorithm .", "entities": [[2, 3, "DatasetName", "CAP"]]}
{"text": "NLP - CAP use only CNN to construct capsules , and it applies the squashing function onto capsules .", "entities": [[2, 3, "DatasetName", "CAP"]]}
{"text": "Maximum length of AAPD , RCV1 and EUR - LEX57 K is set to 500 , while maximum length of ZHIHU is 50 .", "entities": [[5, 6, "DatasetName", "RCV1"]]}
{"text": "To compose the word vector representations , pre - trained 300 - dimensional GLOVE ( Pennington et al , 2014 ) word embeddings are used for AAPD , RCV1 and EUR - LEX57 K , while ZHIHU uses its specified 256 - dimensional word embeddings .", "entities": [[21, 23, "TaskName", "word embeddings"], [28, 29, "DatasetName", "RCV1"], [43, 45, "TaskName", "word embeddings"]]}
{"text": "Adaptive routing layer is not applied on the small - scale datasets AAPD and RCV1 .", "entities": [[14, 15, "DatasetName", "RCV1"]]}
{"text": "nDCG@1 is omitted since it gives the same value as P@1 .", "entities": [[10, 11, "MetricName", "P@1"]]}
{"text": "The encoding - based FASTTEXT is generally inferior to the other baselines as it applies the average pooling on word vector representations , which ig - nores word order for the construction of document representations .", "entities": [[4, 5, "MethodName", "FASTTEXT"], [16, 18, "MethodName", "average pooling"]]}
{"text": "XML - CNN uses a dynamic pooling technique to aggregate the local contextual features extracted by CNN , while SGM uses attention mechanism to aggregate the global contextual features extracted by LSTM .", "entities": [[31, 32, "MethodName", "LSTM"]]}
{"text": "REGGNN is generally superior to both of them as it combines the local and global contextual information dynamically and takes label correlations into consideration using a regularized loss .", "entities": [[27, 28, "MetricName", "loss"]]}
{"text": "However , the two capsulebased methods NLP - CAP and HYPERCAPS consistently outperform all the other methods owing to dynamic routing , which aggregates the fine - grained capsule features in a label - aware manner .", "entities": [[8, 9, "DatasetName", "CAP"]]}
{"text": "Moreover , NLP - CAP only uses CNN to extract the local contextual information , while HYPER - CAPS benefits from the parallel combination of local and global contextual information .", "entities": [[4, 5, "DatasetName", "CAP"]]}
{"text": "In addi - tion , NLP - CAP applies the non - linear squashing function for capsules in the Euclidean space , while HDR is designed for hyperbolic capsules , which take advantage of the representation capacity of the hyperbolic space .", "entities": [[7, 8, "DatasetName", "CAP"]]}
{"text": "Therefore , HYPERCAPS outperforms NLP - CAP as expected .", "entities": [[6, 7, "DatasetName", "CAP"]]}
{"text": "Figure 4 shows the results of the five deep learning based MLC methods , i.e. XML - CNN , SGM , REGGNN , NLP - CAP and HYPERCAPS .", "entities": [[25, 26, "DatasetName", "CAP"]]}
{"text": "nDCG@1 is smaller than nDCG@3 on AAPD , RCV1 and ZHIHU since most of their test instances contain less than three tail labels .", "entities": [[8, 9, "DatasetName", "RCV1"]]}
{"text": "The two capsule - based methods NLP - CAP and HYPERCAPS are both superior to the other methods , which indicates that the label - aware dynamic routing is effective for the prediction on tail labels .", "entities": [[8, 9, "DatasetName", "CAP"]]}
{"text": "In addition , the fact that HYPERCAPS significantly improves the prediction performance compared to NLP - CAP implies that the representation capacity of the hyperbolic space and the combination of local and global contextual information are helpful for learning on tail labels .", "entities": [[16, 17, "DatasetName", "CAP"]]}
{"text": "Multi - label classification ( MLC ) aims at assigning multiple relevant labels to one document .", "entities": [[0, 4, "TaskName", "Multi - label classification"]]}
{"text": "The MLC label set is large compared to Multi - class classification ( MCC ) .", "entities": [[8, 12, "TaskName", "Multi - class classification"]]}
{"text": "The seq2seq model SGM ( Yang et al , 2018b ) uses the attention mechanism to consider the label correlations , while REGGNN ( Xu et", "entities": [[1, 2, "MethodName", "seq2seq"]]}
{"text": "and ( Du et al , 2019a ) investigate capsule networks for text classification .", "entities": [[12, 14, "TaskName", "text classification"]]}
{"text": "Recent research on representation learning ( Nickel and Kiela , 2017 ) indicates that hyperbolic space is superior to Euclidean space in terms of representation capacity , especially in low dimension .", "entities": [[3, 5, "TaskName", "representation learning"]]}
{"text": "Some works lately demonstrate the superiority of the hyperbolic space for serval natural language processing tasks , such as textual entailment ( Ganea et al , 2018a ) , machine translation ( Gulcehre et al , 2019 ) and word embedding ( Tifrea et al , 2019 ) .", "entities": [[29, 31, "TaskName", "machine translation"]]}
{"text": "We present the Hyperbolic Capsule Networks ( HYPERCAPS ) with Hyperbolic Dynamic Routing ( HDR ) and adaptive routing for Multi - Label Classification ( MLC ) .", "entities": [[20, 24, "TaskName", "Multi - Label Classification"]]}
{"text": "In its fifth iteration , # SMM4H 2020 continues to serve as a venue for bringing together researchers interested in addressing the significant opportunities and challenges of utilizing the vast amount of data on social media for health informatics .", "entities": [[6, 7, "DatasetName", "SMM4H"]]}
{"text": "For # SMM4H 2020 , we accepted 5 workshop papers ( acceptance rate of 56 % ) and 26 shared task system description papers .", "entities": [[2, 3, "DatasetName", "SMM4H"]]}
{"text": "The accepted workshop papers span a range of social media data - Twitter , Facebook , Reddit , and online health forums - and health domains , including diabetes , depression , COVID - 19 , medical misinformation , and adverse drug reactions .", "entities": [[16, 17, "DatasetName", "Reddit"]]}
{"text": "Mo\u00dfburger et al use various text mining techniques to compare features of depression forums on Reddit and a curated , moderated site .", "entities": [[15, 16, "DatasetName", "Reddit"]]}
{"text": "The # SMM4H 2020 shared tasks sought to advance the use of Twitter data ( tweets ) for pharmacovigilance , toxicovigilance , and epidemiology of birth defects .", "entities": [[2, 3, "DatasetName", "SMM4H"]]}
{"text": "In addition to re - reruns of three tasks , # SMM4H 2020 included new tasks for detecting adverse drug reactions in French and Russian tweets , characterizing chatter related to prescription medication abuse , and detecting self reports of birth defect pregnancy outcomes .", "entities": [[11, 12, "DatasetName", "SMM4H"]]}
{"text": "The five tasks required methods for binary classification , multiclass classification , and named entity recognition ( NER ) .", "entities": [[13, 16, "TaskName", "named entity recognition"], [17, 18, "TaskName", "NER"]]}
{"text": "With 29 teams and a total of 130 system submissions , participation in the # SMM4H shared tasks continues to grow .", "entities": [[15, 16, "DatasetName", "SMM4H"]]}
{"text": "The organizing committee of # SMM4H 2020 would like to thank the program committee , the additional reviewers of system description papers , the organizers of COLING 2020 ( especially the workshop cochairs ) , the annotators of the shared task data , and , of course , everyone who submitted a paper or participated in the shared tasks .", "entities": [[5, 6, "DatasetName", "SMM4H"]]}
{"text": "# SMM4H 2020 would not have been possible without all of them .", "entities": [[1, 2, "DatasetName", "SMM4H"]]}
{"text": "Specifically , the \" student \" is an extended dialog manager based on a new ontology , and the \" teacher \" is existing resources used for guiding the learning process of the \" student \" .", "entities": [[15, 16, "MethodName", "ontology"]]}
{"text": "With the flourish development of virtual personal assistants ( e.g. , Amazon Alexa and Google Assistant ) , task - oriented dialog systems , which can help users accomplish tasks naturally , have been a focal point in both academic and industry research .", "entities": [[14, 15, "DatasetName", "Google"]]}
{"text": "Based on such ontology , developers can extract dialog features and train the dialog manager model in an interaction environment .", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "However , adding new intents or slots will change the predefined ontology .", "entities": [[11, 12, "MethodName", "ontology"]]}
{"text": "As a consequence , developers need to extract additional dialog features based on new ontology .", "entities": [[14, 15, "MethodName", "ontology"]]}
{"text": "Recently , deep reinforcement learning ( Mnih et al , 2013 ( Mnih et al , , 2015 has been applied to optimize the dialog manager in an \" endto - end \" way , including deep Q - Network ( Lipton et al , 2017 ; Li et al , 2017b ; Peng et al , 2017 ; Zhao and Eskenazi , 2016 ) and policy gradient methods ( Williams et al , 2017 ; Su et al , 2016b ; Dhingra et al , 2017 ) .", "entities": [[36, 40, "MethodName", "deep Q - Network"], [66, 69, "TaskName", "policy gradient methods"]]}
{"text": "However , defining an appropriate kernel function is nontrivial when the ontology has changed drastically .", "entities": [[11, 12, "MethodName", "ontology"]]}
{"text": "Lipton et al ( 2017 ) proposed to use BBQ - Networks to extend the domain .", "entities": [[9, 10, "DatasetName", "BBQ"]]}
{"text": "However , similar to Shah et al ( 2016 ) , the BBQ - Networks have reserved a few bits in the feature vector for new intents and slots .", "entities": [[12, 13, "DatasetName", "BBQ"]]}
{"text": "Knowledge Distillation Our proposed framework is inspired by recent work in knowledge distillation ( Bucilu et al , 2006 ; Ba and Caruana , 2014 ; Li et al , 2014 ) .", "entities": [[0, 2, "MethodName", "Knowledge Distillation"], [11, 13, "MethodName", "knowledge distillation"]]}
{"text": "Knowledge distillation means training a compact model to mimic a larger teacher model by approximating the function learned by the teacher .", "entities": [[0, 2, "MethodName", "Knowledge distillation"]]}
{"text": "Hinton et al ( 2015 ) introduced knowledge distillation to transfer knowledge from", "entities": [[7, 9, "MethodName", "knowledge distillation"]]}
{"text": "The output \u03c0 ( a | h t ; \u03b8 ) of the policy network is a probability distribution over a predefined system action set A s .", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "Lastly , the system samples an action a s t A s based on \u03c0 ( a | h t ; \u03b8 ) and receives a new observation x t+1 with an assigned reward r t .", "entities": [[21, 22, "HyperparameterName", "\u03b8"]]}
{"text": "The policy parameters \u03b8 can be learned by maximizing the expected discounted cumulative rewards : J ( \u03b8 )", "entities": [[3, 4, "HyperparameterName", "\u03b8"], [17, 18, "HyperparameterName", "\u03b8"]]}
{"text": "= E T \u2212t k=0 \u03b3", "entities": [[5, 6, "HyperparameterName", "\u03b3"]]}
{"text": "The policy gradient can be empirically estimated as : \u2207 \u03b8 J ( \u03b8 )", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "N N i=1 T t=1 \u2207 \u03b8 log \u03c0 ( a s i , t |", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b3 k r i , t+k is the sum of discounted reward at step t in the episode i , and b is a baseline to estimate the average reward of current policy .", "entities": [[0, 1, "HyperparameterName", "\u03b3"]]}
{"text": "The system will return an action a s t A s according to the dialog manager \u03c0 ( \u03b8 ) .", "entities": [[18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "The extended dialog manager and new system action set are denoted as \u03c0 ( \u03b8 ) and A s respectively .", "entities": [[14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "Although the ontology of the new system is different from the original one , the extended dialog manager can still reuse dialog policy of the illconsidered system circuitously .", "entities": [[2, 3, "MethodName", "ontology"]]}
{"text": "Given user logs D and the original dialog manager \u03c0 ( \u03b8 ) , we define a loss L ( \u03b8 ; D , \u03b8 ) to minimize the difference between new dialog manager \u03c0 ( \u03b8 ) and the old one : L ( \u03b8 ; D , \u03b8 )", "entities": [[11, 12, "HyperparameterName", "\u03b8"], [17, 18, "MetricName", "loss"], [20, 21, "HyperparameterName", "\u03b8"], [24, 25, "HyperparameterName", "\u03b8"], [36, 37, "HyperparameterName", "\u03b8"], [45, 46, "HyperparameterName", "\u03b8"], [49, 50, "HyperparameterName", "\u03b8"]]}
{"text": "= d D | d | t=1 KL ( \u03c0 ( a | ht ; \u03b8 )", "entities": [[15, 16, "HyperparameterName", "\u03b8"]]}
{"text": "| \u03c0 ( a | ht ; \u03b8 ) )", "entities": [[7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "To distill the knowledge of rules to a new system , we define a loss function L ( \u03b8 ; D , R ) to embed such constraints in the new system : L ( \u03b8 ; D , R )", "entities": [[14, 15, "MetricName", "loss"], [18, 19, "HyperparameterName", "\u03b8"], [35, 36, "HyperparameterName", "\u03b8"]]}
{"text": "So , the learning objective of new dialog manager \u03c0 ( \u03b8 ) can be defined as follows : L ( \u03b8 ; D , \u03b8 , R )", "entities": [[11, 12, "HyperparameterName", "\u03b8"], [21, 22, "HyperparameterName", "\u03b8"], [25, 26, "HyperparameterName", "\u03b8"]]}
{"text": "Sim2 LU Error Rate Succ .", "entities": [[2, 3, "MetricName", "Error"]]}
{"text": "The LU module is implemented with an SVM 5 for intent detection and a CRF 6 for slot filling .", "entities": [[7, 8, "MethodName", "SVM"], [10, 12, "TaskName", "intent detection"], [14, 15, "MethodName", "CRF"], [17, 19, "TaskName", "slot filling"]]}
{"text": "The hidden dialog state representation is inferred by a GRU ( Chung et al , 2014 ) .", "entities": [[9, 10, "MethodName", "GRU"]]}
{"text": "We set the hidden states of the GRU to be 120 .", "entities": [[7, 8, "MethodName", "GRU"]]}
{"text": "The policy network is implemented as a Multilayer Perceptron ( MLP ) with one hidden layer .", "entities": [[10, 11, "DatasetName", "MLP"]]}
{"text": "Adadelta ( Zeiler , 2012 ) method is used to update model parameters .", "entities": [[0, 1, "MethodName", "Adadelta"]]}
{"text": "Due to the change in ontology , we add a new status in dialog features to represent the \" confirm \" intent of users .", "entities": [[5, 6, "MethodName", "ontology"]]}
{"text": "To do so , we reimplement the sentence planning and realization components of a classic NLG system , Methodius , using LSTM sequence - to - sequence ( seq2seq ) models .", "entities": [[21, 22, "MethodName", "LSTM"], [28, 29, "MethodName", "seq2seq"]]}
{"text": "We find that although seq2seq models can learn to generate fluent and grammatical texts remarkably well with sufficiently representative Methodius training data , they can not learn to correctly express Methodius 's SIMILARITY and CONTRAST comparisons unless the corresponding RST relations are included in the inputs .", "entities": [[4, 5, "MethodName", "seq2seq"]]}
{"text": "Traditional approaches to the task of natural language generation ( NLG ) have employed a pipeline of modules , moving from an initial abstract meaning representation ( MR ) to human - readable natural language ( Reiter and Dale , 2000 ) .", "entities": [[27, 28, "DatasetName", "MR"]]}
{"text": "In the last decade , the success of neural methods in other domains of natural language processing ( NLP ) has led to the development of neural ' end - to - end ' ( e2e ) *", "entities": [[35, 36, "DatasetName", "e2e"]]}
{"text": "By contrast , more recent neural approaches - in particular , those developed for the E2E and WebNLG shared task challenges - have mostly mapped simple , flat inputs to texts without representing discourse relations explicitly .", "entities": [[15, 16, "DatasetName", "E2E"], [17, 18, "DatasetName", "WebNLG"]]}
{"text": "However , they do note that only 6 % of the crowd - sourced E2E Challenge texts contain discourse connectives ex - pressing CONTRAST , and though they introduce a conversational weather dataset that uses both CONTRAST and JUSTIFY relations with greater frequency , it is fair to say that the use of hierarchical MRs that incorporate discourse relations remains far from common practice .", "entities": [[14, 15, "DatasetName", "E2E"]]}
{"text": "To do so , we reimplement the sentence planning and realization components of a classic NLG system , Methodius ( Isard , 2016 ) , using LSTM sequenceto - sequence ( seq2seq ) models , since Methodius makes similarity or contrast comparisons in most of its outputs .", "entities": [[26, 27, "MethodName", "LSTM"], [31, 32, "MethodName", "seq2seq"]]}
{"text": "Specifically , rather than crowd - source output texts for Methodius 's content plans , we run the existing system to obtain target texts for training seq2seq models , and experiment with input MRs ( derived from the content plans ) that contain discourse relations as well as ones that leave them out .", "entities": [[26, 27, "MethodName", "seq2seq"]]}
{"text": "In our experiments , we observe that the seq2seq models learn to generate fluent and grammatical texts remarkably well .", "entities": [[8, 9, "MethodName", "seq2seq"]]}
{"text": "Using these automatic checks , we find that even with sufficiently representative Methodius training data , LSTM seq2seq models can not learn to correctly express Methodius 's similarity and contrast comparisons unless the corresponding RST relations are included in the inputs .", "entities": [[16, 17, "MethodName", "LSTM"], [17, 18, "MethodName", "seq2seq"]]}
{"text": "The Methodius system ( Isard , 2016 ) was developed for multilingual text generation , based on the M - PIRO project ( Isard et al , 2003 ; Isard , 2007 ) which focused on museum exhibit descriptions .", "entities": [[12, 14, "TaskName", "text generation"]]}
{"text": "Here , best means the one that has lowest perplexity with respect to forced decoding .", "entities": [[9, 10, "MetricName", "perplexity"]]}
{"text": "We trained the following models : LBL : A standard LSTM seq2seq model with attention on the labeled data , which is also the base model for the other methods .", "entities": [[10, 11, "MethodName", "LSTM"], [11, 12, "MethodName", "seq2seq"]]}
{"text": "3 show the results , chiefly the uniform improvement of the self - training and reranking models over the baseline LSTM models .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "After fine tuning and reranking , the errors reduced to 0 and 2 respectively .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "On the challenge test , no model achieved perfect accuracy .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "While traditional natural language generation systems , e.g. Methodius , often employ knowledge graphs , the use of such structure in neural NLG is underdeveloped .", "entities": [[12, 14, "TaskName", "knowledge graphs"]]}
{"text": "An exception in this respect is WebNLG ( Gardent et al , 2017 ) , which is a multilingual corpus for natural language generation .", "entities": [[6, 7, "DatasetName", "WebNLG"]]}
{"text": "SunBear at WNUT - 2020 Task 2 : Improving RoBERTa - Based Noisy Text Classification with Knowledge of the Data domain", "entities": [[9, 10, "MethodName", "RoBERTa"], [13, 15, "TaskName", "Text Classification"]]}
{"text": "We improve experiment with the effectiveness of fine - tuning methodologies for state - of - the - art language model RoBERTa ( Liu et al , 2019 ) .", "entities": [[21, 22, "MethodName", "RoBERTa"]]}
{"text": "We make a preliminary instantiation of this formal model for the text classification approaches .", "entities": [[11, 13, "TaskName", "text classification"]]}
{"text": "Based on the dataset provided in WNUT - 2020 Task 2 : Identification of informative COVID - 19 English Tweets ( Nguyen et al , 2020 ) , we propose a fine - tuning strategy to adopt the universal language model RoBERTa as an backbone model for text classification purposes .", "entities": [[41, 42, "MethodName", "RoBERTa"], [47, 49, "TaskName", "text classification"]]}
{"text": "We also conduct several experiments in varied fine - tuning architectures on the pre - trained RoBERTa .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}
{"text": "One of the most important parts in text classification problems is input representation .", "entities": [[7, 9, "TaskName", "text classification"]]}
{"text": "While Word2Vec is a selfsupervised algorithm , GloVe ( Pennington et al , 2014 ) is trained unsupervised to form word embeddings .", "entities": [[7, 8, "MethodName", "GloVe"], [20, 22, "TaskName", "word embeddings"]]}
{"text": "GloVe factorizes co - occurrence matrix of words , resulting in dense word vectors .", "entities": [[0, 1, "MethodName", "GloVe"]]}
{"text": "However , both GloVe and Word2Vec fail representing rare or out - of - vocabulary words .", "entities": [[3, 4, "MethodName", "GloVe"]]}
{"text": "FastText ( Mikolov et al , 2018 ) mitigates this problem by decomposing words as a sum of character n - grams .", "entities": [[0, 1, "MethodName", "FastText"]]}
{"text": "Bidirectional Encoder Representations from Transformers ( Devlin et al , 2018 ) , or BERT for short , outperforms the previous best result with GLUE score of 80.4 % , which is 7.6 % improvement .", "entities": [[14, 15, "MethodName", "BERT"], [24, 25, "DatasetName", "GLUE"]]}
{"text": "There are two variants of BERT : base and large ; the large model is a stack of 24 Transformers ' encoders for a total of 340 M parameters while the base one has only 12 encoders .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "GPT - 2 ( Radford et al , 2019 ) by OpenAI is a gigantic model with 1.5 billion parameters and 48 layers , setting new state - of - the - art results on 7 out of 8 datasets .", "entities": [[0, 1, "MethodName", "GPT"]]}
{"text": "Face - book Research team improves training procedures for BERT , introducing RoBERTa ( Liu et al , 2019 ) .", "entities": [[9, 10, "MethodName", "BERT"], [12, 13, "MethodName", "RoBERTa"]]}
{"text": "The improvements include extended training time on a ten - times bigger dataset , increased batch size , using byte - level encoding with larger vocabulary , excluding next sentence predicting task , and dynamic masking pattern modifying .", "entities": [[15, 17, "HyperparameterName", "batch size"]]}
{"text": "For MLM tuning we propose hierarchical tuning process that consists of two steps : Domain adaptation using extra COVID data and Task adaptation using the given training data .", "entities": [[1, 2, "DatasetName", "MLM"], [14, 16, "TaskName", "Domain adaptation"]]}
{"text": "After MLM Tuning , we utilize different training techniques for text classification such as back translation , warm - up learning rate , layer freezing and layer - wise learning rates .", "entities": [[1, 2, "DatasetName", "MLM"], [10, 12, "TaskName", "text classification"], [20, 22, "HyperparameterName", "learning rate"]]}
{"text": "Taking advantage of RoBERTa as a backbone , we propose a customized network with appreciably modifications .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}
{"text": "The \" base \" version of RoBERTa is used .", "entities": [[6, 7, "MethodName", "RoBERTa"]]}
{"text": "It has 12 Transformer blocks , each block outputs a 768 - D vector for each token .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "Since the output of different Transformer blocks represent different semantic levels for the inputs , in our experiments we combine outputs of those Transformer blocks by concatenation .", "entities": [[5, 6, "MethodName", "Transformer"], [23, 24, "MethodName", "Transformer"]]}
{"text": "We propose two types of the head : MLP Head : A simple feed forward network with one hidden layer .", "entities": [[8, 9, "DatasetName", "MLP"]]}
{"text": "BiLSTM Head : A recurrent neural network with one Bidirectional LSTM layer .", "entities": [[0, 1, "MethodName", "BiLSTM"], [9, 11, "MethodName", "Bidirectional LSTM"]]}
{"text": "The hyperparameters are shown in Section 4 . 3.2 Fine - tuning Masked Language Model ( MLM )", "entities": [[16, 17, "DatasetName", "MLM"]]}
{"text": "RoBERTa apparently is an excellent language model since it was trained on a huge dataset in a broad domain .", "entities": [[0, 1, "MethodName", "RoBERTa"]]}
{"text": "Therefore , in order to produce high - quality outputs from the model , there is a need of fine - tuning MLM task on the task dataset for RoBERTa .", "entities": [[22, 23, "DatasetName", "MLM"], [29, 30, "MethodName", "RoBERTa"]]}
{"text": "Choosing learning rate is the key factor for the convergence .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "If learning rate is too small , the model may converge too slow causing harder to fit to new data distribution .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "On the other hand , large learning rate can lead to the problem of useful feature forgetting .", "entities": [[6, 8, "HyperparameterName", "learning rate"]]}
{"text": "Hence , we propose a hierarchical fine - tuning strategy for RoBERTa : the first phase we train with custom domain COVID Tweets dataset for domain adaptation , then the second phase is a fine - tuning process with WNUT Task 2 dataset for task adaptation .", "entities": [[11, 12, "MethodName", "RoBERTa"], [25, 27, "TaskName", "domain adaptation"]]}
{"text": "The input is tokenized into a sequence of BPE tokens .", "entities": [[8, 9, "MethodName", "BPE"]]}
{"text": "RoBERTa , the \" base \" version , takes this sequence and propagates it through 12 Transformer layers .", "entities": [[0, 1, "MethodName", "RoBERTa"], [16, 17, "MethodName", "Transformer"]]}
{"text": "By concatenating outputs from these 12 layers , we form a long sentence representation for the follow - up classification head , which is a simple Multi - layer Perceptron / Long Short - Term Memory network .", "entities": [[31, 36, "MethodName", "Long Short - Term Memory"]]}
{"text": "Recently research ( Xie et al , 2019 ; Edunov et al , 2018 ) have shown that back - translating monolingual data can be used as a potential form of data augmentation in Text Classification .", "entities": [[31, 33, "TaskName", "data augmentation"], [34, 36, "TaskName", "Text Classification"]]}
{"text": "Since RoBERTa has been trained on a huge dataset , we would not want the model to derive too far from its pre - train weights .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}
{"text": "The training procedure is divided into 2 steps : Step 1 : We freeze RoBERTa to train the classification head for the first epoch .", "entities": [[14, 15, "MethodName", "RoBERTa"]]}
{"text": "Warm - up learning rate ( Section 3.2.1 ) is also applied .", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}
{"text": "Because RoBERTa 's weights are already well trained , this step helps escape from narrow local optimum .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}
{"text": "Step 2 : RoBERTa is unfrozen , a whole network is trained .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}
{"text": "In RoBERTa , upper layers produce embeddings with more contextspecific than lower layers .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}
{"text": "This motivates us to further apply layer - wise learning rate : set a small learning rate for the shallowest layer , increase the learning rate as the layer goes deeper .", "entities": [[9, 11, "HyperparameterName", "learning rate"], [15, 17, "HyperparameterName", "learning rate"], [24, 26, "HyperparameterName", "learning rate"]]}
{"text": "This phenomenon occurs when the model gives predictions with confidence higher than its accuracy .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "In this study , we employ label smoothing ( Szegedy et al , 2015 ) to prevent model from being too certain about its predictions .", "entities": [[6, 8, "MethodName", "label smoothing"]]}
{"text": "Instead of assigning \" hard \" one - hot encoded ground truth , label smoothing adds a small perturbation into the label by a smoothing parameter \u03b1 .", "entities": [[13, 15, "MethodName", "label smoothing"], [26, 27, "HyperparameterName", "\u03b1"]]}
{"text": "Moreover , label smoothing also helps stabilize the training process .", "entities": [[2, 4, "MethodName", "label smoothing"]]}
{"text": "When using cross - entropy loss , one - hot encoded labels cause numerical instabilities if the prediction is close to one - hot form .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "The numbers of hidden units of MLP Head and BiLSTM Head to 768 and 256 respectively .", "entities": [[6, 7, "DatasetName", "MLP"], [9, 10, "MethodName", "BiLSTM"]]}
{"text": "Evaluation metrics for assessing are Accuracy , F1score , Recall and Precision metrics on public validation set .", "entities": [[5, 6, "MetricName", "Accuracy"], [9, 10, "MetricName", "Recall"], [11, 12, "MetricName", "Precision"]]}
{"text": "Accuracy can be used when the class distribution is similar while F1 - score is a better choice of metric when there are imbalanced classes .", "entities": [[0, 1, "MetricName", "Accuracy"], [11, 14, "MetricName", "F1 - score"]]}
{"text": "Table 1 compares the performance of multiple trial architectures training with pre - trained method using RoBERTa in our base settings .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}
{"text": "The original RoBERTa with MLP Head shows the better result than LSTM head , but the difference is not really noticeable ( 0.9082 vs. 0.9061 ) .", "entities": [[2, 3, "MethodName", "RoBERTa"], [4, 5, "DatasetName", "MLP"], [11, 12, "MethodName", "LSTM"]]}
{"text": "When applying direct tuning MLM and label smoothing , the gap has been widened , specifically , 0.9218 for MLP Head and 0.9170 for LSTM Head .", "entities": [[4, 5, "DatasetName", "MLM"], [6, 8, "MethodName", "label smoothing"], [19, 20, "DatasetName", "MLP"], [24, 25, "MethodName", "LSTM"]]}
{"text": "By conducting a lot of experiments , we have demonstrated that the use of RoBERTa and our fine - tuning strategy is highly effective in text classification tasks .", "entities": [[14, 15, "MethodName", "RoBERTa"], [25, 27, "TaskName", "text classification"]]}
{"text": "Relations between comprehensibility and adequacy errors in machine translation output", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "Further analysis of misleading translations revealed that the most frequent error types are ambiguity , mistranslation , noun phrase error , word - by - word translation , untranslated word , subject - verb agreement , and spelling error in the source text .", "entities": [[25, 27, "TaskName", "word translation"]]}
{"text": "While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation ( MT ) systems , they are only a substitution for human assessment of translation quality .", "entities": [[14, 16, "TaskName", "machine translation"]]}
{"text": "Two types of publicly available user reviews written in English have been analysed : IMDb movie reviews 1 ( Maas et al , 2011 ) and Amazon product reviews 2 ( McAuley et al , 2015 ) .", "entities": [[14, 17, "DatasetName", "IMDb movie reviews"]]}
{"text": "Google Translate 4 , Bing 5 and Amazon translate 6 .", "entities": [[0, 1, "DatasetName", "Google"]]}
{"text": "word - by - word translation A sequence of source words is translated as single words - the translation choice of each word looks random , both lexically and morphologically , without taking into account any context .", "entities": [[4, 6, "TaskName", "word translation"]]}
{"text": "This research is being conducted with the financial support of the European Association for Machine Translation ( EAMT ) under its programme \" 2019 Sponsorship of Activities \" at the ADAPT Research Centre at Dublin City University .", "entities": [[14, 16, "TaskName", "Machine Translation"]]}
{"text": "Bilingual Character Representation for Efficiently Addressing Out - of - Vocabulary Words in Code - Switching Named Entity Recognition", "entities": [[16, 19, "TaskName", "Named Entity Recognition"]]}
{"text": "We propose an LSTM - based model with hierarchical architecture on named entity recognition from code - switching Twitter data .", "entities": [[3, 4, "MethodName", "LSTM"], [11, 14, "TaskName", "named entity recognition"]]}
{"text": "Our model uses bilingual character representation and transfer learning to address out - of - vocabulary words .", "entities": [[7, 9, "TaskName", "transfer learning"]]}
{"text": "Named Entity Recognition ( NER ) predicts which word tokens refer to location , people , organization , time , and other entities from a word sequence .", "entities": [[0, 3, "TaskName", "Named Entity Recognition"], [4, 5, "TaskName", "NER"]]}
{"text": "Deep neural network models have successfully achieved the state - of - the - art performance in NER tasks ( Cohen ; Chiu and Nichols , 2016 ; Lample et al , 2016 ;", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "For example \" todos los Domingos en Westland Mall \" where \" Westland Mall \" is an English named entity .", "entities": [[8, 9, "DatasetName", "Mall"], [13, 14, "DatasetName", "Mall"]]}
{"text": "Our contributions are two - fold : ( 1 ) bilingual character bidirectional RNN is used to capture character - level information and tackle OOV words issue ( 2 ) we apply transfer learning from monolingual pre - trained word vectors to adapt the model with different domains in a bilingual setting .", "entities": [[32, 34, "TaskName", "transfer learning"]]}
{"text": "In our model , we use LSTM to capture long - range dependencies of the word sequence and character sequence in bilingual character RNN .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "Convolutional Neural Network ( CNN ) was used in NER task as word decoder by Collobert et al ( 2011 ) and a few years later , Huang et al ( 2015 ) introduced Bidirectional Long - Short Term Memory ( BiLSTM ) ( Sundermeyer et al , 2012 ) .", "entities": [[9, 10, "TaskName", "NER"], [41, 42, "MethodName", "BiLSTM"]]}
{"text": "Lample et al ( 2016 ) also showed Conditional Random Field ( CRF )", "entities": [[8, 11, "MethodName", "Conditional Random Field"], [12, 13, "MethodName", "CRF"]]}
{"text": "( Lafferty et al , 2001 ) decoders to improve the results and used Stack memory - based LSTMs for their work in sequence chunking .", "entities": [[24, 25, "TaskName", "chunking"]]}
{"text": "Aguilar et al ( 2017 ) proposed multi - task learning by combining Part - of - Speech tagging task with NER and using gazetteers to provide language - specific knowledge .", "entities": [[7, 11, "TaskName", "multi - task learning"], [13, 19, "TaskName", "Part - of - Speech tagging"], [21, 22, "TaskName", "NER"]]}
{"text": "Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER ( Lample et al , 2016 ) , POS tagging , and language modeling .", "entities": [[15, 16, "TaskName", "NER"]]}
{"text": "Tweets data from Twitter provided by 62.62 % 16.76 % 19.12 % 3.91 % 54.59 % + FastText ( spa ) 49.76 % 12.38 % 11.98 % 3.91 % 39.45 % + token replacement 12.43 % 12.35 % 7.18 % 3.91 % 9.60 % + token normalization 7.94 % 8.38 % 5.01 % 1.67 % 6.08 % Aguilar et al ( 2018 ) .", "entities": [[17, 18, "MethodName", "FastText"]]}
{"text": "We use 300 - dimensional English and Spanish FastText pre - trained word vectors which comprise two million words vocabulary each and they are trained using Common Crawl and Wikipedia .", "entities": [[8, 9, "MethodName", "FastText"], [26, 28, "DatasetName", "Common Crawl"]]}
{"text": "2 . Token normalization : Concatenate Spanish and English FastText word vector vocabulary .", "entities": [[9, 10, "MethodName", "FastText"]]}
{"text": "Normalize OOV words by using one out of these heuristics and check if the word exists in the vocabulary sequentially Then , the effectiveness of the preprocessing and transfer learning in handling OOV words are analyzed .", "entities": [[28, 30, "TaskName", "transfer learning"]]}
{"text": "It is clear that using FastText word vectors reduce the OOV words rate especially when we concatenate the vocabulary of both languages .", "entities": [[5, 6, "MethodName", "FastText"]]}
{"text": "Then , we use it as the input for a Bidirectional LSTM as character encoder , wherein every time step , a character is input to the network .", "entities": [[10, 12, "MethodName", "Bidirectional LSTM"]]}
{"text": "Afterwards , we pass the vectors to bidirectional LSTM .", "entities": [[7, 9, "MethodName", "bidirectional LSTM"]]}
{"text": "LSTM ( u t , \u2212 \u2212 h t\u22121 ) , \u2212 h t = \u2212\u2212\u2212 \u2212 LSTM ( u t , \u2212 \u2212 h t\u22121 )", "entities": [[0, 1, "MethodName", "LSTM"], [17, 18, "MethodName", "LSTM"]]}
{"text": "Dropout is applied to the recurrent layer .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "A softmax function is used to calculate the probability distribution of all possible named - entity tags .", "entities": [[1, 2, "MethodName", "softmax"]]}
{"text": "Since there is a variable number of sequence length , we padded the sequence and applied mask when calculating cross - entropy loss function .", "entities": [[22, 23, "MetricName", "loss"]]}
{"text": "We trained our LSTM models with a hidden size of 200 .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "Dropout ( Srivastava et al , 2014 ) of 0.4 was applied to all LSTMs .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "The performance of the model improves significantly after transfer learning with FastText word vectors while it also reduces the number of OOV words in the development and test set .", "entities": [[8, 10, "TaskName", "transfer learning"], [11, 12, "MethodName", "FastText"]]}
{"text": "We try to use sub - words representation from Spanish FastText , however , it does not improve the result since the OOV words consist of many special characters , for example , \" /IAtrevido / Provocativo \" , \" Twets / wek \" , and possibly create noisy vectors and most of them are not entity words .", "entities": [[10, 11, "MethodName", "FastText"]]}
{"text": "This paper presents a bidirectional LSTM - based model with hierarchical architecture using bilingual character RNN to address the OOV words issue .", "entities": [[4, 6, "MethodName", "bidirectional LSTM"]]}
{"text": "Moreover , token replacement , token normalization , and transfer learning reduce OOV words rate even further and significantly improves the performance .", "entities": [[9, 11, "TaskName", "transfer learning"]]}
{"text": "This work is partially funded by ITS/319/16FP of the Innovation Technology Commission , HKUST 16214415 & 16248016 of Hong Kong Research Grants Council , and RDC 1718050 - 0 of EMOS.AI .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "The algorithm , which finds all of the original fragments in a ground - truth corpus and can reveal whether a generated fragment copies an original without attribution , has a run - time complexity of \u03b8 ( n log n ) where n is the number of sentences in the ground truth .", "entities": [[36, 37, "HyperparameterName", "\u03b8"]]}
{"text": "Multiple language generators may be trained on the same ground truth ( e.g. , Wikipedia ) with the same embedding vectors ( e.g. , BERT ( Devlin et al , 2018 ) and GPT ( Vaswani et", "entities": [[24, 25, "MethodName", "BERT"], [33, 34, "MethodName", "GPT"]]}
{"text": "al , 2017 ; Radford et al , 2018 ) ) and the same technologies ( deep neural networks , LSTM cells ( Hochreiter and Schmidhuber , 1997 ) , transformers ( Vaswani et al , 2017 ) ) .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "High PPL and a low BLEU score may be achieved when there is little overlap between the generated language and the ground truth , but nonesense and off - topic sentences are rewarded .", "entities": [[5, 7, "MetricName", "BLEU score"]]}
{"text": "We also want a generation originality test that flags potential plagiarism of original fragments in the ground truth , which neither BLEU nor PPL does .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "GOT is equally appropriate for stylized text generation , where novelty is desirable , and for other generation tasks where there is not an imposed style but the generation is open - ended , including summarization tasks .", "entities": [[6, 8, "TaskName", "text generation"], [35, 36, "TaskName", "summarization"]]}
{"text": "Original C = 1 Not Original C \u2265 2 Generated Fragment Original C = 0", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "Searching , insertion and deletion in such trees take \u03b8 ( log n ) comparisons .", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "Since the length of fragments is assumed to be constant on average , then each comparison takes constant time , implying that each search / insert / delete operation in O and F take \u03b8 ( log n ) time .", "entities": [[34, 35, "HyperparameterName", "\u03b8"]]}
{"text": "Therefore , the runtime complexity is : \u03b8 ( n log n ) .", "entities": [[7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "do wl = length of window 5 : for each i in range 0 to l \u2212 wl", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "To see how GOT performed on a generation task , we applied it to a metaphor generator that we built , based on an RNN ( Elman , 1990 ) architecture with LSTM cells ( Hochreiter and Schmidhuber , 1997 ) for training a language model on the language of metaphors , using only metaphors and their topics as input .", "entities": [[32, 33, "MethodName", "LSTM"]]}
{"text": "In this paper , we compare the performance of previous literature with Transformer models experimenting on a public and a private dataset .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "Moreover , if an additional corpus of related documents is available , Transformers can leverage that information to further improve calibration accuracy .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "Pretesting leads indeed to an accurate and consistent calibration , but i ) introduces a long delay between question generation and when the question can be used to score students , and ii ) requires to deploy the new questions before actually using them for scoring .", "entities": [[18, 20, "TaskName", "question generation"]]}
{"text": "Although such works showed promising results , none of them experimented with the latest NLP research , such as Transformer - based models ( Vaswani et al , 2017 ) .", "entities": [[19, 20, "MethodName", "Transformer"]]}
{"text": "We evaluate several models built upon the pre - trained BERT ( Devlin et al , 2019 ) and DistilBERT ( Sanh et al , 2019 ) language models on the publicly available ASSISTments dataset and the private CloudAcademy dataset .", "entities": [[10, 11, "MethodName", "BERT"], [19, 20, "MethodName", "DistilBERT"]]}
{"text": "More precisely , we show that - after being fine - tuned on the task of QDE from text - Transformer models are capable of calibrating newly generated questions more accurately than previously proposed approaches .", "entities": [[20, 21, "MethodName", "Transformer"]]}
{"text": "On top of that , we explore the possibility of leveraging additional textual information which might be available ( e.g. transcript of video lectures ) to perform an additional pre - training of the Transformers before fine - tuning them for QDE , and show that such approach can be used to further improve the accuracy .", "entities": [[55, 56, "MetricName", "accuracy"]]}
{"text": "There is a large interest in understanding how textual features affect item difficulty ( El Masri et al , 2017 ; Hickendorff , 2013 ) , and this is not limited to the educational domain : for instance Wang et al ( 2014 ) focus on difficulty estimation in community question answering systems .", "entities": [[49, 52, "TaskName", "community question answering"]]}
{"text": "To the best of our knowledge , ( Xue et al , 2020 ) is the first work that explored the effects of transfer learning for QDE from text .", "entities": [[23, 25, "TaskName", "transfer learning"]]}
{"text": "Specifically , the authors fine - tune pre - trained ELMo embeddings ( Peters et al , 2018 ) for the task of response time prediction , and subsequently perform a second finetuning for the task of p - value estimation .", "entities": [[10, 11, "MethodName", "ELMo"]]}
{"text": "First of all , we adopt transfer learning on Transformers , which are yet to be explored for QDE from text .", "entities": [[6, 8, "TaskName", "transfer learning"]]}
{"text": "Appendix A. Thirdly , the Transformer models presented here do not necessarily require an additional dataset of documents from the same topics as the questions , and they can be trained using only question texts ; however , if such dataset is available , it can be leveraged to further improve the accuracy of calibration .", "entities": [[5, 6, "MethodName", "Transformer"], [52, 53, "MetricName", "accuracy"]]}
{"text": "BERT ( Devlin et al , 2019 ) is a pre - trained language model that reached state of the art performance in many language tasks .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "Its key technical innovation was the application of the Transformer , a popular self - attention model ( Vaswani et al , 2017 ) , to language modeling .", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "BERT is originally trained to address two tasks : Masked Language Modeling ( MLM ) and Next Sentence Prediction ( NSP ) .", "entities": [[0, 1, "MethodName", "BERT"], [9, 12, "TaskName", "Masked Language Modeling"], [13, 14, "DatasetName", "MLM"]]}
{"text": "MLM consists in removing one word from the input text and asking the model to fill the gap , while in NSP the model is asked - given two input sentences - to tell whether the second sentence is a reasonable continuation to the first one .", "entities": [[0, 1, "DatasetName", "MLM"]]}
{"text": "Crucially , BERT can be used for many different downstream tasks , as we do here for QDE : starting from the pre - trained model , it is sufficient to stack an additional layer on top of the original network and then retrain it on the desired task ( process named \" finetuning \" ) .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "BERT is a large model and therefore requires many resources for training and fine - tuning .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "For this reason , we also experiment with DistilBERT ( Sanh et al , 2019 ) , which is a language model obtained by distilling BERT .", "entities": [[8, 9, "MethodName", "DistilBERT"], [25, 26, "MethodName", "BERT"]]}
{"text": "Knowledge distillation is a compression technique in which a small model is trained to reproduce the full output distribution of a larger model ( Hinton et al , 2015 ) .", "entities": [[0, 2, "MethodName", "Knowledge distillation"]]}
{"text": "Similarly to BERT , Distil - BERT can be fine - tuned on downstream tasks and it is therefore worth exploring for QDE from text .", "entities": [[2, 3, "MethodName", "BERT"], [6, 7, "MethodName", "BERT"]]}
{"text": "The second approach is made of two steps : we i ) further pre - train the pre - trained model on the task of Masked Language Modeling ( MLM ) to improve domain knowledge , and subsequently ii ) fine - tune it on the task of QDE from text .", "entities": [[25, 28, "TaskName", "Masked Language Modeling"], [29, 30, "DatasetName", "MLM"]]}
{"text": "We believe that the model , in that case , does not have enough training questions to learn the meaning of the additional separators ( BERT and DistilBERT are pre - trained to use only one [ SEP ] token ) .", "entities": [[25, 26, "MethodName", "BERT"], [27, 28, "MethodName", "DistilBERT"]]}
{"text": "Masked Language Modeling ( MLM ) is a fill - in - theblank task , where a word of the input text is substituted by a [ MASK ] token and the model is trained to use the surrounding words to predict the word that was masked .", "entities": [[0, 3, "TaskName", "Masked Language Modeling"], [4, 5, "DatasetName", "MLM"]]}
{"text": "We leverage MLM to perform an additional pre - training of the pre - trained language models before the fine - tuning on QDE from text .", "entities": [[2, 3, "DatasetName", "MLM"]]}
{"text": "In order for MLM to be effective , though , we need an additional dataset of documents about the same topics that are assessed by the questions : this is available only for the CloudAcademy dataset , which contains the transcript of some of the video - lectures on the e - learning platform .", "entities": [[3, 4, "DatasetName", "MLM"]]}
{"text": "In practice , we perform pretraining with MLM as follows .", "entities": [[7, 8, "DatasetName", "MLM"]]}
{"text": "The actual prediction is performed by stacking a fully connected layer and a softmax layer on top of the original pre - trained model : for each masked sentence , this additional layer consumes as input the contextual embedding corresponding to the [ MASK ] token , and tries to predict the word that should be inserted in its place .", "entities": [[13, 14, "MethodName", "softmax"]]}
{"text": "After pre - training the model on the task of MLM , the additional dense and softmax layers are removed from the network , thus leaving us with and the [ SEP ] token is still used only to indicate the end of the question .", "entities": [[10, 11, "DatasetName", "MLM"], [16, 17, "MethodName", "softmax"]]}
{"text": "a pre - trained model which has the same architecture as the original one , with the only difference that all the internal weights were updated during the additional MLM pre - training .", "entities": [[29, 30, "DatasetName", "MLM"]]}
{"text": "For the experiments on CloudAcademy data , we also have access to an additional dataset - referred to as Lectures ( L ) which contains the transcripts of some online lectures available on the platform and is used for the additional MLM pre - training .", "entities": [[41, 42, "DatasetName", "MLM"]]}
{"text": "The only difference is that the regression model , before being fine - tuned on the task of QDE from text using Q TRAIN , is pre - trained on the task of MLM on L. Being an unsupervised task , we use the whole L for this .", "entities": [[33, 34, "DatasetName", "MLM"]]}
{"text": "Before comparing the Transformer models with the state of the art , we show here the performance of the different configurations , both with and without the additional pre - training on MLM .", "entities": [[3, 4, "MethodName", "Transformer"], [32, 33, "DatasetName", "MLM"]]}
{"text": "Even though there is not one model configuration which clearly outperforms all the others , it can be seen that both the additional pre - training and the textual information of the possible choices are helpful in improving the accuracy of the estimation .", "entities": [[39, 40, "MetricName", "accuracy"]]}
{"text": "For DistilBERT without the additional MLM pre - training the best configuration is Q+all , while in all the other cases the best input configuration is Q+correct .", "entities": [[1, 2, "MethodName", "DistilBERT"], [5, 6, "DatasetName", "MLM"]]}
{"text": "We implemented R2DE using the available code 9 . iii ) ELMo ( Xue et al , 2020 ) :", "entities": [[11, 12, "MethodName", "ELMo"]]}
{"text": "For each Transformer model , only one input configuration is shown , as obtained in subsection 7.1 .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "The table shows that the Transformer models generally outperform the proposed baselines on both metrics , both with and without the additional pre - training on MLM .", "entities": [[5, 6, "MethodName", "Transformer"], [26, 27, "DatasetName", "MLM"]]}
{"text": "The model that consistently and significantly outperforms all the others is BERT with Q+correct and the additional MLM pre - training .", "entities": [[11, 12, "MethodName", "BERT"], [17, 18, "DatasetName", "MLM"]]}
{"text": "It is also interesting to remark that ELMo seems to perform estimations a bit biased towards high and low difficulties : indeed , considering overall MAE and RMSE it performs at the same level of R2DE but it is better for the estimation of \" extreme \" questions .", "entities": [[7, 8, "MethodName", "ELMo"], [25, 26, "MetricName", "MAE"], [27, 28, "MetricName", "RMSE"]]}
{"text": "This might also be the reason why ELMo performs better than DistilBERT without MLM on \" extreme \" questions but worse overall .", "entities": [[7, 8, "MethodName", "ELMo"], [11, 12, "MethodName", "DistilBERT"], [13, 14, "DatasetName", "MLM"]]}
{"text": "Table 3 shows results similar to Table 2 : BERT is the best performing model and both Transformer models outperform the baselines .", "entities": [[9, 10, "MethodName", "BERT"], [17, 18, "MethodName", "Transformer"]]}
{"text": "We analyze here some of the characteristics of the best performing model ( i.e. BERT ) , trying to understand whether there are some question properties which particularly influence its accuracy .", "entities": [[14, 15, "MethodName", "BERT"], [30, 31, "MetricName", "accuracy"]]}
{"text": "We perform the same analysis for R2DE , to understand whether such characteristics are a peculiarity of BERT or are shared among different models ; the choice of R2DE is motivated by the fact that it is the second best performing model on the CloudAcademy dataset ( excluding the other Transformer models ) and it uses TF - IDF to create the features , thus a non - neural approach .", "entities": [[17, 18, "MethodName", "BERT"], [50, 51, "MethodName", "Transformer"]]}
{"text": "We report here the results of three analyses , studying possible differences in the accuracy of QDE depending on i ) input length and question difficulty , ii ) number of correct choices , and iii ) whether it is a cloze question .", "entities": [[14, 15, "MetricName", "accuracy"]]}
{"text": "Specifically , we keep questions i ) with | b | < 3 and len < = 110 for CloudAcademy and BERT ( 96.4 % of the questions ) ; ii ) with | b | < 3 and len < = 110 for CloudAcademy and R2DE ( 96.9 % ) ; iii ) with | b | < 4 and len < = 80 for ASSISTments ( 94.3 % ) , there is no difference between BERT and R2DE because they use the same input configuration .", "entities": [[21, 22, "MethodName", "BERT"], [77, 78, "MethodName", "BERT"]]}
{"text": "Figure 4 shows how the estimation error ( represented by the color ) of BERT and R2DE depends on the input length and the target difficulty of the questions .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "The error heavily depends on the target difficulty : this suggests that the two models tend to estimate difficulties closer to 0 than the target values ( especially R2DE ) .", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "There is no clear correlation between error and input length , but in some cases it seems that the error increases with the input length ( e.g. row b = \u22121 in BERT ) .", "entities": [[32, 33, "MethodName", "BERT"]]}
{"text": "The findings are very similar , but it is even more evident the fact that R2DE tends to perform predictions close to 0 ; the error of BERT depends less heavily on the difficulty .", "entities": [[22, 23, "DatasetName", "0"], [27, 28, "MethodName", "BERT"]]}
{"text": "Again , there are no clear correlations between the input length and the accuracy of the estimation .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "Instead , BERT uses contextual embeddings which depend on the position of each word and the encoding of multiple correct choices we performed ( i.e. concatenation ) might not be the better choice , especially considering that probably there are not enough questions to learn the encoding .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "In this paper we have performed a study of how Transformer models perform in the task of QDE from text , and have proposed a model which outperforms previous approaches .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "Specifically , the proposed model is built upon a pre - trained BERT language model , which is fine - tuned for the task of QDE from text .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "As an outcome from our analysis , we can say that : i ) if an additional dataset is available , BERT with MLM pre - training seems to be the best performing model ; ii ) if the only available data is the text of the questions , DistilBERT might be a better option , as it has basically the same performance as BERT but at a fraction of the computational cost .", "entities": [[21, 22, "MethodName", "BERT"], [23, 24, "DatasetName", "MLM"], [49, 50, "MethodName", "DistilBERT"], [64, 65, "MethodName", "BERT"]]}
{"text": "Furthermore , we studied the effect of some questions characteristics on BERT and R2DE , comparing the two models .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "We have observed that the magnitude of the error naturally increases with the magnitude of the difficulty ( especially for R2DE ) , but there is not a clear correlation between the input length and the accuracy of the estimation .", "entities": [[36, 37, "MetricName", "accuracy"]]}
{"text": "We have also observed that both models are less accurate in estimating the difficulty of cloze questions , compared to questions that end with a question mark , and that the decrease in accuracy is lower for BERT .", "entities": [[33, 34, "MetricName", "accuracy"], [37, 38, "MethodName", "BERT"]]}
{"text": "We believe that this happens because underscores are not frequent in natural language and thus the model has a chance of learning them only during the fine - tuning on QDE , not during MLM pre - training .", "entities": [[34, 35, "DatasetName", "MLM"]]}
{"text": "Lastly , BERT performs better on questions with only one correct choice than on questions with multiple correct choices ( for the latter , it is also outperformed by R2DE ) .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "Future works could continue to dig deeper into the analysis of the model , as we believe that the accuracy of the estimation could be further improved by using an ensemble model in which different sub - models are used depending on some characteristic of the question under calibration .", "entities": [[19, 20, "MetricName", "accuracy"]]}
{"text": "Also , future works will try to explore the attention layers of the proposed model , as it might provide useful information about the reasons why the model works better on some questions .", "entities": [[9, 11, "HyperparameterName", "attention layers"]]}
{"text": "Q = q 1 , q 2 , ... , q Nq .", "entities": [[11, 12, "DatasetName", "Nq"]]}
{"text": "that a student i with skill level \u03b8 i correctly answers the question .", "entities": [[7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "According to the intuition , a student with a given skill \u03b8 i has a lower probability of correctly answering more difficult questions .", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "Interactive topic models are powerful tools for understanding large collections of text .", "entities": [[1, 3, "TaskName", "topic models"]]}
{"text": "Topic models distill large collections of text into topics , giving a high - level summary of the thematic structure of the data without manual annotation .", "entities": [[0, 2, "TaskName", "Topic models"]]}
{"text": "In addition to facilitating discovery of topical trends ( Gardner et al , 2010 ) , topic modeling is used for a wide variety of problems including document classification ( Rubin et al , 2012 ) , information retrieval ( Wei and Croft , 2006 ) , author identification ( Rosen - Zvi et al , 2004 ) , and sentiment analysis ( Titov and McDonald , 2008 ) .", "entities": [[27, 29, "TaskName", "document classification"], [37, 39, "TaskName", "information retrieval"], [60, 62, "TaskName", "sentiment analysis"]]}
{"text": "However , the most compelling use of topic models is to help users understand large datasets ( Chuang et al , 2012 ) .", "entities": [[7, 9, "TaskName", "topic models"]]}
{"text": "Interactive topic modeling allows non - experts to refine automatically generated topics , making topic models less of a \" take it or leave it \" proposition .", "entities": [[14, 16, "TaskName", "topic models"]]}
{"text": "We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models .", "entities": [[19, 21, "TaskName", "topic models"]]}
{"text": "Compared to existing methods such as Interactive Topic Models , our method is much faster .", "entities": [[7, 9, "TaskName", "Topic Models"]]}
{"text": "G K , where G k is a set of anchor facets which will form the kth pseudoword anchor .", "entities": [[16, 17, "DatasetName", "kth"]]}
{"text": "We use the well - known 20 Newsgroups dataset ( 20NEWS ) used in previous interactive topic modeling work : 18 , 846 Usenet postings from 20 different newgroups in the early 1990s .", "entities": [[6, 8, "DatasetName", "20 Newsgroups"]]}
{"text": "The anchor algorithm only gives the topic - word distributions and not word - level topic assignments , so we infer token - level topic assignments using LDA Latent Dirichlet Allocation ( Blei et al , 2003 ) with fixed topics discovered by the anchor method .", "entities": [[27, 28, "MethodName", "LDA"]]}
{"text": "For both training and test , we exclude words outside the LDA vocabulary .", "entities": [[11, 12, "MethodName", "LDA"]]}
{"text": "For example , accuracy penalizes a classifier just as much for labeling a document from ' rec.sport.baseball ' with ' rec.sport.hockey ' as with ' alt.atheism ' despite the similarity between sports newsgroups .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "Adjusted Rand index ( ARI ) also accounts for chance groupings of documents .", "entities": [[4, 5, "MetricName", "ARI"]]}
{"text": "Next we use F - measure , which also considers pairwise groups , balancing the contribution of false negatives , but without the true negatives .", "entities": [[3, 6, "MetricName", "F - measure"]]}
{"text": "As with accuracy , this is true regardless of which combination function we use .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "Figure 1 : Using metadata can improve anchor - based topic models .", "entities": [[10, 12, "TaskName", "topic models"]]}
{"text": "Even on the small InfoVis - VAST dataset which contains only 515 documents , Utopian takes 48 seconds to converge .", "entities": [[6, 7, "DatasetName", "VAST"]]}
{"text": "Given high quality anchor facets , the tandem anchor algorithm can produce high quality topic models ( particularly when the harmonic mean combiner is used ) .", "entities": [[14, 16, "TaskName", "topic models"]]}
{"text": "We also describe common problems with topic models including intruding topic words , duplicate topics , and ambiguous topics .", "entities": [[6, 8, "TaskName", "topic models"]]}
{"text": "As before , we report not only accuracy , but also multiple clustering metrics using the confusion matrix from the classification task .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "For interactive topic modeling , using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use .", "entities": [[17, 19, "TaskName", "topic models"]]}
{"text": "Separating Retention from Extraction in the Evaluation of End - to - end Relation Extraction", "entities": [[13, 15, "TaskName", "Relation Extraction"]]}
{"text": "Such heuristics include lexical overlap with the training set in Named - Entity Recognition ( Taill\u00e9 et al , 2020a ) and Event or Type heuristics in Relation Extraction ( Rosenman et al , 2020 ) .", "entities": [[27, 29, "TaskName", "Relation Extraction"]]}
{"text": "Hence , Named Entity Recognition ( NER ) and Relation Extraction ( RE ) are two key IE tasks among others such as Coreference Resolution ( CR ) , Entity Linking or Event Extraction .", "entities": [[2, 5, "TaskName", "Named Entity Recognition"], [6, 7, "TaskName", "NER"], [9, 11, "TaskName", "Relation Extraction"], [23, 25, "TaskName", "Coreference Resolution"], [29, 31, "TaskName", "Entity Linking"], [32, 34, "TaskName", "Event Extraction"]]}
{"text": "Following the general trend in Natural Language Processing ( NLP ) , the recent quantitative improvements reported on Entity and Relation Extraction benchmarks are at least partly explained by the use of larger and larger pretrained Language Models ( LMs ) such as BERT ( Devlin et al , 2019 ) to obtain contextual word representations .", "entities": [[20, 22, "TaskName", "Relation Extraction"], [35, 38, "TaskName", "pretrained Language Models"], [43, 44, "MethodName", "BERT"]]}
{"text": "This lexical overlap has been shown to be correlated to neural networks performance in NER ( Augenstein et al , 2017 ; Taill\u00e9 et al , 2020a ) .", "entities": [[14, 15, "TaskName", "NER"]]}
{"text": "In end - to - end Relation Extraction , we can expect that these NER and RE heuristics are combined .", "entities": [[6, 8, "TaskName", "Relation Extraction"], [14, 15, "TaskName", "NER"]]}
{"text": "We study three recent end - to - end RE models on CoNLL04 ( Roth and Yih , 2004 ) , ACE05 ( Walker et al , 2006 ) and SciERC ( Luan et al , 2018 ) .", "entities": [[30, 31, "DatasetName", "SciERC"]]}
{"text": "They rely on various pretrained LMs and for a fairer comparison , we use BERT ( Devlin et al , 2019 ) on ACE05 and CoNLL04 and SciBERT ( Beltagy et al , 2019 ) on SciERC 1 .", "entities": [[14, 15, "MethodName", "BERT"], [36, 37, "DatasetName", "SciERC"]]}
{"text": "The NER model is a classical span - based model ( Sohrab and Miwa , 2018 ) .", "entities": [[1, 2, "TaskName", "NER"]]}
{"text": "Special tokens corresponding to each predicted entity span are added and used as representation for Relation Classification .", "entities": [[15, 17, "TaskName", "Relation Classification"]]}
{"text": "SpERT ( Eberts and Ulges , 2020 ) uses a similar span - based NER module .", "entities": [[14, 15, "TaskName", "NER"]]}
{"text": "While Entity Filtering is close to the pipeline approach , the NER and RE modules share a common entity representation and are trained jointly .", "entities": [[11, 12, "TaskName", "NER"]]}
{"text": "Because it is not based on span - level predictions , this model can not detect nested entities , e.g. on SciERC .", "entities": [[21, 22, "DatasetName", "SciERC"]]}
{"text": "Similarly , mentions in SciERC are common names which can be tagged with different labels and they can also be nested .", "entities": [[4, 5, "DatasetName", "SciERC"]]}
{"text": "For RE , while SciERC has almost no exact overlap between test and train relations , ACE05 and CoNLL04 have similar levels of exact match .", "entities": [[4, 5, "DatasetName", "SciERC"], [23, 25, "MetricName", "exact match"]]}
{"text": "As expected , this first evaluation setting enables to expose an important lexical overlap bias , already SpERT 89.0 0.1 74.1 1.0 86.5 0.2 77.0 1.1 52.2 1.1 38.9 1.0 57.0 0.8 75.1 1.2 48.4 1.0 36.3 2.0 53.9 0.8 SpERT 89.4 0.2 74.2 0.8 86.8 0.2 84.8 0.8 59.6 0.7 42.3 1.1 64.0 0.6 82.6 0.8 55.6 0.7 38.4 1.1 60.6 0.5 TABTO 89.7 0.1 77.4 0.8 87.5 0.2 85.9 0.9 62.6 1.8 44.6 2.9 66.4 1.3 81.6 1.5 58.1 1.6 38.5 3.1 61.7 1.1 PURE 90.5 0.2 80.0 0.3 88.7 0.1 86.0 1.3 60.5 1.0 47.1 1.6 65.1 0.7 84.1 1.1 57.9 1.3 44.0 2.0 62.6 discussed in NER , in end - to - end Relation Extraction .", "entities": [[110, 111, "TaskName", "NER"], [118, 120, "TaskName", "Relation Extraction"]]}
{"text": "On every dataset and for every model micro F1 scores are the highest for Exact Match relations , then Partial Match and finally totally unseen relations .", "entities": [[7, 9, "MetricName", "micro F1"], [14, 16, "MetricName", "Exact Match"]]}
{"text": "While we can not evaluate TABTO on SciERC because it is unfit for extraction of nested entities , we can notice different hierarchies of models on every dataset suggesting that there is no one - size - fits - all best model , at least in current evaluation settings .", "entities": [[7, 8, "DatasetName", "SciERC"]]}
{"text": "This results in a loss of performance on the RE part and especially on partially matching or new relations for which the entity representations pairs have not been seen .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "However , the PURE pipeline setting seems to only be more effective on ACE05 where its NER performance is significantly better , probably because learning a separate NER and RE encoder enables to learn and capture more specific information for each distinctive task .", "entities": [[16, 17, "TaskName", "NER"], [27, 28, "TaskName", "NER"]]}
{"text": "On SciERC , performance of all models is already compromised at the NER level before the RE step , which makes further distinction between model performance even more difficult .", "entities": [[1, 2, "DatasetName", "SciERC"], [12, 13, "TaskName", "NER"]]}
{"text": "Swapping arguments has a limited effect on NER , mostly for the \" Located in \" relation .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "The pipeline approach allows the use of argument type representations in the Relation Classifier whereas most end - to - end models use lexical features in a shared entity representation used for both NER and RE .", "entities": [[33, 34, "TaskName", "NER"]]}
{"text": "Several works on generalization of NER models mention lexical overlap with the training as a key indicator of performance .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "Augenstein et al ( 2017 ) separate mentions in the test set as seen and unseen during training and measure out - of - domain generalization in an extensive study of two CRF based models and SENNA combining a Convolutional Neural Network with a CRF ( Collobert and Weston , 2011 ) .", "entities": [[24, 26, "TaskName", "domain generalization"], [32, 33, "MethodName", "CRF"], [44, 45, "MethodName", "CRF"]]}
{"text": "Taill\u00e9 et al ( 2020a ) compare the effect of introducing contextual embeddings in the classical BiLSTM - CRF architecture in a similar setting and show that they help close the performance gap on unseen mentions and domains .", "entities": [[16, 17, "MethodName", "BiLSTM"], [18, 19, "MethodName", "CRF"]]}
{"text": "Lexical overlap has also been mentioned in Coreference Resolution ( Moosavi and Strube , 2017 ) where coreferent mentions tend to co - occur in the test and train sets .", "entities": [[7, 9, "TaskName", "Coreference Resolution"]]}
{"text": "McCoy et al ( 2019 ) expose lexical overlap as a shallow heuristic adopted by state - of - the - art Natural Language Inference models , especially by swapping subject and object of verbs in the hypothesis of some examples where the premise entails the hypothesis .", "entities": [[22, 25, "TaskName", "Natural Language Inference"]]}
{"text": "While such a modification changes the label of these examples to non - entailment , all models tested show a spectacular drop of accuracy on these models .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "We propose to extend previous work on NER and RE to the more realistic end - to - end RE setting with two of the previously described approaches : 1 ) separating performance by lexical overlap of mentions or argument pairs and 2 ) modifying some CoNLL04 test examples by swapping relations heads and tails .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "In this paper , we study three state - of - the - art endto - end Relation Extraction models in order to highlight their tendency to retain seen relations .", "entities": [[17, 19, "TaskName", "Relation Extraction"]]}
{"text": "We confirm that retention of seen mentions and relations play an important role in overall RE performance and can explain the relatively higher scores on CoNLL04 and ACE05 compared to SciERC .", "entities": [[30, 31, "DatasetName", "SciERC"]]}
{"text": "Furthermore , pretrained Language Models can already capture relational information between phrases ( Petroni et al , 2019 ) and further experiments could help distinguish their role in the retention behaviour of RE models .", "entities": [[2, 5, "TaskName", "pretrained Language Models"]]}
{"text": "For CoNLL04 and ACE05 , we train each model with both the cased and uncased versions of BERT BASE and only keep the best performing setting .", "entities": [[17, 18, "MethodName", "BERT"], [18, 19, "MethodName", "BASE"]]}
{"text": "1 We use the approximation model and limit use a context window of 0 to only use the current sentence for prediction and be able to compare with other models .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "HacRED : A Large - Scale Relation Extraction Dataset Toward Hard Cases in Practical Applications", "entities": [[6, 8, "TaskName", "Relation Extraction"]]}
{"text": "Relation extraction ( RE ) is an essential topic in natural language processing and has attracted extensive attention .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}
{"text": "To make RE models more robust on such practical hard cases , we propose a case - oriented construction framework to build a Hard Case Relation Extraction Dataset ( HacRED ) .", "entities": [[25, 27, "TaskName", "Relation Extraction"]]}
{"text": "Relation extraction ( RE ) is one of the core NLP tasks and plays an increasingly important role in knowledge graph completion ( Bordes et al , 2013 ) and question answering ( Dong et al , 2015 ) .", "entities": [[0, 2, "TaskName", "Relation extraction"], [19, 22, "TaskName", "knowledge graph completion"], [30, 32, "TaskName", "question answering"]]}
{"text": "To answer the question , we employ CasRel on 300 randomly selected samples of WebNLG and the same number of data from practical DuIE 1 .", "entities": [[14, 15, "DatasetName", "WebNLG"]]}
{"text": "As illustrated in Figure 1 , CasRel extracts correct triples ( Elliot See , place_of_birth , Dallas ) and ( Elliot See , place_of_death , St. Louis ) in WebNLG where keywords such as born and died explicitly express the relation information .", "entities": [[29, 30, "DatasetName", "WebNLG"]]}
{"text": "The most significant reason why CasRel performs well on WebNLG but struggles on practical data is that more challenging instances which we refer to as hard cases exist in the practical applications .", "entities": [[9, 10, "DatasetName", "WebNLG"]]}
{"text": "Moreover , according to the statistics of entity description documents in CN - DBpedia , at least 40.1 % relational facts can only be extracted from hard cases .", "entities": [[13, 14, "DatasetName", "DBpedia"]]}
{"text": "Therefore , relation extraction from hard cases can not be neglected and demands more attention .", "entities": [[2, 4, "TaskName", "relation extraction"]]}
{"text": "To this end , we propose a case - oriented construction framework based on the challenging instances and build a Hard Case Relation Extraction Dataset ( HacRED ) .", "entities": [[22, 24, "TaskName", "Relation Extraction"]]}
{"text": "Specifically , we first obtain general , massive , and various contexts as well as relational facts from CN - DBpedia to construct a distantly supervised dataset .", "entities": [[20, 21, "DatasetName", "DBpedia"]]}
{"text": "Then , we conduct feature engineering based on the valid indicators .", "entities": [[4, 6, "TaskName", "feature engineering"]]}
{"text": "A large - scale dataset TACRED ( Zhang et al , 2017 ) is obtained via crowdsourcing to satisfy the training of data - hungry models .", "entities": [[5, 6, "DatasetName", "TACRED"]]}
{"text": "DocRED ( Yao et al , 2019 ) is constructed to accelerate the research on document - level RE .", "entities": [[0, 1, "DatasetName", "DocRED"]]}
{"text": "To meet the challenges of fewshot RE , FewRel ( Han et al , 2018 ) as well as FewRel 2.0 ( Gao et al , 2019 ) have been presented .", "entities": [[8, 9, "DatasetName", "FewRel"], [19, 21, "DatasetName", "FewRel 2.0"]]}
{"text": "RELX ( Koksal and Ozgur , 2020 ) is a benchmark for cross - lingual RE .", "entities": [[0, 1, "DatasetName", "RELX"]]}
{"text": "Jia et al ( 2020 ) propose the task of interpersonal RE in dyadic dialogues and further construct a corresponding dataset called DDRel .", "entities": [[22, 23, "DatasetName", "DDRel"]]}
{"text": "GraphRel ( Fu et al , 2019 ) uses graph convolutional network ( GCN ) to capture features of words and text .", "entities": [[9, 12, "MethodName", "graph convolutional network"], [13, 14, "MethodName", "GCN"]]}
{"text": "DGCNN - BERT is a powerful pipeline method that first identifies multiple relations and then labels the head and tail entities given a relation .", "entities": [[0, 1, "MethodName", "DGCNN"], [2, 3, "MethodName", "BERT"]]}
{"text": "SSAN ( Xu et al , 2021 ) designs several transformations to incorporate mention structural dependencies for document - level relation classification ( DocRC ) .", "entities": [[20, 22, "TaskName", "relation classification"]]}
{"text": "To analyze where models struggle in practical instances and distinguish the hard cases , we conduct a manual exploratory analysis on the errorprone instances of SOTA models ( CGCN , CasRel , DGCNN - BERT ) on NYT , DuIE and industry data .", "entities": [[32, 33, "MethodName", "DGCNN"], [34, 35, "MethodName", "BERT"]]}
{"text": "The experiments of Alt et al ( 2020 ) also reflect that RE models get a relatively higher error rate with the length of sentence greater than 30 in TACRED .", "entities": [[29, 30, "DatasetName", "TACRED"]]}
{"text": "Different from previous works ( Zhang et al , 2017 , Zaporojets et al , 2020 which start crowdsourcing annotation straight after the data collection stage , we introduce additional stages of hard case feature engineering and target instance prediction .", "entities": [[34, 36, "TaskName", "feature engineering"]]}
{"text": "To avoid data bias to high - frequency entities and relations , we first obtain about 5 million plain texts and 800 thousand triples from CN - DBpedia .", "entities": [[27, 28, "DatasetName", "DBpedia"]]}
{"text": "We use fine - grained named entity recognition ( NER ) toolkit TexSmart ( Zhang et al , 2020 ) and entity linking ( Chen et al , 2018 ) to align mentioned entities in texts to those in triples .", "entities": [[5, 8, "TaskName", "named entity recognition"], [9, 10, "TaskName", "NER"], [21, 23, "TaskName", "entity linking"]]}
{"text": "More details of feature engineering are described in Appendix A.", "entities": [[3, 5, "TaskName", "feature engineering"]]}
{"text": "After hard case oriented feature engineering , we discard the instances in D ds without any indicator of hard cases .", "entities": [[4, 6, "TaskName", "feature engineering"]]}
{"text": "To further evaluate the quality of selected hard cases , we utilize DGCNN - BERT to test the selected and unselected data .", "entities": [[12, 13, "MethodName", "DGCNN"], [14, 15, "MethodName", "BERT"]]}
{"text": "If the F1 score drops \u03b4=10 % on the hard cases , we reserve the data to constitute the high quality seeds of hard case", "entities": [[2, 4, "MetricName", "F1 score"], [21, 22, "DatasetName", "seeds"]]}
{"text": "In total , we obtain 1 , 431 seeds of hard cases .", "entities": [[8, 9, "DatasetName", "seeds"]]}
{"text": "Then , we form the representation vector as recommended in Baldini Soares et al ( 2019 et al , 1998 ) and BiLSTM ( Hochreiter and Schmidhuber , 1997 ) , to capture the context information .", "entities": [[22, 23, "MethodName", "BiLSTM"]]}
{"text": "When all relations are annotated , NER toolkit recommends multiple entity mentions with the corresponding type based on schema information .", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "Compared with the document - level datasets , DocRED aims at common document - level RE but not consider performance gaps and various hard cases in practical scenarios .", "entities": [[8, 9, "DatasetName", "DocRED"]]}
{"text": "BC5CDR is specially designed for biomedical domain .", "entities": [[0, 1, "DatasetName", "BC5CDR"]]}
{"text": "Besides , HacRED is larger in scale and contains much more various relational facts than BC5CDR and DocRED but with lower duplicated triples ratio .", "entities": [[15, 16, "DatasetName", "BC5CDR"], [17, 18, "DatasetName", "DocRED"]]}
{"text": "No biased relation existing in HacRED reduces the risk of selection bias .", "entities": [[10, 12, "TaskName", "selection bias"]]}
{"text": "As a comparison , NYT contains about 31 % noise instances ( Riedel et al , 2010 ) and TACRED has poor annotation quality ( Alt et al , 2020 ) .", "entities": [[19, 20, "DatasetName", "TACRED"]]}
{"text": "As DGCNN - BERT has been used in the main process of construction , we evaluate other strong RE models including joint RE models , pipeline RE models , and DocRC models on HacRED .", "entities": [[1, 2, "MethodName", "DGCNN"], [3, 4, "MethodName", "BERT"]]}
{"text": "We notice that F1 value of easy cases is generally greater than that of hard cases in different substitution ratio settings , which illustrates that RE models indeed struggle when tackling hard cases .", "entities": [[3, 4, "MetricName", "F1"]]}
{"text": "The joint and pipeline learning strategies do not contribute to a great F1 on triple extraction .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "On the other hand , the relation classification performances of DocRC models are far from satisfactory .", "entities": [[6, 8, "TaskName", "relation classification"]]}
{"text": "As for relation classification task , three volunteers select the relation , including NA regarded as negative , of the given entity pair .", "entities": [[2, 4, "TaskName", "relation classification"]]}
{"text": "BiLSTM - based neural models like NovelTagging and CopyRE .", "entities": [[0, 1, "MethodName", "BiLSTM"]]}
{"text": "The performance improvement on CasRel suggests the powerfulness of BERT encoder in the long - distance context .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "The deep models output the probability of the instance belonging to hard cases and are optimized with the binary cross entropy loss objective .", "entities": [[21, 22, "MetricName", "loss"]]}
{"text": "+ max { 0 , E p ( x )", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "We choose the double hinge loss l", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "= max ( \u2212z , max ( 0 , 1 2 \u2212 1 2 z ) ) proposed by ( du Plessis et al , 2015 ) .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "When all relation mentions are annotated , NER toolkit recommend multiple entity mentions with the corresponding type .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "We specifically discuss two types of common parallel resources : bilingual corpus and bilingual dictionary , and design different transfer learning strategies accordingly .", "entities": [[19, 21, "TaskName", "transfer learning"]]}
{"text": "al , 2017 ; Bordes and Weston , 2017 ; Williams et al , 2017 ; Li et al , 2017 ) , with Google Duplexbeing the most recent example .", "entities": [[24, 25, "DatasetName", "Google"]]}
{"text": "For a common dialog shown in Figure 1 , a typical data acquisition process ( Rojas - Barahona et al , 2017 ) not only requires two human users to converse for multiple turns but also requires annotators to identify user 's intention in each turn .", "entities": [[30, 31, "DatasetName", "converse"]]}
{"text": "Depending on the popularity and availability of target language resources , we study two kinds of parallel data : bilingual corpus and bilingual dictionary , and we respectively design two transfer learning strategies .", "entities": [[30, 32, "TaskName", "transfer learning"]]}
{"text": "We use the popular Wizard - of - Oz ( Rojas - Barahona et", "entities": [[4, 9, "DatasetName", "Wizard - of - Oz"]]}
{"text": "al , 2017 ) dataset as our DST benchmark to evaluate the effectiveness of our crosslingual transfer learning .", "entities": [[16, 18, "TaskName", "transfer learning"]]}
{"text": "Compared with an array of alternative transfer learning strategies , our cross - lingual DST models consistently achieve promising results in both scenarios for both zero - annotation languages .", "entities": [[6, 8, "TaskName", "transfer learning"]]}
{"text": "We systematically study different scenarios for this problem based on the availability of parallel data and propose novel transfer learning methods to tackle the problem .", "entities": [[18, 20, "TaskName", "transfer learning"]]}
{"text": "Though the discriminative models are reported to achieve fairly high accuracy , their applications are heavily restricted by the domain , ontology , and language .", "entities": [[10, 11, "MetricName", "accuracy"], [21, 22, "MethodName", "ontology"]]}
{"text": "Recently , a pointer network based algorithm ( Xu and Hu , 2018 ) and another multi - domain algorithm ( Rastogi et al , 2017 ) have been proposed to break the ontology and domain boundary .", "entities": [[3, 5, "MethodName", "pointer network"], [33, 34, "MethodName", "ontology"]]}
{"text": "Cross - lingual transfer learning has been a very popular topic during the years , which can be seen as a transductive process .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"]]}
{"text": "Recently , algorithms have been successfully designed for POS tagging ( Zhang et al , 2016 ; , NER ( Pan et al , 2017 ; Ni et al , 2017 ) as well as image captioning ( Miyazaki and Shimizu , 2016 ) .", "entities": [[18, 19, "TaskName", "NER"], [35, 37, "TaskName", "image captioning"]]}
{"text": "Our method addresses the transfer learning using a teacher - student framework and proposes to use the teacher to gradually guide the student to make more proper decisions .", "entities": [[4, 6, "TaskName", "transfer learning"]]}
{"text": "We define the ontology of the dialog system to be the set of all the possible words the dialog slot and value can take .", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "We here mainly consider two different types of parallel resources to assist the transfer learning : ( 1 ) Bilingual Corpus , where abundant bilingual corpora exist between the source and the target languages .", "entities": [[13, 15, "TaskName", "transfer learning"]]}
{"text": "That is , the ontology of dialog among different languages is known with a one - to - one mapping between them ( e.g. , greek = griechisch = greco , food = essen = cibo ) .", "entities": [[4, 5, "MethodName", "ontology"]]}
{"text": "Based on that , we could construct a mapping function M to associate the ontology terms from different languages with predesigned language - agnostic concepts : for exam - ple , M ( f oods )", "entities": [[14, 15, "MethodName", "ontology"]]}
{"text": "With a slight abuse of notation , we still use c s , c v , t s , t v , t q R H to denote the vector representations of themselves , where H is the embedding dimension .", "entities": [[38, 40, "HyperparameterName", "embedding dimension"]]}
{"text": "To enable cross - lingual transfer learning , we first re - interpret the architecture of the original NBT by decomposing it into three components :", "entities": [[2, 6, "TaskName", "cross - lingual transfer"]]}
{"text": "the encoder using the same convolutional neural network ( CNN ) as the original NBT , with a slight modification of adding a top batch normalization layer .", "entities": [[24, 26, "MethodName", "batch normalization"]]}
{"text": "The second part is the context gate , which takes the system acts a t = ( t q , t s , t v ) 2 tq represents the system request , ts , tv represents the system confirmation .", "entities": [[33, 34, "MethodName", "ts"]]}
{"text": "If the system wants to confirm some information from a user by asking \" should I try Persian restaurants in the north ? \" then NBT sets ts , tv=\"area , north \" .", "entities": [[27, 28, "MethodName", "ts"]]}
{"text": "We assume the ontology mapping M is known a priori ( see Figure 3 ) .", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "Since we use batch normalization layer to normalize the encoder output ( described in Figure 3 ) , | | r f ( u f t )", "entities": [[3, 5, "MethodName", "batch normalization"]]}
{"text": "Recall that we can easily simulate the target - side system acts , slot - value pairs ( c f s , c f v , a f ) by using the ontology mapping M .", "entities": [[0, 1, "MetricName", "Recall"], [32, 33, "MethodName", "ontology"]]}
{"text": "The cost function ( 9 ) is minimized by stochastic gradient descent .", "entities": [[9, 12, "MethodName", "stochastic gradient descent"]]}
{"text": "al , 2017 ) dataset is used for training and evaluation , which consists of user conversations with taskoriented dialog systems designed to help users find suitable restaurants around Cambridge , UK .", "entities": [[29, 30, "DatasetName", "Cambridge"]]}
{"text": "In the experiments , we do not have access to any training or validation dataset for German and Italian , and we only have access to their testing dataset which is composed of 400 dialogs .", "entities": [[13, 15, "DatasetName", "validation dataset"]]}
{"text": "We use Panlex ( Kamholz et al , 2014 ) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary .", "entities": [[2, 3, "DatasetName", "Panlex"]]}
{"text": "We specifically investigate two kinds of pretrained embedding , and we use Glove ( Pennington et al , 2014 ) as the monolingual embedding and MUSE ( Conneau et al , 2017 ) as the bilingual embedding to see their impacts on the DST performance .", "entities": [[25, 26, "DatasetName", "MUSE"]]}
{"text": "Here we highlight the baselines we use to compare with our cross - lingual algorithm as follows : ( 1 ) Supervised : this baseline algorithm assumes the existence of annotated dialog belief tracking datasets , and it determines the upper bound of the DST model .", "entities": [[21, 23, "DatasetName", "Supervised :"]]}
{"text": "( 3 ) Ontology - match : this algorithm directly uses exact string matching against the utterance to discover the perceived slot - value pairs , it directly assigns a high score to the appearing candidates .", "entities": [[3, 4, "MethodName", "Ontology"]]}
{"text": "( 4 ) Translation - based : this system pre - trains a translator on the external bilingual corpus and then translates the English dialog and ontology into target language as \" annotated \" data , which is used to train the NBT in the target language domain ( more details about the implementation , performance and examples are listed in the appendix ) .", "entities": [[3, 4, "TaskName", "Translation"], [26, 27, "MethodName", "ontology"]]}
{"text": "Furthermore , for both cases , our teacher - student framework can make use of the knowledge learned in source - side NBT to assist its decision making , while translator - based methods learn from scratch .", "entities": [[26, 28, "TaskName", "decision making"]]}
{"text": "From the table , we can easily observe that bilingual corpus is obviously a more informative parallel resource to perform cross - lingual transfer learning .", "entities": [[20, 24, "TaskName", "cross - lingual transfer"]]}
{"text": "The accuracy of XL - NBT - D is lower than XL - NBT - C. We conjecture that our replace - ment strategy to generate \" mixed \" language utterance can sometimes break the semantic coherence and cause additional noises during the transfer process , which remarkably degrades the DST performance .", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "German vs. Italian As can be seen , the transfer learning results for Italian are remarkably higher than German , especially for the \" Goal \" accuracy .", "entities": [[9, 11, "TaskName", "transfer learning"], [26, 27, "MetricName", "accuracy"]]}
{"text": "Here we want to further highlight the comparison between our transfer learning algorithm with the MT - based approach .", "entities": [[10, 12, "TaskName", "transfer learning"]]}
{"text": "In our further ablation studies , we found that using Google Translator 6 can actually achieve a better score than our transfer algorithm , which is understandable considering the complexity of Google Translator and the much larger parallel corpus it leverages .", "entities": [[10, 11, "DatasetName", "Google"], [31, 32, "DatasetName", "Google"]]}
{"text": "Apparently , we need to trade off the efficiency for the accuracy .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "Here we investigate the effect ' of hyper - parameter \u03b1 , \u03c4 on the evaluation results .", "entities": [[10, 11, "HyperparameterName", "\u03b1"]]}
{"text": "The \u03b1 is used to balance the optimization of encoder constraint and gate constraint , where larger \u03b1 means more optimization on gate constraint .", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "We also draw the learning curve ( Precision vs. Iteration ) in the Appendix for both XL - NBT - C and", "entities": [[7, 8, "MetricName", "Precision"]]}
{"text": "In our paper , we propose a novel teacher - student framework to perform cross - lingual transfer learning for DST .", "entities": [[14, 18, "TaskName", "cross - lingual transfer"]]}
{"text": "We are also very thankful for the public belief tracking code and multilingual state - tracking datasets released by Nikola Mrksic from the University of Cambridge .", "entities": [[25, 26, "DatasetName", "Cambridge"]]}
{"text": "A Multi - Type Multi - Span Network for Reading Comprehension that Requires Discrete Reasoning", "entities": [[9, 11, "TaskName", "Reading Comprehension"]]}
{"text": "Rapid progress has been made in the field of reading comprehension and question answering , where several systems have achieved human parity in some simplified settings .", "entities": [[9, 11, "TaskName", "reading comprehension"], [12, 14, "TaskName", "question answering"]]}
{"text": "In this paper , we introduce the Multi - Type Multi - Span Network ( MTMSN ) , a neural reading comprehension model that combines a multi - type answer predictor designed to support various answer types ( e.g. , span , count , negation , and arithmetic expression ) with a multi - span extraction method for dynamically producing one or multiple text spans .", "entities": [[20, 22, "TaskName", "reading comprehension"]]}
{"text": "This paper considers the reading comprehension task in which some discrete - reasoning abilities are needed to correctly answer questions .", "entities": [[4, 6, "TaskName", "reading comprehension"]]}
{"text": "Specifically , we focus on a new reading comprehension dataset called DROP ( Dua et al , 2019 ) , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer .", "entities": [[7, 9, "TaskName", "reading comprehension"], [11, 12, "DatasetName", "DROP"]]}
{"text": "Unlike previous benchmarks such as CNN / DM ( Hermann et al , 2015 ) and SQuAD ( Rajpurkar et al , 2016 ) that have been well solved Devlin et al , 2019 ) , DROP is substantially more challenging in three ways .", "entities": [[16, 17, "DatasetName", "SQuAD"], [36, 37, "DatasetName", "DROP"]]}
{"text": "Second , rather than restricting the answer to be a span of text , DROP loosens the constraint so that answers may be a set of multiple text strings .", "entities": [[14, 15, "DatasetName", "DROP"]]}
{"text": "First , to produce various answer types , Dua et al ( 2019 ) extend previous one - type answer prediction ( Seo et al , 2017 ) to multi - type prediction that supports span extraction , counting , and addition / subtraction .", "entities": [[31, 33, "TaskName", "type prediction"]]}
{"text": "Second , previous reading comprehension models ( Wang et al , 2017 ; Yu et al , 2018 ; Hu et al , 2018 ) are designed to produce one single span as the answer .", "entities": [[3, 5, "TaskName", "reading comprehension"]]}
{"text": "To address the above issues , we introduce the Multi - Type Multi - Span Network ( MTMSN ) , a neural reading comprehension model for predicting various types of answers as well as dynamically extracting one or multiple spans .", "entities": [[22, 24, "TaskName", "reading comprehension"]]}
{"text": "MTMSN utilizes a series of pre - trained Transformer blocks ( Devlin et al , 2019 ) to obtain a deep bidirectional context representation .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "To make a fair comparison , we also construct a baseline that uses the same BERT - based encoder .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "In the reading comprehension task that requires discrete reasoning , a passage and a question are given .", "entities": [[2, 4, "TaskName", "reading comprehension"]]}
{"text": "Unlike previous dataset such as SQuAD ( Rajpurkar et al , 2016 ) where the answer is limited to be a single span of text , DROP loosens the constraint so that the answer involves various types such as number , date , or span of text ( Figure 1 ) .", "entities": [[5, 6, "DatasetName", "SQuAD"], [26, 27, "DatasetName", "DROP"]]}
{"text": "A4 : 77.5 Figure 1 : Question - answer pairs along with a passage from the DROP dataset .", "entities": [[16, 17, "DatasetName", "DROP"]]}
{"text": "Figure 2 gives an overview of our model that aims to combine neural reading comprehension with numerical reasoning .", "entities": [[13, 15, "TaskName", "reading comprehension"]]}
{"text": "Our model uses BERT ( Devlin et al , 2019 ) as encoder : we map word embeddings into contextualized representations using pre - trained Transformer blocks ( Vaswani et al , 2017 ) ( 3.1 ) .", "entities": [[3, 4, "MethodName", "BERT"], [16, 18, "TaskName", "word embeddings"], [25, 26, "MethodName", "Transformer"]]}
{"text": "To obtain a universal representation for both the question and the passage , we utilize BERT ( Devlin et al , 2019 ) , a pre - trained deep bidirectional Transformer model that achieves state - of - the - art performance across various tasks , as the encoder .", "entities": [[15, 16, "MethodName", "BERT"], [30, 31, "MethodName", "Transformer"]]}
{"text": "the passage using the WordPiece vocabulary ( Wu et al , 2016 ) , and then generate the input sequence by concatenating a [ CLS ] token , the tokenized question , a [ SEP ] token , the tokenized passage , and a final [ SEP ] token .", "entities": [[4, 5, "MethodName", "WordPiece"]]}
{"text": "For each token in the sequence , its input representation is the elementwise addition of WordPiece embeddings , positional embeddings , and segment embeddings ( Devlin et al , 2019 ) .", "entities": [[15, 16, "MethodName", "WordPiece"]]}
{"text": "As a result , a list of input embeddings H 0 2 R T \u21e5 D can be obtained , where D is the hidden size and T is the sequence length .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "A series of L pre - trained Transformer blocks are then used to project the input embeddings into contextualized representations", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "Rather than restricting the answer to always be a span of text , the discrete - reasoning reading comprehension task involves different answer types ( e.g. , number , date , span of text ) .", "entities": [[17, 19, "TaskName", "reading comprehension"]]}
{"text": "Answer type prediction Inspired by the Augmented QANet model ( Dua et al , 2019 ) , we use the contextualized token representations from the last four blocks ( H L 3 , ... , H L ) as the inputs to our answer predictor , which are denoted as M 0 , M 1 , M 2 , M 3 , respectively .", "entities": [[1, 3, "TaskName", "type prediction"], [3, 4, "DatasetName", "Inspired"], [51, 52, "DatasetName", "0"]]}
{"text": "Then the model computes two vectors h Q 2 and h P 2 that summarize the question and passage information respectively : \u21b5 Q = softmax ( W Q Q 2 ) , h Q 2 = \u21b5 Q Q 2 where h P 2 is computed in a similar way over P 2 .", "entities": [[25, 26, "MethodName", "softmax"]]}
{"text": "Next , we calculate a probability distribution to represent the choices of different answer types as : p type = softmax ( FFN ( [ h Q 2 ; h P 2 ; h CLS ] ) )", "entities": [[20, 21, "MethodName", "softmax"]]}
{"text": "Here , h CLS is the first vector in the final contextualized representation M 3 , and FFN denotes a feed - forward network consisting of two linear projections with a GeLU activation ( Hendrycks and Gimpel , 2016 ) followed by a layer normalization ( Lei Ba et", "entities": [[31, 32, "MethodName", "GeLU"], [43, 45, "MethodName", "layer normalization"]]}
{"text": "Specifically , we first compute three vectors , namely g Q 0 , g Q 1 , g Q 2 , that summarize the question information among different levels of question representations : Q = softmax ( FFN ( Q 2 ) ,", "entities": [[11, 12, "DatasetName", "0"], [35, 36, "MethodName", "softmax"]]}
{"text": "g Q 2 = Q Q 2 where g Q 0 and g Q 1 are computed over Q 0 and Q 1 respectively , in a similar way as described above .", "entities": [[10, 11, "DatasetName", "0"], [19, 20, "DatasetName", "0"]]}
{"text": "[ M 2 ; M 0 ; g Q 2 \u2326 M 2 ; g Q 0 \u2326 M 0 ] , M end = [ M 2 ; M 1 ; g Q 2 \u2326 M 2 ; g Q 1 \u2326 M 1 ] , p start = softmax ( W SMstart ) , p end = softmax ( W EMend ) where \u2326 denotes the outer product between the vector g and each token representation in M.", "entities": [[5, 6, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [50, 51, "MethodName", "softmax"], [59, 60, "MethodName", "softmax"]]}
{"text": "Then the probabilities of the i - th number being assigned a plus , minus or zero is computed as : p sign i = softmax ( FFN ( [ u i ; h Q 2 ; h P 2 ; h CLS ] ) )", "entities": [[25, 26, "MethodName", "softmax"]]}
{"text": "Count We consider the ability of counting entities and model it as a multi - class classification problem .", "entities": [[13, 17, "TaskName", "multi - class classification"]]}
{"text": "To achieve this , the model first produces a vector h U that summarizes the important information among all mentioned numbers , and then computes a counting probability distribution as : \u21b5 U = softmax ( W U U ) , h U = \u21b5 U U , p count = softmax ( FFN ( [ h U ; h Q 2 ; h P 2 ; h CLS ] ) )", "entities": [[34, 35, "MethodName", "softmax"], [51, 52, "MethodName", "softmax"]]}
{"text": "We find there are many cases in DROP that require to perform logical negation on numbers .", "entities": [[7, 8, "DatasetName", "DROP"]]}
{"text": "Then we compute the probabilities of logical negation on the i - th number as : p negation i = softmax ( FFN ( [ u i ; h Q 2 ; h P 2 ;", "entities": [[20, 21, "MethodName", "softmax"]]}
{"text": "Although existing reading comprehension tasks focus exclusively on finding one span of text as the final answer , DROP loosens the restriction so that the answer to the question may be several text spans .", "entities": [[2, 4, "TaskName", "reading comprehension"], [18, 19, "DatasetName", "DROP"]]}
{"text": "This is achieved by computing a probability distribution on span amount as p span = softmax ( FFN ( [ h Q 2 ; h P 2 ; h CLS ] ) )", "entities": [[15, 16, "MethodName", "softmax"]]}
{"text": "We also delete any remaining span s j that overlaps with s i , where the degree of overlap is measured using the text - level F1 function .", "entities": [[26, 27, "MetricName", "F1"]]}
{"text": "Add span si toS 8 : Remove span si from S 9 : for sj in S do 10 : if f1 ( si , sj ) > 0", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "Then the expression representation along with the reranking probability can be calculated as : \u21b5 V i = softmax ( W V ( V i + C i ) ) ,", "entities": [[18, 19, "MethodName", "softmax"]]}
{"text": "h V i = \u21b5 V i ( V i + C i ) , p arith i = softmax ( FFN ( [ h V i ; h Q 2 ; h P 2 ; h CLS ] ) )", "entities": [[19, 20, "MethodName", "softmax"]]}
{"text": "Since DROP does not indicate the answer type but only provides the answer string , we therefore adopt the weakly supervised annotation scheme , as suggested in Berant et al ( 2013 ) ;", "entities": [[1, 2, "DatasetName", "DROP"]]}
{"text": "Notice that there are two objective functions for the multi - span component : one is a distantly - supervised loss that maximizes the probabilities of all matching spans , and the other is a classification loss that maximizes the probability on span amount .", "entities": [[20, 21, "MetricName", "loss"], [36, 37, "MetricName", "loss"]]}
{"text": "Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs ( DROP ) ( Dua et al , 2019 ) prehensive understanding of the context as well as the ability of numerical reasoning are required .", "entities": [[4, 6, "TaskName", "reading comprehension"], [14, 15, "DatasetName", "DROP"]]}
{"text": "Model settings We build our model upon two publicly available uncased versions of BERT : BERT BASE and BERT LARGE 2 , and refer readers to Devlin et al ( 2019 ) for details on model sizes .", "entities": [[13, 14, "MethodName", "BERT"], [15, 16, "MethodName", "BERT"], [16, 17, "MethodName", "BASE"], [18, 19, "MethodName", "BERT"]]}
{"text": "Baselines Following the implementation of Augmented QANet ( NAQANet ) ( Dua et al , 2019 ) , we introduce a similar baseline called Augmented BERT ( NABERT ) .", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "The main difference is that we replace the encoder of QANet ( Yu et al , 2018 ) with the pre - trained Transformer blocks ( Devlin et al , 2019 ) .", "entities": [[23, 24, "MethodName", "Transformer"]]}
{"text": "Two metrics , namely Exact Match ( EM ) and F1 score , are utilized to evaluate models .", "entities": [[4, 6, "MetricName", "Exact Match"], [7, 8, "MetricName", "EM"], [10, 12, "MetricName", "F1 score"]]}
{"text": "The results reveal that it has a more negative impact on the F1 score .", "entities": [[12, 14, "MetricName", "F1 score"]]}
{"text": "Hence for multi - span scenarios , the gain of our method on F1 is relatively easier to obtain than the one on EM .", "entities": [[13, 14, "MetricName", "F1"], [23, 24, "MetricName", "EM"]]}
{"text": "We list the annotation statistics on the DROP train set in Table 6 .", "entities": [[7, 8, "DatasetName", "DROP"]]}
{"text": "As we can see , only annotating matching spans results in a labeled ratio of 56.4 % , indicating that DROP includes various answer types beyond text spans .", "entities": [[20, 21, "DatasetName", "DROP"]]}
{"text": "More importantly , the F1 score constantly increases as more answer types are considered .", "entities": [[4, 6, "MetricName", "F1 score"]]}
{"text": "Reading comprehension benchmarks Promising advancements have been made for reading comprehension due to the creation of many large datasets .", "entities": [[0, 2, "TaskName", "Reading comprehension"], [9, 11, "TaskName", "reading comprehension"]]}
{"text": "Recently , Dua et al ( 2019 ) released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers .", "entities": [[13, 14, "DatasetName", "DROP"]]}
{"text": "We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities .", "entities": [[5, 6, "DatasetName", "DROP"]]}
{"text": "Dua et al ( 2019 ) enhanced prior single - type prediction to support various answer types such as span , count number , and addition / subtraction .", "entities": [[10, 12, "TaskName", "type prediction"]]}
{"text": "Later works on program induction ( Reed and De Freitas , 2016 ; Neelakantan et al , 2016 ; Liang et al , 2017 ) extended this idea by using several built - in logic operations along with a key - value memory to learn different types of compositional programs such as addition or sorting .", "entities": [[3, 5, "TaskName", "program induction"]]}
{"text": "Visual question answering In computer vision community , the most similar work to our approach is Neural Module Networks ( Andreas et al , 2016b ) , where a dependency parser is used to lay out a neural network composed of several pre - defined modules .", "entities": [[0, 3, "DatasetName", "Visual question answering"]]}
{"text": "We introduce MTMSN , a multi - type multi - span network for reading comprehension that requires discrete reasoning over the content of paragraphs .", "entities": [[13, 15, "TaskName", "reading comprehension"]]}
{"text": "Abstract Text Summarization : A Low Resource Challenge", "entities": [[1, 3, "TaskName", "Text Summarization"]]}
{"text": "Text summarization is considered as a challenging task in the NLP community .", "entities": [[0, 2, "TaskName", "Text summarization"]]}
{"text": "The availability of datasets for the task of multilingual text summarization is rare , and such datasets are difficult to construct .", "entities": [[9, 11, "TaskName", "text summarization"]]}
{"text": "In this work , we build an abstract text summarizer for the German language text using the state - of - the - art \" Transformer \" model .", "entities": [[25, 26, "MethodName", "Transformer"]]}
{"text": "We propose an iterative data augmentation approach which uses synthetic data along with the real summarization data for the German language .", "entities": [[4, 6, "TaskName", "data augmentation"], [15, 16, "TaskName", "summarization"]]}
{"text": "To generate synthetic data , the Common Crawl ( German ) dataset is exploited , which covers different domains .", "entities": [[6, 8, "DatasetName", "Common Crawl"]]}
{"text": "The obtained summarization performance is measured in terms of ROUGE and BLEU score .", "entities": [[2, 3, "TaskName", "summarization"], [11, 13, "MetricName", "BLEU score"]]}
{"text": "Automatic text summarization is considered as a challenging task because while summarizing a piece of text , we read it entirely to develop our understanding to prepare highlighting its main points .", "entities": [[1, 3, "TaskName", "text summarization"]]}
{"text": "Due to the lack of human knowledge and language processing abilities in computers , automatic text summarization is a major non - trivial task ( Allahyari et al , 2017 ) .", "entities": [[15, 17, "TaskName", "text summarization"]]}
{"text": "Two major approaches for automatic summarization are : extractive and abstractive .", "entities": [[5, 6, "TaskName", "summarization"]]}
{"text": "The extractive summarization approach produces summaries by choosing a subset of sentences in the original text .", "entities": [[1, 3, "TaskName", "extractive summarization"]]}
{"text": "The abstract text summarization approach aims to shorten the long text into a humanreadable form that contains the most important fact from the original text ( Allahyari et al , 2017 ;", "entities": [[2, 4, "TaskName", "text summarization"]]}
{"text": "The deep learning - based neural attention model when applying to abstract text summarization performs well compared to standard learning - based approaches ( Rush et al , 2015 ) .", "entities": [[12, 14, "TaskName", "text summarization"]]}
{"text": "Abstract text summarization using the attentional encoder - decoder recurrent neural network approach shows a stateof - the - art performance and sets a baseline model", "entities": [[1, 3, "TaskName", "text summarization"]]}
{"text": "There is an inherent limitation to natural language processing tasks such as text summarization for resource - poor and morphological complex languages owing to a shortage of quality linguistic data available ( Kurniawan and Louvan , 2018 ) .", "entities": [[12, 14, "TaskName", "text summarization"]]}
{"text": "The use of synthetic data along with the real data is one of the popular approaches followed in machine translation domain for the low resource conditions to improve the translation quality ( Bojar and Tamchyna , 2011 ; Hoang et al , 2018 ; Chinea - R\u0131os et al , 2017 ) .", "entities": [[18, 20, "TaskName", "machine translation"]]}
{"text": "The iterative back - translation ( e.g. training back - translation systems multiple times ) were also found effective in machine translation ( Hoang et al , 2018 ) .", "entities": [[20, 22, "TaskName", "machine translation"]]}
{"text": "We explore similar approaches in our experiments for the text summarization task .", "entities": [[9, 11, "TaskName", "text summarization"]]}
{"text": "The organizations of this paper is as follows : Section 1 describes related work on abstract text summarization .", "entities": [[16, 18, "TaskName", "text summarization"]]}
{"text": "Across all experiments performed in this paper , we have used the Transformer model as implemented in OpenNMT - py 1 ( Vaswani et al , 2018 ; See et al , 2017 ) .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "The Transformer model is based on encoder / decoder architecture .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "We use German wiki data ( spread across different domain ) collected from the SwissText 2019 2 ( real data ) and Common Crawl 3 data ( synthetic data ) in our experiment .", "entities": [[22, 24, "DatasetName", "Common Crawl"]]}
{"text": "The data crawled from the Internet ( Common Crawl ) used to prepare synthetic data to boost the training .", "entities": [[7, 9, "DatasetName", "Common Crawl"]]}
{"text": "Step 2 : Sentence selection : The sentences from the Common Crawl data are selected with respect to the vocabulary based on the threshold we provide ( e.g. a sentence has 10 words and the threshold is 10 % ( 0.1 ) ) .", "entities": [[10, 12, "DatasetName", "Common Crawl"]]}
{"text": "Select random sentences ( e.g. 100 K ) from the selected Common Crawl data in the previous step .", "entities": [[11, 13, "DatasetName", "Common Crawl"]]}
{"text": "This section describes our experiments conducted for the text summarization task .", "entities": [[8, 10, "TaskName", "text summarization"]]}
{"text": "The Transformer model is implemented in OpenNMT - py .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "We use 3 settings : ( i ) real data ( we set this as the baseline in our experiment ) , ( ii ) real data and synthetic data , and ( iii ) real and regenerated synthetic data for the summarization task , described as follows : 1 . S1 : Transformer model using Train Real data In this setup , we use the \" Train Real \" data for training the Transformer model .", "entities": [[42, 43, "TaskName", "summarization"], [53, 54, "MethodName", "Transformer"], [74, 75, "MethodName", "Transformer"]]}
{"text": "In this setup , we use the \" Train RealSynth \" data for training the Transformer model .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "Transformer Model using Train RealSynthRegen data We propose an iterative approach to improve the quality of synthetic summaries .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "It allows the summarizer to fall back and copy the source text when encounters < unk > tokens by referencing to the softmax of the multiplication between attention scores of the output with the attention scores of the source ( See et al , 2017 ) .", "entities": [[22, 23, "MethodName", "softmax"]]}
{"text": "In the Middle Ages , the ship was under the name \" Vuolle \" 1 \" for the Finnish Navy 1 tion task under low resource condition , which helps improving the text summarization system in terms of automatic evaluation metrics .", "entities": [[32, 34, "TaskName", "text summarization"]]}
{"text": "As the next step , we plan to investigate : i ) synthetic summarization data , and ii ) applying transfer learning on text summarization for the multilingual low resource data set with little or no ground truth summaries ( Keneshloo et al , 2018 ) .", "entities": [[13, 14, "TaskName", "summarization"], [20, 22, "TaskName", "transfer learning"], [23, 25, "TaskName", "text summarization"]]}
{"text": "The work is supported by an innovation project ( under an InnoSuisse grant ) oriented to improve the automatic speech recognition and natural language understanding technologies for German .", "entities": [[18, 21, "TaskName", "automatic speech recognition"], [22, 25, "TaskName", "natural language understanding"]]}
{"text": "A Negative Case Analysis of Visual Grounding Methods for VQA", "entities": [[5, 7, "TaskName", "Visual Grounding"], [9, 10, "TaskName", "VQA"]]}
{"text": "Existing Visual Question Answering ( VQA ) methods tend to exploit dataset biases and spurious statistical correlations , instead of producing right answers for the right reasons .", "entities": [[1, 4, "DatasetName", "Visual Question Answering"], [5, 6, "TaskName", "VQA"]]}
{"text": "To address this issue , recent bias mitigation methods for VQA propose to incorporate visual cues ( e.g. , human attention maps ) to better ground the VQA models , showcasing impressive gains .", "entities": [[10, 11, "TaskName", "VQA"], [27, 28, "TaskName", "VQA"]]}
{"text": "However , we show that the performance improvements are not a result of improved visual grounding , but a regularization effect which prevents over - fitting to linguistic priors .", "entities": [[14, 16, "TaskName", "visual grounding"]]}
{"text": "Based on this observation , we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state - of - theart performance on VQA - CPv2 1 .", "entities": [[29, 30, "TaskName", "VQA"]]}
{"text": "Visual Question Answering ( VQA ) ( Antol et al , 2015 ) , the task of answering questions about visual content , was proposed to facilitate the development of models with human - like visual and linguistic understanding .", "entities": [[0, 3, "DatasetName", "Visual Question Answering"], [4, 5, "TaskName", "VQA"]]}
{"text": "However , existing VQA models often exploit superficial statistical biases to produce responses , instead of producing the right answers for the right reasons ( Kafle et al , 2019 ) .", "entities": [[3, 4, "TaskName", "VQA"]]}
{"text": "The VQA - CP dataset showcases this phenomenon by incorporating different question type / answer distributions in the train and test sets .", "entities": [[1, 4, "DatasetName", "VQA - CP"]]}
{"text": "To tackle this issue , recent works have endeavored to enforce proper visual grounding , where the goal is to make models produce answers by looking at relevant visual regions ( Gan et al , 2017 ; Selvaraju et al , Figure 1 : We find that existing visual sensitivity enhancement methods improve performance on VQA - CPv2 through regularization as opposed to proper visual grounding . 2019 ; Wu and Mooney , 2019 ) , instead of exploiting linguistic priors .", "entities": [[12, 14, "TaskName", "visual grounding"], [55, 56, "TaskName", "VQA"], [64, 66, "TaskName", "visual grounding"]]}
{"text": "We find that their improved accuracy does not actually emerge from proper visual grounding , but from regularization effects , where the model forgets the linguistic priors in the train set , thereby performing better on the test set .", "entities": [[5, 6, "MetricName", "accuracy"], [12, 14, "TaskName", "visual grounding"]]}
{"text": "Third , we show that these methods degrade performance when the priors remain intact and instead work on VQA - CPv2 by hurting its train accuracy .", "entities": [[18, 19, "TaskName", "VQA"], [25, 26, "MetricName", "accuracy"]]}
{"text": "Based on these observations , we hypothesize that controlled degradation on the train set al ows models to forget the training priors to improve test accuracy .", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "We find that this approach also achieves near state - of - the - art performance ( 48.9 % on VQA - CPv2 ) , providing further support for our claims .", "entities": [[20, 21, "TaskName", "VQA"]]}
{"text": "While we agree that visual grounding is a useful direction to pursue , our experiments show that the community requires better ways to test if systems are actually visually grounded .", "entities": [[4, 6, "TaskName", "visual grounding"]]}
{"text": "As expected of any real world dataset , VQA datasets also contain dataset biases ( Goyal et al , 2017 ) .", "entities": [[8, 9, "TaskName", "VQA"]]}
{"text": "The VQA - CP dataset was introduced to study the robustness of VQA methods against linguistic biases .", "entities": [[1, 4, "DatasetName", "VQA - CP"], [12, 13, "TaskName", "VQA"]]}
{"text": "Since it contains different answer distributions in the train and test sets , VQA - CP makes it nearly impossible for the models that rely upon linguistic correlations to perform well on the test set Shrestha et al , 2019 ) .", "entities": [[13, 16, "DatasetName", "VQA - CP"]]}
{"text": "VQA algorithms without explicit bias mitigation mechanisms fail on VQA - CP , so recent works have focused on the following solutions :", "entities": [[0, 1, "TaskName", "VQA"], [9, 12, "DatasetName", "VQA - CP"]]}
{"text": "Given a question Q and an image I , e.g. , represented by bottom - up region proposals : v ( Anderson et al , 2018 ) , a VQA model is tasked with predicting the answer a : P ( a | Q , I )", "entities": [[29, 30, "TaskName", "VQA"]]}
{"text": "Without additional regularization , existing VQA models such as the baseline model used in this work : UpDn ( Anderson et al , 2018 ) , tend to rely on the linguistic priors : P ( a | Q ) to answer questions .", "entities": [[5, 6, "TaskName", "VQA"]]}
{"text": "Such models fail on VQA - CP , because the priors in the test set differ from the train set .", "entities": [[4, 7, "DatasetName", "VQA - CP"]]}
{"text": "Is this actually due to better visual grounding ? 4", "entities": [[6, 8, "TaskName", "visual grounding"]]}
{"text": "Then , we analyze the regularization effects by evaluating the performance on VQA - CPv2 's train split ( Sec . 4.5 ) and the behavior on a dataset without changing priors ( Sec . 4.6 ) .", "entities": [[12, 13, "TaskName", "VQA"]]}
{"text": "We present a new metric to assess visual grounding in Sec . 4.7 and describe our regularization method in Sec .", "entities": [[7, 9, "TaskName", "visual grounding"]]}
{"text": "We compare the baseline UpDn model with HINT and SCR - variants trained on VQAv2 or VQA - CPv2 to study the causes behind the improvements .", "entities": [[16, 17, "TaskName", "VQA"]]}
{"text": "We assign random importance scores to the visual regions : S rand \u223c uniform ( 0 , 1 ) .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "Percentage of Overlaps : To further check if the variants trained on irrelevant or random regions gain performance in a manner similar to the models trained on relevant regions , we compute the overlap between their predictions on VQA - CPv2 's test set .", "entities": [[38, 39, "TaskName", "VQA"]]}
{"text": "We hypothesize that degrading performance on the train set helps forget linguistic biases , which in turn helps accuracy on VQA - CPv2 's test set but hurts accuracy on VQAv2 's val set .", "entities": [[18, 19, "MetricName", "accuracy"], [20, 21, "TaskName", "VQA"], [28, 29, "MetricName", "accuracy"]]}
{"text": "However , if we were to compare against the improvements in VQA - CPv2 in a fair manner , i.e. , only use the instances with visual cues while fine - tuning , then , the performance on VQAv2 drops continuously during the course of the training .", "entities": [[11, 12, "TaskName", "VQA"]]}
{"text": "This indicates that HINT and SCR help forget linguistic priors , which is beneficial for VQA - CPv2 but not for VQAv2 .", "entities": [[15, 16, "TaskName", "VQA"]]}
{"text": "In order to quantitatively assess visual grounding , we propose a new metric called : Correctly Predicted but Improperly Grounded ( CPIG ) : %", "entities": [[5, 7, "TaskName", "visual grounding"]]}
{"text": "The usage of visual cues and sensitivities in existing methods is superfluous because the results indicate that performance improves through degradation of training accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "We hypothesize that simple regularization that does not rely on cues or sensitivities can also achieve large performance gains for VQA - CP .", "entities": [[20, 23, "DatasetName", "VQA - CP"]]}
{"text": "It is also interesting to note that the drop in training accuracy is lower with this regularization scheme as compared to the state - of - the - art methods .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "While our results indicate that current visual grounding based bias mitigation approaches do not suffice , we believe this is still a good research direction .", "entities": [[6, 8, "TaskName", "visual grounding"]]}
{"text": "We recommend that both train and test accuracy be reported , because a model truly capable of visual grounding would not cause drastic drops in training accuracy to do well on the test sets .", "entities": [[7, 8, "MetricName", "accuracy"], [17, 19, "TaskName", "visual grounding"], [26, 27, "MetricName", "accuracy"]]}
{"text": "Another alternative is to use tasks that explicitly test grounding , e.g. , in visual query detection an agent must output boxes around any regions of a scene that match the natural language query ( Acharya et al , 2019a ) .", "entities": [[18, 19, "DatasetName", "agent"]]}
{"text": "Here , we showed that existing visual grounding based bias mitigation methods for VQA are not working as intended .", "entities": [[6, 8, "TaskName", "visual grounding"], [13, 14, "TaskName", "VQA"]]}
{"text": "We found that the accuracy improvements stem from a regularization effect rather than proper visual grounding .", "entities": [[4, 5, "MetricName", "accuracy"], [14, 16, "TaskName", "visual grounding"]]}
{"text": "We proposed a simple regularization scheme which , despite not requiring additional annotations , rivals state - of - theart accuracy .", "entities": [[20, 21, "MetricName", "accuracy"]]}
{"text": "Future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation .", "entities": [[1, 3, "TaskName", "visual grounding"]]}
{"text": "Following ( Selvaraju et al , 2019 ) , we train HINT on the subset with human - based attention maps ( Das et al , 2017 ) , which are available for 9 % of the VQA - CPv2 train and test sets .", "entities": [[37, 38, "TaskName", "VQA"]]}
{"text": "Our Zero - Out Regularizer Our regularization method , which is a binary cross entropy loss between the model predictions and a zero vector , does not use additional cues or sensitivities and yet achieves near state - of - the - art performance on VQA - CPv2 .", "entities": [[15, 16, "MetricName", "loss"], [45, 46, "TaskName", "VQA"]]}
{"text": "We set the learning rate to : 2\u00d710 \u22126", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}
{"text": "Since HINT trained on relevant cues obtains the highest correlation values , it does indicate improvement in visual grounding .", "entities": [[17, 19, "TaskName", "visual grounding"]]}
{"text": "Presentation of qualitative examples in visual grounding models for VQA suffers from confirmation bias i.e. , while it is possible to find qualitative samples that look at relevant regions to answer questions properly , it is also possible to find samples that produce correct answers without looking at relevant regions .", "entities": [[5, 7, "TaskName", "visual grounding"], [9, 10, "TaskName", "VQA"]]}
{"text": "We next present a quantitative assessment of visual grounding , which does not suffer from the confirmation bias .", "entities": [[7, 9, "TaskName", "visual grounding"]]}
{"text": "Table A4 shows VQA accuracy for each answer type on VQACPv2 's test set .", "entities": [[3, 4, "TaskName", "VQA"], [4, 5, "MetricName", "accuracy"]]}
{"text": "We hypothesize that the methods help forget linguistic priors , which improves test accuracy of such questions .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "Regularization effects caused by HINT / SCR and our method cause the models to weaken this prior i.e. , reduce the tendency to just predict ' no ' , which would increase accuracy at test because ' yes ' is more frequent in the test set .", "entities": [[32, 33, "MetricName", "accuracy"]]}
{"text": "Next , all of the methods perform poorly on ' Number ( Num ) ' answer type , showing that methods find it difficult to answer questions that are most reliant on correct visual grounding such as : localizing and counting objects .", "entities": [[33, 35, "TaskName", "visual grounding"]]}
{"text": "These results support our claims that it is possible to improve performance without actually performing visual grounding .", "entities": [[15, 17, "TaskName", "visual grounding"]]}
{"text": "The regularization effect of our loss is invariant with respect to the dataset size .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "The Hebrew Universal Dependency Treebank : Past , Present and Future", "entities": [[2, 5, "DatasetName", "Universal Dependency Treebank"]]}
{"text": "In this work we manually validate the UDv2 version of the HTB , and , according to our findings , we apply scheme changes that bring the UD HTB to the same theoretical grounds as the rest of UD .", "entities": [[27, 28, "DatasetName", "UD"], [38, 39, "DatasetName", "UD"]]}
{"text": "Our experimental parsing results with UDv2New confirm that improving the coherence and internal consistency of the UD HTB indeed leads to improved parsing performance .", "entities": [[16, 17, "DatasetName", "UD"]]}
{"text": "At the same time , our analysis demonstrates that there is more to be done at the point of intersection of UD with other linguistic processing layers , in particular , at the points where UD interfaces external morphological and lexical resources .", "entities": [[21, 22, "DatasetName", "UD"], [35, 36, "DatasetName", "UD"]]}
{"text": "This modest start , however , defined linguistic conventions and annotation principles that would continue to affect many treebank versions derived from the HTB for many years , including the universal dependencies ( UD ) HTB version .", "entities": [[30, 32, "DatasetName", "universal dependencies"], [33, 34, "DatasetName", "UD"]]}
{"text": "Bar - haim et al , 2008 ; Shacham and Wintner , 2007 ; Tsarfaty , 2006 ; Goldberg and Tsarfaty , 2008 ; Goldberg and Elhadad , 2009 ; Tsarfaty , 2010 ; Goldberg andElhadad , 2010 , 2011 ; More and Tsarfaty , 2016 ; More et al , In Press ) , but these models were trained on vastly different versions of the treebank , obeying different theories and annotation schemes , which then rendered the reported results mostly non - comparable . Hebrew dependency parsing presents an acute version of this syndrome .", "entities": [[87, 89, "TaskName", "dependency parsing"]]}
{"text": "Studies such as Goldberg and Elhadad ( 2011 ) , Tsarfaty et al ( 2012 ) , More et al ( In Press ) , as well as the SPMRL shared tasks ( Seddah et al , 2013 ( Seddah et al , , 2014 , all present attachment scores on Hebrew dependency parsing .", "entities": [[52, 54, "TaskName", "dependency parsing"]]}
{"text": "This is why the UD initiative comes as a blessing , not only for the cross - linguistic parsing community but also for the Hebrew NLP community - by presenting a unique opportunity to standardize the resources and metrics used for Hebrew parsing .", "entities": [[4, 5, "DatasetName", "UD"]]}
{"text": "This resulted in odd , and sometime plain wrong , dependency structures , with respect to the UD scheme .", "entities": [[17, 18, "DatasetName", "UD"]]}
{"text": "In this work we take the opportunity to validate the UDv2 HTB , by manually going through the published trees , identifying systematic errors or annotation inconsistencies , and locating cases where the annotated structures contradict the UD guidelines ( or spirit ) .", "entities": [[37, 38, "DatasetName", "UD"]]}
{"text": "We identified and corrected three main points of failure in the UD HTB : ( i ) the classification of argument types , deriving from the classification in the original HTB ( ii ) a mix - up of morphological and syntactic properties , where morphological features serve as syntactic sub - relations and vice versa , and ( iii ) a mix up of language - specific versus universal phenomena , where label sub - typing is exploited to indicate a supposedly language - specific phenomenon , which in fact has a designated universal label elsewhere .", "entities": [[11, 12, "DatasetName", "UD"]]}
{"text": "We use UDv2 and UDv2New to train a morphosyntactic parser ( More et al , In Press ) and provide baseline results on Hebrew UD parsing , in both ideal and realistic scenarios .", "entities": [[24, 25, "DatasetName", "UD"]]}
{"text": "Lessons learned from our empirical analysis concern the systematic organization of natural language grammar in UD , and in particular ( i ) the need to standardize the interface of UD treebanks to external morphological and lexical resources , and ( ii ) the need to organize the form - function mapping in a language - specific vs. family - specific vs. strictly - universal relations taxonomy , within and across treebanks .", "entities": [[15, 16, "DatasetName", "UD"], [30, 31, "DatasetName", "UD"]]}
{"text": "Following the first treebanking efforts , in English ( Marcus et al , 1993 ) , Chinese ( Xue et al , 2005 ) , and Arabic ( Maamouri and Bies , 2004 ) , and with the surge of interest in developing statistical , broad - coverage , parsing models , Sima'an et al ( 2001 ) introduced a pilot treebanking study and a Hebrew treebank ( HTB ) , which included 500 sentences from the Hebrew newspaper ha'aretz , morphologically segmented and morpho - syntactically annotated with part - of - speech tags , morphological features , and labeled phrase - structure trees .", "entities": [[89, 92, "DatasetName", "part - of"]]}
{"text": "+ ATH ( you ) , EM ( with )", "entities": [[6, 7, "MetricName", "EM"]]}
{"text": "The POS tags labeling scheme in the HTB includes quite a few changes from PTB , including the addition of special tags lexicalizing important functional elements in Hebrew : AT ( for the accusative marker ) , H ( the definite article ) , POSS ( the possesive marker ) , and HAM ( the yes / no question marker ) .", "entities": [[14, 15, "DatasetName", "PTB"], [52, 53, "DatasetName", "HAM"]]}
{"text": "In addition , the HTB introduces the NNT , JJT , CDT labels , marking the constructstate variants of NN , JJ , CD in the PTB , and a specific tag MOD that tags modifier words which is neither an adjective nor an adverb .", "entities": [[26, 27, "DatasetName", "PTB"], [32, 33, "DatasetName", "MOD"]]}
{"text": "The syntactic labels in the phrase structure trees of the HTB were adopted from the Penn Treebank ( PTB ) almost as is , with the addition of a PREDP label for marking verbless predicates .", "entities": [[15, 17, "DatasetName", "Penn Treebank"], [18, 19, "DatasetName", "PTB"]]}
{"text": "The syntactic trees themselves looked superficially like the PTB but they differ in several aspects .", "entities": [[8, 9, "DatasetName", "PTB"]]}
{"text": "To overcome this , Tsarfaty ( 2010 ) devised a set of rules based on the daughter - dependencies , function tags and empty elements , to automatically derive the relationalrealizational ( RR ) version of the HTB .", "entities": [[32, 33, "DatasetName", "RR"]]}
{"text": "In the RR HTB , each node is marked with its relational network ( an unordered set of grammatical functions ) mapped to the ordered syntactic constituents .", "entities": [[2, 3, "DatasetName", "RR"]]}
{"text": "The RR HTB retained the morphological conventions and core non - core distinction of the original HTB .", "entities": [[1, 2, "DatasetName", "RR"]]}
{"text": "In a parallel effort , and with the surge of interest in dependency parsing ( Buchholz and Marsi , 2006 ; Nivre et al , 2007 ) , 6 Goldberg ( 2011 ) automatically converted the HTB into its first , unlabeled , dependency version .", "entities": [[12, 14, "TaskName", "dependency parsing"]]}
{"text": "Based on this version , Goldberg and Elhadad ( 2009 ) presented the first Hebrew dependency parsing results , only unlabeled attachment scores ( UAS ) at this point .", "entities": [[15, 17, "TaskName", "dependency parsing"]]}
{"text": "Tsarfaty used the U - SD labels to edit three versions of the HTB : ( i ) to mark the original phrasestructure trees in the HTB with the labels as dashfeatures , ( ii ) to relabel the relational networks in RR trees with U - SD labels , and ( iii ) to derive a labeled dependencies version of the HTB .", "entities": [[42, 43, "DatasetName", "RR"]]}
{"text": "3 The Hebrew UD Treebank", "entities": [[3, 4, "DatasetName", "UD"]]}
{"text": "The RR version of the Unified - SD HTB provided the basis for automatically converting the Hebrew trees into UDv1 trees .", "entities": [[1, 2, "DatasetName", "RR"]]}
{"text": "The UD HTB assumes the same segmentation principles as the first edition of the HTB , segmenting off prefixes and suffixes , with the addition of splitting off genitive pronominal clitics from nouns .", "entities": [[1, 2, "DatasetName", "UD"]]}
{"text": "Goldberg and Tsarfaty ( 2014 ) devised an automatic process that chooses a lexical head in each relational network of each constituent in the RR treebank .", "entities": [[24, 25, "DatasetName", "RR"]]}
{"text": "They also mapped the fine - grained POS categories to the coarse - grained UPOS categories in UD , and remaining POS distinctions in HebLex ( HebBinyan , construct - states , etc . ) are stored in FEATS .", "entities": [[17, 18, "DatasetName", "UD"]]}
{"text": "The label set of U - SD was automatically mapped to UD , and relations from U - SD outside of UD were kept as relation : subtype .", "entities": [[11, 12, "DatasetName", "UD"], [21, 22, "DatasetName", "UD"]]}
{"text": "The converted HTB is documented on the UD webpage .", "entities": [[7, 8, "DatasetName", "UD"]]}
{"text": "In order to validate the current UDv2 trees , we reviewed the list of UD POS tags , relation labels and features , and for each of these items we identified the dependency structures in the HTB dev set that contain them .", "entities": [[14, 15, "DatasetName", "UD"]]}
{"text": "At this point , for each item , a linguist characterized the role such item actually fulfills in the Hebrew grammatical structures , ( as opposed to the role it was designed to fulfill in the UD scheme ) .", "entities": [[36, 37, "DatasetName", "UD"]]}
{"text": "That is : linguistically adequate , typologically adequate , suitable for rapid , consistent annotation , suitable for parsing with high accuracy , easily comprehended by non - linguists , and provides good support for downstream NLP tasks .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "Some UD definitions stand in clear contrast with the canonical syntactic analysis of Hebrew .", "entities": [[1, 2, "DatasetName", "UD"]]}
{"text": "In line with Netzer et al s conclusion , these are tagged as AUX in the UD HTB .", "entities": [[16, 17, "DatasetName", "UD"]]}
{"text": "The automatic conversion to UD has kept fine - grained labels as subrelations , resulting with the language - specific label acl : inf .", "entities": [[4, 5, "DatasetName", "UD"]]}
{"text": "Since the UD guidelines permit infinitive structures in acl , it is unnecessary to mark infinity as a sub - relation .", "entities": [[2, 3, "DatasetName", "UD"]]}
{"text": "As UD aspires to present a set of tags which are relevant to as many languages as possible , natu - 11 All analyses are visualized in the supp .", "entities": [[1, 2, "DatasetName", "UD"]]}
{"text": "To allow representation of these , the UD scheme allows for sub - relations in the form of relation : subtype , as exemplified above .", "entities": [[7, 8, "DatasetName", "UD"]]}
{"text": "As a result , the following subtypes were reduced to their parent relation : ( i ) det : quant , originally marking an arbitrary subset of existential quantifiers , was reduced to simply det , and ( ii ) advmod : phrase , originally marking multi - word adverbials , were re - structured as advmod+fixed , in line with the UD guidelines for multi - word - expressions .", "entities": [[17, 18, "DatasetName", "det"], [34, 35, "DatasetName", "det"], [62, 63, "DatasetName", "UD"]]}
{"text": "From conj : discourse to parataxis An interesting case is with labels not used at all in the older versions of the UD HTB , while language - specific labels stand to mark their function .", "entities": [[22, 23, "DatasetName", "UD"]]}
{"text": "The UD label parataxis , for instance , describes a relation between two ( or more ) sentences which are syntactically independent ( i.e. do not stand in subordination or conjunction relation to one another ) , but are thematically connected , and consequently punctuated as the same sentence .", "entities": [[1, 2, "DatasetName", "UD"]]}
{"text": "In our revised version , we comply with UD guidelines and label this relation ' parataxis ' .", "entities": [[8, 9, "DatasetName", "UD"]]}
{"text": "Here we report its performance on the UD HTB . Scenarios : Because of its rich morphology and orthographic convention to attach or fuse adpositions and pronominals onto open - class categories , there is severe ambiguity in the morphological analysis of the Hebrew input tokens .", "entities": [[7, 8, "DatasetName", "UD"], [39, 41, "TaskName", "morphological analysis"]]}
{"text": "Hence , it is unknown upfront how many morphemes ( in the HTB terminology ) or syntactic words ( in the UD terminology ) are in the space - delimited tokens .", "entities": [[21, 22, "DatasetName", "UD"]]}
{"text": "We examine two kinds of scenarios : ideal : assuming gold morphological analysis and disambiguation given by an oracle .", "entities": [[11, 13, "TaskName", "morphological analysis"]]}
{"text": "realistic : assuming automatically predicted morphological analysis and disambiguation .", "entities": [[5, 7, "TaskName", "morphological analysis"]]}
{"text": "We use yap for predicting morphological analysis ( MA ) and morphological disambiguation ( More , 2016 ) , and we contrast the use of a data - driven lexicon baselinelex with an external broad - coverage lexicon HebLex .", "entities": [[5, 7, "TaskName", "morphological analysis"], [11, 13, "TaskName", "morphological disambiguation"]]}
{"text": "However , the correct morphological disambiguation is guaranteed to be one of the morphological MA provided to the system as input .", "entities": [[4, 6, "TaskName", "morphological disambiguation"]]}
{"text": "Table 3 shows the parsing results in realistic scenarios , where we assume automatically predicted morphological analysis and disambiguation .", "entities": [[15, 17, "TaskName", "morphological analysis"]]}
{"text": "To test this , we execute an infused scenario where the morphological analysis lattices are guaranteed to also include the correct analysis .", "entities": [[11, 13, "TaskName", "morphological analysis"]]}
{"text": "Our endeavor here has been to manually verify the current version of the UD HTB resulting analyses , and to correct lingering errors .", "entities": [[13, 14, "DatasetName", "UD"]]}
{"text": "We conjecture that in order to obtain decent performance , the work on the treebank should be complemented by adapting language - specific lexica to the set of guidelines for word segmentation and for representing morphology , as defined by UD .", "entities": [[40, 41, "DatasetName", "UD"]]}
{"text": "Even when external lexica assumes the same labeling scheme as UD , gaps between the theories underlying the development of these resources could lead to lack of coverage that substantially harms parsing performance .", "entities": [[10, 11, "DatasetName", "UD"]]}
{"text": "Additional lessons learned from our manual verification process have to do with the organization of morphological features and syntactic subtypes within the HTB and in the UD treebanks collection in general .", "entities": [[26, 27, "DatasetName", "UD"]]}
{"text": "We argue that clearer guidelines are needed in the general UD scheme , instructing directly what kind of linguistic information should go where , by which formal means .", "entities": [[10, 11, "DatasetName", "UD"]]}
{"text": "An example to this is the feature HebBinyan in the UD HTB , which stores the value of the morphological template of the verb .", "entities": [[10, 11, "DatasetName", "UD"]]}
{"text": "We propose that the next major revision of the UD treebank scheme could ideally focus on the universal organization of the grammar , and will center around these themes : subtypes : A universal inventory and management of the sub - label system which will define what linguistic phenomena can count as subtype of a label , and will maintain crosslinguistic consistency in its use for shared phenomena .", "entities": [[9, 10, "DatasetName", "UD"]]}
{"text": "lexical resources : For languages that have external lexica , especially in the case of morphologically rich and resource scarce languages , an effort is needed to verify that the labeling scheme theoretical guidelines underlying lexica are harmonized with the UD guidelines .", "entities": [[40, 41, "DatasetName", "UD"]]}
{"text": "Such lexica can be made available via the CoNLL - UL format ( More et al , 2018 ) to benefit the entire UD community .", "entities": [[23, 24, "DatasetName", "UD"]]}
{"text": "semantic applications : in addition to aligning lexical resources , it is important to advance the usability of UD in down - stream application scenarios , by making available the additional layer of enhanced dependencies .", "entities": [[18, 19, "DatasetName", "UD"]]}
{"text": "In this work we manually verified the recent UD HTB version and corrected lingering errors .", "entities": [[8, 9, "DatasetName", "UD"]]}
{"text": "Our future plans include a comprehensive revision of the lexical and morphological resources associated with the UD scheme , to improve the empirical parsing results in realistic scenarios , and the addition of enhanced dependencies , which would be more adequate for downstream semantic tasks .", "entities": [[16, 17, "DatasetName", "UD"]]}
{"text": "To enrich language models with some of this missing experience , we leverage two sources of information : ( 1 ) the Lancaster Sensorimotor norms , which provide ratings ( means and standard deviations ) for over 40 , 000 English words along several dimensions of embodiment , and which capture the extent to which something is experienced across 11 different sensory modalities , and ( 2 ) vectors from coefficients of binary classifiers trained on images for the BERT vocabulary .", "entities": [[79, 80, "MethodName", "BERT"]]}
{"text": "We pre - trained the ELECTRA model and fine - tuned the RoBERTa model with these two sources of information then evaluate using the established GLUE benchmark and the Visual Dialog benchmark .", "entities": [[5, 6, "MethodName", "ELECTRA"], [12, 13, "MethodName", "RoBERTa"], [25, 26, "DatasetName", "GLUE"], [29, 31, "TaskName", "Visual Dialog"]]}
{"text": "Children learn their first spoken language in a highly interactive setting where generally the first words children learn are concrete words that denote physical objects , which is an important developmental step in child first language acquisition ( Kuperman et al , 2012a ; McCune , 2008 ; Clark , 2013 ) .", "entities": [[35, 37, "TaskName", "language acquisition"]]}
{"text": "In particular , what does this mean for language models that are trained purely on text ( likely largely written by adults ) , such as BERT ( Devlin et al , 2018 ) or GPT - 3 ?", "entities": [[26, 27, "MethodName", "BERT"], [35, 36, "MethodName", "GPT"]]}
{"text": "In this paper , we contribute to a growing body of recent work that attempts to addresses these limitations by ( 1 ) leveraging multimodal and sensorimotor knowledge of the Lancaster Sensorimotor Norms ( Lynott et al , 2019 ) and ( 2 ) using vectorized representations of images by treating both ( 1 ) and ( 2 ) as embeddings of language models for GLUE and Visual Dialog benchmarks .", "entities": [[65, 66, "DatasetName", "GLUE"], [67, 69, "TaskName", "Visual Dialog"]]}
{"text": "We explore how these embeddings can be used to enrich the ELEC - TRA language model 's pre - training and fine - tuning , and evaluate on the GLUE benchmark ( Experiment 1 , Section 4 ) , and how they can be used to replace input embeddings for a pre - trained RoBERTa model for the Visual Dialog task ( Experiment 2 , Section 5 ) .", "entities": [[29, 30, "DatasetName", "GLUE"], [54, 55, "MethodName", "RoBERTa"], [58, 60, "TaskName", "Visual Dialog"]]}
{"text": "However , this only works if an agent that has learned the language has the knowledge of horses , black , white , stripes , and vertical concepts - i.e. , via direct experience , not just through linguistic exposure or encyclopedic definitions .", "entities": [[7, 8, "DatasetName", "agent"]]}
{"text": "Rogers et al ( 2020 ) provides a recent primer and overview of research that has attempted to uncover strengths and weaknesses of BERT and related language models ( so - called BERTology ) .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "This criticism is born out in Forbes et al ( 2019 ) which showed that BERT can guess affordances and properties of objects because that information can be found in text ( e.g. , a typical chair has the affordance of being sittable , and a property of having legs ) , but has no notion of how objects are related semantically to each other , and Da and Kasai ( 2019 ) further showed that real - world perceptual properties are likely to be assumed instead of inferred .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "Furthermore , Bender and Koller ( 2020 ) make a strong case that BERT learns form instead of meaning , and while the fact that BERT performs so well on many tasks is difficult to dispute , models trained on text are missing semantic information crucial for holistic language understanding .", "entities": [[13, 14, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"]]}
{"text": "Since before BERT which has proven powerful in many language processing tasks , efforts have been made to encode multimodal ( i.e. , more than just text as a learning modality ) information into embeddings and language models ( Takano and Utsumi , 2016 ; Kiros et al , 2014 ; Zellers et al , 2021 ) and recent , continued efforts towards bridging grounded visual representations to distributional representations of word meanings give credence to the claim that text - only models like BERT are missing crucial semantic information because enriching BERT with visual information improves performance in several known tasks ( Kim et al , 2019 ;", "entities": [[2, 3, "MethodName", "BERT"], [84, 85, "MethodName", "BERT"], [92, 93, "MethodName", "BERT"]]}
{"text": "Auditory - sound ; ping Gustatory - having to do with eating ; cream Haptic - muscle movement ; handshake Interoceptive - having to do with affect or emotion ; headache Olfactory - smell ; incense Visual - visual ; barcode Foot - leg - haptics for foot / leg ; run Hand - arm - haptics for hand / arm ; pointing Head - having to do with the head ; eye Mouth - haptics for mouth ; kiss Torso - haptics for torso ; breath Max - strength.perceptual - the highest rating across the 11 sensorimotor dimensions Minkowski3.perceptual - treating the 11 modalities as a vector , this represents the distance of the vector from the origin with influence of weaker dimensions attenuated Exclusivity.perceptual - the extent to which a concept ( out of the 11 ) which is experienced through a single perceptual modalitiy The last three can be seen as aggregates from the 11 modalities ; they also have .action values representing the extent to which a concept is experienced as an action ( as opposed to .perceptual ) , and .sensorimotor values representing the extent a concept is experience as sensorimotor .", "entities": [[28, 29, "DatasetName", "emotion"]]}
{"text": "We normalize each value in the vector independently to a value between 0 - 1 by dividing each value over its max value .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "We follow Kiros et al ( 2018 ) and use Google Image Search to find images using the BERT vocabulary , resulting in 27 , 152 words and corresponding images ( some words did not result in images , and we did not download images for filler words ) .", "entities": [[10, 11, "DatasetName", "Google"], [18, 19, "MethodName", "BERT"]]}
{"text": "We then follow Schlangen et al ( 2016 ) and process each image by passing them through the recent CLIP ( Jia et al , 2021 )", "entities": [[19, 20, "MethodName", "CLIP"]]}
{"text": "convolutional neural network ( trained on ImageNet , using CLIP 's ViT - B/32 model ) , yielding a vector of size 512 for each image .", "entities": [[6, 7, "DatasetName", "ImageNet"], [9, 10, "MethodName", "CLIP"]]}
{"text": "We then use a logistic regression classifier ( C=0.25 , max iterations=1000 ) , one for each word , trained on the images for each word .", "entities": [[4, 6, "MethodName", "logistic regression"]]}
{"text": "4 Experiment 1 : Tying embedding weights and pre - training ELECTRA , fine - tuning on GLUE In this experiment , and crucially for our ongoing work that aligns with child - inspired language acquisition , we use ELECTRA", "entities": [[11, 12, "MethodName", "ELECTRA"], [17, 18, "DatasetName", "GLUE"], [34, 36, "TaskName", "language acquisition"], [39, 40, "MethodName", "ELECTRA"]]}
{"text": "Task & Procedure Wang et al ( 2018 ) introduced the GLUE benchmark which consists of nine English sentence understanding tasks covering several domains ( e.g. , movie reviews and online question answering ) .", "entities": [[11, 12, "DatasetName", "GLUE"], [31, 33, "TaskName", "question answering"]]}
{"text": "2 Our aim is to achieve improved results over the text - only baseline with a specified number of training steps using the openwebtext data for training .", "entities": [[23, 24, "DatasetName", "openwebtext"]]}
{"text": "We only report the results for the MRPC ( a paraphrase task that uses accuracy and f1 metrics ) , COLA ( a grammatical acceptibility task ; uses Matthew 's Correlation ) , and WNLI ( ambiguity resolution ; uses an accuracy metric ) tasks because they are sufficient to illustrate the utility of our method when applied to ELECTRA .", "entities": [[7, 8, "DatasetName", "MRPC"], [14, 15, "MetricName", "accuracy"], [20, 21, "MethodName", "COLA"], [34, 35, "DatasetName", "WNLI"], [41, 42, "MetricName", "accuracy"], [59, 60, "MethodName", "ELECTRA"]]}
{"text": "To give ELECTRA knowledge about additional modalities from the Lancaster and WAC vectors , we tie the vectors to the the weights of the generator and discriminator of ELECTRA depicted in Figure 2 , and vary whether the embeddings are frozen or not during pre - training , then train for 100 , 000 steps .", "entities": [[2, 3, "MethodName", "ELECTRA"], [28, 29, "MethodName", "ELECTRA"]]}
{"text": "As the WAC vectors were larger than ELECTRA 's expected embedding size of 128 , we applied UMAP to reduce the dimensionality to 128 ; similarly for the WAC and Lancaster concatenated embeddings .", "entities": [[7, 8, "MethodName", "ELECTRA"]]}
{"text": "For Lancaster vectors , we set the ELECTRA embedding size to 39 .", "entities": [[7, 8, "MethodName", "ELECTRA"]]}
{"text": "For a broader comparison , we also compared to GloVE ( Pennington et al , 2014 ) and several ablations where we concatenate multimodal vectors with the GloVe vectors ( we used the evaluation script for GloVE provided by Wang et al ( 2018 ) ) .", "entities": [[27, 28, "MethodName", "GloVe"]]}
{"text": "We also use the same training and evaluation regime for the WAC and Lancaster vectors , and a concatenation of the two , on their own treating them as word - level embeddings similar to GloVe .", "entities": [[35, 36, "MethodName", "GloVe"]]}
{"text": "Results Table 1 shows the results on the GLUE benchmark .", "entities": [[8, 9, "DatasetName", "GLUE"]]}
{"text": "The word - level embeddings of GloVe , WAC , and Lancaster are shown in the top 5 rows of the table .", "entities": [[6, 7, "MethodName", "GloVe"]]}
{"text": "Of note is a significant advantage of using the Lancaster vectors alone compared to using any other embedding or combination for the WNLI task which is co - reference and natural language inference for fiction books .", "entities": [[22, 23, "DatasetName", "WNLI"], [30, 33, "TaskName", "natural language inference"]]}
{"text": "Interestingly , the best performing model for COLA was GloVE and Lancaster word - level embeddings ; COLA is a grammaticality test , which is important in language understanding , but arguably less critical in early - stage child language acquisition .", "entities": [[7, 8, "MethodName", "COLA"], [17, 18, "MethodName", "COLA"], [39, 41, "TaskName", "language acquisition"]]}
{"text": "All other rows show the ELECTRA baseline and ELECTRA that uses some variation of WAC , Lancaster , or both as embeddings ( denoted with the EL - prefix ) .", "entities": [[5, 6, "MethodName", "ELECTRA"], [8, 9, "MethodName", "ELECTRA"]]}
{"text": "The bottom part of the table compares ELECTRA with a variant of ELECTRA that uses WAC embeddings ( both with and without freezing the embedding weights ) , ELECTRA with lancaster embeddings and ELECTRA with WAC embeddings concatenated with the Lancaster embeddings ( where the length of the WAC embeddings plus the size of ELECTRA is 128 ) .", "entities": [[7, 8, "MethodName", "ELECTRA"], [12, 13, "MethodName", "ELECTRA"], [28, 29, "MethodName", "ELECTRA"], [33, 34, "MethodName", "ELECTRA"], [54, 55, "MethodName", "ELECTRA"]]}
{"text": "Contrary to our hypothesis , we observe that when ELECTRA uses WAC with frozen weights , the performance on the benchmark performs better than all others , including the ELECTRA baseline .", "entities": [[9, 10, "MethodName", "ELECTRA"], [19, 21, "DatasetName", "the benchmark"], [29, 30, "MethodName", "ELECTRA"]]}
{"text": "This could suggest that ELECTRA can make effective use of the visual and Lancaster embeddings by adjusting weights in the other layers of the model .", "entities": [[4, 5, "MethodName", "ELECTRA"]]}
{"text": "The EL - lan - wac variant performed well above the ELECTRA baseline , substantiating the hypothesis that enriching the model with multimodal knowledge can improve results .", "entities": [[11, 12, "MethodName", "ELECTRA"]]}
{"text": "Taken together , we find the results encouraging because the relatively short training regime still yielded respectable results , suggesting that ELECTRA with a visual or other multimodal embedding can be useful with less training as is the case when children learn language .", "entities": [[21, 22, "MethodName", "ELECTRA"]]}
{"text": "In this experiment , we use an evaluation that requires knowledge of the visual world by evaluating the Lancaster and WAC vectors on the Visual Dialog task , termed visdial .", "entities": [[24, 26, "TaskName", "Visual Dialog"], [29, 30, "DatasetName", "visdial"]]}
{"text": "In this experiment , we evaluate using a fully pre - trained RoBERTa model by replacing its embeddings with the WAC and Lancaster vectors .", "entities": [[12, 13, "MethodName", "RoBERTa"]]}
{"text": "Task Following Murahari et al ( 2019 ) , given an image , dialogue history consisting of questionanswer pairs , and a follow - up question about the image , the task of visdial is to predict a free - form natural language answer to the question .", "entities": [[33, 34, "DatasetName", "visdial"]]}
{"text": "The visdial dataset introduced in Das et al ( 2019 ) also includes evaluation metrics and human - annotated answers to the natural language queries about the image .", "entities": [[1, 2, "DatasetName", "visdial"]]}
{"text": "Metrics We report the following metrics : R@1 Rate of times the top - ranked answer is a correct one ; i.e. , accuracy .", "entities": [[7, 8, "MetricName", "R@1"], [23, 24, "MetricName", "accuracy"]]}
{"text": "R@5", "entities": [[0, 1, "MetricName", "R@5"]]}
{"text": "MRR Mean Reciprocal Rank is the multiplicative inverse of the rank of the first correct answer .", "entities": [[0, 1, "MetricName", "MRR"]]}
{"text": "Baseline and Procedure We report the values for the model described in Murahari et al ( 2019 ) for our baseline - work which builds on VilBERT ( Lu et", "entities": [[26, 27, "MethodName", "VilBERT"]]}
{"text": "Murahari et al ( 2019 ) adapted the VilBERT model for the visdial task by using a pre - trained language model trained on English Wikipedia and the BooksCorpus ( Zhu et al , 2015 ) using masked language modeling and next sentence prediction losses .", "entities": [[8, 9, "MethodName", "VilBERT"], [12, 13, "DatasetName", "visdial"], [37, 40, "TaskName", "masked language modeling"]]}
{"text": "They then frame the task as a next - sentence prediction task ( whereas the original VilBERT was modeled to generate descriptions of images ) .", "entities": [[16, 17, "MethodName", "VilBERT"]]}
{"text": "They then use the Conceptual Captions ( Sharma et al , 2018 ) and Visual Question Answering ( VQA ) ( Antol et al , 2015 ) datasets ( using masked image region , masked language modeling , and next sentence prediction losses ) to train the two transformers .", "entities": [[4, 6, "DatasetName", "Conceptual Captions"], [14, 17, "DatasetName", "Visual Question Answering"], [18, 19, "TaskName", "VQA"], [34, 37, "TaskName", "masked language modeling"]]}
{"text": "They then fine - tune on the visdial task ( also using masked image region , masked language modeling , and next sentence prediction losses ) .", "entities": [[7, 8, "DatasetName", "visdial"], [16, 19, "TaskName", "masked language modeling"]]}
{"text": "The underlying architecture uses a pre - trained BERT language model ( i.e. , bert - base - uncased ) as a starting point before training on the Wikipedia , BooksCorpus , Conceptual Captions , and VQA datasets .", "entities": [[8, 9, "MethodName", "BERT"], [32, 34, "DatasetName", "Conceptual Captions"], [36, 37, "TaskName", "VQA"]]}
{"text": "We altered their architecture by replacing the RoBERTa pre - trained embedding layer with the Lancaster and WAC vectors , as depicted in Figure 3 .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}
{"text": "We then fine - tuned on the visdial task using their training regime .", "entities": [[7, 8, "DatasetName", "visdial"]]}
{"text": "Vocabulary : RoBERTa & AoA", "entities": [[2, 3, "MethodName", "RoBERTa"]]}
{"text": "To explore if RoBERTa could make use of a WAC embedding that uses words that are more aimed at a child vocabulary , we report results of filtering out words not in the the Age - of - Acquisition ( AoA ) list ( Kuperman et al , 2012b ) .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}
{"text": "Lancaster vectors Similar to AoA , the Lancaster Norms has a predefined vocabulary , which , when compared to the RoBERTa vocabulary results in 11 , 402 words in both .", "entities": [[20, 21, "MethodName", "RoBERTa"]]}
{"text": "For each word in the RoBERTa vocabulary that was also in the Lancaster norms , we replaced the RoBERTa embedding with the Lancaster vector for that word ; otherwise words retained the original RoBERTa embedding .", "entities": [[5, 6, "MethodName", "RoBERTa"], [18, 19, "MethodName", "RoBERTa"], [33, 34, "MethodName", "RoBERTa"]]}
{"text": "As their model expects vectors of size 768 ( the embedding size for RoBERTa ) , but the Lancaster vectors are only size 39 , we padded the rest of the vector with zeros .", "entities": [[13, 14, "MethodName", "RoBERTa"]]}
{"text": "WAC vectors We use the vocabulary from the RoBERTa tokenizer as with the Lancaster Vectors , which results in a a 27 , 152 - word overlap with the WAC vectors .", "entities": [[8, 9, "MethodName", "RoBERTa"]]}
{"text": "As the WAC vectors have a dimensionality of 513 , smaller than the required size of RoBERTa 's 768 , we padded zeros after each vector .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}
{"text": "6 Compared to Experiment 1 with the GLUE benchmark , the approach taken in this section fundamentally changes how the Lancaster and WAC embeddings are applied to RoBERTa ; here the Lancaster and WAC embeddings are used on a pre - trained model .", "entities": [[7, 8, "DatasetName", "GLUE"], [27, 28, "MethodName", "RoBERTa"]]}
{"text": "no freeze MRR R@1 R@5 NDCG pothesize that RoBERTa will improve with WAC embeddings , as well as the Lancaster concatenated to WAC ( denoted lanwac ) , though the Lancaster embedding on its own may be too small to make a difference .", "entities": [[2, 3, "MetricName", "MRR"], [3, 4, "MetricName", "R@1"], [4, 5, "MetricName", "R@5"], [8, 9, "MethodName", "RoBERTa"]]}
{"text": "As words that are learned earlier in a child 's life are generally more concrete , we hypothesize that RoBERTa will improve when WAC only uses words from the AoA data as more abstract terms are represented by zero vectors .", "entities": [[19, 20, "MethodName", "RoBERTa"]]}
{"text": "Results Table 2 shows the results for the visdial task .", "entities": [[8, 9, "DatasetName", "visdial"]]}
{"text": "Though it is clear that RoBERTa is doing the heavy lifting , when added to RoBERTa , the Lancaster and WAC vectors show improvements over the RoBERTa baseline for some metrics .", "entities": [[5, 6, "MethodName", "RoBERTa"], [15, 16, "MethodName", "RoBERTa"], [26, 27, "MethodName", "RoBERTa"]]}
{"text": "For cases where the Lancaster and WAC models yield better performance , these results suggest that a pre - trained language model can make use of adding multimodal knowledge in the form of vectors derived from multimodal knowledge ( Lancaster ) and visual ( WAC ) for the visdial task .", "entities": [[48, 49, "DatasetName", "visdial"]]}
{"text": "RoBERTA that uses the WAC embedding especially shows respectable results in the visdial task , particularly when the embedding uses the AoA vocabulary ( we only considered AoA for WAC because WAC peformed better than lanwac in this experiment ) .", "entities": [[12, 13, "DatasetName", "visdial"]]}
{"text": "The main contribution of this paper is to explore using the Lancaster Sensorimotor Norms and the Words - as - Classifiers model as vectorized knowledge from the physical world on the GLUE and Visual Dialog tasks .", "entities": [[31, 32, "DatasetName", "GLUE"], [33, 35, "TaskName", "Visual Dialog"]]}
{"text": "Lancaster norms performed well on their own in one GLUE task compared to other word embeddings like GloVe , and coupled with the WAC vectors as the embedding in an ELECTRA model , they performed respectably on the GLUE task .", "entities": [[9, 10, "DatasetName", "GLUE"], [14, 16, "TaskName", "word embeddings"], [17, 18, "MethodName", "GloVe"], [30, 31, "MethodName", "ELECTRA"], [38, 39, "DatasetName", "GLUE"]]}
{"text": "The WAC vectors , when used as embeddings in the RoBERTa model performed well on the Visual Dialog task , particularly when the vocabulary was more restricted to the Age of Acquisition vocabulary .", "entities": [[10, 11, "MethodName", "RoBERTa"], [16, 18, "TaskName", "Visual Dialog"]]}
{"text": "Moreover , standard language models can not actually identify denotations when they are present ; i.e. , ELECTRA and RoBERTa are not actually capable of determining if an object is red or soft from observing that object - a basic ability for a language learning child - simply because those models can not observe the world outside of text , though the purpose of the WAC ( and models like VilBERT ) model is to do just that : identify denotations ; by coupling WAC with ELECTRA and RoBERTa , both models can make use of that capability .", "entities": [[17, 18, "MethodName", "ELECTRA"], [19, 20, "MethodName", "RoBERTa"], [70, 71, "MethodName", "VilBERT"], [86, 87, "MethodName", "ELECTRA"], [88, 89, "MethodName", "RoBERTa"]]}
{"text": "In particular , our knowledge from this paper informs us that the ELECTRA model with embeddings tied to WAC classifier weights is a good candidate for live interaction of a robot that is learning words from a human collaborator because the ELECTRA - WAC model can function with small amounts of data and the embedding layer can successfully be tied to weights of the WAC classifiers .", "entities": [[12, 13, "MethodName", "ELECTRA"], [41, 42, "MethodName", "ELECTRA"]]}
{"text": "Model predictions are compared with the sentence completion data obtained from Hindi native speakers .", "entities": [[6, 8, "TaskName", "sentence completion"]]}
{"text": "In this section we summarize the key results of a recent study by Apurva and Husain ( 2020 ) who investigated the nature of verbal prediction in Hindi using a series sentence completion studies .", "entities": [[31, 33, "TaskName", "sentence completion"]]}
{"text": "Apurva and Husain ( 2020 ) used the sentence completion paradigm ( Taylor , 1953 ) to probe the nature of clause final verbal prediction when differing the number of preverbal nouns that precede the tobe - completed target verb .", "entities": [[8, 10, "TaskName", "sentence completion"]]}
{"text": "Moreover , as the sentence completion experiment included only animate nouns in various items ( see Section 2 ) , we use an additional animacy annotation ( Jena et al , 2013 ) to label the nouns accordingly .", "entities": [[4, 6, "TaskName", "sentence completion"]]}
{"text": "Such an abstraction is well motivated considering that humans are known to be sensitive to both syntactic part - of - speech tags as well as lexical semantics during sentence processing ( e.g. , Demberg and Keller , 2008 ;", "entities": [[17, 20, "DatasetName", "part - of"]]}
{"text": "All the models are evaluated by comparing the model output with the sentence completion data obtained from the native speakers ; specifically , model output is evaluated in terms of the nature of the predicted verb class .", "entities": [[12, 14, "TaskName", "sentence completion"]]}
{"text": "Additionally , see Section 2 of the supplementary material for examples of various errors .", "entities": [[7, 9, "DatasetName", "supplementary material"]]}
{"text": "Note that if s is not predicted in the ith run , then p i ( s ) = 0 .", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "Formally , F ( h | | m ) \u221d x VC m ( x ) = 0 h ( x ) ( 5 )", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "= 0 h ( x ) log h ( x ) m ( x ) ( 6 ) where h ( x ) is normalized from h", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "See Section 6 of the supplementary material for more details about error types .", "entities": [[5, 7, "DatasetName", "supplementary material"]]}
{"text": "Due to space constraint , details of this model have been mentioned as supplementary material ( Section 7 ) .", "entities": [[13, 15, "DatasetName", "supplementary material"]]}
{"text": "15 See Section 3 of the supplementary material for more details on training data .", "entities": [[6, 8, "DatasetName", "supplementary material"]]}
{"text": "We evaluate five methods for producing attribution heatmaps by applying them to white - box LSTM classifiers for tasks based on formal languages .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "Given an input , an attribution method produces a vector of attribution or relevance scores , which is typically visualized as a heatmap that highlights portions of the input that contribute to model behavior .", "entities": [[22, 23, "MethodName", "heatmap"]]}
{"text": "One of the main challenges facing the evaluation of attribution methods is that it is difficult to assess the quality of a heatmap when the network in question is not understood in the first place .", "entities": [[22, 23, "MethodName", "heatmap"]]}
{"text": "On a theoretical level , axiomatic approaches propose formal desiderata that attribution methods should satisfy , such as implementation invariance ( Sundararajan et al , 2017 ) , input translation invariance ( Kindermans et al , 2019 ) , continuity with respect to inputs ( Montavon et al , 2018 ; Ghorbani et al , 2019 ) , or the existence of relationships between attribution scores and logit or softmax scores ( Sundararajan et al , 2017 ; Ancona et al , 2018 ; Montavon , 2019 ) .", "entities": [[69, 70, "MethodName", "softmax"]]}
{"text": "Merrill 's ( 2019 ) asymptotic analysis shows that LSTM acceptors accept only counter languages when their weights are fully saturated .", "entities": [[9, 10, "MethodName", "LSTM"]]}
{"text": "The Dyck language is the language D generated by the following context - free grammar , where \u03b5 is the empty string .", "entities": [[17, 18, "HyperparameterName", "\u03b5"]]}
{"text": "| \u03b5 D contains all balanced strings of parentheses and square brackets .", "entities": [[1, 2, "HyperparameterName", "\u03b5"]]}
{"text": "In the automaton - based approach , we use the LSTM to simulate an automaton , with the cell state containing a representation of the automaton 's state .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "\u2248 1 , where m \u226b 0 is a large constant .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "= tanh ( u [ 1 \u22121 ] x ( t ) ) , where u > 0 is a hyperparameter that scales the counter by a factor of v", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "For example , after seeing the input aaabbc , the counterbased network for the SP task satisfies c ( 6 ) = v [ 3 2 1 0 2 1 0 ] \u22a4 .", "entities": [[27, 28, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}
{"text": "To simulate A using an LSTM , we use | Q", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "The first k \u2212 1 positions contain all stack items except the top item , with ( represented by the value 1 , [ represented by \u22121 , and empty positions represented by 0 .", "entities": [[33, 34, "DatasetName", "0"]]}
{"text": "The kth position contains the top item of the stack .", "entities": [[1, 2, "DatasetName", "kth"]]}
{"text": "1 \u22121 0 1 1 1 1 0 0 ] \u22a4 .", "entities": [[2, 3, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "The 1 in position 4 indicates that the top item of the stack is ( , and the 1 , \u22121 , and 0 in positions 1 - 3 indicate that the remainder of the stack is ( [ .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "The three 1s in positions 5 - 8 indicate that the stack height is 3 , and the 0 in position 9 indicates that the stack is not empty .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "The empty stack bit is then set to 0 , marking the stack as non - empty .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "= Xt , i \u222b 1 0", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "= [ , and 0 otherwise .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "If the current input symbol is a closing bracket , then \u03b1 ( t )", "entities": [[11, 12, "HyperparameterName", "\u03b1"]]}
{"text": "= 0 , so the sign of u ( t ) is determined by the highest item of h ( t\u22121 ) : k\u22121 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Given X , an LSTM classifier produces a vector y of logit scores .", "entities": [[4, 5, "MethodName", "LSTM"]]}
{"text": "In this method , R ( c ) t , i ( X ) is the change in\u0177 c observed when X t , : is replaced by 0 .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "\u0177 i , i = c 0 , otherwise .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "For each layer l with activation z ( l ) , activation function f ( l ) , and output a ( l )", "entities": [[11, 13, "HyperparameterName", "activation function"]]}
{"text": "3 For the LSTM gate interactions , we follow Arras et al ( 2017b ) in treating multiplicative connections of the form a ( l 1 ) a ( l 2 ) as activation functions of the form", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "When c = False , all relevance scores are 0 .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "Saliency scores satisfy R ( True ) t , 1 ( X ) = \u2212R ( True ) t , 2 ( X ) , resulting in token - level scores of 0 for all inputs .", "entities": [[32, 33, "DatasetName", "0"]]}
{"text": "Heatmaps # 3 and # 4 show that LRP assigns scores of 0 to prefixes containing equal numbers of as and bs .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "We will see in Subsection 7.1 that this phenomenon appears to be related to the fact that the LSTM gates are saturated .", "entities": [[18, 19, "MethodName", "LSTM"]]}
{"text": "Note that the as in heatmap # 9 receive scores of 0 from occlusion and G \u00d7 I , since removing only one of the two as does not destroy the subsequence .", "entities": [[5, 6, "MethodName", "heatmap"], [11, 12, "DatasetName", "0"]]}
{"text": "Although occlusion appears to be erratic at first glance , its behavior can be explained by the fact that changing x ( t ) to 0 causes h ( t ) to be 0 , which the LSTM interprets as the initial state of the FSA ; thus , R ( c ) t ( X ) \u0338 = 0 precisely when X t+1 : , : is classified differently from X.", "entities": [[25, 26, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [37, 38, "MethodName", "LSTM"], [59, 60, "DatasetName", "0"]]}
{"text": "As mentioned in the previous section , network saturation causes gradients to be approximately 0 when using sigmoid or tanh activation functions .", "entities": [[14, 15, "DatasetName", "0"], [19, 21, "MethodName", "tanh activation"]]}
{"text": "We hypothesize that this phenomenon is related to the fact c ( t ) = 0 after reading such prefixes , since the counter has been incremented and decremented in equal amounts .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "Accordingly , we test whether this phenomenon can be mitigated by desaturating the gates so that c ( t ) does not exactly reach 0 .", "entities": [[24, 25, "DatasetName", "0"]]}
{"text": "\u2248 \u03c3 ( m ) using a constant m \u226b 0 .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "Indeed , the percentage of prefixes receiving scores of 0 increases as the approximation c ( t )", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "\u2248 0 becomes more exact .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "= [ 1 0 ] h ( t )", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ [ 0 v/2 ]", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "I 4 0 1 0 0 0 0 1 0 0 0 0 1 \uf8f9 \uf8fa \uf8fa \uf8fb x ( t ) \uf8f6", "entities": [[2, 3, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "For example , if h ( t\u22121 ) 1 = 0 , then no as have been encountered in the input string before time t.", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "\u2248 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "2 m \uf8ee \uf8ef \uf8ef \uf8f0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb h ( t\u22121 ) + m [ 1 1 1 1 \u22121 \u22121 \u22121 ] \u22a4 ) All other gates are fixed to 1 .", "entities": [[6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [14, 15, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [20, 21, "DatasetName", "0"]]}
{"text": "= [ 0 1 1 1 0 0 0 0 ] h ( t )", "entities": [[2, 3, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "[ 0 v/2 ]", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "A.3 FSA Network Here we describe a general construction of an LSTM simulating an FSA with states Q , accepting states Q F \u2286 Q , alphabet \u03a3 , and transition function \u03b4 : Q \u00d7 \u03a3 Q. Recall that h ( t ) contains a one - hot representation of pairs in Q \u00d7 \u03a3 encoding the current state of the FSA and the most recent input symbol .", "entities": [[11, 12, "MethodName", "LSTM"], [32, 33, "HyperparameterName", "\u03b4"], [38, 39, "MetricName", "Recall"]]}
{"text": "The initial state h ( 0 ) = 0 represents the starting configuration of the FSA .", "entities": [[5, 6, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "6 g ( t ) \u27e8q , x\u27e9 = { v , x = x ( t ) 0 , otherwise The input gate then filters out any positions that do not represent valid transitions from the previous state q \u2032 , which is recovered from h ( t\u22121 ) .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "= q 0 , otherwise Now , we describe how this behavior is implemented in our LSTM .", "entities": [[2, 3, "DatasetName", "0"], [16, 17, "MethodName", "LSTM"]]}
{"text": "x ( t ) ) , where W ( c , x ) \u27e8q , x\u27e9 , j = { 1 , j is the index for x 0 , otherwise .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "First , the bias term handles the case where the current case is the starting state q 0 .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "This is necessary because the initial configuration of the network is represented by h ( 0 )", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "= q \u2212m , otherwise The bias vector sets i ( t ) \u27e8q , x\u27e9 to be 1 if the FSA transitions from q 0 to q after reading x , and 0 otherwise .", "entities": [[25, 26, "DatasetName", "0"], [33, 34, "DatasetName", "0"]]}
{"text": "where W ( i ) \u27e8q , x\u27e9 , \u27e8q \u2032 , x \u2032 \u27e9 = { m \u2212 b ( i ) \u27e8q , x\u27e9 , \u03b4 ( q \u2032 , x )", "entities": [[27, 28, "HyperparameterName", "\u03b4"]]}
{"text": "= W h ( t ) , where W c , \u27e8q , x\u27e9 = \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 , c = True and q Q F 1 , c = False and q / Q F 0 , otherwise .", "entities": [[38, 39, "DatasetName", "0"]]}
{"text": "= 0 is treated as an empty stack .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "= [ 0 2 ] W ( c , x ) serves three functions when x ( t ) is an open bracket , and does nothing when x ( t ) is a closing bracket .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "0 0 0 0 2 k \u22122 k 0 0 1 1 0 0 \u22122 \u22122 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb W W ( c , h ) = \uf8ee \uf8ef \uf8ef \uf8f0 0 1 0 0 2 4 2 k\u22121 0 0 0 0 0 0 0 0 0 \u22121 0 \uf8f9 \uf8fa \uf8fa \uf8fb Finally , the \u22121s serve to decrease the empty stack indicator by an amount proportional to the stack height at time t \u2212 1 .", "entities": [[0, 1, "DatasetName", "0"], [1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"], [3, 4, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [34, 35, "DatasetName", "0"], [36, 37, "DatasetName", "0"], [37, 38, "DatasetName", "0"], [42, 43, "DatasetName", "0"], [43, 44, "DatasetName", "0"], [44, 45, "DatasetName", "0"], [45, 46, "DatasetName", "0"], [46, 47, "DatasetName", "0"], [47, 48, "DatasetName", "0"], [48, 49, "DatasetName", "0"], [49, 50, "DatasetName", "0"], [50, 51, "DatasetName", "0"], [52, 53, "DatasetName", "0"]]}
{"text": "( i , x ) sets the input gate for the first k \u2212 1 positions to 0 when x ( t ) is a closing bracket .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "Suppose v represents the number s in unary notation : v j is 1 if j \u2264 s and 0 otherwise .", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "= 2 \uf8ee \uf8f0 0 0 \u22121 \u22121 1 1 0 0 0 \uf8f9 \uf8fb W Based on this , W ( i , h ) is defined as follows .", "entities": [[4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "W ( i , h ) = 2 \uf8ee \uf8ef \uf8ef \uf8f0 0", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "( T k ) : k\u22121 , : 0 ( T k ) :", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "k\u22121 , : 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb W ( i , h ) : k\u22121 , k+1:2k contains T k , with the last row truncated .", "entities": [[3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"]]}
{"text": "= 2 \uf8ee \uf8ef \uf8ef \uf8f0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb Next , W ( f , h ) marks the second highest stack position and the top of the unary counter for deletion , in case an item needs to be popped .", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [20, 21, "DatasetName", "0"], [21, 22, "DatasetName", "0"]]}
{"text": "= 2 \uf8ee \uf8ef \uf8ef \uf8f0 0 \u2212 ( T k ) 2 : , : 0 \u2212T k 0 0 \uf8f9 \uf8fa \uf8fa \uf8fb Finally , the bias term ensures that the top stack item and empty stack indicator are always cleared .", "entities": [[6, 7, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [20, 21, "DatasetName", "0"]]}
{"text": "+ 1 0 , otherwise .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Besides , recent studies on argument mining explore to build links or clusters for topic - dependent arguments ( Wachsmuth et al , 2018 ; Shnarch et al , 2018 ; Reimers et al , 2019 ) .", "entities": [[5, 7, "TaskName", "argument mining"]]}
{"text": "Since the DTC structure can provide relatively rich and low - noise information about certain topic aspects of articles , it is meaningful for various NLP tasks like summarization ( Perez - Beltrachini et", "entities": [[28, 29, "TaskName", "summarization"]]}
{"text": "al , 2019 ) , document similarity measuring ( Gong et al , 2018 ) , and response generation ( Dziri et al , 2019 ) .", "entities": [[17, 19, "TaskName", "response generation"]]}
{"text": "DTC structures for the 385 Wall Street Journal ( WSJ ) articles in the RST - DT corpus aiming to build a bridge between discourse rhetorical structure and DTC structure for discourse researchers to utilize .", "entities": [[14, 17, "DatasetName", "RST - DT"]]}
{"text": "In addition , previous studies on argument mining usually manually define some coarse - grain topic categories for either topic - dependent argument classification or clustering ( Reimers et al , 2019 ) .", "entities": [[6, 8, "TaskName", "argument mining"]]}
{"text": "For better understanding of our annotation , we present some preliminary definitions as following : Discourse Topic Unit ( DTU ) refers to the elementary topic unit in our annotated DTC structure .", "entities": [[19, 20, "DatasetName", "DTU"]]}
{"text": "Different from them , we study macro DTC structures in this work where each sentence is taken as an independent DTU 1 .", "entities": [[20, 21, "DatasetName", "DTU"]]}
{"text": "It is worth mentioning that not all the 1 Although we built the corpus based on RST - DT , it remains DTUs are topic - bearing , there are also some sentences with no topic meaning , e.g. , the sentence \" Oops ! \" .", "entities": [[16, 19, "DatasetName", "RST - DT"]]}
{"text": "Topic Object ( TO ) could be subject , object , or other noun or noun phrase in the DTU which can provide a certain basis for topic chain parsing .", "entities": [[19, 20, "DatasetName", "DTU"]]}
{"text": "Usually , each TO is closely related to the topic of its DTU , and each DTU maintains an independent TO set .", "entities": [[12, 13, "DatasetName", "DTU"], [16, 17, "DatasetName", "DTU"]]}
{"text": "For example , given the DTU \" Drexel Burnham Lambert Inc. is the adviser on the transaction . \"", "entities": [[5, 6, "DatasetName", "DTU"]]}
{"text": ", if the surrounding context of the DTU is mainly about the company , then we choose \" Drexel Burnham Lambert Inc. \" as a TO ; if the context is mainly about the transaction , then we choose \" transaction \" as a TO , and we can also select both of them if necessary .", "entities": [[7, 8, "DatasetName", "DTU"]]}
{"text": "Topic Event ( TE ) refers to the main phrase which most clearly expresses an event occurrence or a description of the TOs in the DTU .", "entities": [[25, 26, "DatasetName", "DTU"]]}
{"text": "For the DTU u4 in Figure 1 , we select \" develop vaccines against the virus \" as the topic event of the DTU .", "entities": [[2, 3, "DatasetName", "DTU"], [23, 24, "DatasetName", "DTU"]]}
{"text": "With the above - mentioned definitions in mind , we argue that each DTU is composed of a set of TOs and a core TE .", "entities": [[13, 14, "DatasetName", "DTU"]]}
{"text": "For the two DTUs u3 and u4 in Figure 1 , although the two corresponding TO sets , { WHO , risky to directly take each elementary discourse unit ( EDU ) as a DTU since there are many competing hypotheses about what constitutes an EDU but without \" topic \" .", "entities": [[34, 35, "DatasetName", "DTU"]]}
{"text": "Previous work on topic - dependent argument mining usually take each independent sentence as an elementary unit , and this work is inspired by these researches .", "entities": [[6, 8, "TaskName", "argument mining"]]}
{"text": "Therefore , for any two adjacent DTUs on a topic chain , the TE in the second DTU should evolve from the TEs in the established chain where the first DTU is located .", "entities": [[17, 18, "DatasetName", "DTU"], [30, 31, "DatasetName", "DTU"]]}
{"text": "Sometimes , a DTU may have topic relevance to multiple subsequent DTUs , we only opt for the closest and most relevant one for annotation .", "entities": [[3, 4, "DatasetName", "DTU"]]}
{"text": "To achieve this , we follow two principles to build each arc in a topic chain : ( 1 ) For each DTU , we search its topic - related DTU from near to far ; ( 2 ) We label topic links for DTUs in order and the annotated DTC structure is dynamically optimized during the human annotation process .", "entities": [[22, 23, "DatasetName", "DTU"], [30, 31, "DatasetName", "DTU"]]}
{"text": "For example , when comparing the current DTU ( U - j ) with previous ones , we directly replace the previously annotated arc ( U - i , U - k ) with ( U - i , U - j ) if the topic relevancy between U - i and U - j obviously surpasses that between U - i and", "entities": [[7, 8, "DatasetName", "DTU"]]}
{"text": "In other words , we do not require all topic chains to be labeled , but we try to ensure the accuracy of the annotated chains as much as possible .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "Simply put , the annotation process is also the process of comparing the TO and TEs of the current DTU with that of the previous ones .", "entities": [[19, 20, "DatasetName", "DTU"]]}
{"text": "For the DTU u1 , its TO set contains two topic objects , i.e. , \" coronavirus \" and \" COVID - 19 \" , and its core topic event can be sketched as \" coronavirus outbreak in Wuhan \" .", "entities": [[2, 3, "DatasetName", "DTU"]]}
{"text": "As depicted in Figure 2 , each DTU is preceded by an index pair ( i , j ) according to which u - i and u - j are connected through a topic link .", "entities": [[7, 8, "DatasetName", "DTU"]]}
{"text": "Specifically , given the annotation results of the pre - trained topic model , ( \u03c4 , \u03b9 ) , and that of three annotators , ( \u03c4 , \u03bd ) , ( \u03c4 , \u03b9 ) , and ( \u03c4 , \u03bd ) , on the DTU \u03c4 , we set the confidence of the pre - trained topic model to 0.5 and that of human annotators to 1 , then the confidence score of each annotation on \u03c4 can be calculated as : ( \u03c4 , \u03b9 ) ( 0.5 + 1 ) /3.5", "entities": [[47, 48, "DatasetName", "DTU"]]}
{"text": "According to our statistics , the averaged confidence score of each DTU annotation is around 0.73 .", "entities": [[11, 12, "DatasetName", "DTU"]]}
{"text": "The annotated corpus contains 385 news articles ( 7962 DTUs ) from RST - DT .", "entities": [[12, 15, "DatasetName", "RST - DT"]]}
{"text": "For supervised learning , we have divided the dataset into three parts ( the test corpus is consist with that of RST - DT ) , as shown in Table 2 .", "entities": [[21, 24, "DatasetName", "RST - DT"]]}
{"text": "Recent years have witnessed the great effects of pre - trained language models ( Devlin et al , Cui et al , 2020 ) on natural language understanding .", "entities": [[25, 28, "TaskName", "natural language understanding"]]}
{"text": "Given a discourse with k - 1 DTUs , we use the pretrained Bert 3 model to encode the entire discourse where each DTU is surrounded by the [ CLS ] and [ SEP ] tokens .", "entities": [[23, 24, "DatasetName", "DTU"]]}
{"text": "And we take the Bert output corresponding to [ CLS ] as our DTU representation .", "entities": [[13, 14, "DatasetName", "DTU"]]}
{"text": "For the convenience of calculation , a zero - initialized vector u z is added at the end of the DTU sequence for the tail DTUs of the topic chains or the isolated DTUs to point to , obtaining U", "entities": [[20, 21, "DatasetName", "DTU"]]}
{"text": "For dependency parsing , we simply build a bi - linear function between U and its duplicate to achieve it , as following :", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "U \u03b1 = W \u03b1 U +", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [4, 5, "HyperparameterName", "\u03b1"]]}
{"text": "b \u03b1 U \u03b2", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [3, 4, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b2 U", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "b \u03b2 s", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "= U T \u03b1 WU", "entities": [[3, 4, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b2 where U \u03b1 and U \u03b2 are ( D\u00d7k ) matrices representing U and its duplicate , W R D\u00d7D denotes the parameters of the bilinear term , and s R k\u00d7k refers to the scores for each DTU upon its candidate successor DTUs .", "entities": [[0, 1, "HyperparameterName", "\u03b2"], [3, 4, "HyperparameterName", "\u03b1"], [6, 7, "HyperparameterName", "\u03b2"], [39, 40, "DatasetName", "DTU"]]}
{"text": "We measure the micro - averaged F1 scores of both topic links and chains for performance , and we do not take those isolated DTUs into consideration to avoid the overestimation of performance .", "entities": [[6, 7, "MetricName", "F1"]]}
{"text": "It is worth mentioning that we annotated the WSJ articles in the RST - DT corpus also aim to allow the discourse researchers to explore the potential correlation between RST - and DTC - style discourse analysis in future work .", "entities": [[12, 15, "DatasetName", "RST - DT"]]}
{"text": "We used the 768D Bert - base and 1024D Bert - large model for DTU representation .", "entities": [[14, 15, "DatasetName", "DTU"]]}
{"text": "The number of parameters in each model and the runtime time of each system are shown in the table below .", "entities": [[1, 4, "HyperparameterName", "number of parameters"]]}
{"text": "XMU Neural Machine Translation Systems for WMT 17", "entities": [[2, 4, "TaskName", "Machine Translation"]]}
{"text": "This paper describes the Neural Machine Translation systems of Xiamen University for the translation tasks of WMT 17 .", "entities": [[5, 7, "TaskName", "Machine Translation"]]}
{"text": "Neural Machine Translation ( NMT ) (", "entities": [[1, 3, "TaskName", "Machine Translation"]]}
{"text": "The decoder is a stacked Long Short - Term Memory ( LSTM )", "entities": [[5, 10, "MethodName", "Long Short - Term Memory"], [11, 12, "MethodName", "LSTM"]]}
{"text": "We use both Byte Pair Encoding ( BPE ) ( Sennrich et al , 2016c ) and mixed word / character segmentation ( Wu et al , 2016 ) to achieve open - vocabulary translation .", "entities": [[3, 6, "MethodName", "Byte Pair Encoding"], [7, 8, "MethodName", "BPE"]]}
{"text": "We use LSTM as the main recurrent unit and residual connections ( He et al , 2016 ) to help training .", "entities": [[2, 3, "MethodName", "LSTM"]]}
{"text": "Both the encoder and decoder adopt LSTM as its main recurrent unit .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "LSTM", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "i t = LSTM", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "b i ( \u2212 x i\u22121 t , \u2212 s i t+ ( \u22121 ) i+1 ) ( 2 ) Here x 0 t R e is the word embedding of word x t , x i t R h is the output of LSTM unit and s i", "entities": [[22, 23, "DatasetName", "0"], [44, 45, "MethodName", "LSTM"]]}
{"text": "t = ( c i t , m i t ) denotes the memory and hidden state of LSTM .", "entities": [[18, 19, "MethodName", "LSTM"]]}
{"text": "LSTM", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "LSTM b i ( x i\u22121 t , \u2212 s i t+1 )", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "The annotation vectors are taken from the output x Lenc of top LSTM layer .", "entities": [[12, 13, "MethodName", "LSTM"]]}
{"text": "At each time - step t , let y 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "t\u22121 R e denotes the word embedding of y t\u22121 and y 1 t\u22121 R h denotes the output of bottom LSTM from previous time - step .", "entities": [[21, 22, "MethodName", "LSTM"]]}
{"text": "Different from GNMT ( Wu et al , 2016 ) , we use the concatenation of y 0", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "[ y 0 t\u22121 T ; y 1 t\u22121 T ] T ( 7 ) e t , i = v T", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "The probability of the next word y t is simply modeled using a softmax layer on the output of top LSTM : p", "entities": [[13, 14, "MethodName", "softmax"], [20, 21, "MethodName", "LSTM"]]}
{"text": "x , y < t ) = softmax ( y t , y L dec t ) ( 10 ) We set L dec to 8 in all our experiments .", "entities": [[7, 8, "MethodName", "softmax"]]}
{"text": "To enable open - vocabulary , we use two approaches : BPE and mixed word / character segmentation .", "entities": [[11, 12, "MethodName", "BPE"]]}
{"text": "In most of our experiments , we use BPE 3 ( Sennrich et al , 2016c ) with 50 K operations .", "entities": [[8, 9, "MethodName", "BPE"]]}
{"text": "In our preliminary experiments , we found that BPE works better than UNK replacement techniques .", "entities": [[8, 9, "MethodName", "BPE"]]}
{"text": "For all our models , we adopt Adam ( Kingma and Ba , 2015 )", "entities": [[7, 8, "MethodName", "Adam"]]}
{"text": "We gradually halve the learning rate during the training process .", "entities": [[4, 6, "HyperparameterName", "learning rate"]]}
{"text": "Table 1 show the results of English - German Translation .", "entities": [[9, 10, "TaskName", "Translation"]]}
{"text": "Test ( BLEU )", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "As shown in Table 3 show the results of English - Chinese Translation .", "entities": [[12, 13, "TaskName", "Translation"]]}
{"text": "We describe XMU 's neural machine translation systems for the WMT 17 shared news translation tasks .", "entities": [[5, 7, "TaskName", "machine translation"]]}
{"text": "Rigid Formats Controlled Text Generation", "entities": [[3, 5, "TaskName", "Text Generation"]]}
{"text": "Neural text generation has made tremendous progress in various tasks .", "entities": [[1, 3, "TaskName", "text generation"]]}
{"text": "To the best of our knowledge , text generation based on the predefined rigid formats has not been well investigated .", "entities": [[7, 9, "TaskName", "text generation"]]}
{"text": "Therefore , we propose a simple and elegant framework named SongNet to tackle this problem .", "entities": [[10, 11, "MethodName", "SongNet"]]}
{"text": "The backbone of the framework is a Transformer - based auto - regressive language model .", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks ( RNN ) or Convolutional Neural Networks ( CNN ) based sequence - tosequence ( seq2seq ) frameworks ( Bahdanau et al , 1 Code : http://github.com/lipiji/SongNet Let me not to the marriage of true minds", "entities": [[41, 42, "MethodName", "seq2seq"]]}
{"text": "al , 2017 ) , Transformer and its variants ( Vaswani et al , 2017 ; , pre - trained auto - regressive language models such as XLNet and GPT2 ( Radford et al , 2019 ) , etc .", "entities": [[5, 6, "MethodName", "Transformer"], [27, 28, "MethodName", "XLNet"]]}
{"text": "Performance has been improved significantly in lots of tasks such as machine translation", "entities": [[11, 13, "TaskName", "machine translation"]]}
{"text": "Vaswani et al , 2017 ) , dialogue systems ( Vinyals and Le , 2015 ; Shang et al , 2015 ; Li , 2020 ) , text summarization ( Rush et al , 2015 ; Li et al , 2017 ; See et al , 2017 ) , story telling ( Fan et al , 2018 ; See et al , 2019 ) , poetry writing ( Zhang and Lapata , 2014 ; Lau et al , 2018 ;", "entities": [[27, 29, "TaskName", "text summarization"]]}
{"text": "Generally , most of the above mentioned tasks can be regarded as free text generation , which means that no constraints on the format and structure , say the number of words and rhyming rules .", "entities": [[13, 15, "TaskName", "text generation"]]}
{"text": "Note that tasks of dialogue generation and story telling are almost in an open - ending generation style as long as the generated content is relevant with the conditional input text .", "entities": [[4, 6, "TaskName", "dialogue generation"]]}
{"text": "To the best of our knowledge , text generation based on the predefined rigid formats constraints has not been well investigated yet .", "entities": [[7, 9, "TaskName", "text generation"]]}
{"text": "In this work , we propose a simple and elegant framework named SongNet to address this challenging problem .", "entities": [[12, 13, "MethodName", "SongNet"]]}
{"text": "The backbone of the framework is a Transformer - based auto - regressive language model .", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "Inspired by BERT ( Devlin et al , 2019 ) and GPT ( Radford et", "entities": [[0, 1, "DatasetName", "Inspired"], [2, 3, "MethodName", "BERT"], [11, 12, "MethodName", "GPT"]]}
{"text": "Extensive experiments on the collected datasets demonstrate that our proposed framework can generate satisfying results in terms of both the tailor - designed automatic metrics including format accuracy , rhyming accuracy , sentence integrity , as well as the human evaluation results on relevance , fluency , and style .", "entities": [[27, 28, "MetricName", "accuracy"], [30, 31, "MetricName", "accuracy"]]}
{"text": "In summary , our contributions are as follows : We propose to tackle a new challenging task : rigid formats controlled text generation .", "entities": [[21, 23, "TaskName", "text generation"]]}
{"text": "A pre - training and fine - tuning framework named SongNet is designed to address the problem .", "entities": [[10, 11, "MethodName", "SongNet"]]}
{"text": "To verify the performance of our framework SongNet , we collect two corpora , SongCi and Sonnet , in Chinese and English respectively .", "entities": [[7, 8, "MethodName", "SongNet"]]}
{"text": "The task of rigid formats controlled text generation is defined as follows : Input : a rigid format C C : C = { c 0 c 1 c 2 c 3 , c 0 c 1 c 2 c 3 c 4 c 5 . } ( 1 ) where C is the set of all possible formats .", "entities": [[6, 8, "TaskName", "text generation"], [25, 26, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}
{"text": "Finally , the target of this problem is to find a mapping function G to conduct the rigid formats controlled text generation : Y = G ( C ) ( 2 ) 3 Framework Description", "entities": [[20, 22, "TaskName", "text generation"]]}
{"text": "As shown in Figure 2 , the backbone of our framework is a Transformer - based auto - regressive language model .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "We tailor - design several sets of indicating symbols to enhance the performance in terms of accuracy on format , rhyme , and sentence integrity .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "Similar to BERT ( Devlin et al , 2019 ) and GPT ( Radford et", "entities": [[2, 3, "MethodName", "BERT"], [11, 12, "MethodName", "GPT"]]}
{"text": "We use two sentences ( as shown in Figure 1 ) \" love is not love , ... , bends with the remover to remove \" extracted from the Shakespeare 's Sonnets ( Shakespeare , 2000 ) as examples to describe the details of our framework SongNet .", "entities": [[46, 47, "MethodName", "SongNet"]]}
{"text": "Since our basic model is a Transformer - based auto - regressive language model , during training , the input is \" bos love is not love , /s ... , bends with the remover to remove .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "The target of our framework is to conduct the formats controlled text generation .", "entities": [[11, 13, "TaskName", "text generation"]]}
{"text": "Format and Rhyme Symbols : C = { c 0 , c 0 , c 0 , c 2 , c 1 , /s", "entities": [[9, 10, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}
{"text": "c 0 , c 0 , c 0 , c 0 , c 0 , c 2 , c 1 , /s , eos } ( 3 ) where we use { c 0 } to represent the general tokens ; { c 1 } depict the punctuation characters ; { c 2 } represent the rhyming tokens \" love \" and \" remove \" .", "entities": [[1, 2, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [33, 34, "DatasetName", "0"]]}
{"text": "Intra - Position Symbols : P = { p 4 , p 3 , p 2 , p 1 , p 0 , /s", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "p 6 , p 5 , p 4 , p 3 , p 2 , p 1 , p 0 , /s , eos } ( 4 ) { p i } denote the local positions of tokens within the same clause or sentence .", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "For example , { p 0 } usually denote punctuation characters , thus { p 1 } should be the ending words of sentences .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Segment Symbols : S = { s 0 , s 0 , s 0 , s 0 , s 0 , /s s 1 , s 1 , s 1 , s 1 , s 1 , s 1 , s 1 , /s , eos } ( 5 ) where s i is the symbol index for", "entities": [[7, 8, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [19, 20, "DatasetName", "0"]]}
{"text": "Contrast to Transformer ( Vaswani et al , 2017 ) , BERT ( Devlin et al , 2019 ) , and GPT2 ( Radford et al , 2019 ) , we modify the traditional attention strategies slightly to fit our problem .", "entities": [[2, 3, "MethodName", "Transformer"], [11, 12, "MethodName", "BERT"]]}
{"text": "Specifically , for the input , we first obtain the representations by summing all the embeddings of the input tokens and symbols , as shown in the red solid box of Figure 2 : H 0 t = E wt + E ct + E pt + E st + E gt ( 6 ) where 0 is the layer index and t is the state index .", "entities": [[35, 36, "DatasetName", "0"], [56, 57, "DatasetName", "0"]]}
{"text": "g is the global position index same as position symbols used in Transformer ( Vaswani et al , 2017 ) .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "To represent the global dynamic information , we introduce another variable F 0 by only summing the pre - defined symbols as shown in the blue dash box of Figure 2 : F 0 t = E ct", "entities": [[12, 13, "DatasetName", "0"], [33, 34, "DatasetName", "0"]]}
{"text": "The first block is a masking multi - head self - attention component , and the second block is named global multi - head attention .", "entities": [[21, 25, "MethodName", "multi - head attention"]]}
{"text": "Masking Multi - Head Self - Attention : C 1 t = LN FFN ( C 1 t ) + C 1 t C 1 t = LN SLF - ATT ( Q 0 t , K 0", "entities": [[33, 34, "DatasetName", "0"], [37, 38, "DatasetName", "0"]]}
{"text": "\u2264t , V 0 \u2264t ) + H 0 t Q 0", "entities": [[3, 4, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "= H 0 W Q K 0 , V 0 = H 0 W K , H 0 W V ( 8 ) where SLF - ATT ( ) , LN ( ) , and FFN ( ) represent self - attention mechanism , layer normalization , and feed - forward network respectively .", "entities": [[2, 3, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [44, 46, "MethodName", "layer normalization"]]}
{"text": "After obtaining C 1 t from Equation ( 8 ) , we feed it into the second attention block to capture the global dynamic information from F 0 .", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "Global Multi - Head Attention : H 1 t = LN FFN ( H 1 t )", "entities": [[1, 5, "MethodName", "Multi - Head Attention"]]}
{"text": "+ H 1 t H 1 t = LN GLOBAL - ATT ( Q 1 t , K 1 , V 1 ) + C 1 t Q 1 = C 1 W Q K 1 , V 1 = F 0 W K , F 0 W V ( 9 )", "entities": [[41, 42, "DatasetName", "0"], [46, 47, "DatasetName", "0"]]}
{"text": "We can observe that all the context information from F 0 are considered .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "Note that H is renewed layerly , however the global variable F 0 is fixed .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "To achieve this goal , we adjust the masking strategy used in BERT ( Devlin et al , 2019 ) to our framework according to our definitions .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "Adam ( Kingma and Ba , 2014 ) optimization method with Noam learning - rate decay strategy and 10 , 000 warmup steps is employed to conduct the pre - training .", "entities": [[0, 1, "MethodName", "Adam"]]}
{"text": "For English , same as BERT , we employ English Wikipedia ( 2400 M words ) and BooksCorpus ( 980 M words )", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "We did not use BPE operation ( Sennrich et al , 2015 ) on this corpus considering the format controlling purpose .", "entities": [[4, 5, "MethodName", "BPE"]]}
{"text": "Assume that the number of format - correct sentences is n , then we can obtain Precision p", "entities": [[16, 17, "MetricName", "Precision"]]}
{"text": "n /n , Recall r = n /m , and F1 - measure .", "entities": [[3, 4, "MetricName", "Recall"], [10, 11, "MetricName", "F1"]]}
{"text": "We report both the Macro - F1 and Micro - F1 in the results tables .", "entities": [[4, 7, "MetricName", "Macro - F1"], [8, 11, "MetricName", "Micro - F1"]]}
{"text": "We report the Macro - F1 and Micro - F1 numbers in the results tables as well .", "entities": [[3, 6, "MetricName", "Macro - F1"], [7, 10, "MetricName", "Micro - F1"]]}
{"text": "i 0 , y", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "The rating criteria are as follows : Relevance : +2 : all the sentences are relevant to the same topic ; +1 : partial sentences are relevant ; 0 : not relevant at all .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "Fluency : +2 : fluent ; +1 : readable but with some grammar mistakes ; 0 : unreadable .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "Style : +2 : match with SongCi or Sonnet genres ; +1 : partially match ; 0 : mismatch .", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "SongNet Out proposed framework with both the per - training and fine - tuning stages .", "entities": [[0, 1, "MethodName", "SongNet"]]}
{"text": "SongNet ( only pre - tuning )", "entities": [[0, 1, "MethodName", "SongNet"]]}
{"text": "SongNet ( only fine - tuning )", "entities": [[0, 1, "MethodName", "SongNet"]]}
{"text": "SongNet - GRU Employ GRU to replace Transformer as the core structure .", "entities": [[0, 1, "MethodName", "SongNet"], [2, 3, "MethodName", "GRU"], [4, 5, "MethodName", "GRU"], [7, 8, "MethodName", "Transformer"]]}
{"text": "SongNet - SongCi CiPai : Zhe Gu Tian , Format :", "entities": [[0, 1, "MethodName", "SongNet"]]}
{"text": "Translation for Chinese poetry is also a challenging task . )", "entities": [[0, 1, "TaskName", "Translation"]]}
{"text": "SongNet - SongCi CiPai : Bu Suan Zi , Format : 5 , 5 . 7 , 5 . 5 , 5 . 7 , 5 .", "entities": [[0, 1, "MethodName", "SongNet"]]}
{"text": "_ ( 1 ) ( 2 ) SongNet - Sonnet _ _", "entities": [[7, 8, "MethodName", "SongNet"]]}
{"text": "But the Rhyme accuracy and the sentence integrity will drop simultaneously .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "For human evaluation , we just conduct the judging on the results generated by our final model SongNet .", "entities": [[17, 18, "MethodName", "SongNet"]]}
{"text": "The examples of the generated results under this setting are shown in Table 6 , which show that our model SongNet can generate satisfying results especially on SongCi .", "entities": [[20, 21, "MethodName", "SongNet"]]}
{"text": "We propose to tackle a challenging task called rigid formats controlled text generation .", "entities": [[11, 13, "TaskName", "text generation"]]}
{"text": "A pre - training and fine - tuning framework SongNet is designed to address the problem .", "entities": [[9, 10, "MethodName", "SongNet"]]}
{"text": "In this paper , we report on the shared task on metaphor identification on VU Amsterdam Metaphor Corpus and on a subset of the TOEFL Native Language Identification Corpus .", "entities": [[25, 28, "TaskName", "Native Language Identification"]]}
{"text": "In terms of methods , approaches based on feature - engineering in a supervised machine learning paradigm explored features based on concreteness and imageability , semantic classification using WordNet , FrameNet , VerbNet , SUMO ontology , property norms , and distributional semantic models , syntactic dependency patterns , sensorial and vision - based features K\u00f6per and i m Walde , 2017 ; Tekiroglu et al , 2015 ; Tsvetkov et al , 2014 ; Beigman Klebanov", "entities": [[30, 31, "DatasetName", "FrameNet"], [35, 36, "MethodName", "ontology"]]}
{"text": "The metric used for evaluation is the F1 score ( least frequent class / label , which is \" metaphor \" ) with Precision and Recall also available via the detailed results link in CodaLab .", "entities": [[7, 9, "MetricName", "F1 score"], [23, 24, "MetricName", "Precision"], [25, 26, "MetricName", "Recall"]]}
{"text": "We make available to shared task participants a number of features from prior published work on metaphor detection , including unigram features , features based on WordNet , VerbNet , and those derived from a distributional semantic model , POS - based , concreteness and difference in concreteness , as well as topic models .", "entities": [[52, 54, "TaskName", "topic models"]]}
{"text": "6 Baseline 2 : bot.zen is one of the top - ranked systems in the first metaphor shared task in 2018 by Stemle and Onysko ( 2018 ) that uses a bi - directional recursive neural network architecture with long - term short - term memory ( LSTM BiRNN ) and implements a flat sequenceto - sequence neural network with one hidden layer using TensorFlow and Keras in Python .", "entities": [[47, 48, "MethodName", "LSTM"]]}
{"text": "The system uses fastText word embeddings from different corpora , including learner corpus and BNC data .", "entities": [[3, 4, "MethodName", "fastText"], [4, 6, "TaskName", "word embeddings"]]}
{"text": "Finally , Baseline 3 : BERT is constructed by finetuning the BERT model ( Devlin et al , 2018 ) in a standard token classification task : After obtaining the contextualized embeddings of a sentence , we apply a linear layer followed by softmax on each token to predict whether it is metaphorical or not .", "entities": [[5, 6, "MethodName", "BERT"], [11, 12, "MethodName", "BERT"], [23, 25, "TaskName", "token classification"], [39, 41, "MethodName", "linear layer"], [43, 44, "MethodName", "softmax"]]}
{"text": "Character embeddings + Similarity Networks + Bi - LSTM + Transformer Kumar and Sharma ( 2020 ) added lexical and orthographic information via character embeddings in addition to GloVe and ELMo embeddings for an enriched input representation .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "Transformer"], [11, 12, "DatasetName", "Kumar"], [28, 29, "MethodName", "GloVe"], [30, 31, "MethodName", "ELMo"]]}
{"text": "A Bi - LSTM network and Transformer network are trained independently and combined in an ensemble .", "entities": [[3, 4, "MethodName", "LSTM"], [6, 7, "MethodName", "Transformer"]]}
{"text": "Specifically , the concreteness value of a word is formulated as a linear interpolation between two reference vectors ( concrete and abstract ) which were randomly initialized and learned from data . iiegn : LSTM BiRNN + metadata ; combine TOEFL and VUA data Stemle and Onysko ( 2020 ) used an LSTM BiRNN classifier to study the relationship between the metadata in the TOEFL corpus ( proficiency , L1 of the author , and the prompt to which the essay is responding ) and classifier performance .", "entities": [[34, 35, "MethodName", "LSTM"], [52, 53, "MethodName", "LSTM"]]}
{"text": "Duke Data Science : BERT , XNET language models + POS tags as features for a Bi - LSTM classifier Liu et al ( 2020 ) use pre - trained BERT and XLNet language models to create contextualized embeddings , which are combined with POS tags to generate features for a Bi - LSTM for token - level metaphor classification .", "entities": [[4, 5, "MethodName", "BERT"], [18, 19, "MethodName", "LSTM"], [30, 31, "MethodName", "BERT"], [32, 33, "MethodName", "XLNet"], [53, 54, "MethodName", "LSTM"]]}
{"text": "For the testing phase , the authros used an ensemble strategy , training four copies of the Bi - LSTM with different initializations and averaging their predictions .", "entities": [[19, 20, "MethodName", "LSTM"]]}
{"text": "chasingkangaroos : RNN + BiLSTM + Attention + Ensemble Brooks and Youssef ( 2020 ) use an ensemble of RNN models with Bi - LSTMs and bidirectional attention mechanisms .", "entities": [[4, 5, "MethodName", "BiLSTM"]]}
{"text": "Each word was represented by an 11 - gram and appeared at the center of the 11 - gram ; each word in the 11 - gram was represented by a 1 , 324 dimensional word embedding ( concatenation of ELMo and GloVe embeddings ) .", "entities": [[40, 41, "MethodName", "ELMo"], [42, 44, "MethodName", "GloVe embeddings"]]}
{"text": "baseline system ( also one of the shared task baselines , see section 4.1 ) uses BERT - after obtaining the contextualized embeddings of a sentence , a linear layer is applied followed by softmax on each token to predict whether it is metaphorical or not .", "entities": [[16, 17, "MethodName", "BERT"], [28, 30, "MethodName", "linear layer"], [34, 35, "MethodName", "softmax"]]}
{"text": "UoB team : Bi - LSTM + GloVe embeddings + concreteness Alnafesah et al ( 2020 ) explore ways of using concreteness information in a neural metaphor detection context .", "entities": [[5, 6, "MethodName", "LSTM"], [7, 9, "MethodName", "GloVe embeddings"]]}
{"text": "GloVe embeddings are used as features to an SVM classifier to learn concreteness values , training it using human labels of concreteness .", "entities": [[0, 2, "MethodName", "GloVe embeddings"], [8, 9, "MethodName", "SVM"]]}
{"text": "Then , for metaphor detection , every input word is represented as a 304 - dimensional vector - 300 dimensions are GloVe pre - trained embeddings , plus probabilities for the four concreteness classes .", "entities": [[21, 22, "MethodName", "GloVe"]]}
{"text": "These representations of words are given as input to a Bi - LSTM which outputs a sequence of labels .", "entities": [[12, 13, "MethodName", "LSTM"]]}
{"text": "Results suggest that explicit concreteness information helps improve metaphor detection , relative to a baseline that uses GloVe embeddings only .", "entities": [[17, 19, "MethodName", "GloVe embeddings"]]}
{"text": "ALBERT + BiLSTM Li et al ( 2020 ) use a sequence labeling model based on ALBERT - LSTM - Softmax .", "entities": [[0, 1, "MethodName", "ALBERT"], [2, 3, "MethodName", "BiLSTM"], [16, 17, "MethodName", "ALBERT"], [18, 19, "MethodName", "LSTM"], [20, 21, "MethodName", "Softmax"]]}
{"text": "Embeddings produced by BERT serve as input to BiLSTM , as well as to the final softmax layer .", "entities": [[3, 4, "MethodName", "BERT"], [8, 9, "MethodName", "BiLSTM"], [16, 17, "MethodName", "softmax"]]}
{"text": "The authors report on experiments with inputs to BERT ( single - sentence vs pairs ; variants using BERT tokenization ) , spellcorrection of the TOEFL data , and CRF vs softmax at the classification layer .", "entities": [[8, 9, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [29, 30, "MethodName", "CRF"], [31, 32, "MethodName", "softmax"]]}
{"text": "PolyU - LLT : Sensorimotor and embodiment features + embeddings + n - grams + logistic regression classifier Wan et al ( 2020 ) use sensorimotor and embodiment features .", "entities": [[15, 17, "MethodName", "logistic regression"]]}
{"text": "The authors also use word , lemma , and POS n - grams ; word2vec and GloVe word embeddings , as well as cosine distance measurements using the embeddings .", "entities": [[6, 7, "DatasetName", "lemma"], [16, 17, "MethodName", "GloVe"], [17, 19, "TaskName", "word embeddings"]]}
{"text": "The different features are combined using logistic regression and other classifiers .", "entities": [[6, 8, "MethodName", "logistic regression"]]}
{"text": "The clearest trend in the 2020 submissions is the use of deep learning architectures based on BERT ( Devlin et al , 2018 ) - more than half of the participating systems used BERT or its variant .", "entities": [[16, 17, "MethodName", "BERT"], [33, 34, "MethodName", "BERT"]]}
{"text": "We note , however , that participating systems that used BERT showed a smaller performance gap between VUA and TOEFL data ; in zhengchang the gap is all but eliminated .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "This suggests that a BERT - based system with parameters optimized for performance on TOEFL data can close this gap .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "Considering TOEFL data as an additional genre , along with the four genres represented in VUA , we observe that it is generally harder than Academic and News , and is commensurate with Fiction in terms of performance , for the three systems with best VUA All POS performance ( Deep - Met : 0.72 both , Go Figure ! : 0.69 both , illiniMet : 0.69 for VUA Fiction , .70 for TOEFL ) ; a caveat to this observation is that the difference between VUA and TOEFL is not only in genre but in the metaphor annotation guidelines as well .", "entities": [[52, 53, "DatasetName", "Met"]]}
{"text": "Interactive Query - Assisted Summarization via Deep Reinforcement Learning", "entities": [[4, 5, "TaskName", "Summarization"]]}
{"text": "Interactive summarization is a task that facilitates user - guided exploration of information within a document set .", "entities": [[1, 2, "TaskName", "summarization"]]}
{"text": "While one would like to employ state of the art neural models to improve the quality of interactive summarization , many such technologies can not ingest the full document set or can not operate at sufficient speed for interactivity .", "entities": [[18, 19, "TaskName", "summarization"]]}
{"text": "We compare our solution to a recent interactive summarization system , and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience .", "entities": [[8, 9, "TaskName", "summarization"]]}
{"text": "We focus on the task of interactive summarization ( INTSUMM : Shapira et al , 2021b ) which enables information exploration within a document set on a topic , by means of user - guided summarization .", "entities": [[7, 8, "TaskName", "summarization"], [35, 36, "TaskName", "summarization"]]}
{"text": "Indeed , all previous interactive summarization systems we know of either apply traditional methods or are inadequate for real - time processing due to high latency ( 2 ) .", "entities": [[5, 6, "TaskName", "summarization"]]}
{"text": "The model for the query - assisted summarization subtask , M Summ , incorporates the query sequence by ( 1 ) encoding a query into the contextual sentence representations , ( 2 ) attending the representations using a new query - biased variant of the maximal marginal relevance ( MMR : Carbonell and Goldstein , 1998 ) function , and ( 3 ) a dual reward mechanism for policy optimization ( Pasunuru and Bansal , 2018 ) which we adapt to consider both reference summaries and the query ( 3 ) .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "The models are trained on the DUC 2 2007 multidocument summarization ( MDS ) news - domain dataset , with adaptions for our task setting .", "entities": [[10, 11, "TaskName", "summarization"]]}
{"text": "For testing , we follow the INTSUMM evaluation framework of Shapira et al ( 2021b ) to run simulations , collect real user sessions , and assess the results , using DUC 2006 .", "entities": [[31, 33, "DatasetName", "DUC 2006"]]}
{"text": "Interactive summarization facilitates user - guided information navigation within document sets .", "entities": [[1, 2, "TaskName", "summarization"]]}
{"text": "Hirsch et al ( 2021 ) applied advanced coreference resolution algorithms that take several hours for preprocessing a document set .", "entities": [[8, 10, "TaskName", "coreference resolution"]]}
{"text": "The two INTSUMM baseline systems of Shapira et al ( 2021b ) use sentence clustering or TextRank ( Mihalcea and Tarau , 2004 ) for summarization , sentence similarity heuristics for query - responses , and n - gram frequency or TextRank for suggested query extraction .", "entities": [[25, 26, "TaskName", "summarization"]]}
{"text": "We next review some recent techniques in MDS , query - focused summarization and multi - document keyphrase extraction , all of which relate to the INTSUMM task and our choice of algorithms .", "entities": [[12, 13, "TaskName", "summarization"], [17, 19, "TaskName", "keyphrase extraction"]]}
{"text": "The subtask of query - assisted summarization .", "entities": [[6, 7, "TaskName", "summarization"]]}
{"text": "For example , Wang et al ( 2020 ) use graph neural networks to globally score sentence salience , Xiao et al ( 2021 ) summarize using Longformers ( Beltagy et al , 2020 ) , and combine a Longformer with BART ( Lewis et al , 2020 ) and incorporate graphical representation of information .", "entities": [[39, 40, "MethodName", "Longformer"], [41, 42, "MethodName", "BART"]]}
{"text": "In the query - focused summarization ( QFS ) task summaries are biased on a query .", "entities": [[5, 6, "TaskName", "summarization"]]}
{"text": "Relatedly , incremental update summarization ( Mc - Creadie et al , 2014 ; Lin et al , 2017 ) marks queryrelevant information as reported texts stream in , avoiding repeating information marked earlier .", "entities": [[4, 5, "TaskName", "summarization"]]}
{"text": "Extracting suggested queries on a document set most resembles the multi - document keyphrase extraction ( MDKE ) task since it aims to identify salient keyphrases ( Shapira et al , 2021a ) .", "entities": [[13, 15, "TaskName", "keyphrase extraction"]]}
{"text": "While previous methods for keyphrase extraction could potentially be adapted for our dynamic setting , we choose to focus in this work on a deep RL architecture for suggested queries that resonates our model for query - assisted summarization and allows sharing insights between the models .", "entities": [[4, 6, "TaskName", "keyphrase extraction"], [38, 39, "TaskName", "summarization"]]}
{"text": "The subtask of query - assisted summarization covers two main components of the INTSUMM task : the generators of an initial summary and of queryresponses .", "entities": [[6, 7, "TaskName", "summarization"]]}
{"text": "The input to the query - assisted summarization subtask is tuple ( D , q , E in , m ) , such that : D is a document set on a topic where the j - th sentence in the concatenation of D 's documents is denoted s j ; q is a query , and can be empty ( denoted _ ) for an unbiased generic summary ; E in = { e in 1 , ... , e in k } is a sequence of sentences from D termed the history , containing texts previously output in the session ; and m is the number of sentences to output .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "= { e out 1 , ... , e out m } from D ( extractive summarization ) .", "entities": [[15, 17, "TaskName", "extractive summarization"]]}
{"text": "Our query - assisted summarization model , M Summ , is autoregressive , outputting the requested number of summary sentences one - by - one .", "entities": [[4, 5, "TaskName", "summarization"]]}
{"text": "encodes s j on the sentence level and then a bi - LSTM ( Huang et al , 2015 ) forms representation c j on the document level , given the CNN encodings .", "entities": [[12, 13, "MethodName", "LSTM"]]}
{"text": "This sentence+query represen - Contextual sentence embeddings are concatenated to the current query embedding .", "entities": [[5, 7, "TaskName", "sentence embeddings"]]}
{"text": "The query - focused MMR scores are incorporated into M Summ by softly attending on the sentence representations with their respective translated query - focused MMR scores : \u00b5 t = softmax ( MLP ( m t ) )", "entities": [[31, 32, "MethodName", "softmax"], [33, 34, "DatasetName", "MLP"]]}
{"text": "At time t , a representation z t of the summary - so - far is computed by applying an LSTM encoder on { c idx ( e in 1 ) , ... , c idx ( e in k ) , c idx ( e out 1 ) , ... , c idx ( e out t\u22121 ) } , i.e. , on the plain sentence representations of E t , where idx ( e ) is the index of sentence", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "Finally , a sentence s j at time t is assigned a selection probability softmax ( p t )", "entities": [[14, 15, "MethodName", "softmax"]]}
{"text": "At time step t , for selected sentence e out t ( based on softmax ( p t ) ) , reward r t is computed and weighted into M Summ 's loss function .", "entities": [[14, 15, "MethodName", "softmax"], [32, 33, "MetricName", "loss"]]}
{"text": "SEMSIM computes the cosine similarity between the average of word embeddings ( spaCy : Honnibal and Montani , 2021 ) of e out t and that of q t .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "Chiefly , we modify their model for handling an input query - sequence and a sentence history , and employ a different summarization reward function .", "entities": [[22, 23, "TaskName", "summarization"]]}
{"text": "To provide a warm start for training M Summ , a reduced version of M Summ is first pre - trained for generic extractive single - document summarization using the large - scale CNN / Daily Mail corpus ( Hermann et al , 2015 ) , as proposed by Chen and Bansal ( 2018 )", "entities": [[26, 28, "TaskName", "document summarization"], [33, 37, "DatasetName", "CNN / Daily Mail"]]}
{"text": "For the given document set , all noun phrases are extracted using a standard part - of - speech regular expression method ( Mihalcea and Tarau , 2004 ; Wan and Xiao , 2008 ) .", "entities": [[14, 17, "DatasetName", "part - of"]]}
{"text": "We obtain document - level contextual phrase embeddings , c j for phrase \u03c1 j , with the CNN and bi - LSTM networks , and softly attend the embeddings with a standard MMR score : m t j = \u03bb SIM ( \u03c1 j , D ) \u2212 ( 1 \u2212 \u03bb ) max e Et SIM ( \u03c1 j , e ) ( 12 )", "entities": [[22, 23, "MethodName", "LSTM"]]}
{"text": "We use the same DUC 2007 training data , with document sets and reference summaries , and additionally prepare three \" histories \" per topic : one empty and two non - empty .", "entities": [[4, 6, "DatasetName", "DUC 2007"]]}
{"text": "The AUC is normalized with the total token - length of all suggested queries to mitigate for lengthy phrase extractions .", "entities": [[1, 2, "MetricName", "AUC"]]}
{"text": "To simulate the query - assisted summarization algorithms , we utilize the real sessions recorded by Shapira et al ( 2021b ) : 3 - 4 user sessions on 20 topics from DUC 2006 collected with S 2 .", "entities": [[6, 7, "TaskName", "summarization"], [32, 34, "DatasetName", "DUC 2006"]]}
{"text": "6 Using the DUC 2006 INTSUMM test set , we prepared two complementing user sets of 20 topics , each with 10 of the topics to be run on our system and the other 10 on the baseline .", "entities": [[3, 5, "DatasetName", "DUC 2006"]]}
{"text": "Interactive summarization for information exploration is a task that requires compliance to user requests and session history , while comprehensively handling a large input document set .", "entities": [[1, 2, "TaskName", "summarization"]]}
{"text": "We note that while M Summ is designed for the INTSUMM task , it may potentially be serviceable for standard MDS , QFS , update summarization and combinations thereof .", "entities": [[25, 26, "TaskName", "summarization"]]}
{"text": "The DUC 2006 and 2007 datasets were obtained according to the DUC website ( duc . nist.gov ) requirements .", "entities": [[1, 3, "DatasetName", "DUC 2006"]]}
{"text": "To provide a warm start for training M Summ and M Sugg , a reduced version of the models , which is the same for both , is first pre - trained for generic extractive single - document summarization using the CNN / Daily Mail corpus ( Hermann et al , 2015 ) with about 287k samples , as proposed by Chen and Bansal ( 2018 ) .", "entities": [[37, 39, "TaskName", "document summarization"], [41, 45, "DatasetName", "CNN / Daily Mail"]]}
{"text": "The MLP in Equation 3 transforms the MMR score with a feed - forward network with one - hidden layer of dimension 80 following .", "entities": [[1, 2, "DatasetName", "MLP"]]}
{"text": "In our experiments , to cancel out this component ( both at training and inference time ) , we simply set \u03b2", "entities": [[21, 22, "HyperparameterName", "\u03b2"]]}
{"text": "We also tried sentence level ROUGE - L , like in , eventually outputting sentences that were much less compliant to queries .", "entities": [[5, 8, "MetricName", "ROUGE - L"]]}
{"text": "Importantly , since DUC 2007 is most similar to the test DUC 2006 set , it seems to be more beneficial to include DUC 2007 in the training set .", "entities": [[3, 5, "DatasetName", "DUC 2007"], [11, 13, "DatasetName", "DUC 2006"], [23, 25, "DatasetName", "DUC 2007"]]}
{"text": "( 6 ) To represent the sentences , we also tried using average w2v vectors ( Honnibal and Montani , 2021 ) and Sentence - BERT ( Reimers and Gurevych , 2019 ) instead of the CNN network .", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "( 7 ) For the sentence similarity in the query - MMR component , we tried w2v and Sentence - BERT representations instead of TF - IDF vectors .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "We tried training on other query types , synthesized with various keyphrase extraction techniques , and found that our final choice of queries more consistently gave good results overall .", "entities": [[11, 13, "TaskName", "keyphrase extraction"]]}
{"text": "Tuning was performed in accordance to the validation scores and generic keyphrase extraction scores on the MK - DUC - 01 multi - document keyphrase extraction dataset of Shapira et al ( 2021a ) .", "entities": [[11, 13, "TaskName", "keyphrase extraction"], [24, 26, "TaskName", "keyphrase extraction"]]}
{"text": "We tested several values between 0 and 1 for both hyper - parameters .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "All functions tested were adequate overall , though our final choice of reward function was closest to the keyphrase extraction task unigram overlap metric , and gave best results overall .", "entities": [[18, 20, "TaskName", "keyphrase extraction"]]}
{"text": "We extracted all noun - phrases from the document set by first mapping all tokens to their part - of - speech tags , and then applying a regularexpression chunker with regex : { ( < JJ > * < NN . *", "entities": [[17, 20, "DatasetName", "part - of"]]}
{"text": "While DUC 2006 ( our test set ) and 2007 ( our train / validation set ) were originally designed for the query - focused summarization task , they contain excessive topic concentration due to their long and descriptive topic queries ( Baumel et al , 2016 ) .", "entities": [[1, 3, "DatasetName", "DUC 2006"], [25, 26, "TaskName", "summarization"]]}
{"text": "By computing the area under the full curve , and dividing by the full length , the normalized AUC score is obtained .", "entities": [[18, 19, "MetricName", "AUC"]]}
{"text": "Specifically , we use the Advantage Actor Critic ( A2C ) policy gradient training method .", "entities": [[9, 10, "MethodName", "A2C"]]}
{"text": "At a high level , an output reward ( subtracted by a baseline reward - computed on a version of the model without MMR attention ) is used to weight the output selection in the loss function .", "entities": [[35, 36, "MetricName", "loss"]]}
{"text": "Since the reward function is not differentiable , it is used as a weight on the probability of the selected output , which is then given to the loss function .", "entities": [[28, 29, "MetricName", "loss"]]}
{"text": "We conduct extensive experiments with four prominent NLP models - TextRNN , BERT , RoBERTa and XLNetover eight types of textual perturbations on three datasets .", "entities": [[12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "RoBERTa"]]}
{"text": "To improve the robustness under perturbation , it is common practice to leverage data augmentation ( Li and Specia , 2019 ; Min et al , 2020 ; Tan and Joty , 2021 ) .", "entities": [[13, 15, "TaskName", "data augmentation"]]}
{"text": "Similarly , how much data augmentation through the perturbation improves model robustness varies between models and perturbations .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "In this work , we aim to investigate two Research Questions ( RQ ) : RQ1 : Why are NLP models less robust to some perturbations than others ? RQ2 : Why does data augmentation work better at improving the model robustness to some perturbations than others ?", "entities": [[33, 35, "TaskName", "data augmentation"]]}
{"text": "We also validate another hypothesis for RQ2 that the learnability metric is predictive of the improvement on robust performance brought by data augmentation along a perturbation .", "entities": [[21, 23, "TaskName", "data augmentation"]]}
{"text": "Our proposed learnability is inspired by the concepts of Randomized Controlled Trial ( RCT ) and Average Treatment Effect ( ATE ) from Causal Inference ( Rubin , 1974 ; Holland , 1986 ) .", "entities": [[23, 25, "MethodName", "Causal Inference"]]}
{"text": "Perturbation Training Examples Test Examples 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Standard original l ( x i , 0 ) , ( x j , 1 )", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "( x i , 0 ) , ( x j , 1 ) 1 Robustness original l { 0 , 1 } ( x i , 0 ) , ( x j , 1 ) ( x * i , 0 ) , ( x * j , 1 ) 2 Data Augmentation original l { 0 , 1 } ( x i , 0 ) , ( x j , 1 ) ( x * i , 0 ) , ( x * j , 1 )", "entities": [[4, 5, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [51, 53, "TaskName", "Data Augmentation"], [56, 57, "DatasetName", "0"], [64, 65, "DatasetName", "0"], [78, 79, "DatasetName", "0"]]}
{"text": "x * i , 0 ) , ( x * j , 1 ) 3 Learnability random l \u2032 { 1 \u2032 } ( x j , 0 \u2032 ) , ( x * i , 1 \u2032 )", "entities": [[4, 5, "DatasetName", "0"], [27, 28, "DatasetName", "0"]]}
{"text": "( x * i , 1 \u2032 ) 4 random l \u2032 { 1 \u2032 } ( x j , 0 \u2032 ) , ( x * i , 1 \u2032 )", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "( x i , 1 \u2032 ) Table 1 : Example experiment settings for measuring learnability , robustness and improvement by data augmentation .", "entities": [[21, 23, "TaskName", "data augmentation"]]}
{"text": "Training / test examples are the expected input data , assuming we have only one negative ( x i , 0 ) and positive ( x j , 1 ) example in our original training / test set .", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "Combining these two findings , we further show that data augmentation is only more effective at improving robustness against perturbations that a model is more sensitive to , contributing to the interpretation of robustness and data augmentation .", "entities": [[9, 11, "TaskName", "data augmentation"], [35, 37, "TaskName", "data augmentation"]]}
{"text": "As a pilot study , we consider the task of binary text classification .", "entities": [[11, 13, "TaskName", "text classification"]]}
{"text": "The training set is denoted as D train = { ( x 1 , l 1 ) , ... , ( x n , l n ) } , where x i is the i - th example and l i { 0 , 1 } is the corresponding label .", "entities": [[42, 43, "DatasetName", "0"]]}
{"text": "A textual perturbation is a transformation g \u2236 ( x ; \u03b2 )", "entities": [[11, 12, "HyperparameterName", "\u03b2"]]}
{"text": "x * that injects a specific type of noise into an example x with parameters \u03b2 and the resulting perturbed example is x * .", "entities": [[15, 16, "HyperparameterName", "\u03b2"]]}
{"text": "Experiment 0 in Table 1 is the standard learning setup , where we train and evaluate a model on the original dataset .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "We apply the perturbations to test examples and measure the robustness of model to said perturbations as the decrease in accuracy .", "entities": [[20, 21, "MetricName", "accuracy"]]}
{"text": "= A 1 ( f , g , D * test ) \u2212A 0 ( f , D test ) .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "Improvement by Data Augmentation ( Post Augmentation \u2206 ) .", "entities": [[2, 4, "TaskName", "Data Augmentation"]]}
{"text": "We simulate the data augmentation process by appending perturbed data to the training set ( Experiment 2 of Table 1 ) .", "entities": [[3, 5, "TaskName", "data augmentation"]]}
{"text": "We calculate the improvement on performance after data augmentation as the difference of test accuracies : \u2206 post_aug ( f , g , D )", "entities": [[7, 9, "TaskName", "data augmentation"]]}
{"text": "Each data point has equal probability of being assigned to positive ( l \u2032 = 1 ) or negative ( l \u2032 = 0 ) pseudo label .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "We control the perturbation probability p [ 0 , 1 ] , i.e. , an example has a specific probability p of being perturbed .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "* i is : Z \u223c U ( 0 , 1 ) , \u2200i { 1 , 2 , ... , n } x", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "( ) 3 Here Z is a random variable drawn from a uniform distribution U ( 0 , 1 ) .", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "A model for which a perturbation is more learnable experiences bigger robustness gains with data augmentation along such a perturbation .", "entities": [[14, 16, "TaskName", "data augmentation"]]}
{"text": "We provide a brief introduction to basic concepts of causal inference in Appendix B.", "entities": [[9, 11, "MethodName", "causal inference"]]}
{"text": "i ( 0 ) \u2236= 1 { f", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "\u2212 E [ Y ( 0 ) ]", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "where A ( f , g , p , D ) is the accuracy of model f ( \u22c5 ) trained with perturbation g ( \u22c5 ) at perturbation probability p on test set D. Therefore , we show that ATE is exactly the difference of accuracy on the perturbed and unperturbed test sets with random labels .", "entities": [[13, 14, "MetricName", "accuracy"], [46, 47, "MetricName", "accuracy"]]}
{"text": "We compare between the probability - based and accuracy - based metrics there .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "We find that our accuracy - based metric yields better resolution , so we report this metric in the main text of this paper .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "To test the learnability , robustness and improvement by data augmentation with different NLP models and perturbations , we experiment with four modern and representative neural NLP models : TextRNN ( Liu et al , 2016 ) , BERT ( Devlin et al , 2019 ) , RoBERTa ( Liu et al , 2019b ) and XLNet ( Yang et al , 2019 ) .", "entities": [[9, 11, "TaskName", "data augmentation"], [38, 39, "MethodName", "BERT"], [47, 48, "MethodName", "RoBERTa"], [56, 57, "MethodName", "XLNet"]]}
{"text": "For TextRNN , we use the implementation by an open - source text classification toolkit NeuralClassifier ( Liu et al , 2019a ) .", "entities": [[12, 14, "TaskName", "text classification"]]}
{"text": "For the other three pretrained models , we use the bert - base - cased , roberta - base , xlnet - base - cased versions from Hugging Face ( Wolf et al , 2020 ) , respectively .", "entities": [[20, 21, "MethodName", "xlnet"]]}
{"text": "We use three common binary text classification datasets - IMDB movie reviews ( IMDB ) ( Pang and Lee , 2005 ) , Yelp polarity reviews ( YELP ) ( Zhang et al , 2015 ) , Quora Question Pair ( QQP ) ( Iyer et al , 2017 ) - as our testbeds .", "entities": [[5, 7, "TaskName", "text classification"], [9, 12, "DatasetName", "IMDB movie reviews"], [13, 14, "DatasetName", "IMDB"], [41, 42, "DatasetName", "QQP"]]}
{"text": "IMDB and YELP datasets present the task of sentiment analysis , where each sentence is labelled as positive or negative sentiment .", "entities": [[0, 1, "DatasetName", "IMDB"], [8, 10, "TaskName", "sentiment analysis"]]}
{"text": "QQP is a paraphrase detection task , where each pair of sentences is marked as semantically equivalent or not .", "entities": [[0, 1, "DatasetName", "QQP"]]}
{"text": "To control the effect of dataset size and imbalanced classes , all datasets are randomly subsampled to the same size as IMDB ( 50k ) with balanced classes .", "entities": [[21, 22, "DatasetName", "IMDB"]]}
{"text": "We run all experiments across three random seeds and report the average results .", "entities": [[7, 8, "DatasetName", "seeds"]]}
{"text": "In fact , the major difference between different p \u2212 learnability curves is the area of lower perturbation probabilities and this provides motivation for using log AU C instead of AU C as the summarization of learnability at different p ( Section 2.1 ) .", "entities": [[34, 35, "TaskName", "summarization"]]}
{"text": "Table 2 shows the average learnability over all perturbation probabilities of each modelperturbation pair on IMDB dataset in Figure 3 . 4", "entities": [[15, 16, "DatasetName", "IMDB"]]}
{"text": "As a result , the curve for 4 Please refer to Appendix E for benchmark results on YELP ( Table 5 ) and QQP ( this perturbation substantially deviates from others in Figure 3 .", "entities": [[23, 24, "DatasetName", "QQP"]]}
{"text": "Table 2 also quantifies the trend that data augmentation with a perturbation the model is less robust to has more improvement on robustness ( Hypothesis 2 ) .", "entities": [[7, 9, "TaskName", "data augmentation"]]}
{"text": "We plot the correlations on IMDB dataset in Figure 4a and 4b .", "entities": [[5, 6, "DatasetName", "IMDB"]]}
{"text": "5 Both the correlations between 1 ) learnability vs. robustness and 2 ) learnability vs. improvement by data augmentation are strong ( Spearman | \u03c1 | > 0.6 ) and highly significant ( p - value < 0.001 ) , which firmly supports our hypotheses .", "entities": [[17, 19, "TaskName", "data augmentation"]]}
{"text": "Our findings provide insight about when the model is less robust and when data augmentation works better for improving robustness .", "entities": [[13, 15, "TaskName", "data augmentation"]]}
{"text": "Figure 4c shows that the more learnable a perturbation is for a model , the greater the likelihood that its robustness can be improved through data augmentation along this perturbation .", "entities": [[25, 27, "TaskName", "data augmentation"]]}
{"text": "We argue that this is not simply because there is more room for improvement by data augmentation .", "entities": [[15, 17, "TaskName", "data augmentation"]]}
{"text": "From a causal perspective , learnability acts as a common cause ( confounder ) for both robustness and improvement by data augmentation .", "entities": [[20, 22, "TaskName", "data augmentation"]]}
{"text": "This indicates a potential limitation of using data augmentation for improving robustness to perturbations : data augmentation is only more effective at improving robustness against perturbations more learnable for a model .", "entities": [[7, 9, "TaskName", "data augmentation"], [15, 17, "TaskName", "data augmentation"]]}
{"text": "Contrastive learning ( Gao et al , 2021 ; Yan et al , 2021 ) that pulls the representations of the original and perturbed text together , makes it difficult for the model to identify the perturbation ( reducing learnability ) and thus may help improve robustness .", "entities": [[0, 2, "MethodName", "Contrastive learning"], [9, 12, "DatasetName", "Yan et al"]]}
{"text": "We do not assume that the test accuracy of the original test set , a.k.a in - distribution accuracy , is invariant invariant against training with augmentation or not .", "entities": [[7, 8, "MetricName", "accuracy"], [18, 19, "MetricName", "accuracy"]]}
{"text": "It would be interesting to investigate the trade - off between robust accuracy and in - distribution accuracy in the future .", "entities": [[12, 13, "MetricName", "accuracy"], [17, 18, "MetricName", "accuracy"]]}
{"text": "This could be explored with other approaches in causal inference for deconfounding besides simulation on randomized control trial , such as working with real data but stratifying it ( Frangakis and Rubin , 2002 ) , to bring the learnability experiment closer to more naturalistic settings .", "entities": [[8, 10, "MethodName", "causal inference"]]}
{"text": "Although we restrict to balanced , binary classification for simplicity in this pilot study , our framework can also be extended to imbalanced , multi - class classification .", "entities": [[24, 28, "TaskName", "multi - class classification"]]}
{"text": "Interpretation of Data Augmentation .", "entities": [[2, 4, "TaskName", "Data Augmentation"]]}
{"text": "Although data augmentation has been widely used in CV ( Sato et al , 2015 ; DeVries and Taylor , 2017 ; Dwibedi et al , 2017 ) and NLP ( Wang and Yang , 2015 ; Kobayashi , 2018 ; Wei and Zou , 2019 ) , the underlying mechanism of its effectiveness remains under - researched .", "entities": [[1, 3, "TaskName", "data augmentation"]]}
{"text": "Recent studies aim to quantify intuitions of how data augmentation improves model generalization .", "entities": [[8, 10, "TaskName", "data augmentation"]]}
{"text": "Gontijo - Lopes et al ( 2020 ) introduce affinity and diversity , and find a correlation between the two metrics and augmentation performance in image classification .", "entities": [[25, 27, "TaskName", "image classification"]]}
{"text": "Our proposed learnability metric implies when data augmentation works better and thus acts as a complement to this line of research .", "entities": [[6, 8, "TaskName", "data augmentation"]]}
{"text": "( x ; \u03b2 )", "entities": [[3, 4, "HyperparameterName", "\u03b2"]]}
{"text": "1 : // \u2460 assigning random labels 2 : Initialize an empty dataset D \u2032 3 : for i in { 1 , 2 , ... , n + m } do 4 : l \u2032 i randint [ 0 , 1 ] 5 : D \u2032 D \u2032 \u222a { ( x i , l \u2032 i ) } 6 : end for 7 : // \u2461 perturbing with probabilities 8 : Initialize an empty dataset D \u2032 * 9 : for i in { 1 , 2 , ... , n + m } do 10 : z rand ( 0 , 1 ) 11 : x", "entities": [[39, 40, "DatasetName", "0"], [102, 103, "DatasetName", "0"]]}
{"text": "The aim of causal inference is to investigate how a treatment T affects the outcome Y .", "entities": [[3, 5, "MethodName", "causal inference"]]}
{"text": "Y i ( 1 ) \u2212 Y i ( 0 ) .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "( 9 ) Here , Y i ( 1 ) is the outcome Y of individual i that receives treatment ( T = 1 ) , while Y i ( 0 ) is the opposite .", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "\u2212 E [ Y ( 0 ) ] .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "( 10 ) ATE quantifies how the outcome Y is expected to change if we modify the treatment T from 0 to 1 .", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "Y i ( 0 ) \u2236= P f ( L \u2032 = l \u2032", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "( 0 , 1 ) .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "( 0 , 1 ) .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "i ( 0 )", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "E [ Y ( 0 ) ]", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "Y i ( 0 ) \u2236= 1 { f", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "= x i ) > 0.5 } { 0 , 1 } .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "That is to say , accuracy - based AT E tends to vary more drastically than probability - based if inconsistent predictions occur more often , and thus can better capture the nuance of perturbation learnability .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "As a result , we choose accuracy - based ATE as the primary measurement of learnability in this paper .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Inspired by Precision @ K in Information Retrieval ( IR ) , we propose a similar metric dubbed Learnability @ p , which is the learnability of a perturbation for a model at a specific perturbation probability p.", "entities": [[0, 1, "DatasetName", "Inspired"], [2, 3, "MetricName", "Precision"], [6, 8, "TaskName", "Information Retrieval"]]}
{"text": "We are primarily interested in whether a selected p can represent the learnability over different perturbation probabilities and correlates well with robustness and post data augmentation \u2206. We calculate the standard deviation ( \u03c3 ) of Learnability @ p and average learnability ( log AU C ) over all model - perturbation pairs to measure how well it can distinguish between different models and perturbations .", "entities": [[24, 26, "TaskName", "data augmentation"]]}
{"text": "To investigate the strength of the correlations , we also calculate Spearman \u03c1 between accuracy - based / probability - based learnability @ p vs. average learnability / robustness / post data augmentation \u2206 over all model - perturbation pairs .", "entities": [[14, 15, "MetricName", "accuracy"], [31, 33, "TaskName", "data augmentation"]]}
{"text": "Table 4 shows that generally average learnability has stronger correlation than Learnability @ p. Correlations with both robustness and post data augmentation \u2206 peak at p = 0.02 for accuracybased / probability - based measurements , and the correlations with average learnability ( 0.816*/0.886 * ) are also strong at these perturbation probabilities .", "entities": [[20, 22, "TaskName", "data augmentation"]]}
{"text": "Overall , Learnability @ p with higher standard deviation correlates better with average learnability , robustness and post data augmentation \u2206. Our analysis shows that if p is carefully selected by \u03c3 , Learnability @ p is also a promising metric , though not as accurate as average learnability .", "entities": [[18, 20, "TaskName", "data augmentation"]]}
{"text": "3 ) of each model - perturbation pair on QQP dataset .", "entities": [[9, 10, "DatasetName", "QQP"]]}
{"text": "Explanation Graph Generation via Pre - trained Language Models : An Empirical Study with Contrastive Learning", "entities": [[1, 3, "TaskName", "Graph Generation"], [14, 16, "MethodName", "Contrastive Learning"]]}
{"text": "Next , we leverage these graphs in different contrastive learning models with Max - Margin and InfoNCE losses .", "entities": [[8, 10, "MethodName", "contrastive learning"], [16, 17, "MethodName", "InfoNCE"]]}
{"text": "Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks .", "entities": [[11, 12, "MetricName", "accuracy"], [21, 23, "TaskName", "graph generation"]]}
{"text": "Lastly , we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human - like negative graphs can lead to further improvements .", "entities": [[12, 14, "MethodName", "contrastive learning"]]}
{"text": "Pre - trained sequence - to - sequence language models ( PLMs ) like BART ( Lewis et al , 2020 )", "entities": [[14, 15, "MethodName", "BART"]]}
{"text": "and ( Saha et al , 2021b ) showing the belief , argument , stance , gold explanation graph , and T5 - generated explanation graph .", "entities": [[21, 22, "MethodName", "T5"]]}
{"text": "T5 ( Raffel et al , 2020 ) have led to significant advances in many natural language generation tasks like text summarization and machine translation .", "entities": [[0, 1, "MethodName", "T5"], [20, 22, "TaskName", "text summarization"], [23, 25, "TaskName", "machine translation"]]}
{"text": "For example , consider a recently proposed commonsense explanation graph generation task shown in Fig .", "entities": [[9, 11, "TaskName", "graph generation"]]}
{"text": "These explanation graphs encode structured knowledge ( augmented with commonsense ) and consist of concepts as nodes and relations from ConceptNet ( Liu and Singh , 2004 ) as edges .", "entities": [[20, 21, "DatasetName", "ConceptNet"]]}
{"text": "Following Saha et al ( 2021b ) , we represent graphs as strings composed of concatenated edges and fine - tune T5 to generate graphs in an autoregressive manner .", "entities": [[21, 22, "MethodName", "T5"]]}
{"text": "1 shows a graph generated by T5 that is disconnected and hence structurally incorrect .", "entities": [[6, 7, "MethodName", "T5"]]}
{"text": "Both T5 - generated graphs shown in Fig . 1 contain incoherent or noncommonsensical edges ( marked by dashed arrows ) like \" fast food ; has context ; salads \" .", "entities": [[1, 2, "MethodName", "T5"]]}
{"text": "While a general recipe towards improving the structural and semantic aspects of graph generation can be via large - scale training with more humanannotated graphs , it is prohibitive under most practical scenarios because of the cognitive load associated with a complex data creation task like graph annotation Saha et", "entities": [[12, 14, "TaskName", "graph generation"]]}
{"text": "Overall , we leverage three types of negative graphs ( synthetic structural , synthetic semantic , and human - created semantic ) and develop multiple contrastive learning models ( Hjelm et al , 2018 ; Chen et al , 2020a ; Khosla et al , 2020 ; Gunel et al , 2020 ) for effectively distinguishing between correct and incorrect graphs .", "entities": [[25, 27, "MethodName", "contrastive learning"]]}
{"text": "Our first method is a Generate - and - Refine model that first generates an initial graph and further refines it using another T5 model .", "entities": [[23, 24, "MethodName", "T5"]]}
{"text": "On two real - world tasks of explanation graph generation and temporal graph generation , with varied node and edge semantics , we observe that our proposed methods and graph perturbation techniques generalize well and lead to improvements in both structural and semantic accuracy of graphs .", "entities": [[8, 10, "TaskName", "graph generation"], [12, 14, "TaskName", "graph generation"], [43, 44, "MetricName", "accuracy"]]}
{"text": "Further analysis of different types of negative graphs reveal that the human - error graphs are the hardest , most diverse , and hence the best type of negatives to learn from in contrastive learning .", "entities": [[33, 35, "MethodName", "contrastive learning"]]}
{"text": "We present a detailed analysis of graph structure and semantics for end - to - end explanation graph generation via pre - trained language models .", "entities": [[17, 19, "TaskName", "graph generation"]]}
{"text": "We propose simple yet effective graph perturbation techniques for constructing positive and negative graphs and use them in different graph contrastive learning models .", "entities": [[20, 22, "MethodName", "contrastive learning"]]}
{"text": "Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks .", "entities": [[11, 12, "MetricName", "accuracy"], [21, 23, "TaskName", "graph generation"]]}
{"text": "Graph Generation from Language Models .", "entities": [[0, 2, "TaskName", "Graph Generation"]]}
{"text": "Representative works on graph generation from language models include knowledge graph completion models like Comet Hwang et al , 2021 )", "entities": [[3, 5, "TaskName", "graph generation"], [9, 12, "TaskName", "knowledge graph completion"]]}
{"text": "that fine - tune GPT ( Radford et al , 2019 ; Brown et al , 2020 ) and BART ( Lewis et al , 2020 ) , generation of event influence graphs ( Tandon et al , 2019 ; Madaan et al , 2020 ) , partially ordered scripts ( Sakaguchi et al , 2021 ) , temporal graphs ( Madaan and Yang , 2021 ) , entailment trees , proof graphs ( Saha et al , 2020 ; Saha et al , 2021a ) and commonsense explanation graphs ( Saha et al , 2021b ) .", "entities": [[4, 5, "MethodName", "GPT"], [19, 20, "MethodName", "BART"]]}
{"text": "Linguistic tasks like syntactic parsing Mohammadshahi and Henderson , 2021 ; Kondratyuk and Straka , 2019 ) and semantic parsing ( Chen et al , 2020b ; Shin et al , 2021 ) have also made use of language models .", "entities": [[18, 20, "TaskName", "semantic parsing"]]}
{"text": "Our novelty lies in presenting the first systematic analysis of structure and semantics of graph generation for two downstream NLP tasks using pre - trained language models and improving them via constrastive learning .", "entities": [[14, 16, "TaskName", "graph generation"]]}
{"text": "Data Augmentation and Contrastive Learning .", "entities": [[0, 2, "TaskName", "Data Augmentation"], [3, 5, "MethodName", "Contrastive Learning"]]}
{"text": "Data Augmentation for NLP ( Hedderich et al , 2020 ; has been a powerful tool in low - data settings , ranging from its early usages with synonym replacement ( Kolomiyets et al , 2011 ; Wang and Yang , 2015 ) to more recent methods of perturbing hidden representations ( Miyato et al , 2016 ; .", "entities": [[0, 2, "TaskName", "Data Augmentation"]]}
{"text": "Contrastive learning , beyond its historical use in learning robust image representations ( Chopra et al , 2005 ; Hadsell et al , 2006 ; Gutmann and Hyv\u00e4rinen , 2010 ; Hoffer and Ailon , 2015 ; Hjelm et al , 2018 ; Chen et al , 2020a ; He et al , 2020 ) has been explored in supervised scenarios ( Khosla et al , 2020 ; Gunel et al , 2020 ) and for NLP , in training self - supervised language models ( Fang et al , 2020 ) , learning sentence representations ( Gao et al , 2021 ) , document clustering , summarization Cao and Wang , 2021 ) and generic text generation .", "entities": [[0, 2, "MethodName", "Contrastive learning"], [107, 108, "TaskName", "summarization"], [116, 118, "TaskName", "text generation"]]}
{"text": "It has also been used in unconditional graph representation learning ( You et al , 2020 ; Hassani and Khasahmadi , 2020 ; .", "entities": [[7, 10, "TaskName", "graph representation learning"]]}
{"text": "We follow this rich line of work to explore their applicability in supervised graph generation tasks from pretrained language models in low - resource settings .", "entities": [[13, 15, "TaskName", "graph generation"], [17, 20, "TaskName", "pretrained language models"]]}
{"text": "al , 2020 ; Sakaguchi et al , 2020 ; Talmor et al , 2021 ) , recent focus on generative evaluation have led to the development of tasks and benchmarks that explore unstructured commonsense sentence generation ( Lin et al , 2020 ) , event influence graph generation ( Madaan et al , 2020 ) , commonsense explanation graph generation ( Saha et al , 2021b ) , etc .", "entities": [[47, 49, "TaskName", "graph generation"], [59, 61, "TaskName", "graph generation"]]}
{"text": "We experiment with two graph generation tasks , primarily focusing on ExplaGraphs ( Saha et al , 2021b ) because of the clear distinction in the underlying structural constraints and the semantic aspect dealing with commonsense .", "entities": [[4, 6, "TaskName", "graph generation"]]}
{"text": "Our primary task of interest is a recently proposed commonsense explanation graph generation task called ExplaGraphs ( Saha et al , 2021b ) .", "entities": [[11, 13, "TaskName", "graph generation"]]}
{"text": "In Sec . 6.4 , we also experiment with another related task of temporal graph generation ( Madaan et al , 2020 ) .", "entities": [[14, 16, "TaskName", "graph generation"]]}
{"text": "In this task , given a belief and an argument , an agent has to perform two sub - tasks - predict the stance ( support / counter ) and also generate an explanation graph explaining the stance .", "entities": [[12, 13, "DatasetName", "agent"]]}
{"text": "Baseline T5 Model .", "entities": [[1, 2, "MethodName", "T5"]]}
{"text": "The stance prediction model is a fine - tuned RoBERTa model ( Liu et al , 2019 ) which we keep unaltered from prior work and focus on the graph generation sub - task .", "entities": [[9, 10, "MethodName", "RoBERTa"], [29, 31, "TaskName", "graph generation"]]}
{"text": "We generate graphs as linearized strings in an endto - end manner by leveraging an encoder - decoder pre - trained language model , T5 ( Raffel et al , 2020 ) .", "entities": [[24, 25, "MethodName", "T5"]]}
{"text": "While we choose T5 because of its superior performance ( Saha et al , 2021b ) , we do not make any model - specific assumptions and graphs can be generated via any encoder - decoder style pre - trained language model ( e.g. , see Appendix E for results with BART ) .", "entities": [[3, 4, "MethodName", "T5"], [51, 52, "MethodName", "BART"]]}
{"text": "Analysis of T5 Baseline .", "entities": [[2, 3, "MethodName", "T5"]]}
{"text": "We analyze the quality of the explanation graphs generated by T5 in Table 1 .", "entities": [[10, 11, "MethodName", "T5"]]}
{"text": "While the structural accuracy improves with increase in training data , the gain saturates quickly and even after training on the entire data , we find a significant fraction of graphs to violate the structural constraints .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "We note that a high 91 % of T5 's generations are valid graph encodings i.e. , the generated strings can be parsed into graphical structures ( without any post - processing ) , suggesting that T5 is able to learn the graph encoding from a fairly small amount of supervision .", "entities": [[8, 9, "MethodName", "T5"], [36, 37, "MethodName", "T5"]]}
{"text": "In Fig . 1 , we show examples of structurally incorrect and semantically incorrect graphs generated by T5 .", "entities": [[17, 18, "MethodName", "T5"]]}
{"text": "Hence , our approach towards improving explanation graph generation is through data augmentation techniques that perturb human - curated graphs to construct positive and negative graphs .", "entities": [[7, 9, "TaskName", "graph generation"], [11, 13, "TaskName", "data augmentation"]]}
{"text": "Semantic incorrectness typically arises from inappropriate relations that do not adhere to human commonsense ( \" loss of jobs ; is a ; humane \" ) .", "entities": [[16, 17, "MetricName", "loss"]]}
{"text": "4 Humans make subtle errors , thus making them ideal negative candidates for contrastive learning .", "entities": [[13, 15, "MethodName", "contrastive learning"]]}
{"text": "Next we propose different methods of leveraging these positive and negative graphs for explanation graph generation .", "entities": [[14, 16, "TaskName", "graph generation"]]}
{"text": "Our models either use only positive graphs as simple data augmentation , only negative graphs in a max - margin model , or both in a Generate & Refine model and a Contrastive model .", "entities": [[9, 11, "TaskName", "data augmentation"]]}
{"text": "In this first simple approach , we augment the training data with the synthetically created positive graphs and retrain the baseline T5 model .", "entities": [[21, 22, "MethodName", "T5"]]}
{"text": "i } l i=1 respectively , we define the loss function L as a linear combination of the standard crossentropy loss L CE and a max - margin loss L MM , defined between a word y ( g )", "entities": [[9, 10, "MetricName", "loss"], [20, 21, "MetricName", "loss"], [28, 29, "MetricName", "loss"]]}
{"text": "i \u2212logP \u03b8 ( y ( g )", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "\u2212 log P \u03b8 ( y ( n )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "+ \u03b2 )", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "L = L CE + \u03b1L MM where \u03b1 and \u03b2 ( margin ) are hyperparameters .", "entities": [[8, 9, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b2"]]}
{"text": "As noted earlier , the baseline model often makes commonsense mistakes in distinguishing between positive and negative relations ( \" causes \" vs \" not causes \" ) and our relation perturbing negative graphs and the max - margin loss component facilitate learning a better boundary between them .", "entities": [[39, 40, "MetricName", "loss"]]}
{"text": "Specifically , our approach is a 2 - stage pipeline - first , an initial graph is generated by the baseline T5 model and second , an Explanation Graph Refinement model conditions on the initial graph , along with the belief , argument and the stance to refine the graph .", "entities": [[21, 22, "MethodName", "T5"]]}
{"text": "The refiner is also a T5 model fine - tuned with the prefix \" Refine the Explanation Graph for \" on all positive and negative graphs described in Sec . 4 .", "entities": [[5, 6, "MethodName", "T5"]]}
{"text": "Our Contrastive Graph Generation Model ( Fig . 2 ) also leverages both positive and negative graphs but instead of doing so in a 2 - stage Generate & Refine model , uses a contrastive learning framework ( Khosla et al , 2020 ; Gunel et al , 2020 ) .", "entities": [[2, 4, "TaskName", "Graph Generation"], [34, 36, "MethodName", "contrastive learning"]]}
{"text": "Given a ground - truth graph G ( g ) , a positive graph G ( p ) and a set of negative graphs { G ( n ) i } M i=1 , contrastive learning aims to learn the graph representations such that the gold graph 's representation is close to that of the synthetic positive graph while being distant from those of the negative graphs .", "entities": [[34, 36, "MethodName", "contrastive learning"]]}
{"text": "Similar to Cao and Wang ( 2021 ) , we use the last layer of the decoder in T5 as the representation of each token in the graph and obtain the graph representation by averaging over the constituent token representations .", "entities": [[18, 19, "MethodName", "T5"]]}
{"text": "+ where \u03b1 and the temperature \u03c4 are the hyperparameters and sim ( ) denotes the cosine similarity function between the graph representations .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "While our primary metrics of interest are Graph Structural Accuracy ( StCA ) and Semantic Accuracy ( SeCA ) , following prior work ( Saha et al , 2021b ) , we also report Stance Accuracy ( SA ) , Graph - BertScore ( G - BS ) , Graph Edit Distance ( GED ) and Edge Accuracy ( EA ) .", "entities": [[9, 10, "MetricName", "Accuracy"], [15, 16, "MetricName", "Accuracy"], [35, 36, "MetricName", "Accuracy"], [53, 54, "DatasetName", "GED"], [57, 58, "MetricName", "Accuracy"]]}
{"text": "We observe that using a larger T5 model improves StCA by 12 % and SeCA by 16 % .", "entities": [[6, 7, "MethodName", "T5"]]}
{"text": "Together with the results reported in Table 1 , we conclude that much of the improvement in explanation graph generation comes from increasing the training data and using a larger model .", "entities": [[18, 20, "TaskName", "graph generation"]]}
{"text": "Given its superior performance , we build our proposed models on T5 - large .", "entities": [[11, 12, "MethodName", "T5"]]}
{"text": "We compare the graphs generated by T5 and our Max - Margin model on Amazon Mechanical Turk where three annotators choose which graph is better or if they are mostly similar ( instructions in Appendix F ) .", "entities": [[6, 7, "MethodName", "T5"]]}
{"text": "With majority voting on 150 samples , we observe that our Max - Margin model 's graphs are preferred 13 % more times compared to those of the T5 model ( 43 % vs 30 % and statistically significant with p < 0.05 ) while in 22 % cases , the graphs are marked similar ( remaining have no majority ) .", "entities": [[28, 29, "MethodName", "T5"]]}
{"text": "In ness and diversity in these graphs and hence are the best candidates for contrastive learning .", "entities": [[14, 16, "MethodName", "contrastive learning"]]}
{"text": "We test the generalizability of constructing structurally and semantically perturbed graphs for contrastive learning by also experimenting on a temporal graph generation task ( Madaan and Yang , 2021 ) that requires constructing a temporal graph from a document .", "entities": [[12, 14, "MethodName", "contrastive learning"], [20, 22, "TaskName", "graph generation"]]}
{"text": "Following our overall goal of improving graph generation with limited data , we randomly sample 1.3 % of the overall corpus ( \u223c9.5k samples ) as the training data such that all graphs are connected DAGs .", "entities": [[6, 8, "TaskName", "graph generation"]]}
{"text": "Unlike T5 , our models ' graphs are both structurally and semantically correct with diverse commonsense nodes ( \" Groupthink \" , \" Good Thing \" ) .", "entities": [[1, 2, "MethodName", "T5"]]}
{"text": "While our models generate more correct graphs , they lack in structural diversity - the Contrastive model generates 77 % of linear graphs ( i.e. , the nodes are in a linear chain ) which is comparable to 75 % in the T5 model .", "entities": [[42, 43, "MethodName", "T5"]]}
{"text": "Structural diversity is not a measure of graph correctness ; however , like diverse text generation ( Vijayakumar et al , 2018 ) , generating diverse graphs is an interesting direction for future work .", "entities": [[14, 16, "TaskName", "text generation"]]}
{"text": "Moreover , we see in the previous section that humanerror graphs are the best negative candidates for contrastive learning ( which is intuitive since tricky and subtle errors made by expert human annotators would make for some of the hardest negatives / distractors for a contrastive learning model to learn from ) .", "entities": [[17, 19, "MethodName", "contrastive learning"], [45, 47, "MethodName", "contrastive learning"]]}
{"text": "We first fine - tune a T5 model that conditions on the belief , argument and the stance to generate a set of incorrect edges ( which is the set of edges that are present in the incorrect graph and not in the refined graph ) .", "entities": [[6, 7, "MethodName", "T5"]]}
{"text": "Human - like Negative Graph Construction .", "entities": [[4, 6, "TaskName", "Graph Construction"]]}
{"text": "Hence , we control the quality of the generated incorrect graphs by the following two techniques - ( a ) Thresholding via fraction of Acceptable Edges ( AE ) :", "entities": [[27, 28, "MethodName", "AE"]]}
{"text": "We compute the fraction of acceptable edges for every generated negative graph and choose only those graphs with AE above a certain threshold \u03b4 .", "entities": [[18, 19, "MethodName", "AE"], [23, 24, "HyperparameterName", "\u03b4"]]}
{"text": "We use our SeCA metric model ( that classifies a graph into support , counter , or incorrect class ) to compute the probability of the generated graph being incorrect and choose those graphs that are above a certain threshold \u03b3 of incorrect probability .", "entities": [[40, 41, "HyperparameterName", "\u03b3"]]}
{"text": "These initial promising results for emulating hard / tricky human errors as strong negatives for contrastive learning will hopefully lead to further future work in this interesting direction .", "entities": [[15, 17, "MethodName", "contrastive learning"]]}
{"text": "We presented an empirical study of graph structure and semantics for end - to - end explanation graph generation from pre - trained language models and showed that the generated graphs often violate structural constraints or are semantically incorrect .", "entities": [[17, 19, "TaskName", "graph generation"]]}
{"text": "We significantly improve both the structural and semantic accuracy of graph generation by proposing contrastive learning models that leverage simple yet efficient methods of graph perturbations and also generalize to similar graph generation tasks .", "entities": [[8, 9, "MetricName", "accuracy"], [10, 12, "TaskName", "graph generation"], [14, 16, "MethodName", "contrastive learning"], [31, 33, "TaskName", "graph generation"]]}
{"text": "Explanation graph generation improves the interpretability of neural commonsense reasoning systems and could prove to be effective in understanding and debugging such models .", "entities": [[1, 3, "TaskName", "graph generation"]]}
{"text": "It treats a graph as a set of edges and computes the best match between the gold edges and the predicted edges , where the matching score between a pair of edges is given by the BertScore F1 .", "entities": [[37, 38, "MetricName", "F1"]]}
{"text": "Graph Edit Distance ( GED ) .", "entities": [[4, 5, "DatasetName", "GED"]]}
{"text": "GED is the standard Graph Edit Distance for graphs , measuring the number of edit operations ( addition , deletion , and replacement of nodes and edges ) to transform one graph to the other and further normalized by an appropriate normalizing constant .", "entities": [[0, 1, "DatasetName", "GED"]]}
{"text": "Edge Accuracy ( EA ) .", "entities": [[1, 2, "MetricName", "Accuracy"]]}
{"text": "The final metric , Edge Accuracy ( EA ) measures the fraction of edges in the graph that are important .", "entities": [[5, 6, "MetricName", "Accuracy"]]}
{"text": "The task of temporal graph generation requires constructing a temporal graph from a document ( see Fig . 4 ) .", "entities": [[4, 6, "TaskName", "graph generation"]]}
{"text": "For Temporal Graph Generation , we randomly sample 1.3 % of the overall corpus ( Madaan and Yang , 2021 ) .", "entities": [[2, 4, "TaskName", "Graph Generation"]]}
{"text": "Following our overall goal of improving graph generation in limited data settings , we randomly sample 1.3 % of the overall corpus ( \u223c 9.5k samples ) as the training corpus such that all graphs are connected DAGs .", "entities": [[6, 8, "TaskName", "graph generation"]]}
{"text": "\u03b1 is manually tuned in the range", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "T5 model on the facts based on ConceptNet relations from ATOMIC - 2020 ( Hwang et al , 2021 , a large - scale commonsense knowledge base .", "entities": [[0, 1, "MethodName", "T5"], [7, 8, "DatasetName", "ConceptNet"], [10, 11, "DatasetName", "ATOMIC"]]}
{"text": "Next , we fine - tune this model further on the end - task of graph generation which leads to small improvements in both StCA and SeCA .", "entities": [[15, 17, "TaskName", "graph generation"]]}
{"text": "In Fig . 6 and 7 , our proposed models improve upon Contrastive Graph T5 - generated Graph Semantically Incorrect Figure 7 : Example of explanation graphs generated by different models .", "entities": [[14, 15, "MethodName", "T5"]]}
{"text": "The baseline T5 - generated graph is semantically incorrect ( incoherent relations marked in dashed red ) while our proposed models generate both structurally and semantically correct graphs .", "entities": [[2, 3, "MethodName", "T5"]]}
{"text": "This work was supported by DARPA MCS Grant N66001 - 19 - 2 - 4031 , NSF - CAREER Award 1846185 , DARPA YFA17 - D17AP00022 , ONR Grant N00014 - 18 - 1 - 2871 , Microsoft Investigator Fellowship , and Munroe & Rebecca Cobey Fellowship .", "entities": [[5, 6, "DatasetName", "DARPA"], [22, 23, "DatasetName", "DARPA"]]}
{"text": "Structural Correctness Accuracy of Graphs ( StCA ) .", "entities": [[2, 3, "MetricName", "Accuracy"]]}
{"text": "For computing SeCA , prior work trains a 3 - way RoBERTa ( Liu Figure 6 : Example of explanation graphs generated by different models .", "entities": [[11, 12, "MethodName", "RoBERTa"]]}
{"text": "The baseline T5 - generated graph is semantically incorrect ( incoherent relations marked in dashed red ) while our proposed models generate both structurally and semantically correct graphs .", "entities": [[2, 3, "MethodName", "T5"]]}
{"text": "the incorrect semantic relations from the T5 baseline graphs .", "entities": [[6, 7, "MethodName", "T5"]]}
{"text": "Overall , our quantitative results and human evaluation suggest that there is significant room for improvement on the task of commonsense explanation graph generation .", "entities": [[22, 24, "TaskName", "graph generation"]]}
{"text": "Structurally Incorrect T5 - generated Graph Semantically Incorrect Figure 9 : Example of explanation graphs generated by different models .", "entities": [[2, 3, "MethodName", "T5"]]}
{"text": "T5 generates a semantically incorrect graph .", "entities": [[0, 1, "MethodName", "T5"]]}
{"text": "Syntactic analyses and named entity recognition for PubMed and PubMed Central - up - to - the - minute", "entities": [[3, 6, "TaskName", "named entity recognition"]]}
{"text": "In this paper we present a publicly available resource distributing preprocessed biomedical literature including sentence splitting , tokenization , part - of - speech tagging , syntactic parses and named entity recognition .", "entities": [[19, 25, "TaskName", "part - of - speech tagging"], [29, 32, "TaskName", "named entity recognition"]]}
{"text": "For instance the GENIA shared tasks focusing on extracting biological events , such as gene regulations , have consistently gathered wide interest and have led to the development of several text mining tools ( Miwa et al , 2012 ; Bj\u00f6rne and Salakoski , 2013 ) .", "entities": [[3, 4, "DatasetName", "GENIA"]]}
{"text": "All the data is downloadable in an easily handleable XML format , also used by the widely adapted event extraction system TEES ( Bj\u00f6rne and Salakoski , 2015 ) .", "entities": [[18, 20, "TaskName", "event extraction"]]}
{"text": "These documents are subsequently split into sentences using GENIA sentence splitter ( Saetre et al , 2007 ) as most linguistic analyses are done on the sentence level .", "entities": [[8, 9, "DatasetName", "GENIA"]]}
{"text": "GENIA sentence splitter is trained on biomedical text ( GENIA corpus ) and has state - of - the - art performance on this domain .", "entities": [[0, 1, "DatasetName", "GENIA"], [9, 10, "DatasetName", "GENIA"]]}
{"text": "We chose to use this tool as the performance of the TEES software has been previously evaluated on a large - scale together with this parsing pipeline ( Van Landeghem et al , 2013b ) and it should be a reliable choice for biomedical relation extraction .", "entities": [[44, 46, "TaskName", "relation extraction"]]}
{"text": "Since dependency parsing has become the prevalent approach in modeling syntactic relations , we also provide conversions to the collapsed Stanford dependency scheme ( De Marneffe et al , 2006 ) .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "Named entity recognition ( NER ) is one of the fundamental tasks in BioNLP as most of the cru - ( Okazaki , 2007 ) .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"]]}
{"text": "NERsuite was selected as several biological models are readily available for this software ( Kaewphan et al , 2016 ; Pyysalo and Ananiadou , 2014 ) and as it supports label weighting ( Minkov et al , 2006 ) unlike many other NER tools .", "entities": [[42, 43, "TaskName", "NER"]]}
{"text": "For cell line names we use a publicly available state - of - the - art model ( Kaewphan et al , 2016 ) , whereas for the other entity types we train our own models with manually annotated data from GENETAG ( Tanabe et al , 2005 ) , CHEMDNER ( Krallinger et al , 2015 ) , SPECIES ( Pafilis et al , 2013 ) and NCBI disease (", "entities": [[68, 70, "DatasetName", "NCBI disease"]]}
{"text": "In addition , named entity recognition for several entity types is carried out .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}
{"text": "We are currently improving the pipeline to automatically reprocess the failed batches with the problematic articles excluded to minimize the loss of data .", "entities": [[20, 21, "MetricName", "loss"]]}
{"text": "Unfortunately we do not have exact processing time statistics for named entity recognition and thus estimate its computational requirements by extrapolating from a smaller test run .", "entities": [[10, 13, "TaskName", "named entity recognition"]]}
{"text": "Based on this experiment NER has demanded 4 , 100 CPU hours thus far .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "As many of the NER training corpora include only abstracts and are limited to specific domains , the generalizability of the trained NER models to full articles and to the wide spectrum of topics covered in PubMed is not clear .", "entities": [[4, 5, "TaskName", "NER"], [22, 23, "TaskName", "NER"]]}
{"text": "BOUN - ISIK Participation : An Unsupervised Approach for the Named Entity Normalization and Relation Extraction of Bacteria Biotope\u1e61", "entities": [[14, 16, "TaskName", "Relation Extraction"]]}
{"text": "For the normalization of entities , we utilized word embeddings and syntactic re - ranking .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "For the relation extraction task , pre - defined rules are used .", "entities": [[2, 4, "TaskName", "relation extraction"]]}
{"text": "This year 's task has presented the opportunity to the participants to develop solutions for three subproblems : normalization ( BB - norm ) , relation extraction ( BB - rel ) , and knowledge base extraction ( BB - kb ) .", "entities": [[25, 27, "TaskName", "relation extraction"]]}
{"text": "For the BB - norm task of the Bacteria Biotope Task of the BioNLP Shared Task 2019 , the participants are expected to develop systems to link the named entities ( Microorganism , Habitat , and Phenotype ) in a given text through a given ontology , when the entities are given with their boundaries .", "entities": [[45, 46, "MethodName", "ontology"]]}
{"text": "For instance , the sample sentence \" Atypical mycobacteria causing non - pulmonary disease in Queensland . \" consists of the following mentions : \" mycobacteria \" microorganism mention , \" causing non - pulmonary disease \" phenotype mention , and \" pulmonary \" habitat mention , which should be normalized to the \" Mycobacteria \" term in the NCBI taxonomy , and \" human pathogen \" and \" lung \" terms in the Onto - Biotope ontology , respectively .", "entities": [[77, 78, "MethodName", "ontology"]]}
{"text": "This paper presents our participating system for two sub - tasks : one for the BB - norm ( Entity Normalization ) sub - task and one for the BB - rel ( Relation Extraction ) sub - task .", "entities": [[33, 35, "TaskName", "Relation Extraction"]]}
{"text": "For the entity normalization sub - task , we utilized word embeddings and syntactic re - ranking to normalize the entities .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "On the other hand , for the relation extraction sub - task , we proposed a rule - based method .", "entities": [[7, 9, "TaskName", "relation extraction"]]}
{"text": "Relation extraction in the bacteria biotopes domain has also attracted considerable attention owing to the BioNLP Bacteria Biotope Shared Tasks .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}
{"text": "Previous work in the bacteria biotopes domain consists of the extraction of relations between bacteria entities and habitat entities ( Localization Relation Extraction ) and of relations between two habitat entities ( Part Of Relation Extraction ) .", "entities": [[21, 23, "TaskName", "Relation Extraction"], [34, 36, "TaskName", "Relation Extraction"]]}
{"text": "The participants of the BioNLP Shared Task 2011 , which is the first shared task that addressed the relation extraction task of bacteria biotopes , utilized both machine learning and rule - based approaches for detecting the Localization and Part - of relations among bacteria and habitats ( Bossy et al , 2011 ) .", "entities": [[18, 20, "TaskName", "relation extraction"], [39, 42, "DatasetName", "Part - of"]]}
{"text": "For this subtask , the best F - score ( 42 % ) was obtained by the TEES 2.1 system ( Bj\u00f6rne and Salakoski , 2013 ) , which used support vector machine classification .", "entities": [[30, 33, "MethodName", "support vector machine"]]}
{"text": "In the BioNLP Shared Task 2016 , the VERSE team ( Lever and Jones , 2016 ) achieved the best F - score , which is 56 % , on the relation extraction sub - task of Bacteria Biotopes by utilizing support vector machines .", "entities": [[8, 9, "MethodName", "VERSE"], [31, 33, "TaskName", "relation extraction"]]}
{"text": "Since our system for the named entity normalization and relation extraction of bacteria biotopes is based on unsupervised approaches and does not require any labeled training data , the errors of the developed system are analyzed on the provided training and the development sets .", "entities": [[9, 11, "TaskName", "relation extraction"]]}
{"text": "The BB - norm task includes the normalization of Habitat entities and Phenotype entities in a given set of documents through the Onto - Biotope ontology and the normalization of Microorganism entities through the NCBI Taxonomy .", "entities": [[25, 26, "MethodName", "ontology"]]}
{"text": "According to this approach , for the normalization of an entity mention , the top k semantically most similar ontology concepts are found at the first step using the word embedding representations of the entity mention and the ontology concepts .", "entities": [[19, 20, "MethodName", "ontology"], [38, 39, "MethodName", "ontology"]]}
{"text": "At the second step , these top k semantically most similar concepts are re - ranked according to a similarity metric that utilizes the constituency parses of the entity mention and ontology concept phrases .", "entities": [[31, 32, "MethodName", "ontology"]]}
{"text": "The resulting most similar ontology concept is assigned as the normalized concept for the corresponding mention .", "entities": [[4, 5, "MethodName", "ontology"]]}
{"text": "In the pre - processing step , the named entity mentions and the ontology concept names are tokenized , and the stop - words are removed from the mentions and the ontology concept names .", "entities": [[13, 14, "MethodName", "ontology"], [31, 32, "MethodName", "ontology"]]}
{"text": "Following this intuition , the semantic similarity between named entity mentions and ontology concept terms would be higher for the similar pairs , and lower for the dissimilar pairs , if the words can be converted into a machine processable format such as real - valued vectors .", "entities": [[5, 7, "TaskName", "semantic similarity"], [12, 13, "MethodName", "ontology"]]}
{"text": "For the multiword named entity mentions and ontology concept terms , the vector representations are obtained by averaging the real - valued vectors of their composing words .", "entities": [[7, 8, "MethodName", "ontology"]]}
{"text": "After the vector representations are obtained for each entity mention and for each ontology concept term , the semantic similarity between each pair is computed by using the cosine similarity .", "entities": [[13, 14, "MethodName", "ontology"], [18, 20, "TaskName", "semantic similarity"]]}
{"text": "For each entity mention , the top k most similar ontology concepts are retained as candidates for further processing , i.e. , for syntactic weighting based re - ranking .", "entities": [[10, 11, "MethodName", "ontology"]]}
{"text": "The semantic similarities are recomputed using the mathematical formulation shown in Equation ( 1 ) , which considers also the similarity between the head words of the entity mention and ontology concept pair .", "entities": [[30, 31, "MethodName", "ontology"]]}
{"text": "In Equation ( 1 ) , S RR ( m , c ) is the final computed similarity between mention m and the candidate concept c , and S S is the semantic similarity , in which m head is the head word of the mention m and c head is the head word of the concept c ,", "entities": [[7, 8, "DatasetName", "RR"], [32, 34, "TaskName", "semantic similarity"]]}
{"text": "S S ( m , c ) is the similarity between mention m and concept c computed as described in Section 3.1.1 , and w is a weighting parameter which can take values between 0 and 1 .", "entities": [[34, 35, "DatasetName", "0"]]}
{"text": "Error analysis on the training and developments data sets revealed that applying some rules may improve the results .", "entities": [[0, 1, "MetricName", "Error"]]}
{"text": "In this kind of cases , if an exact match does not exist , the previously mentioned similar entities in the text are searched .", "entities": [[8, 10, "MetricName", "exact match"]]}
{"text": "4 Relation Extraction", "entities": [[1, 3, "TaskName", "Relation Extraction"]]}
{"text": "Our system for the relation extraction sub - task is based on the naive assumption that the related entities for most of the relations appear within the same sentence .", "entities": [[4, 6, "TaskName", "relation extraction"]]}
{"text": "Since our rule - based system for relation extraction is based on the assumption that most of the relations appear within the same sentences , our system is not able to catch the relations that cross sentence boundaries .", "entities": [[7, 9, "TaskName", "relation extraction"]]}
{"text": "In the BioNLP Shared Task 2019 Bacteria Biotopes relation extraction sub - task , entities are given with their boundaries in the text and the participants are asked to predict the relations between the entities .", "entities": [[8, 10, "TaskName", "relation extraction"]]}
{"text": "The performances of the submitted systems are evaluated with their F1 ( F - measure ) , recall and precision values .", "entities": [[10, 11, "MetricName", "F1"], [12, 15, "MetricName", "F - measure"]]}
{"text": "The aim of the first system is the normalization of the entity mentions in a biomedical text through the corresponding ontology , whereas the goal of the second system is the extraction of localization and property relations between the related entities when the entities are given .", "entities": [[20, 21, "MethodName", "ontology"]]}
{"text": "Both systems are unsupervised in the sense that they do not require domainspecific labeled data , while the normalization system makes use of word embeddings and syntactic re - ranking .", "entities": [[23, 25, "TaskName", "word embeddings"]]}
{"text": "Second Language Acquisition Modeling", "entities": [[1, 3, "TaskName", "Language Acquisition"]]}
{"text": "We present the task of second language acquisition ( SLA ) modeling .", "entities": [[6, 8, "TaskName", "language acquisition"]]}
{"text": "We propose second language acquisition ( SLA ) modeling as a new computational task to help broaden our understanding in this area .", "entities": [[3, 5, "TaskName", "language acquisition"]]}
{"text": "Each token from the reference answer is labeled according to the alignment with the learner 's response ( the final column : 0 for correct and 1 for incorrect ) .", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "The overall format is inspired by the Universal Dependencies ( UD ) format 2 .", "entities": [[7, 9, "DatasetName", "Universal Dependencies"], [10, 11, "DatasetName", "UD"]]}
{"text": "Column 1 is a unique B64 - encoded token ID , column 2 is a token ( word ) , and columns 3 - 6 are morpho - syntactic features from the UD tag set ( part of speech , morphology features , and dependency parse labels and edges ) .", "entities": [[32, 33, "DatasetName", "UD"]]}
{"text": "These were generated by processing the aligned reference answers with Google SyntaxNet ( Andor et al , 2016 ) .", "entities": [[10, 11, "DatasetName", "Google"]]}
{"text": "Because UD tags are meant to be language - agnostic , it was our goal to help make cross - lingual SLA modeling more straightforward by providing these features .", "entities": [[1, 2, "DatasetName", "UD"]]}
{"text": "AUC is a common measure of ranking quality in classification tasks , and can be interpreted as the probability that the system will rank a randomly - chosen error above a randomlychosen non - error .", "entities": [[0, 1, "MetricName", "AUC"]]}
{"text": "CECL ( Bestgen , 2018 ) used a logistic regression approach .", "entities": [[8, 10, "MethodName", "logistic regression"]]}
{"text": "Cambridge ( Yuan , 2018 ) trained two RNNsa sequence labeler , and a sequence - to - sequence model taking into account previous answers - and found that averaging their predictions yielded the best results .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}
{"text": "These include proxies for student engagement , spacing effect , response time , etc . nihalnayak ( Nayak and Rao , 2018 ) used a logistic regression model similar to the baseline , but added features inspired by research in codemixed language - learning where context plays an important role .", "entities": [[25, 27, "MethodName", "logistic regression"]]}
{"text": "Grotoco ( Klerke et al , 2018 ) also used logistic regression , including word lemmas , frequency , cognates , and user - specific features such as word error rate .", "entities": [[10, 12, "MethodName", "logistic regression"]]}
{"text": "jilljenn ( Vie , 2018 ) used a deep factorization machine ( DeepFM ) , a neural architecture developed for click - through rate prediction in recommender systems .", "entities": [[20, 25, "TaskName", "click - through rate prediction"]]}
{"text": "The DeepFM outperformed a simple logistic regression baseline without much additional feature engineering .", "entities": [[5, 7, "MethodName", "logistic regression"], [11, 13, "TaskName", "feature engineering"]]}
{"text": "However , according to a task organizer survey ymatusevych used a linear model with multilingual word embeddings , corpus frequency , and several L1 - L2 features such as cognates .", "entities": [[15, 17, "TaskName", "word embeddings"]]}
{"text": "It is a simple logistic regression using data set features , trained separately for each track using stochastic gradient descent on the TRAIN set only .", "entities": [[4, 6, "MethodName", "logistic regression"], [17, 20, "MethodName", "stochastic gradient descent"]]}
{"text": "In its simplest form ( Rasch , 1980 ) , an IRT model is a logistic regression with two weights : one representing the learner 's ability ( i.e. , user ID ) , and the other representing the difficulty of the exercise or test item ( i.e. , token ID ) .", "entities": [[15, 17, "MethodName", "logistic regression"]]}
{"text": "For decades , tutoring systems have also employed sequence models like HMMs to perform knowledge tracing ( Corbett and Anderson , 1995 ) , a way of estimating a learner 's mastery of knowledge over time .", "entities": [[14, 16, "TaskName", "knowledge tracing"]]}
{"text": "RNN - based approaches that encode user performance over time ( i.e. , that span across exercises ) are therefore variants of deep knowledge tracing ( Piech et al , 2015 ) .", "entities": [[23, 25, "TaskName", "knowledge tracing"]]}
{"text": "SLA modeling also bears some similarity to research in grammatical error detection ( Leacock et al , 2010 ) and correction ( Ng et al , 2013 ) .", "entities": [[9, 12, "TaskName", "grammatical error detection"]]}
{"text": "Given these similarities , a few teams adapted state - of - the - art GEC / GED approaches to create their SLA modeling systems .", "entities": [[17, 18, "DatasetName", "GED"]]}
{"text": "For example , recent work in machine translation has demonstrated gains through learning to translate multiple languages with a unified model ( Dong et al , 2015 ) .", "entities": [[6, 8, "TaskName", "machine translation"]]}
{"text": "In this section , we analyze the various modeling choices explored by the different teams in order to shed light on what kinds of algorithmic and feature engineering decisions appear to be useful for the SLA modeling task .", "entities": [[26, 28, "TaskName", "feature engineering"]]}
{"text": "To answer this question , we partitioned the TEST set into 6.4k subsets ( one for each learner ) , and computed per - user AUC scores for each team 's predictions ( 83.9k observations total ) .", "entities": [[25, 26, "MetricName", "AUC"]]}
{"text": "We also coded each team with indicator variables to describe their algorithmic approach , and used a regression analysis to determine if these algorithmic variations had any significant effects on learnerspecific AUC scores .", "entities": [[31, 32, "MetricName", "AUC"]]}
{"text": "Note that most teams ' systems that were not based on RNNs or tree ensembles used logistic regression , hence the \" linear model \" effect is negligible ( effectively treated as a control condition in the analysis ) .", "entities": [[16, 18, "MethodName", "logistic regression"]]}
{"text": "Broadly speaking , results suggest that feature engineering had a much smaller impact on system performance than the choice of learning algorithm .", "entities": [[6, 8, "TaskName", "feature engineering"]]}
{"text": "This echoes singsound 's finding that their linguistic encoder contributed the least to system performance , and Cambridge determined through ablation studies that these features in fact hurt their system .", "entities": [[17, 18, "DatasetName", "Cambridge"]]}
{"text": "We also experiment with stacking ( Wolpert , 1992 ) by training a logistic regression classifier using each team 's prediction as an input feature 6 .", "entities": [[13, 15, "MethodName", "logistic regression"]]}
{"text": "In this work , we presented the task of second language acquisition ( SLA ) modeling , described a large data set for studying this task , and reported on the results of a shared task challenge that explored this new domain .", "entities": [[10, 12, "TaskName", "language acquisition"]]}
{"text": "Among our key findings is the observation that , for this particular formulation of the task , the choice of learning algorithm appears to be more important than clever feature engineering .", "entities": [[29, 31, "TaskName", "feature engineering"]]}
{"text": "Still , many teams opted for a simpler algorithm ( e.g. , logistic regression ) and concentrated instead on more psychologically - motivated features .", "entities": [[12, 14, "MethodName", "logistic regression"]]}
{"text": "In particular , we conduct an experiment wherein we investigate the correspondence between human processing difficulty ( as reflected by gaze fixation measurements ) and the representations induced by popular pretrained language models .", "entities": [[30, 33, "TaskName", "pretrained language models"]]}
{"text": "While these approaches provide valuable insights into how neural networks process a large variety of phenomena , they rely on decoding accuracy as a probe for encoded linguistic information .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "Similarly , Bouchacourt and Baroni ( 2018 ) used the framework to measure the similarity between input image embeddings and the representations of the same image by an agent in an language game setting .", "entities": [[28, 29, "DatasetName", "agent"]]}
{"text": "When n i = n j , the dissimilarity between an experimental condition and itself is intuitively 0 , thus making the N \u00d7 N RDM symmetric along a diagonal of zeros ( Kriegeskorte et al , 2008 ) .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "Because layer disagreement for a sentence measures the extent to which two layers ( e.g. within BERT ) disagree with each other about the pairwise similarity of the sentence ( with other sentences in the corpus ) , a sentence with high layer disagreement will have unstable similarity relationships to other sentences in the corpus .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "We consider the following fixation features : TOTAL FIXATION DURATION and FIRST PASS DURATION .", "entities": [[12, 13, "DatasetName", "PASS"]]}
{"text": "Pretrained encoders We conduct our analysis on pretrained BERT - large ( Devlin et al , 2018 ) and ELMo ( Peters et al , 2018 ) , two widely employed contextual sentence encoders .", "entities": [[8, 9, "MethodName", "BERT"], [19, 20, "MethodName", "ELMo"]]}
{"text": "We refer to ELMo 's lowest layer as E1 , BERT 's 11th layer as B11 , etc .", "entities": [[3, 4, "MethodName", "ELMo"], [10, 11, "MethodName", "BERT"]]}
{"text": "The top section of the table shows correlations when L i and L j are the three final adjacent layers in BERT and ELMo .", "entities": [[21, 22, "MethodName", "BERT"], [23, 24, "MethodName", "ELMo"]]}
{"text": "The middle section shows the results for top three BERT layer pairs L i and L j which maximize the correlation scores .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "Finally , Figure 2 shows Spearman 's \u03c1 correlations between V Corr L i \u2212L j and each of V totf ix , and V Y ngve for all combinations of the 24 BERT layers .", "entities": [[33, 34, "MethodName", "BERT"]]}
{"text": "These findings confirm the hypothesis that the sentences that are most challenging for humans to process , are the sentences ( a ) the layers of BERT disagree most on among themselves ; and ( b ) that ELMo and BERT disagree most on , indicating that there may be common factors which affect human processing difficulty and result in disagreement between layers .", "entities": [[26, 27, "MethodName", "BERT"], [38, 39, "MethodName", "ELMo"], [40, 41, "MethodName", "BERT"]]}
{"text": "It is important to note that these encoders are trained with a language modelling objective , unlike models where reading behaviour is explicitly modelled ( Hahn and Keller , 2016 ) or predicted ( Matthies and S\u00f8gaard , 2013 ) .", "entities": [[12, 14, "TaskName", "language modelling"]]}
{"text": "This can be seen as analogous to the case of similarities observed between neural networks trained to perform object recognition and spatio - temporal cortical dynamics ( Cichy et al , 2016 ) .", "entities": [[18, 20, "TaskName", "object recognition"]]}
{"text": "Syntactic complexity Figure 2 shows that , for all combinations of BERT layers , total fixation time and Yngve scores have strong negative and positive correlations ( respectively ) with layer disagreement .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "Mostafa Abdou and Anders S\u00f8gaard are supported by a Google Focused Research Award and a Facebook Research Award .", "entities": [[9, 10, "DatasetName", "Google"]]}
{"text": "Investigating the Generative Approach for Question Answering in E - Commerce", "entities": [[5, 7, "TaskName", "Question Answering"]]}
{"text": "Many e - commerce websites provide Productrelated Question Answering ( PQA ) platform where potential customers can ask questions related to a product , and other consumers can post an answer to that question based on their experience .", "entities": [[7, 9, "TaskName", "Question Answering"]]}
{"text": "We use state - of - the - art generative models proposed by Deng et al ( 2020 ) and Lu et al ( 2020 ) for this purpose .", "entities": [[13, 16, "DatasetName", "Deng et al"]]}
{"text": "On closer examination , we find several drawbacks in this approach : ( 1 ) input reviews are not always utilized significantly for answer generation , ( 2 ) the performance of the models is abysmal while answering the numerical questions , ( 3 ) many of the generated answers contain phrases like \" I do not know \" which are taken from the reference answer in training data , and these answers do not convey any information to the customer .", "entities": [[23, 25, "TaskName", "answer generation"]]}
{"text": "Earlier works on product question answering ( PQA ) focus on retrieval - based approaches and binary answer prediction tasks .", "entities": [[4, 6, "TaskName", "question answering"]]}
{"text": "With the success of machine translation ( Sutskever et al , 2014 ) and summarization ( See et al , 2017 ) , the PQA approaches are shifting towards natural answer generation from relevant product reviews ( Gao et al , 2019 ;", "entities": [[4, 6, "TaskName", "machine translation"], [14, 15, "TaskName", "summarization"], [30, 32, "TaskName", "answer generation"]]}
{"text": "Chen et al , 2019b ; Deng et al , 2020 ; Lu et al , 2020 ; Gao et al , 2021 ) .", "entities": [[6, 9, "DatasetName", "Deng et al"]]}
{"text": "( Deng et al , 2020 ) and CHIME ( Lu et al , 2020 ) in detail beyond their traditional scores on popular metrics such as ROUGE ( Lin , 2004 ) .", "entities": [[1, 4, "DatasetName", "Deng et al"]]}
{"text": "We use Opinion - aware Answer Generation ( OAAG ) model ( Deng et al , 2020 ) and Crosspassage Hierarchical Memory Network ( CHIME ) model ( Lu et al , 2020 ) for our analysis .", "entities": [[5, 7, "TaskName", "Answer Generation"], [12, 15, "DatasetName", "Deng et al"], [21, 23, "MethodName", "Memory Network"]]}
{"text": "Following the generative approach , these two models achieve state - of - the - art performance on the Amazon Question Answering dataset .", "entities": [[20, 22, "TaskName", "Question Answering"]]}
{"text": "Upon retrieving the relevant reviews , OAAG uses an encoder - decoder model for answer generation .", "entities": [[14, 16, "TaskName", "answer generation"]]}
{"text": "OAAG encodes the question and each review corresponding to that question using a Bi - LSTM ( Hochreiter and Schmidhuber , 1997 ) network .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "It extends pretrained XLNet", "entities": [[3, 4, "MethodName", "XLNet"]]}
{"text": "Each training instance is fed into an XLNet encoder to get the hidden representations that are used to update the two memories .", "entities": [[7, 8, "MethodName", "XLNet"]]}
{"text": "Since there is no validation dataset , we take the 10 % of the train data as validation data .", "entities": [[4, 6, "DatasetName", "validation dataset"]]}
{"text": "Different types of questions are asked on the Amazon product page like numerical , \" yes / no \" , descriptive .", "entities": [[9, 11, "DatasetName", "product page"]]}
{"text": "In this paper , we extensively analyze the generative approach of question - answering in e - commerce using a state - of - the - art OAAG model ( Deng et al , 2020 ) and CHIME model ( Lu et al , 2020 ) .", "entities": [[30, 33, "DatasetName", "Deng et al"]]}
{"text": "Score", "entities": [[0, 1, "MetricName", "Score"]]}
{"text": "This paper describes the Air Force Research Laboratory ( AFRL ) machine translation systems and the improvements that were developed during the WMT19 evaluation campaign .", "entities": [[11, 13, "TaskName", "machine translation"]]}
{"text": "This year , we refine our approach to training popular neural machine translation toolkits , experiment with a new domain adaptation technique and again measure improvements in performance on the Russian - English language pair .", "entities": [[11, 13, "TaskName", "machine translation"], [19, 21, "TaskName", "domain adaptation"]]}
{"text": "As part of the 2019 Conference on Machine Translation ( Bojar et al , 2019 ) news - translation shared task , the AFRL Human Language Technology team participated in the Russian - English portion of the competition .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}
{"text": "For Russian - English we again submitted an entry comprising our best systems trained with Marian ( Junczys - Dowmunt et al , 2018 ) , Sockeye ( Hieber et al , 2017 ) with Elastic Weight Consolidation ( EWC ) ( Thompson et al , 2019 ) , OpenNMT ( Klein et al , 2018 ) , and Moses ( Koehn et al , 2007 ) combined using the Jane system combination method ( Freitag et al , 2014 ) .", "entities": [[35, 38, "MethodName", "Elastic Weight Consolidation"], [39, 40, "MethodName", "EWC"]]}
{"text": "For all systems trained , we applied either byte - pair encoding ( BPE ) ( Sennrich et al , 2016 ) or SentencePiece ( Kudo and Richardson , 2018 ) subword strategies to address the vocabulary - size problem .", "entities": [[13, 14, "MethodName", "BPE"], [23, 24, "MethodName", "SentencePiece"]]}
{"text": "For this year , we also employed a language ID filtering step for the BPE - based systems .", "entities": [[14, 15, "MethodName", "BPE"]]}
{"text": "Using the pre - built language ID model developed by the authors of fastText ( Joulin et al , 2016a , b ) , we developed a utility that examined the source and target sentence pairs and discarded that pair if either side fell below 0.8 1 probability of the desired language .", "entities": [[13, 14, "MethodName", "fastText"]]}
{"text": "This process was particularly effective when used to filter the Paracrawl corpus where 57.1 % of lines were removed .", "entities": [[10, 11, "DatasetName", "Paracrawl"]]}
{"text": "One of the problems faced when addressing the closed - vocabulary problem is the granularity of the subword units either produced by SentencePiece or BPE .", "entities": [[22, 23, "MethodName", "SentencePiece"], [24, 25, "MethodName", "BPE"]]}
{"text": "To that end , we examined varying the number of BPE merge operations in order to determine an optimal setting to maximize performance for the Russian - English language pair .", "entities": [[10, 11, "MethodName", "BPE"]]}
{"text": "For the OpenNMT - based systems , a vocabulary size of 32k entries was employed during training of a SentencePiece segmentation model 2 .", "entities": [[19, 20, "MethodName", "SentencePiece"]]}
{"text": "Alternatively , for the BPE - based systems , we systematically examined varying sizes of BPE merge operations and vocabulary sizes in 10k increments from 30k to 80k .", "entities": [[4, 5, "MethodName", "BPE"], [15, 16, "MethodName", "BPE"]]}
{"text": "Results in Table 3 show that 40k BPE merge operations perform best across all test sets decoded for this language pair .", "entities": [[7, 8, "MethodName", "BPE"]]}
{"text": "All subsequent Marian experiments in this work utilize this 40k BPE training corpus .", "entities": [[10, 11, "MethodName", "BPE"]]}
{"text": "While most of our experimentation builds off of previous years ' efforts , we did examine domain adaptation via continued training , including Elastic Weight Consolidation ( EWC ) ( Thompson et al , 2019 ) .", "entities": [[16, 18, "TaskName", "domain adaptation"], [23, 26, "MethodName", "Elastic Weight Consolidation"], [27, 28, "MethodName", "EWC"]]}
{"text": "As with last year 's efforts , we train multiple Marian ( Junczys - Dowmunt et al , 2018 ) models with both University of Edinburgh 's \" bi - deep \" and Google 's transformer ( Vaswani et", "entities": [[33, 34, "DatasetName", "Google"]]}
{"text": "One method to mitigate this performance drop is to prevent certain parameters of the network from changing with Elastic Weight Consolidation ( EWC ) ( Kirkpatrick et al , 2017 ) .", "entities": [[18, 21, "MethodName", "Elastic Weight Consolidation"], [22, 23, "MethodName", "EWC"]]}
{"text": "We first trained a baseline transformer system using the best - performing BPE parameters from Section 2.2 , 512 - dimension word embeddings , 6 layer encoder and decoder , 8 attention heads , label smoothing and transformer attention dropout of 0.1 .", "entities": [[12, 13, "MethodName", "BPE"], [21, 23, "TaskName", "word embeddings"], [34, 36, "MethodName", "label smoothing"], [38, 40, "MethodName", "attention dropout"]]}
{"text": "We also followed the Sockeye EWC training procedure , producing a model more resilient to overfitting due to continued training .", "entities": [[5, 6, "MethodName", "EWC"]]}
{"text": "The best - performing EWC system 5 5 EWC applied with weight - decay of 0.001 and learning - actually improved performance on 2018 with lesspronounced overfitting on 2014 .", "entities": [[4, 5, "MethodName", "EWC"], [8, 9, "MethodName", "EWC"]]}
{"text": "For system combination outlined later in Section 4 , we decoded test sets with an ensemble of the four highest - scoring model checkpoints from the best EWC training run .", "entities": [[27, 28, "MethodName", "EWC"]]}
{"text": "Our first Open - NMT system was trained using the Transformer architecture with the default \" Trans - formerBig \" settings as described in Vaswani et", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "Dropout rates of 0.3 for layers and 0.1 for attention heads and relu 's .", "entities": [[0, 1, "MethodName", "Dropout"], [12, 13, "MethodName", "relu"]]}
{"text": "This data was then processed with a joint 32k word vocabulary SentencePiece model .", "entities": [[11, 12, "MethodName", "SentencePiece"]]}
{"text": "For our second OpenNMT system , we first trained language - specific , 32k word vocabularies using SentencePiece .", "entities": [[17, 18, "MethodName", "SentencePiece"]]}
{"text": "These data , with the addition of the language ID filtered ParaCrawl corpus outlined in Section 2.1 , were used for training the system .", "entities": [[11, 12, "DatasetName", "ParaCrawl"]]}
{"text": "OpenNMT - tf was used to create the system , using the stock \" Transformer \" model .", "entities": [[14, 15, "MethodName", "Transformer"]]}
{"text": "The BPE model used was applied to both the parallel training data and the language modeling corpus .", "entities": [[1, 2, "MethodName", "BPE"]]}
{"text": "We submitted the final 5 - system combination outlined in Section 4 and the four - checkpoint EWC ensemble detailed in Section 3.2 to the Russian - English portion of the WMT19 news task evaluation .", "entities": [[17, 18, "MethodName", "EWC"]]}
{"text": "We presented a series of improvements to our Russian - English systems , including improved preprocessing and domain adaptation .", "entities": [[17, 19, "TaskName", "domain adaptation"]]}
{"text": "We aim to contribute to research in this area through the release of the Unhealthy Comment Corpus ( UCC ) of approximately 44 , 000 comments and corresponding crowdsourced labels and confidence scores .", "entities": [[18, 19, "DatasetName", "UCC"]]}
{"text": "The UCC is open source and available on Github .", "entities": [[1, 2, "DatasetName", "UCC"]]}
{"text": "The UCC contributes further high quality data on attributes like sarcasm , hostility , and condescension , adding to existing datasets on these and related attributes ( Wang and Potts , 2019 ; Davidson et al , 2017 ; Wulczyn et al , 2017 ; Chen et al , 2017 ) , and provides ( to the best of our knowledge ) the first dataset of this scale with labels for dismissiveness , unfair generalisations , antagonistic behavior , and overall assessments of whether those comments fall within ' healthy ' conversation .", "entities": [[1, 2, "DatasetName", "UCC"]]}
{"text": "Section 2 outlines the motivation and background to the UCC attribute typology .", "entities": [[9, 10, "DatasetName", "UCC"]]}
{"text": "Our primary quality control mechanism was to collate a set of ' test comments ' , for which we had manually established the correct answers .", "entities": [[7, 8, "DatasetName", "collate"]]}
{"text": "Annotators encountered one test comment per batch of seven comments they reviewed , without knowing which of the seven was the test comment , and their running accuracy on these test comments was defined as their ' trustworthiness score ' .", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "There are specific attributes , notably sarcasm , for which it can be possible to collate a corpus of self - labelled data , for example by scraping tweets with ' # sarcastic ' from Twitter , or comments followed by ' /s ' on Reddit ( Khodak et al , 2018 ) .", "entities": [[15, 16, "DatasetName", "collate"], [45, 46, "DatasetName", "Reddit"]]}
{"text": "Inspection of random subsets of the new UCC dataset reveals that the data is generally of a high quality , and captures important nuances , accurately identifying these subtle attributes , both when they overlap ( as is common ) , and also when they do not ( see Figure 3 for examples ) .", "entities": [[7, 8, "DatasetName", "UCC"]]}
{"text": "We also include correlations with the ' toxicity ' scores produced by Jigsaw 's Perspective API ( perspectiveapi.com ) , which again confirms that our attributes , in particular those other than antagonistic and hostile , capture something distinct from overt toxicity .", "entities": [[12, 13, "MethodName", "Jigsaw"]]}
{"text": "Despite the above caveat , we conduct analysis using Krippendorff 's \u03b1 ( K - \u03b1 ) for two reasons .", "entities": [[11, 12, "HyperparameterName", "\u03b1"], [15, 16, "HyperparameterName", "\u03b1"]]}
{"text": "They range from 0.31 - 0.39 , which is comparable with other datasets labelling ' similar ' phenomenon , such as sarcasm ( 0.24 - 0.38 ) ( Swanson et al , 2014 ; Justo et al , 2018 ; D'Arcey et al , 2019 ) , and hate speech with sub - attributes from Figure Eight annotators ( 0.21 ) ( Lazaridou et al , 2020 ) .", "entities": [[48, 50, "DatasetName", "hate speech"]]}
{"text": "Secondly , to the extent that K - \u03b1 is an important reliability metric for this form of data , it supports our use of ' trustworthiness ' scores when aggregating judgements on a given comment to decide labels and confidence scores .", "entities": [[8, 9, "HyperparameterName", "\u03b1"]]}
{"text": "Also included in the UCC dataset are the individual annotations for each comment by all ' trusted ' annotators .", "entities": [[4, 5, "DatasetName", "UCC"]]}
{"text": "Use of a pre - trained BERT model ( Devlin et al , 2019 ) and fine - tuning on this dataset produces classifiers with modest performance ( Figure 6 ) , compared to the state of the art for sequence classification .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "For each comment , we hold out one annotator to act as our ' human model ' and use the aggregated score of the other annotators as the ground truth to compute the ROC AUC .", "entities": [[33, 35, "MetricName", "ROC AUC"]]}
{"text": "We use the same test sets to compute the ROC AUC of the trained BERT model and average those scores as well .", "entities": [[9, 11, "MetricName", "ROC AUC"], [14, 15, "MethodName", "BERT"]]}
{"text": "As we can see , for all attributes other than ' sarcastic ' the BERT model outperforms a randomly selected human annotator , indicating that it has sufficiently captured the semantic and syntactic structures for these attributes .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "For ' sarcastic ' , the gap between the BERT model and human annotators indicates a rich area for studying whether model performance can be improved .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "This potential selection bias is likely to be evident across the broader healthy / unhealthy categorisation along with each of the attributes .", "entities": [[2, 4, "TaskName", "selection bias"]]}
{"text": "It is important to note at this stage of the field in general , and with our understanding of this dataset in particular , that the UCC dataset is not designed to train models which are immediately available for automated moderation without human intervention in a live online setting .", "entities": [[26, 27, "DatasetName", "UCC"]]}
{"text": "We also provide results from a modern baseline ML model ( fine tuning BERT ) and note that performance exceeds that of a crowd - worker .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "We investigate competitive classification models with bi - directional recurrent neural networks ( Bi - RNN ) and neural machine translation ( NMT ) models .", "entities": [[19, 21, "TaskName", "machine translation"]]}
{"text": "Our GEC systems ranked the first in the Unrestricted Track , and the third in both the Restricted Track and the Low Resource Track .", "entities": [[8, 9, "DatasetName", "Unrestricted"], [17, 18, "DatasetName", "Restricted"]]}
{"text": "Grammatical error correction ( GEC ) is the task of automatically correcting grammatical errors in text .", "entities": [[0, 3, "TaskName", "Grammatical error correction"]]}
{"text": "Classification methods achieved the best result in CoNLL - 2013 ( Rozovskaya et al , 2013 ) .", "entities": [[0, 1, "TaskName", "Classification"]]}
{"text": "After that , statistical machine translation ( SMT ) methods began to show better performance in CoNLL - 2014 ( Felice et al , 2014 ) .", "entities": [[4, 6, "TaskName", "machine translation"]]}
{"text": "Then after ( Junczys - Dowmunt and Grundkiewicz , 2016 ) , machine translation methods became the mainstream in GEC solutions .", "entities": [[12, 14, "TaskName", "machine translation"]]}
{"text": "As Transformer ( Vaswani et al , 2017 ) plays an increasingly important role in sequence modeling , Transformer - based end - to - end NMT models began to lead the current GEC research Ge et al , 2018 ; Zhao et al , 2019 ) .", "entities": [[1, 2, "MethodName", "Transformer"], [18, 19, "MethodName", "Transformer"]]}
{"text": "We submitted the same system output for the Restricted and Unrestricted tasks .", "entities": [[8, 9, "DatasetName", "Restricted"], [10, 11, "DatasetName", "Unrestricted"]]}
{"text": "The system uses several ensemble methods to combine the CNNbased and Transformer - based translation models , described in details below .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "Unlike ( Chollampatt and Ng , 2018 ) , we did not use fastText ( Bojanowski et al , 2017 ) to initialize word embeddings because we found no improvement on the development set by doing that .", "entities": [[13, 14, "MethodName", "fastText"], [23, 25, "TaskName", "word embeddings"]]}
{"text": "We tuned parameters for the system , such as batch size , word embedding dimension , etc .", "entities": [[9, 11, "HyperparameterName", "batch size"], [12, 15, "HyperparameterName", "word embedding dimension"]]}
{"text": "Transformer is currently considered to be one of the most powerful models for sequence modeling .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "For GEC , some of the best recent results reported on CoNLL - 2014 test set are obtained by Transformer - based translation models .", "entities": [[19, 20, "MethodName", "Transformer"]]}
{"text": "We trained eight Transformer - based translation models in a low resource translation paradigm .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "If two conflicting corrections are the same , we merge them and add \u03b1 to the confidence of the correction ; otherwise , the correction with a lower confidence will be discarded .", "entities": [[13, 14, "HyperparameterName", "\u03b1"]]}
{"text": "After combining outcomes , if the confidence of a correction is lower than \u03b2 , the correction is discarded .", "entities": [[13, 14, "HyperparameterName", "\u03b2"]]}
{"text": "Figure 1 displays three types of ensemble methods based on all of the CNN - based and Transformer - based translation models .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "Combine each CNN - based ensemble model with each of the selected five of the Transformer - based models .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "Perform ensemble over all of the ensemble models relating to either CNN ensemble 1 or CNN ensemble 2 , noted as EoE ( Ensemble over Ensemble ) 1 and 2 . Ensemble each CNN ensemble model with some selected combinations of Transformer - based models to produce 16 strong ensemble system outcomes , represented as ' Hybrid Ensemble ' in Figure 1 .", "entities": [[41, 42, "MethodName", "Transformer"]]}
{"text": "We also explored the noisy Wikipedia edit history corpus for the Transformer - based translation model .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "Note that for the Restricted and Unrestricted tracks , we did not observe any gain from the classification models or the rule - based methods , therefore only the translation systems were used for those tracks .", "entities": [[4, 5, "DatasetName", "Restricted"], [6, 7, "DatasetName", "Unrestricted"]]}
{"text": "( A ) Bi - GRU context model Figure 2 shows the bi - directional GRU context model we use to determine the right grammatical category for a target word .", "entities": [[5, 6, "MethodName", "GRU"], [15, 16, "MethodName", "GRU"]]}
{"text": "C t is then fed through a fully connected layer and softmax layer to produce the predictive distribution .", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "Inspired by the Pointer network model ( Vinyals et al , 2015 ) , we proposed the pointer context model .", "entities": [[0, 1, "DatasetName", "Inspired"], [3, 5, "MethodName", "Pointer network"]]}
{"text": "The computation path is the same as the Bi - GRU model structure .", "entities": [[10, 11, "MethodName", "GRU"]]}
{"text": "We concatenate the target word 's char - based embedding and C t to obtain C 1 t , and then use it as the query to compute dot product a 1 t with each of the word embeddings in the confusion set .", "entities": [[37, 39, "TaskName", "word embeddings"]]}
{"text": "a 1 t is then fed through a softmax layer to produce the predictive distribution .", "entities": [[8, 9, "MethodName", "softmax"]]}
{"text": "We use the same Transformer - based translation model mentioned in Subsection 3.2.2 .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "( Bryant et al , 2019 ) and Common Crawl .", "entities": [[8, 10, "DatasetName", "Common Crawl"]]}
{"text": "We use Common Crawl to pretrain the decoder parameters for the Transformer - based translation model .", "entities": [[2, 4, "DatasetName", "Common Crawl"], [11, 12, "MethodName", "Transformer"]]}
{"text": "FCE , Lang - 8 , NUCLE and W&I are used to train all of the translation models .", "entities": [[0, 1, "DatasetName", "FCE"]]}
{"text": "It is worth noting that we did data augmentation for W&I to train all of the translation models .", "entities": [[7, 9, "TaskName", "data augmentation"]]}
{"text": "The data sets used in Low Resource Track include Wiked , Wikipedia Dumps and Common Crawl .", "entities": [[14, 16, "DatasetName", "Common Crawl"]]}
{"text": "We added the W&I corpus eight times to the training corpus for domain adaptation .", "entities": [[12, 14, "TaskName", "domain adaptation"]]}
{"text": "We trained eight Transformer - based translation models in different combinations of error adaptation , domain adaptation , and GPU set .", "entities": [[3, 4, "MethodName", "Transformer"], [15, 17, "TaskName", "domain adaptation"]]}
{"text": "We set the copy number as 8 , 10 and 15 , and find that domain adaptation has no significant effect on the results .", "entities": [[15, 17, "TaskName", "domain adaptation"]]}
{"text": "As described in Section 2.1.3 , we need to ensemble all of the CNN - based and Transformer - based translation models .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "When combining two models that are not strong , we expect a higher recall so \u03b2 was not high .", "entities": [[15, 16, "HyperparameterName", "\u03b2"]]}
{"text": "Corrections proposed by multiple models are given higher weights ( controlled by \u03b1 ) .", "entities": [[12, 13, "HyperparameterName", "\u03b1"]]}
{"text": "If the confidence of a correction finally reaches \u03b2 , the correction will be adopted .", "entities": [[8, 9, "HyperparameterName", "\u03b2"]]}
{"text": "In the final ensemble , we select the best performance on each type from each single system or ensemble system and discard the corrections with low precision ( controlled by \u03b2 ) .", "entities": [[30, 31, "HyperparameterName", "\u03b2"]]}
{"text": "We can see that the individual CNN or Transformer - based translation models perform reasonably well , and the ensemble methods consistently outperform the individual systems .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "A Transformer - based translation model is trained on the filtered Wiked corpus .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "When there is a sufficient parallel learner corpus , such as in Restricted Track and Unrestricted Track , the NMT ensemble model is the best choice to implement a GEC system .", "entities": [[12, 13, "DatasetName", "Restricted"], [15, 16, "DatasetName", "Unrestricted"]]}
{"text": "We have evaluated two kinds of NMT models : CNN - based and Transformer - based translation models .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "Finally we reached the result of F 0.5 = 0.6678 on the official test set in Restricted Track and Unrestricted Track , ranking the third in the Restricted track 4 .", "entities": [[16, 17, "DatasetName", "Restricted"], [19, 20, "DatasetName", "Unrestricted"], [27, 28, "DatasetName", "Restricted"]]}
{"text": "In this paper we present a new unsupervised approach , \" Attraction to Topics \" - A2 T , for the detection of argumentative units , a sub - task of argument mining .", "entities": [[31, 33, "TaskName", "argument mining"]]}
{"text": "Argument mining involves the automatic discovery of argument components ( i.e. claims , premises ) and the argumentative relations ( i.e. supports , attacks ) among these components in texts .", "entities": [[0, 2, "TaskName", "Argument mining"]]}
{"text": "Primarily aiming to extract arguments from texts in order to provide structured data for computational models of argument and reasoning engines ( Lippi and Torroni , 2015a ) , argument mining has additionally the potential to support applications in various research fields , such as opinion mining ( Goudas et al , 2015 ) , stance detection ( Hasan and Ng , 2014 ) , policy modelling ( Florou et al , 2013 ; Goudas et al , 2014 ) , legal information systems ( Palau and Moens , 2009 ) , etc .", "entities": [[29, 31, "TaskName", "argument mining"], [45, 47, "TaskName", "opinion mining"], [55, 57, "TaskName", "stance detection"]]}
{"text": "Argument mining is usually addressed as a pipeline of several sub - tasks .", "entities": [[0, 2, "TaskName", "Argument mining"]]}
{"text": "Topics seem to be related to the task of argument mining , at least for some types of corpora , as topic identification frequently appears as a step in the process of manual annotation of arguments in texts ( Stab and Gurevych , 2014a ) .", "entities": [[9, 11, "TaskName", "argument mining"]]}
{"text": "However , despite its apparent importance in manual annotation , only a small number of studies have examined the inclusion of topic information in sub - tasks of argument mining .", "entities": [[28, 30, "TaskName", "argument mining"]]}
{"text": "The rest of the paper is organized as follows : Section 2 presents an overview of approaches related to argument mining focusing on the detection of argumentative units , while Section 3 presents our approach on applying topic modeling for identifying sentences that contain argument components .", "entities": [[19, 21, "TaskName", "argument mining"]]}
{"text": "Almost all argument mining frameworks proposed so far employ a pipeline of stages , each of which is addressing a sub - task of the argument mining problem ( Lippi and Torroni , 2015a ) .", "entities": [[2, 4, "TaskName", "argument mining"], [25, 27, "TaskName", "argument mining"]]}
{"text": "The segmentation of text into argumentative units is typically the first sub - task encountered in such an argument mining pipeline , aiming to segment texts into argumentative and non - argumentative text units ( i.e. segments that do contain or do not contain argument components , such as claims or premises ) .", "entities": [[18, 20, "TaskName", "argument mining"]]}
{"text": "A plethora of learning algorithms have been applied on the task , including Naive Bayes ( Moens et al , 2007 ; Park and Cardie , 2014 ) , Support Vector Machines ( SVM ) ( Mochales and Moens , 2011 ; Rooney et al , 2012 ; Park and Cardie , 2014 ; Stab and Gurevych , 2014b ; Lippi and Torroni , 2015b ) , Maximum Entropy ( Mochales and Moens , 2011 ) , Logistic Regression ( Goudas et al , 2014 ( Goudas et al , , 2015Levy et al , 2014 ) , Decision Trees and Random Forests ( Goudas et al , 2014 ( Goudas et al , , 2015Stab and Gurevych , 2014b ) .", "entities": [[33, 34, "MethodName", "SVM"], [77, 79, "MethodName", "Logistic Regression"]]}
{"text": "In ( Petasis and Karkaletsis , 2016 ) an unsupervised approach is presented , which addresses the sub - task of identifying the main claim in a document by exploiting evidence from an extractive summarization algorithm , TextRank ( Mihalcea and Tarau , 2004 ) .", "entities": [[33, 35, "TaskName", "extractive summarization"]]}
{"text": "Regarding semi - supervised approaches , Habernal and Gurevych ( 2015 ) propose new unsupervised features that exploit clustering of unlabeled argumentative data from debate portals based on word embeddings , outperforming several baselines .", "entities": [[28, 30, "TaskName", "word embeddings"]]}
{"text": "This work employs also topic modeling as one of its features , by including as features the distributions of sentences from LDA ( Blei et al , 2003 ) .", "entities": [[21, 22, "MethodName", "LDA"]]}
{"text": "In Lawrence et al ( 2014 ) , LDA is used to decide whether a proposition can be attached to its previous proposition in order to identify non directional relations among propositions detected through classifiers based on words and part - ofspeech tags .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "LDA has been also used to mine lexicons of argument ( words that are topic independent ) and domain words ( Nguyen and Litman , 2015 ) , by post - processing document topics generated by LDA .", "entities": [[0, 1, "MethodName", "LDA"], [36, 37, "MethodName", "LDA"]]}
{"text": "These lexicons have been used as features for supervised approaches for argument mining ( Nguyen and Litman , 2016a , b ) .", "entities": [[11, 13, "TaskName", "argument mining"]]}
{"text": "First , topic modeling returns a set of topics T = { t 0 , . . .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": ", \u03b8 sm } .", "entities": [[1, 2, "HyperparameterName", "\u03b8"]]}
{"text": "In particular , \u03b8 s i =", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "[ p ( t 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "Probabilistic topic models are a suite of algorithms whose aim is to discover the hidden thematic structure in large archives of documents , namely sentences in A2 T .", "entities": [[1, 3, "TaskName", "topic models"]]}
{"text": "Probabilistic topic modeling algorithms infer the distribution \u03b8 of documents over topics and the distribution \u03c6 of words over topics , by sampling from the bag of words of each document .", "entities": [[7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "With respect to other algorithms ( such as LDA ) , HDP has the advantage to provide the optimal number of topics instead of requiring to set such a number as input ( Teh et al , 2006 ) .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "Consider a set of sentences S and the distribution \u03b8 of sentences over the set of topics T .", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "The more the distribution \u03b8 s i of a sentence s i over the topics is unequal , the more s i is focused on a topic , thus suggesting s i as a possible argumentative unit .", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "i = max ( \u03b8 s i ) is a measure of how much s i is focused on a topic and", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "+ \u03b3 is a parabolic function over the position of the sentence in c. In particular , given L ( c ) as the number of sentences in c , f ( pos i )", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "The parameters \u03b1 , \u03b2 , \u03b3 determine the shape of \u03c1 s i .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [4, 5, "HyperparameterName", "\u03b2"], [6, 7, "HyperparameterName", "\u03b3"]]}
{"text": "K [ 0 , 1 ] is a constant value used to balance the role of focus and position in calculating the attraction .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "\u03c4 g\u22121 ( 0 , 1 ] ) are prefixed threshold values .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "Task 3 : Argumentative sentence classification - given a sentence , classify the sentence as major claim , claim , premise , or nonargumentative . Baseline .", "entities": [[4, 6, "TaskName", "sentence classification"]]}
{"text": "Given the text c containing L ( c ) sentences s i , let be \u03b6 c \u223c Dir ( \u03b1 ) the probability distribution of the sentences in c , such that \u03b6 s i c \u223c p", "entities": [[20, 21, "HyperparameterName", "\u03b1"]]}
{"text": "The L ( c ) parameters \u03b1 used to generate \u03b6 c are defined such that", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "As a first experiment , we performed sentence labeling with different threshold ranging from 0 to 1 with step 0.05 .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "UDapter : Language Adaptation for Truly Universal Dependency Parsing", "entities": [[7, 9, "TaskName", "Dependency Parsing"]]}
{"text": "Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality .", "entities": [[4, 6, "TaskName", "dependency parsing"]]}
{"text": "In this paper , we strike a good balance between maximum sharing and language - specific capacity in multilingual dependency parsing .", "entities": [[19, 21, "TaskName", "dependency parsing"]]}
{"text": "Inspired by recently introduced parameter sharing techniques ( Platanios et al , 2018 ; Houlsby et al , 2019 ) , we propose a new multilingual parser , UDapter , that learns to modify its language - specific parameters including the adapter modules , as a function of language embeddings .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Furthermore , we propose not to learn language embeddings from scratch , but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database which supports 3718 languages including all languages represented in UD .", "entities": [[41, 42, "DatasetName", "UD"]]}
{"text": "In our model , typological features are crucial , leading to a substantial LAS increase on zero - shot languages and no loss on high - resource languages when compared to the language embeddings learned from scratch .", "entities": [[22, 23, "MetricName", "loss"]]}
{"text": "Accordingly , we make the following contributions : 1 ) We apply the idea of adapter tuning ( Rebuffi et al , 2018 ; Houlsby et al , 2019 ) to the task of universal dependency parsing .", "entities": [[35, 37, "TaskName", "dependency parsing"]]}
{"text": "2 ) We combine adapters with the idea of contextual parameter generation ( Platanios et al , 2018 ) , leading to a novel language adaptation approach with state - of - the art UD parsing results .", "entities": [[34, 35, "DatasetName", "UD"]]}
{"text": "Multilingual Neural Networks Early models in multilingual neural machine translation ( NMT ) designed dedicated architectures ( Dong et al , 2015 ;", "entities": [[8, 10, "TaskName", "machine translation"]]}
{"text": "As the leading model , multilingual BERT ( mBERT ) 2 ( Devlin et al , 2019 ) which is a deep self - attention network , was trained without language - specific signals on the 104 languages with the largest Wikipedias .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "mBERT"], [22, 26, "MethodName", "self - attention network"]]}
{"text": "It uses a shared vocabulary of 110 K WordPieces ( Wu et al , 2016 ) , and has been shown to facilitate cross - lingual transfer in several applications ( Pires et al , 2019 ; Wu and Dredze , 2019 ) .", "entities": [[23, 27, "TaskName", "cross - lingual transfer"]]}
{"text": "Concurrently to our work , Pfeiffer et al ( 2020 ) have proposed to combine language and task adapters , small bottleneck layers ( Rebuffi et al , 2018 ; Houlsby et al , 2019 ) , to address the capacity issue which limits multilingual pre - trained models for cross - lingual transfer .", "entities": [[50, 54, "TaskName", "cross - lingual transfer"]]}
{"text": "Cross - Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages", "entities": [[3, 5, "TaskName", "Dependency Parsing"]]}
{"text": "Ammar et al ( 2016 ) trains a multilingual parser using multilingual word embeddings , token - level language information , language typology features and fine - grained POS tags .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "More recently , based on mBERT ( Devlin et al , 2019 ) , zero - shot transfer in dependency parsing was investigated ( Wu and Dredze , 2019 ; Tran and Bisazza , 2019 ) .", "entities": [[5, 6, "MethodName", "mBERT"], [19, 21, "TaskName", "dependency parsing"]]}
{"text": "Finally Kondratyuk and Straka ( 2019 ) trained a multilingual parser on the concatenation of all available UD treebanks .", "entities": [[17, 18, "DatasetName", "UD"]]}
{"text": "In dependency parsing , several previous studies ( Naseem et al , 2012 ; Zhang and Barzilay , 2015 ; Ammar et al , 2016 ; Scholivet et al , 2019 ) have suggested that typological features are useful for the selective sharing of transfer information .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "Finally , Lin et al ( 2019 ) use typological features , along with properties of the training data , to choose optimal transfer languages for various tasks , including UD parsing , in a hard manner .", "entities": [[30, 31, "DatasetName", "UD"]]}
{"text": "UDapter consists of a biaffine attention layer stacked on top of the pretrained Transformer encoder ( mBERT ) .", "entities": [[13, 14, "MethodName", "Transformer"], [16, 17, "MethodName", "mBERT"]]}
{"text": "This is similar to ( Wu and Dredze , 2019 ; Kondratyuk and Straka , 2019 ) , except that our mBERT layers are interleaved with special adapter layers inspired by Houlsby et al ( 2019 ) .", "entities": [[21, 22, "MethodName", "mBERT"]]}
{"text": "While mBERT weights are frozen , biaffine attention and adapter layer weights are generated by a contextual parameter generator ( Platanios et al , 2018 ) that takes a language embedding as input and is updated while training on the treebanks .", "entities": [[1, 2, "MethodName", "mBERT"]]}
{"text": "Note that the proposed adaptation approach is not restricted to dependency parsing and is in principle applicable to a range of multilingual NLP tasks .", "entities": [[10, 12, "TaskName", "dependency parsing"], [21, 23, "TaskName", "multilingual NLP"]]}
{"text": "In this model , an encoder generates an internal representation r i for each word ; the decoder takes r i and passes it through separate feedforward layers ( MLP ) , and finally uses deep biaffine attention to score arcs connecting a head and a tail : h ( head )", "entities": [[29, 30, "DatasetName", "MLP"]]}
{"text": "i = MLP ( head ) ( r i ) ( 1 ) h ( tail )", "entities": [[2, 3, "DatasetName", "MLP"]]}
{"text": "i = MLP ( tail ) ( r i ) ( 2 ) s ( arc )", "entities": [[2, 3, "DatasetName", "MLP"]]}
{"text": "To obtain contextualized word representations , UDapter uses mBERT .", "entities": [[8, 9, "MethodName", "mBERT"]]}
{"text": "For a token i in sentence S , BERT builds an input representation w i composed by summing a WordPiece embedding", "entities": [[8, 9, "MethodName", "BERT"], [19, 20, "MethodName", "WordPiece"]]}
{"text": "Each w i S is then passed to a stacked self - attention layers ( SA ) to generate the final encoder representation", "entities": [[12, 14, "HyperparameterName", "attention layers"]]}
{"text": "In UDapter , following Houlsby et al ( 2019 ) , two bottleneck adapters with two feedforward projections and a GELU nonlinearity ( Hendrycks and Gimpel , 2016 ) are inserted into each transformer layer as shown in Figure 1 .", "entities": [[20, 21, "MethodName", "GELU"]]}
{"text": "F 1 1 0 0 1 0 0 1 1 0 1 0 1 1 Biaffine Attention ( for", "entities": [[3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}
{"text": "Since we only train adapters and the biaffine attention ( i.e. adapter tuning ) , the parameter generator is formalized as { \u03b8 ( ad ) , \u03b8 ( bf ) } g ( m ) ( l e ) where g ( m ) denotes the parameter generator with language embedding l e , and \u03b8 ( ad ) and \u03b8 ( bf ) denote the parameters of adapters and biaffine attention respectively .", "entities": [[22, 23, "HyperparameterName", "\u03b8"], [27, 28, "HyperparameterName", "\u03b8"], [56, 57, "HyperparameterName", "\u03b8"], [61, 62, "HyperparameterName", "\u03b8"]]}
{"text": "While this allows UDapter to perform well on the languages in training , even if they are typologically diverse , information sharing is still a problem for languages not seen during training ( zero - shot learning ) as a language embedding is not available .", "entities": [[33, 37, "TaskName", "zero - shot learning"]]}
{"text": "Inspired by Naseem et al ( 2012 ) and", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "We use a multi - layer perceptron MLP ( lang ) with two feedforward layers and a ReLU nonlinear activation to compute a language embedding l e : l e = MLP ( lang ) ( l t ) ( 7 ) where l t is a typological feature vector for a language consisting of all 103 syntactic , 28 phonological and 158 phonetic inventory features from the URIEL language typology database ( Lewis et al , 2015 ) and Glottolog ( Hammarstr\u00f6m et al , 2020 ) .", "entities": [[7, 8, "DatasetName", "MLP"], [17, 18, "MethodName", "ReLU"], [31, 32, "DatasetName", "MLP"]]}
{"text": "As many feature values are not available for each language , we use the values predicted by using a k - nearest neighbors approach based on average of genetic , geographical and feature distances between languages .", "entities": [[19, 23, "MethodName", "k - nearest neighbors"]]}
{"text": "For the encoder , we use BERT - multilingualcased together with its WordPiece tokenizer .", "entities": [[6, 7, "MethodName", "BERT"], [12, 13, "MethodName", "WordPiece"]]}
{"text": "Since dependency annotations are between words , we pass the BERT output corresponding to the first wordpiece per word to the biaffine parser .", "entities": [[10, 11, "MethodName", "BERT"], [16, 17, "MethodName", "wordpiece"]]}
{"text": "Baselines We compare UDapter to the current state of the art in UD parsing : [ 1 ] UUparser+BERT ( Kulmizev et al , 2019 ) , a graph - based BLSTM parser ( de Lhoneux et", "entities": [[12, 13, "DatasetName", "UD"]]}
{"text": "al , 2018 ) using mBERT embeddings as additional features .", "entities": [[5, 6, "MethodName", "mBERT"]]}
{"text": "[ 2 ] UDpipe ( Straka , 2018 ) , a monolingually trained multi - task parser that uses pretrained word embeddings and character representations .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "[ 3 ] UDify ( Kondratyuk and Straka , 2019 ) , the mBERTbased multi - task UD parser on which our UDapter is based , but originally trained on all language treebanks from UD .", "entities": [[17, 18, "DatasetName", "UD"], [34, 35, "DatasetName", "UD"]]}
{"text": "Finally , we evaluate two variants of our model : 1 ) Adapter - only has only task - specific adapter modules and no languagespecific adaptation , i.e. no contextual parameter generator ; and 2 ) UDapter - proxy is trained without typology features : a separate language embedding is learnt from scratch for each in - training language , and for low - resource languages we use one from the same language family , if available , as proxy representation .", "entities": [[12, 13, "MethodName", "Adapter"]]}
{"text": "By comparing UDapter to these parsers , we highlight its unique character that enables language specific parameterization by typological features within a multilingual framework for both supervised and zero - shot learning setup .", "entities": [[28, 32, "TaskName", "zero - shot learning"]]}
{"text": "This result resounds with previous findings in multilingual NMT ( Arivazhagan et al , 2019 ) and highlights the importance of language adaptation even when using high - quality sentence representations like those produced by mBERT .", "entities": [[35, 36, "MethodName", "mBERT"]]}
{"text": "To understand the relevance of adapters , we also evaluate a model which has almost the same architecture as multi - udify except for the adapter modules and the tuning choice ( frozen mBERT weights ) .", "entities": [[33, 34, "MethodName", "mBERT"]]}
{"text": "While it is harder to find a trend here , we notice that UDapter is typically beneficial for the languages not present in the mBERT training corpus : it outperforms multi - udify in 13 out of 22 ( non - mBERT ) languages .", "entities": [[24, 25, "MethodName", "mBERT"], [41, 42, "MethodName", "mBERT"]]}
{"text": "Although dependency parsing is a syntactic task , the network does not only utilize syntactic features , as also observed by Lin et al ( 2019 ) , but exploits all available typological features to learn its representations .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "To answer this question we train another adapteronly model with doubled size ( 2048 instead of the 1024 used in the main experiments ) .", "entities": [[13, 14, "DatasetName", "2048"]]}
{"text": "As seen in 3a , increase in model size brings a slight gain to the high - resource languages , but actually leads to a small loss in the zero - shot setup .", "entities": [[26, 27, "MetricName", "loss"]]}
{"text": "We have presented UDapter , a multilingual dependency parsing model that learns to adapt languagespecific parameters on the basis of adapter modules ( Rebuffi et al , 2018 ; Houlsby et al , 2019 ) and the contextual parameter generation ( CPG ) method ( Platanios et al , 2018 ) which is in principle applicable to a range of multilingual NLP tasks .", "entities": [[7, 9, "TaskName", "dependency parsing"], [60, 62, "TaskName", "multilingual NLP"]]}
{"text": "' * ' shows languages not present in mBERT training data .", "entities": [[8, 9, "MethodName", "mBERT"]]}
{"text": "Languages with ' * ' are not included in mBERT training data .", "entities": [[9, 10, "MethodName", "mBERT"]]}
{"text": "Note that original UDify is trained on all available UD treebanks from 75 languages .", "entities": [[9, 10, "DatasetName", "UD"]]}
{"text": "CompLx@SMM4H'22 : In - domain pretrained language models for detection of adverse drug reaction mentions in English tweets", "entities": [[5, 8, "TaskName", "pretrained language models"]]}
{"text": "The paper describes the system that team CompLx developed for sub - task 1a of the Social Media Mining for Health 2022 ( # SMM4H )", "entities": [[24, 25, "DatasetName", "SMM4H"]]}
{"text": "We finetune a RoBERTa model , a pretrained , transformer - based language model , on a provided dataset to classify English tweets for mentions of Adverse Drug Reactions ( ADRs ) , i.e. negative side effects related to medication intake .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}
{"text": "The Shared Task ( Weissenbacher et al , 2022 ) of the 2022 Social Media Mining for Health Applications ( # SMM4H ) workshop proposed ten sub - tasks in the domain of social media mining for health monitoring and surveillance .", "entities": [[21, 22, "DatasetName", "SMM4H"]]}
{"text": "In the 2022 instantiation of the # SMM4H Shared Task , our team participated in : ( i ) sub - task 1a , the classification of English tweets containing mentions of Adverse Drug Reactions ( ADRs ) ( Magge et al , 2021 ) , ( ii ) sub - task 3 , the classification of English tweets ( 3a ) and WebMD reviews ( 3b ) contain - 1 https://huggingface.co/orestxherija/roberta - base - adr - smm4h2022 2 https://github.com/orestxherija/CompLx - SMM4H2022 3 https://huggingface.co/spaces/orestxherija/adr - mentionclassifier ing mentions of changes in medication treatments , and ( iii ) sub - task 8 , the classification of English tweets self - reporting chronic stress .", "entities": [[7, 8, "DatasetName", "SMM4H"]]}
{"text": "To address these challenges , we finetune a variant of a RoBERTa ( Liu et al , 2019 ) model , a transformer - based ( Vaswani et al , 2017 ) language model pretrained on approximately 128 million tweets ( Loureiro et al , 2022 ) on each sub - task 's provided dataset .", "entities": [[11, 12, "MethodName", "RoBERTa"]]}
{"text": "Without any domain adaptation efforts ( apart from standard finetuning on the downstream task ) or hyperparameter optimizations , the model outperforms the average of all submissions for sub - task 1a by a 9 % absolute difference in F1score .", "entities": [[2, 4, "TaskName", "domain adaptation"]]}
{"text": "Label vyvanse make me so hyper and creative and i think of so many tweets ADR feed an ocd vyvanse and cover him in crayons No ADR trazodone has screwed up my sleep schedule .", "entities": [[18, 19, "DatasetName", "ocd"]]}
{"text": "The establishment of language modeling as the pretraining step in the transfer learning pipeline revolutionized modern NLP with models such as ULMFiT ( Howard and Ruder , 2018 ) , ELMo ( Peters et al , 2018 )", "entities": [[11, 13, "TaskName", "transfer learning"], [21, 22, "MethodName", "ULMFiT"], [30, 31, "MethodName", "ELMo"]]}
{"text": "and , most notably , transformerbased language models such as GPT ( Radford et al , 2018 ) and BERT ( Devlin et al , 2019 ) .", "entities": [[10, 11, "MethodName", "GPT"], [19, 20, "MethodName", "BERT"]]}
{"text": "In recent years , there have been intensive efforts in the research community to produce ever - larger transformer - based pretrained language models that are trained using a variety of datasets , transformermodel architectures , training objectives and optimization techniques .", "entities": [[21, 24, "TaskName", "pretrained language models"]]}
{"text": "This should come as no surprise , since such language models have dominated virtually all NLP leaderboards , most notably GLUE ( Wang et al , 2018 ) and SuperGLUE ( Wang et al , 2019 ) .", "entities": [[20, 21, "DatasetName", "GLUE"], [29, 30, "DatasetName", "SuperGLUE"]]}
{"text": "Considering this overwhelming success , we opt for a RoBERTa ( Liu et al , 2019 ) model 4 that has been trained on approximately 128 million tweets ( Loureiro et al , 2022 ) .", "entities": [[9, 10, "MethodName", "RoBERTa"]]}
{"text": "We opt for a model that has been trained on an in - domain corpus , namely tweets , as transfer learning has been shown to yield improved results when there is indomain pretraining ( Gururangan et al , 2020 ) .", "entities": [[20, 22, "TaskName", "transfer learning"]]}
{"text": "Finally , we employ label smoothing ( Szegedy et al , 2016 ) with a smoothing factor of 0.1 .", "entities": [[4, 6, "MethodName", "label smoothing"]]}
{"text": "We train the models for a maximum of 25 epochs with an early stopping patience level set to 0.001 for 3 epochs .", "entities": [[12, 14, "MethodName", "early stopping"]]}
{"text": "Note that in this set of experiments , the validation set was used both during training ( e.g. for early stopping or selection of batch size ) as well as for the reporting of the systems ' performance .", "entities": [[19, 21, "MethodName", "early stopping"], [24, 26, "HyperparameterName", "batch size"]]}
{"text": "We demonstrated that a RoBERTa model ( Liu et al , 2019 ) pretrained on approximately 128 million tweets performs very competitively when finetuned on English tweet classification for ADRs .", "entities": [[4, 5, "MethodName", "RoBERTa"]]}
{"text": "Text classification and , more generally , binary classification is one of the oldest and most widely researched topics in NLP .", "entities": [[0, 2, "TaskName", "Text classification"]]}
{"text": "Data augmentation methods typically target the initial part of the workflow , the data , aiming to increase the quantity , quality and diversity of the training dataset to ensure that model performance is robust to small syntactic or semantic perturbations in the inputs .", "entities": [[0, 2, "TaskName", "Data augmentation"]]}
{"text": "The research community has devoted intensive efforts in the former approach , as can be observed by the ever - increasing list of transformerbased pretrained language models ( Devlin et al , 2019 ;", "entities": [[24, 27, "TaskName", "pretrained language models"]]}
{"text": "The latter approach would include domain adaptation techniques , such as continued self - supervised pretraining followed by supervised finetuning , which has been shown ( Gururangan et al , 2020 ) to consistently lead to superior results relative to direct finetuning .", "entities": [[5, 7, "TaskName", "domain adaptation"]]}
{"text": "Clause - Wise and Recursive Decoding for Complex and Cross - Domain Text - to - SQL Generation", "entities": [[12, 17, "TaskName", "Text - to - SQL"]]}
{"text": "Most deep learning approaches for text - to - SQL generation are limited to the WikiSQL dataset , which only supports very simple queries over a single table .", "entities": [[5, 10, "TaskName", "text - to - SQL"], [15, 16, "DatasetName", "WikiSQL"]]}
{"text": "We focus on the Spider dataset , a complex and crossdomain text - to - SQL task , which includes complex queries over multiple tables .", "entities": [[11, 16, "TaskName", "text - to - SQL"]]}
{"text": "Text - to - SQL generation is the task of translating a natural language question into the corresponding SQL .", "entities": [[0, 5, "TaskName", "Text - to - SQL"]]}
{"text": "Recently , various deep learning approaches have been proposed based on the WikiSQL dataset ( Zhong et al , 2017 ) .", "entities": [[12, 13, "DatasetName", "WikiSQL"]]}
{"text": "However , because WikiSQL contains only very simple queries over just a single table , these approaches ( Xu et al , 2017 ; Huang et al , 2018 ; Yu et al , 2018a ; Dong and Lapata , 2018 ) can not be applied directly to generate complex queries containing elements such as JOIN , GROUP BY , and nested queries .", "entities": [[3, 4, "DatasetName", "WikiSQL"]]}
{"text": "To overcome this limitation , Yu et al ( 2018c ) introduced Spider , a new complex and crossdomain text - to - SQL dataset .", "entities": [[19, 24, "TaskName", "text - to - SQL"]]}
{"text": "We first predict a sketch for each SQL clause ( e.g. , SELECT , WHERE ) with text classification modules .", "entities": [[17, 19, "TaskName", "text classification"]]}
{"text": "Our work is related to the grammar - based constrained decoding approaches for semantic parsing ( Yin and Neubig , 2017 ;", "entities": [[13, 15, "TaskName", "semantic parsing"]]}
{"text": "While their approaches are focused on general purpose code generation , we instead focus on SQL - specific grammar to address the text - to - SQL task .", "entities": [[8, 10, "TaskName", "code generation"], [22, 27, "TaskName", "text - to - SQL"]]}
{"text": "Our task differs from code generation in two aspects .", "entities": [[4, 6, "TaskName", "code generation"]]}
{"text": "For text - to - SQL generation , several SQLspecific approaches have been proposed ( Zhong et al , 2017 ;", "entities": [[1, 6, "TaskName", "text - to - SQL"]]}
{"text": "Xu et al , 2017 ; Huang et al , 2018 ; Yu et al , 2018a ; Dong and Lapata , 2018 ; Yavuz et al , 2018 ) based on WikiSQL dataset ( Zhong et al , 2017 ) .", "entities": [[32, 33, "DatasetName", "WikiSQL"]]}
{"text": "However , all of them are limited to the specific WikiSQL SQL sketch , which only supports very simple queries .", "entities": [[10, 11, "DatasetName", "WikiSQL"]]}
{"text": "However , they focused only on specific databases such as ATIS ( Price , 1990 ) and GeoQuery ( Zelle and Mooney , 1996 ) .", "entities": [[10, 11, "DatasetName", "ATIS"]]}
{"text": "SyntaxSQLNet ( Yu et al , 2018b ) is the first and state - of - the - art model for the Spider ( Yu et al , 2018c ) , a complex and cross - domain text - to - SQL task .", "entities": [[37, 42, "TaskName", "text - to - SQL"]]}
{"text": "We encode a natural language question with a bidirectional LSTM .", "entities": [[8, 10, "MethodName", "bidirectional LSTM"]]}
{"text": "We denote H Q R d\u00d7 | X | as the question encoding , where d is the number of LSTM units and | X | is the number of tokens in the question .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "First , we apply bi - directional LSTM over this sequence for each column .", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "Then , we apply the self - attention mechanism ( Lin et al , 2017 ) over the LSTM outputs to form a summarized fixedsize vector for each column .", "entities": [[18, 19, "MethodName", "LSTM"]]}
{"text": "For the ith column , its encoding h ( i ) col R d is computed by a weighted sum of the LSTM output", "entities": [[22, 23, "MethodName", "LSTM"]]}
{"text": "We predict the clause - wise sketch via 8 different text classification modules that include the number of SQL expressions in each clause , the presence of LIMIT clause , and the presence of INTERSECT / UNION / EXCEPT as described in Figure 1 .", "entities": [[10, 12, "TaskName", "text classification"]]}
{"text": "For the classification , we applied attention - based bi - directional LSTM following Zhou et al ( 2016 ) .", "entities": [[12, 13, "MethodName", "LSTM"]]}
{"text": "To predict columns and operators , we use the LSTM decoder with the attention mechanism ( Luong et al , 2015 ) such that the number of decoding steps are decided by the sketch prediction module .", "entities": [[9, 10, "MethodName", "LSTM"]]}
{"text": "= LSTM ( d ( t\u22121 ) col , h ( t\u22121 ) col ) ,", "entities": [[1, 2, "MethodName", "LSTM"]]}
{"text": "R d followed by tanh activation .", "entities": [[4, 6, "MethodName", "tanh activation"]]}
{"text": "Finally , the probability for each column at the tth decoding step is computed as a dot product between a ( t ) col R d and the encoding of each column in H col R d\u00d7 | C | followed by softmax .", "entities": [[42, 43, "MethodName", "softmax"]]}
{"text": "P ( t ) col = softmax ( a ( t ) col T H col ) ( 9 )", "entities": [[6, 7, "MethodName", "softmax"]]}
{"text": "d ( t ) op = LSTM ( d ( t\u22121 ) op , h ( t ) col ) ( 10 ) Attentional output a ( t ) op R d is computed identically to Eq .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "Then , the probability for operators corresponding to the t - th predicted column is computed by the softmax classifier as follows : P ( t ) op = softmax ( W o a ( t ) op + b o ) ( 11 ) where W o R no\u00d7d and b o R no are trainable parameters and n o is the number of possible operators .", "entities": [[18, 19, "MethodName", "softmax"], [29, 30, "MethodName", "softmax"]]}
{"text": "We evaluate our model with Spider ( Yu et al , 2018c ) , a large - scale , complex and cross - domain text - to - SQL dataset .", "entities": [[24, 29, "TaskName", "text - to - SQL"]]}
{"text": "Our model and all the baseline models are trained based on only the Spider dataset without data augmentation .", "entities": [[16, 18, "TaskName", "data augmentation"]]}
{"text": "For the word embedding , we apply deep contextualized word representations ( ELMO ) from Peters et al ( 2018 ) and allow them to be fine - tuned during the training .", "entities": [[12, 13, "MethodName", "ELMO"]]}
{"text": "For the question and column encoders , we use a 1 - layer 512 - unit bi - directional LSTM .", "entities": [[19, 20, "MethodName", "LSTM"]]}
{"text": "For the decoders in the columns and operators prediction modules , we use a 1 - layer 1024unit uni - directional LSTM .", "entities": [[21, 22, "MethodName", "LSTM"]]}
{"text": "In this paper , we propose a recursive and SQL clause - wise decoding neural architecture to address the complex and cross - domain text - to - SQL task .", "entities": [[24, 29, "TaskName", "text - to - SQL"]]}
{"text": "For many applications in text simplification , computer - aided language learning ( CALL ) systems , authorship tools , translation , and information retrieval , sentence - level readability metrics are direly needed .", "entities": [[4, 6, "TaskName", "text simplification"], [23, 25, "TaskName", "information retrieval"]]}
{"text": "For instance , an automatic text simplification system must begin by asking which portions of a text need to be simplified .", "entities": [[5, 7, "TaskName", "text simplification"]]}
{"text": "Translation tools can aim to preserve not just meaning but also the approximate difficulty of the sentences they are translating or use a sentence - level difficulty metric to target output that is easier to understand .", "entities": [[0, 1, "TaskName", "Translation"]]}
{"text": "Furthermore , information retrieval systems also benefit when they can return not merely relevant texts , but also texts appropriate to the reading level of the user .", "entities": [[2, 4, "TaskName", "information retrieval"]]}
{"text": "Recently there has been an increased interest in sentential models of text difficulty in the automatic text simplification and summarization communities in particular ( Vajjala and Meurers , 2014 ; Macdonald and Siddharthan , 2016 ) .", "entities": [[16, 18, "TaskName", "text simplification"], [19, 20, "TaskName", "summarization"]]}
{"text": "Petersen trained SVM classifiers to classify texts as belonging to one of four primary school grade levels based on the Weekly Reader educational newspaper 2 .", "entities": [[2, 3, "MethodName", "SVM"]]}
{"text": "3 Coh - Metrix ( Graesser et al , 2004 ) also includes a number of measures related to discourse coherence , for example .", "entities": [[3, 4, "MethodName", "Metrix"]]}
{"text": "Classification Few studies to date have addressed sentence - level readability for English .", "entities": [[0, 1, "TaskName", "Classification"]]}
{"text": "The structure of this task , however , is not suited to text simplification applications , because the sentences are not controlled for meaning .", "entities": [[12, 14, "TaskName", "text simplification"]]}
{"text": "In the medical domain , Kauchak et al ( 2014 ) also looked at sentence - level classification , identifying sentences as being either simple or difficult .", "entities": [[2, 4, "DatasetName", "medical domain"]]}
{"text": "Their features included word length , sentences length , part - of - speech counts , average unigram frequencies and standard deviation , and the proportion of words not on a list of the five thousand most frequent words as well as three domainspecific features based on an ontology of medical terminology .", "entities": [[9, 12, "DatasetName", "part - of"], [48, 49, "MethodName", "ontology"]]}
{"text": "In this example , we are at embedding depth 1 or 0 up until we encounter the word that , which increases the embedding depth by 1 , resulting in a nonzero embedding difference score .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "The English and Simple English Wikipedia corpus of Hwang et al ( 2015 , ESEW ) is a new corpus of more than 150k sentence pairs designed to address the flaws of the Parallel Wikipedia Corpus of Zhu et al ( 2010 , PWKP ) , which was previously dominant in work on text simplification , by using a more sophisticated method of aligning pairs of English and Simple English sentences .", "entities": [[53, 55, "TaskName", "text simplification"]]}
{"text": "Both parsers are trained and perform near the state of the art on the standard sections of the Wall Street Journal section of the Penn Treebank .", "entities": [[24, 26, "DatasetName", "Penn Treebank"]]}
{"text": "The program icy - parses uses part - of - speech tags and head - dependent relations to determine the total , average , and maximum integration cost across a sentence .", "entities": [[6, 9, "DatasetName", "part - of"]]}
{"text": "Therefore we wanted to provide a point of comparison regarding the relative utility of these parsers : grouping features by parser allows us to assess the trade - off between model accuracy and the time necessary for feature extraction .", "entities": [[31, 32, "MetricName", "accuracy"]]}
{"text": "( s 1 \u2212 s 2 ) > 0", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "Since the features examined in our study are complementary to those proposed by these two previous studies , a model combining all of these features should further improve in accuracy .", "entities": [[29, 30, "MetricName", "accuracy"]]}
{"text": "Inspired by this , our task is to generate procedure captions from narrated instructional videos which are a sequence of step - wise clips with a description as shown in Figure 1 .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Previous works on video understanding tend to recognize actions in video clips by detecting pose ( Wang et al , 2013a ; Packer et al , 2012 ) and motion ( Wang et al , 2013b ; Yang et al , 2013 ) or both ( Wang et al , 2014 ) and fine - grained features .", "entities": [[3, 5, "TaskName", "video understanding"]]}
{"text": "al , 2016 ) for video captioning usually consist of two stages : ( 1 ) temporal event proposition ; and ( 2 ) event captioning .", "entities": [[5, 7, "TaskName", "video captioning"]]}
{"text": "Previous models for dense video captioning only use video signals without considering transcripts .", "entities": [[3, 6, "TaskName", "dense video captioning"]]}
{"text": "Then we use temporal convolution to encode these features and generate procedure proposals .", "entities": [[4, 5, "MethodName", "convolution"]]}
{"text": "2 . We employ the pre - trained BERT ( Devlin et al , 2018 ) and self - attention ( Vaswani et", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "Narrated Instructional Video Understanding Previous works aim to ground the description to the video .", "entities": [[2, 4, "TaskName", "Video Understanding"]]}
{"text": "( Naim et al , 2015 ) utilize latent - variable based discriminative models ( CRF , Structured Perceptron ) for unsupervised alignment .", "entities": [[15, 16, "MethodName", "CRF"]]}
{"text": "DAPs in ( Escorcia et al , 2016 ) apply a sliding window and a Long Short - Term Memory ( LSTM ) network for video content encoding and predicting proposals covered by the window .", "entities": [[15, 20, "MethodName", "Long Short - Term Memory"], [21, 22, "MethodName", "LSTM"]]}
{"text": "SST in ( Buch et al , 2017 ) effectively generates proposals in a single pass .", "entities": [[0, 1, "DatasetName", "SST"]]}
{"text": "Different from video captioning and paragraph generation , dense video caption requires segmenting of each video into a sequence of temporal propos - als with corresponding captions .", "entities": [[2, 4, "TaskName", "video captioning"]]}
{"text": "( Krishna et al , 2017 ) resorts to the DAP method ( Escorcia et al , 2016 ) for event detection and apply the contextaware S2VT model ( Venugopalan et al , 2015 ) .", "entities": [[20, 22, "TaskName", "event detection"]]}
{"text": "( Li et al , 2018 ) train jointly on unifying the temporal proposal localization and sentence generation for dense video captioning .", "entities": [[19, 22, "TaskName", "dense video captioning"]]}
{"text": "Besides , there are also some works try to incorporate multi - modal information ( e.g. audio stream ) for dense video captioning task ( Ramanishka et al , 2016 ;", "entities": [[20, 23, "TaskName", "dense video captioning"]]}
{"text": "To embed transcripts , we first split all tokens in the transcript by a sliding window and input them into a uncased BERT - large ( Devlin et al , 2018 ) model .", "entities": [[22, 23, "MethodName", "BERT"]]}
{"text": "Next , we encode these sentences by a Transformer ( Vaswani et al , 2017 ) and take the first output as the context - aware transcript embedding e R e .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "To embed the videos , we uniformly sample T frames and encode each frame v t in V = { v 1 , , v T } to an embedding representation by an ImageNet - pre - trained ResNet - 32 ( He et al , 2016 ) network .", "entities": [[33, 34, "DatasetName", "ImageNet"], [38, 39, "MethodName", "ResNet"]]}
{"text": "Then we adopt another Transformer model to further encode the context information , and output X = { x 1 , , x T } R T \u00d7d .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "C = { c 1 , , c t , , c T | c t = { x t e } } and feed it into a Bi - directional LSTM F = Bi - LSTM ( C ) where F = { f 1 f T } R T \u00d7f , and f is the hidden size of the LSTM layers .", "entities": [[31, 32, "MethodName", "LSTM"], [36, 37, "MethodName", "LSTM"], [61, 62, "MethodName", "LSTM"]]}
{"text": "In order to encode this interaction , we follow the method in ( Zhou et al , 2018a ) which uses an LSTM model to predict a sequence from the K \u00d7 T generated procedure proposal .", "entities": [[22, 23, "MethodName", "LSTM"]]}
{"text": "Score Feature The score feature is a flatten of matrix M s , i.e. s R K T \u00d71 .", "entities": [[0, 1, "MetricName", "Score"]]}
{"text": "We design an LSTM based sequence - to - sequence model ( Sutskever et al , 2014 ) to generate captions for each extracted procedure .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "= { e ts , , e te } \u2282 { e 1 , , e Q } where Q is the total word count of a video 's transcript .", "entities": [[3, 4, "MethodName", "ts"]]}
{"text": "The loss L s aims to enlarge the score of all positive samples and decrease the score otherwise .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "The cross - entropy loss L p aims to exploit the model to correctly select the most similar proposal of each ground - truth procedure from many positive samples .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "The IoU replaces the denominator part with the union of predicted and ground - truth procedures .", "entities": [[1, 2, "MetricName", "IoU"]]}
{"text": "This dataset contains pre - processed frame features ( T = 500 frames for each video , each frame feature is a 512 - d vector , extracted by ResNet - 32 ) which were used in ( Zhou et", "entities": [[29, 30, "MethodName", "ResNet"]]}
{"text": "However , the labels of 210 testing videos are unpublished , we can only adopt the training and validation dataset for our experiment .", "entities": [[18, 20, "DatasetName", "validation dataset"]]}
{"text": "The hidden size of all recurrent model ( LSTM ) is 512 and we conduct a dropout for each layer with a probability of 0.5 .", "entities": [[8, 9, "MethodName", "LSTM"]]}
{"text": "We use two transformer models with 2048 inner hidden sizes , 8 heads , and 6 layers to encode context - aware transcripts and video frame features separately .", "entities": [[6, 7, "DatasetName", "2048"]]}
{"text": "We compare our model with several baseline methods : ( 1 ) SCNN - prop ( Shou et al , 2016 ) ( 2 ) vsLSTM is an LSTM based video summarization model ; ( 3 ) Proc - Nets ( Zhou et", "entities": [[28, 29, "MethodName", "LSTM"], [30, 32, "TaskName", "video summarization"]]}
{"text": "In order to ensure a fair comparison , we first run the ProcNets on the validation dataset of YouCookII and get a comparable result .", "entities": [[15, 17, "DatasetName", "validation dataset"]]}
{"text": "For evaluating procedure captioning , we consider two baseline models : ( 1 ) Bi - LSTM with temporal attention ( Yao et al , 2015 ) ( 2 ) an end - to - end transformer based video dense captioning model proposed in ( Zhou et al , 2018b ) .", "entities": [[16, 17, "MethodName", "LSTM"], [18, 20, "MethodName", "temporal attention"]]}
{"text": "Domain Adaptation for Sentiment Analysis using Keywords in the Target Domain as the Learning Weight", "entities": [[0, 2, "TaskName", "Domain Adaptation"], [3, 5, "TaskName", "Sentiment Analysis"]]}
{"text": "This paper proposes a new method of instance - based domain adaptation for sentiment analysis .", "entities": [[10, 12, "TaskName", "domain adaptation"], [13, 15, "TaskName", "sentiment analysis"]]}
{"text": "Next , the keyword content rate of a document is calculated using the likelihood of keywords and the domain adaptation is performed by giving the keyword content rate to each document in the source domain as the weight .", "entities": [[18, 20, "TaskName", "domain adaptation"]]}
{"text": "This paper proposes a new method of instance - based domain adaptation for sentiment analysis .", "entities": [[10, 12, "TaskName", "domain adaptation"], [13, 15, "TaskName", "sentiment analysis"]]}
{"text": "Sentiment analysis involves judging a polarity , positive or negative , of a review such as a movie review .", "entities": [[0, 2, "TaskName", "Sentiment analysis"]]}
{"text": "This is one of the document classification tasks and supervised learning can be used to solve it .", "entities": [[5, 7, "TaskName", "document classification"]]}
{"text": "However , if the domain of the test data is different from the domain for the learning data ( for example , book reviews ) , the accuracy of the classifier obtained through standard supervised learning reduced .", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "The solution to this problem is domain adaptation .", "entities": [[6, 8, "TaskName", "domain adaptation"]]}
{"text": "Domain adaptation can be roughly divided into two categories : feature - based and instance - based ( Pan and Yang , 2010 ) .", "entities": [[0, 2, "TaskName", "Domain adaptation"]]}
{"text": "Domain adaptation is roughly divided into two types : the supervised approach using labeled data in the target domain and the unsupervised approach that does not use them .", "entities": [[0, 2, "TaskName", "Domain adaptation"]]}
{"text": "The number of files contained in each domain is shown in The learning algorithm is an SVM with scikitlearn .", "entities": [[16, 17, "MethodName", "SVM"]]}
{"text": "The core is linear , the value of the c parameter is fixed at 0.1 , and the scikit - learn SVM supports Weighted - Learning 1 , so the scikit - learn SVM is used here .", "entities": [[21, 22, "MethodName", "SVM"], [33, 34, "MethodName", "SVM"]]}
{"text": "NONE in Table 1 means that the domain adaptation method was not used but simply applies the classifier formed from the training data of the source domain to the result of the test data in the target domain was applied .", "entities": [[7, 9, "TaskName", "domain adaptation"]]}
{"text": "For theae data only , the instance - based method has no effect on domain adaptation .", "entities": [[14, 16, "TaskName", "domain adaptation"]]}
{"text": "After SCL is combined with our method , the accuracy is not high enough .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "In addition , although the weighted - learning SVM is used in this paper , the loss value of the loss function in the neural network is multiplied by the weight , and it is easier to realize weighted - learning as the loss value .", "entities": [[8, 9, "MethodName", "SVM"], [16, 17, "MetricName", "loss"], [20, 21, "MetricName", "loss"], [43, 44, "MetricName", "loss"]]}
{"text": "There are many options for using the domain adaptation method on deep learning , and these solutions combined with instance - based are easier to compute .", "entities": [[7, 9, "TaskName", "domain adaptation"]]}
{"text": "In a simple example , we used the AutoEncoder ( AE ) as the feature - based method .", "entities": [[8, 9, "MethodName", "AutoEncoder"], [10, 11, "MethodName", "AE"]]}
{"text": "Using AE , the dimension of the data in the source and target domain was reduced , that is , encoded .", "entities": [[1, 2, "MethodName", "AE"]]}
{"text": "In learning , as described above , the value of the loss function was multiplied by the weight obtained by our method and was taken as the loss value FIG .", "entities": [[11, 12, "MetricName", "loss"], [27, 28, "MetricName", "loss"]]}
{"text": "This paper proposed a method for instance - based domain adaptation of sentiment analysis .", "entities": [[9, 11, "TaskName", "domain adaptation"], [12, 14, "TaskName", "sentiment analysis"]]}
{"text": "However , using an instance - based alone to perform domain adaptation has a very small effect , the combining instance - based method and feature - based method is assured as shown in this paper .", "entities": [[10, 12, "TaskName", "domain adaptation"]]}
{"text": "Joint Learning of POS and Dependencies for Multilingual Universal Dependency Parsing", "entities": [[9, 11, "TaskName", "Dependency Parsing"]]}
{"text": "This paper describes the system of team LeisureX in the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[21, 23, "DatasetName", "Universal Dependencies"]]}
{"text": "Our system predicts the part - of - speech tag and dependency tree jointly .", "entities": [[4, 7, "DatasetName", "part - of"]]}
{"text": "For the basic tasks , including tokenization , lemmatization and morphology prediction , we employ the official baseline model ( UDPipe ) .", "entities": [[8, 9, "TaskName", "lemmatization"]]}
{"text": "The goal of Universal Dependencies ( UD ) ( Nivre et al , 2016 ; Zeman et al , 2017 ) is to develop multilingual treebank , whose annotations of morphology and syntax are cross - linguistically consistent .", "entities": [[3, 5, "DatasetName", "Universal Dependencies"], [6, 7, "DatasetName", "UD"]]}
{"text": "In this paper , we describe our system for the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies ( Zeman et al , 2018 ) , and we focus only on the subtasks of part - of - speech ( POS ) tagging and dependency parsing .", "entities": [[21, 23, "DatasetName", "Universal Dependencies"], [39, 42, "DatasetName", "part - of"], [49, 51, "TaskName", "dependency parsing"]]}
{"text": "For the intermediate steps , including tokenization , lemmatization and morphology prediction , we tackle them by the official baseline model ( UDPipe ) 1 .", "entities": [[8, 9, "TaskName", "lemmatization"]]}
{"text": "Dependency parsing that aims to predict the existence and type of linguistic dependency relations between words , is a fundamental part in natural language processing ( NLP ) tasks ( Li et al , 2018c ; .", "entities": [[0, 2, "TaskName", "Dependency parsing"]]}
{"text": "Many referential natural language processing studies ( Zhang et al , 2018 ; Cai et al , 2018 ; Li et al , 2018b ; Wang et al , 2018 ; Qin et al , 2017 ) can also contribute to the universal dependency parsing system .", "entities": [[43, 45, "TaskName", "dependency parsing"]]}
{"text": "Universal dependency parsing focuses on learning syntactic dependency structure over many typologically different languages , even low - resource languages in a real - world setting .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "Within the dependency parsing literature , there are two dominant techniques , graph - based ( McDonald et al , 2005 ; Ma and Zhao , 2012 ; Kiperwasser and Goldberg , 2016 ; and transition - based parsing ( Nivre , 2003 ; Dyer et al , 2015 ; .", "entities": [[2, 4, "TaskName", "dependency parsing"]]}
{"text": "In our system , we adopt the transition - based dependency parsing in view of its relatively lower time complexity .", "entities": [[7, 12, "TaskName", "transition - based dependency parsing"]]}
{"text": "Our system implements universal dependency parsing based on the stack - pointer networks ( STACKPTR ) parser introduced by ( Ma et al , 2018 ) .", "entities": [[4, 6, "TaskName", "dependency parsing"]]}
{"text": "Furthermore , previous work ( Straka et al , 2016 ; Nguyen et al , 2017 ) showed that POS tags are helpful to dependency parsing .", "entities": [[24, 26, "TaskName", "dependency parsing"]]}
{"text": "Therefore , we seek to jointly learn POS tagging and dependency parsing .", "entities": [[10, 12, "TaskName", "dependency parsing"]]}
{"text": "As Long short - term memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) have shown significant representational effectiveness to a wide range of NLP tasks , we leverage bidirectional LSTMs ( BiLSTM ) to learn shared representations for both POS tagging and dependency parsing .", "entities": [[1, 6, "MethodName", "Long short - term memory"], [7, 8, "MethodName", "LSTM"], [35, 36, "MethodName", "BiLSTM"], [46, 48, "TaskName", "dependency parsing"]]}
{"text": "In this section , we describe our joint model 2 for POS tagging and dependency parsing in the CoNLL 2018 Shared Task , which is built on the STACKPTR parser introduced by ( Ma et al , 2018 ) .", "entities": [[14, 16, "TaskName", "dependency parsing"]]}
{"text": "In our system , we follow the bidirectional LSTM - CNN architecture ( BiLSTM - CNNs ) ( Chiu and Nichols , 2016 ; Ma and Hovy , 2016 ) , where CNNs encode word information into character - level representation and BiLSTM models context information of each word .", "entities": [[7, 9, "MethodName", "bidirectional LSTM"], [13, 14, "MethodName", "BiLSTM"], [42, 43, "MethodName", "BiLSTM"]]}
{"text": "Due to their ability to capture syntactic and semantic information of words from large scale unlabeled texts , we pre - train the word embeddings from the given training dataset by word2vec ( Mikolov et al , 2013 ) toolkit .", "entities": [[23, 25, "TaskName", "word embeddings"]]}
{"text": "The tagger uses a BiLSTM over the concatenation of word embeddings and character embeddings : s pos", "entities": [[4, 5, "MethodName", "BiLSTM"], [9, 11, "TaskName", "word embeddings"]]}
{"text": "The tag classifier is trained jointly using crossentropy losses that are summed together with the dependency parser loss during optimization .", "entities": [[17, 18, "MetricName", "loss"]]}
{"text": "In order to integrate contextual information , we concatenate the character embedding e c , pre - trained word embedding e w and UPOS tag embedding e pos , then feed them into the BiLSTM .", "entities": [[34, 35, "MethodName", "BiLSTM"]]}
{"text": "The universal dependency parsing component of our system is built on the current state - of - the - art approach STACKPTR , which combines pointer networks ( Vinyals et al , 2015 ) with an internal stack for tracking the status of depth - first search .", "entities": [[2, 4, "TaskName", "dependency parsing"]]}
{"text": "The encoder based on BiLSTM - CNNs architecture takes the sequence of tokens and their POS tags as input , then encodes it into encoder hidden state s i .", "entities": [[4, 5, "MethodName", "BiLSTM"]]}
{"text": "U T h t + V T s i + b a t = sof tmax ( e t )", "entities": [[14, 15, "DatasetName", "sof"]]}
{"text": "\u03b2", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "The training objective of pur system is to learn the probability of UPOS tags P \u03b8 pos ( y pos | x ) and the dependency trees P \u03b8 dep ( y dep | x , y pos ) .", "entities": [[15, 16, "HyperparameterName", "\u03b8"], [28, 29, "HyperparameterName", "\u03b8"]]}
{"text": "Given a sentence x , the probabilities are factorized as : P \u03b8 pos", "entities": [[12, 13, "HyperparameterName", "\u03b8"]]}
{"text": "= k i=1 P \u03b8 pos ( p", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "y pos = arg max ypos Ypos ( P \u03b8 pos", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "P \u03b8 dep ( y dep |", "entities": [[1, 2, "HyperparameterName", "\u03b8"]]}
{"text": "= k i=1 P \u03b8 dep ( p", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "k i=1 l i j=1 P \u03b8 dep ( c i , j | c i , < j ,", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "p < i , x , y pos ) where \u03b8 pos and \u03b8 dep represent the model parameters respectively .", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "Therefore , the whole loss is the sum of three objectives : Loss = Loss pos + Loss arc + Loss label where the Loss pos , Loss arc and Loss label are the conditional likehood of their corresponding target , using the cross - entropy loss .", "entities": [[4, 5, "MetricName", "loss"], [46, 47, "MetricName", "loss"]]}
{"text": "We initialize word vectors with 50 - dimensional pretrained word embeddings , 100 - dimensional tag embeddings and 512 - dimensional recurrent states ( in each direction ) .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "These results show that our joint model could improve the performance of universal dependency parsing .", "entities": [[13, 15, "TaskName", "dependency parsing"]]}
{"text": "Surprisingly , in the case of POS tagging , our joint model obtains lower averaged accuracy in both UPOS and XPOS .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "In this paper , we describe our system in the CoNLL 2018 shared task on UD parsing .", "entities": [[15, 16, "DatasetName", "UD"]]}
{"text": "Our system uses a transition - based neural network architecture for dependency parsing , which predicts the UPOS tag and dependencies jointly .", "entities": [[11, 13, "TaskName", "dependency parsing"]]}
{"text": "Get To The Point : Summarization with Pointer - Generator Networks", "entities": [[5, 6, "TaskName", "Summarization"]]}
{"text": "Neural sequence - to - sequence models have provided a viable new approach for abstractive text summarization ( meaning they are not restricted to simply selecting and rearranging passages from the original text ) .", "entities": [[14, 17, "TaskName", "abstractive text summarization"]]}
{"text": "We apply our model to the CNN / Daily Mail summarization task , outperforming the current abstractive state - of - the - art by at least 2 ROUGE points .", "entities": [[6, 10, "DatasetName", "CNN / Daily Mail"], [10, 11, "TaskName", "summarization"]]}
{"text": "Summarization is the task of condensing a piece of text to a shorter version that contains the main information from the original .", "entities": [[0, 1, "TaskName", "Summarization"]]}
{"text": "There are two broad approaches to summarization : extractive and abstractive .", "entities": [[6, 7, "TaskName", "summarization"]]}
{"text": "Baseline Seq2Seq + Attention : UNK UNK says his administration is confident it will be able to destabilize nigeria 's economy .", "entities": [[1, 2, "MethodName", "Seq2Seq"]]}
{"text": "Figure 1 : Comparison of output of 3 abstractive summarization models on a news article .", "entities": [[9, 10, "TaskName", "summarization"]]}
{"text": "chunks of text from the source document ensures baseline levels of grammaticality and accuracy .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "On the other hand , sophisticated abilities that are crucial to high - quality summarization , such as paraphrasing , generalization , or the incorporation of real - world knowledge , are possible only in an abstractive framework ( see Figure 5 ) .", "entities": [[14, 15, "TaskName", "summarization"]]}
{"text": "Due to the difficulty of abstractive summarization , the great majority of past work has been extractive ( Kupiec et al , 1995 ;", "entities": [[6, 7, "TaskName", "summarization"]]}
{"text": "However , the recent success of sequence - to - sequence models ( Sutskever ... et al , 2014 ) , in which recurrent neural networks ( RNNs ) both read and freely generate text , has made abstractive summarization", "entities": [[39, 40, "TaskName", "summarization"]]}
{"text": "While most recent abstractive work has focused on headline generation tasks ( reducing one or two sentences to a single headline ) , we believe that longer - text summarization is both more challenging ( requiring higher levels of abstraction while avoiding repetition ) and ultimately more useful .", "entities": [[28, 30, "TaskName", "text summarization"]]}
{"text": "Forced - Attention Sentence Compression , that were applied to short - text summarization .", "entities": [[3, 5, "DatasetName", "Sentence Compression"], [12, 14, "TaskName", "text summarization"]]}
{"text": "We propose a novel variant of the coverage vector ( Tu et al , 2016 ) from Neural Machine Translation , which we use to track and control coverage of the source document .", "entities": [[18, 20, "TaskName", "Machine Translation"]]}
{"text": "i are fed one - by - one into the encoder ( a single - layer bidirectional LSTM ) , producing a sequence of encoder hidden states", "entities": [[16, 18, "MethodName", "bidirectional LSTM"]]}
{"text": "On each step t , the decoder ( a single - layer unidirectional LSTM ) receives the word embedding of the previous word ( while training , this is the previous word of the reference summary ; at test time it is the previous word emitted by the decoder ) , and has decoder state s t .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "i + W s s t + b attn ) ( 1 ) a t = softmax ( e t ) ( 2 )", "entities": [[16, 17, "MethodName", "softmax"]]}
{"text": "The attention distribution can be viewed as For each decoder timestep a generation probability p gen [ 0 , 1 ] is calculated , which weights the probability of generating words from the vocabulary , versus copying words from the source text .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "Note that out - of - vocabulary article words such as 2 - 0 are included in the final distribution .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "The context vector , which can be seen as a fixedsize representation of what has been read from the source for this step , is concatenated with the decoder state s t and fed through two linear layers to produce the vocabulary distribution P vocab : P vocab = softmax ( V ( V [ s t , h * t ]", "entities": [[49, 50, "MethodName", "softmax"]]}
{"text": "During training , the loss for timestep t is the negative log likelihood of the target word w * t for that timestep :", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "Our pointer - generator network is a hybrid between our baseline and a pointer network ( Vinyals et al , 2015 ) , as it allows both copying words via pointing , and generating words from a fixed vocabulary .", "entities": [[13, 15, "MethodName", "pointer network"]]}
{"text": "In addition , the generation probability p gen [ 0 , 1 ] for timestep t is calculated from the context vector h * t , the decoder state s t and the decoder input x t : p gen = \u03c3 ( w T h * h * t + w T s s t + w T", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "x x t + b ptr ) ( 8 ) where vectors w h * , w s , w x and scalar b ptr are learnable parameters and \u03c3 is the sigmoid function .", "entities": [[5, 6, "DatasetName", "ptr"], [24, 25, "DatasetName", "ptr"]]}
{"text": "In our coverage model , we maintain a coverage vector c t , which is the sum of attention distributions over all previous decoder timesteps : c t = \u2211 t\u22121 t = 0 a t ( 10 )", "entities": [[33, 34, "DatasetName", "0"]]}
{"text": "Note that c 0 is a zero vector , because on the first timestep , none of the source document has been covered .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "Our loss function is more flexible : because summarization should not require uniform coverage , we only penalize the overlap between each attention distribution and the coverage so far - preventing repeated attention .", "entities": [[1, 2, "MetricName", "loss"], [8, 9, "TaskName", "summarization"]]}
{"text": "Rush et al ( 2015 ) were the first to apply modern neural networks to abstractive text summarization , achieving state - of - the - art performance on DUC - 2004 and Gigaword , two sentence - level summarization datasets .", "entities": [[15, 18, "TaskName", "abstractive text summarization"], [39, 40, "TaskName", "summarization"]]}
{"text": "Their approach , which is centered on the attention mechanism , has been augmented with recurrent decoders , Abstract Meaning Representations ( Takase et al , 2016 ) , hierarchical networks , variational autoencoders ( Miao and Blunsom , 2016 ) , and direct optimization of the performance metric ( Ranzato et al , 2016 ) , further improving performance on those datasets .", "entities": [[33, 34, "MethodName", "autoencoders"]]}
{"text": "However , large - scale datasets for summarization of longer text are rare .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "adapted the DeepMind question - answering dataset ( Hermann et al , 2015 ) for summarization , resulting in the CNN / Daily Mail dataset , and provided the first abstractive baselines .", "entities": [[15, 16, "TaskName", "summarization"], [20, 24, "DatasetName", "CNN / Daily Mail"]]}
{"text": "Prior to modern neural methods , abstractive summarization received less attention than extractive summarization , but Jing ( 2000 ) explored cutting unimportant parts of sentences to create summaries , and Cheung and Penn ( 2014 ) explore sentence fusion using dependency trees .", "entities": [[7, 8, "TaskName", "summarization"], [12, 14, "TaskName", "extractive summarization"], [38, 40, "TaskName", "sentence fusion"]]}
{"text": "The pointer network ( Vinyals et al , 2015 ) is a sequence - tosequence model that uses the soft attention distribution of Bahdanau et al ( 2015 ) to produce an output sequence consisting of elements from the input sequence .", "entities": [[1, 3, "MethodName", "pointer network"]]}
{"text": "The pointer network has been used to create hybrid approaches for NMT ( Gulcehre et al , 2016 ) , language modeling ( Merity et al , 2016 ) , and summarization ( Gu et al , 2016 ; Gulcehre et al , 2016 ; Miao and Blunsom , 2016 ;", "entities": [[1, 3, "MethodName", "pointer network"], [31, 32, "TaskName", "summarization"]]}
{"text": "Our approach is close to the Forced - Attention Sentence Compression model of Miao and Blunsom ( 2016 ) and the CopyNet model of Gu et al ( 2016 ) , with some small differences : ( i ) We calculate an explicit switch probability p gen , whereas Gu et al induce competition through a shared softmax function .", "entities": [[9, 11, "DatasetName", "Sentence Compression"], [57, 58, "MethodName", "softmax"]]}
{"text": "We believe the mixture approach described here is better for abstractive summarization - in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in - vocabulary words , and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying .", "entities": [[11, 12, "TaskName", "summarization"]]}
{"text": "Originating from Statistical Machine Translation ( Koehn , 2009 ) , coverage was adapted for NMT by Tu et al ( 2016 ) and , who both use a GRU to update the coverage vector each step .", "entities": [[3, 5, "TaskName", "Machine Translation"], [29, 30, "MethodName", "GRU"]]}
{"text": "In this respect our approach is similar to Xu et al ( 2015 ) , who apply a coverage - like method to image cap - tioning , and Chen et al ( 2016 ) , who also incorporate a coverage mechanism ( which they call ' distraction ' ) as described in equation ( 11 ) into neural summarization of longer text .", "entities": [[59, 60, "TaskName", "summarization"]]}
{"text": "Temporal attention is a related technique that has been applied to NMT ( Sankaran et al , 2016 ) and summarization .", "entities": [[0, 2, "MethodName", "Temporal attention"], [20, 21, "TaskName", "summarization"]]}
{"text": "We hypothesize that an early intervention method such as coverage is preferable to a post hoc method such as temporal attention - it is better to inform the attention mechanism to help it make better decisions , than to override its decisions altogether .", "entities": [[19, 21, "MethodName", "temporal attention"]]}
{"text": "This theory is supported by the large boost that coverage gives our ROUGE scores ( see Table 1 ) , compared to the smaller boost given by temporal attention for the same task .", "entities": [[27, 29, "MethodName", "temporal attention"]]}
{"text": "We use the CNN / Daily Mail dataset ( Hermann et al , 2015 ; , which contains online news articles ( 781 tokens on average ) paired with multi - sentence summaries ( 3.75 sentences or 56 tokens on average ) .", "entities": [[3, 7, "DatasetName", "CNN / Daily Mail"]]}
{"text": "Both the dataset 's published results ( Nallapati et al , , 2017 use the anonymized version of the data , which has been pre - processed to replace each named entity , e.g. , The United Nations , with its own unique identifier for the example pair , e.g. , @entity5 .", "entities": [[37, 38, "DatasetName", "Nations"]]}
{"text": "For all experiments , our model has 256dimensional hidden states and 128 - dimensional word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "For the pointer - generator models , we use a vocabulary of 50k words for both source and target - note that due to the pointer network 's ability to handle OOV words , we can use For the baseline model , we also try a larger vocabulary size of 150k .", "entities": [[25, 27, "MethodName", "pointer network"]]}
{"text": "Note that the pointer and the coverage mechanism introduce very few additional parameters to the network : for the models with vocabulary size 50k , the baseline model has 21 , 499 , 600 parameters , the pointer - generator adds 1153 extra parameters ( w h * , w s , w x and b ptr in equation 8 ) , and coverage adds 512 extra parameters ( w c in equation 11 ) .", "entities": [[56, 57, "DatasetName", "ptr"]]}
{"text": "Unlike , we do not pretrain the word embeddings - they are learned from scratch during training .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "( This was found to work best of Stochastic Gradient Descent , Adadelta , Momentum , Adam and RM - SProp ) .", "entities": [[8, 11, "MethodName", "Stochastic Gradient Descent"], [12, 13, "MethodName", "Adadelta"], [16, 17, "MethodName", "Adam"]]}
{"text": "We use gradient clipping with a maximum gradient norm of 2 , but do not use any form of regularization .", "entities": [[2, 4, "MethodName", "gradient clipping"]]}
{"text": "We use loss on the validation set to implement early stopping .", "entities": [[2, 3, "MetricName", "loss"], [9, 11, "MethodName", "early stopping"]]}
{"text": "We tried training the coverage model without the loss function , hoping that the attention mechanism may learn by itself not to attend repeatedly to the same locations , but we found this to be ineffective , with no discernible reduction in repetition .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "We find that both our baseline models perform poorly with respect to ROUGE and METEOR , and in fact the larger vocabulary size ( 150k ) does not seem to help .", "entities": [[14, 15, "DatasetName", "METEOR"]]}
{"text": "Further examples of all these problems are provided in the supplementary material .", "entities": [[10, 12, "DatasetName", "supplementary material"]]}
{"text": "Our pointer - generator model achieves much better ROUGE and METEOR scores than the baseline , despite many fewer training epochs .", "entities": [[10, 11, "DatasetName", "METEOR"]]}
{"text": "Our pointer - generator model with coverage improves the ROUGE and METEOR scores further , convincingly surpassing the best abstractive model Article : smugglers lure arab and african migrants by offering discounts to get onto overcrowded ships if people bring more potential passengers , a cnn investigation has revealed .", "entities": [[11, 12, "DatasetName", "METEOR"]]}
{"text": "For example , smugglers profit from desperate migrants is a valid alternative abstractive summary for the first example in Figure 5 , but it scores 0", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "To explore this issue further , we evaluated our systems with the METEOR metric , which rewards not only exact word matches , but also matching stems , synonyms and paraphrases ( from a predefined list ) .", "entities": [[12, 13, "DatasetName", "METEOR"]]}
{"text": "We observe that all our models receive over 1 METEOR point boost by the inclusion of stem , synonym and paraphrase matching , indicating that they may be performing some abstraction .", "entities": [[9, 10, "DatasetName", "METEOR"]]}
{"text": "Some of these abilities are demonstrated in Figure 1 , and the supplementary material contains more examples .", "entities": [[12, 14, "DatasetName", "supplementary material"]]}
{"text": "In any case , encouraging the pointer - generator model to write more abstractively , while retaining the accuracy advantages of the pointer module , is an exciting direction for future work .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "This work was begun while the first author was an intern at Google Brain and continued at Stanford .", "entities": [[12, 13, "DatasetName", "Google"]]}
{"text": "Stanford University gratefully acknowledges the support of the DARPA DEFT Program AFRL contract no .", "entities": [[8, 9, "DatasetName", "DARPA"]]}
{"text": "Re - evaluating Evaluation in Text Summarization", "entities": [[5, 7, "TaskName", "Text Summarization"]]}
{"text": "Automated evaluation metrics as a stand - in for manual evaluation are an essential part of the development of text - generation tasks such as text summarization .", "entities": [[25, 27, "TaskName", "text summarization"]]}
{"text": "However , while the field has progressed , our standard metrics have not - for nearly 20 years ROUGE has been the standard evaluation in most summarization papers .", "entities": [[26, 27, "TaskName", "summarization"]]}
{"text": "In this paper , we make an attempt to re - evaluate the evaluation method for text summarization : assessing the reliability of automatic metrics using top - scoring system outputs , both abstractive and extractive , on recently popular datasets for both systemlevel and summary - level evaluation settings .", "entities": [[16, 18, "TaskName", "text summarization"]]}
{"text": "We release a dataset of human judgments that are collected from 25 top - scoring neural summarization systems ( 14 abstractive and 11 extractive ) :", "entities": [[16, 17, "TaskName", "summarization"]]}
{"text": "In text summarization , manual evaluation , as exemplified by the Pyramid method ( Nenkova and Passonneau , 2004 ) , is the gold - standard in evaluation .", "entities": [[1, 3, "TaskName", "text summarization"]]}
{"text": "and it is not clear whether conclusions found there will hold with modern systems and summarization tasks .", "entities": [[15, 16, "TaskName", "summarization"]]}
{"text": "Constrained by few existing human judgment datasets , it remains unknown how existing metrics behave on current top - scoring summarization systems .", "entities": [[20, 21, "TaskName", "summarization"]]}
{"text": "In this paper , we ask the question : does the rapid progress of model development in summarization models require us to re - evaluate the evaluation process used for text summarization ?", "entities": [[17, 18, "TaskName", "summarization"], [30, 32, "TaskName", "text summarization"]]}
{"text": "To this end , we create and release a large benchmark for meta - evaluating summarization metrics including : Outputs from 25 top - scoring extractive and abstractive summarization systems on the CNN / DailyMail dataset .", "entities": [[15, 16, "TaskName", "summarization"], [28, 29, "TaskName", "summarization"]]}
{"text": "Manual evaluations using the lightweight pyramids method ( Shapira et al , 2019 ) , which we use as a gold - standard to evaluate summarization systems as well as automated metrics .", "entities": [[25, 26, "TaskName", "summarization"]]}
{"text": "Using this benchmark , we perform an extensive analysis , which indicates the need to re - examine our assumptions about the evaluation of automatic summarization systems .", "entities": [[25, 26, "TaskName", "summarization"]]}
{"text": "Calls for Future Research These observations demonstrate the limitations of our current bestperforming metrics , highlighting ( 1 ) the need for future meta - evaluation to ( i ) be across multiple datasets and ( ii ) evaluate metrics on different application scenarios , e.g. summary level vs. system level ( 2 ) the need for more systematic metaevaluation of summarization metrics that updates with our ever - evolving systems and datasets , and ( 3 ) the potential benefit to the summarization community of a shared task similar to the WMT 3 Metrics Task in Machine Translation , where systems and metrics co - evolve . 3 http://www.statmt.org/wmt20/", "entities": [[61, 62, "TaskName", "summarization"], [83, 84, "TaskName", "summarization"], [97, 99, "TaskName", "Machine Translation"]]}
{"text": "- 2008 , 2009 ( Dang and Owczarzak , 2008 , 2009 are multi - document , multi - reference summarization datasets .", "entities": [[20, 21, "TaskName", "summarization"]]}
{"text": "( Hermann et al , 2015 ; Nallapati et al , 2016 ) is a commonly used summarization dataset that contains news articles and associated highlights as summaries .", "entities": [[17, 18, "TaskName", "summarization"]]}
{"text": "Extractive summarization systems .", "entities": [[0, 2, "TaskName", "Extractive summarization"]]}
{"text": "We use CNN - LSTM - BiClassifier ( CLSTM - SL ; Kedzie et al ( 2018 ) ) , Latent ( Zhang et al , 2018 ) , Ban - ditSum ( Dong et al , 2018 ) , REFRESH ( Narayan et al , 2018 ) , NeuSum , HIBERT ( Zhang et al , 2019b ) , Bert - Sum - Ext ( Liu and Lapata , 2019a ) , CNN - Transformer - BiClassifier ( CTrans - SL ; Zhong et al ( 2019 ) ) , CNN - Transformer - Pointer ( CTrans - PN ; Zhong et al ( 2019 ) ) , HeterGraph ( Wang et al , 2020 ) and MatchSum ( Zhong et al , 2020 ) as representatives of extractive systems , totaling 11 extractive system outputs for each document in the CNNDM test set .", "entities": [[4, 5, "MethodName", "LSTM"], [75, 76, "MethodName", "Transformer"], [93, 94, "MethodName", "Transformer"]]}
{"text": "Abstractive summarization systems .", "entities": [[1, 2, "TaskName", "summarization"]]}
{"text": "We use pointer - generator+coverage ( See et al , 2017 ) , fastAbsRL ( Chen and Bansal , 2018 ) , fastAbsRLrank ( Chen and Bansal , 2018 ) , Bottom - up ( Gehrmann et al , 2018 ) , T5 ( Raffel et al , 2019 ) , Unilm - v1 ( Dong et al , 2019 ) , Unilm - v2 ( Dong et al , 2019 ) , twoStageRL ( Zhang et al , 2019a ) , pre - SummAbs ( Liu and Lapata , 2019b ) , preSummAbsext ( Liu and Lapata , 2019b )", "entities": [[42, 43, "MethodName", "T5"]]}
{"text": "BART ( Lewis et al , 2019 ) and Semsim ( Yoon et al , 2020 ) as abstractive systems .", "entities": [[0, 1, "MethodName", "BART"]]}
{"text": "BERTScore ( BScore ) measures soft overlap between contextual BERT embeddings of tokens between the two texts 4 ( Zhang et al , 2020 ) .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "MoverScore ( MScore ) applies a distance measure to contextualized BERT and ELMo word embeddings 5 ( Zhao et al , 2019 ) .", "entities": [[10, 11, "MethodName", "BERT"], [12, 13, "MethodName", "ELMo"], [13, 15, "TaskName", "word embeddings"]]}
{"text": "Sentence Mover Similarity ( SMS ) applies minimum distance matching between text based on sentence embeddings ( Clark et al , 2019 ) .", "entities": [[14, 16, "TaskName", "sentence embeddings"]]}
{"text": "Word Mover Similarity ( WMS ) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings 6 ( Kusner et al , 2015 ) .", "entities": [[21, 23, "TaskName", "word embeddings"]]}
{"text": "We follow a 3 - step process to collect human judgments : ( 1 ) we collect system - generated summaries on the most - commonly used summarization dataset , CNNDM ; ( 2 ) we select representative test samples from CNNDM and ( 3 ) we manually evaluate system - generated summaries of the aboveselected test samples .", "entities": [[27, 28, "TaskName", "summarization"]]}
{"text": "In text summarization , a \" good \" summary should represent as much relevant content from the input document as possible , within the acceptable length limits .", "entities": [[1, 3, "TaskName", "text summarization"]]}
{"text": "( b ) System Summary ( BART , Lewis et al ( 2019 ) ) :", "entities": [[6, 7, "MethodName", "BART"]]}
{"text": "Motivated by the central research question : \" does the rapid progress of model development in summarization models require us to re - evaluate the evaluation process used for text summarization ? \"", "entities": [[16, 17, "TaskName", "summarization"], [29, 31, "TaskName", "text summarization"]]}
{"text": "To answer this we observe the Pearson correlation between different metrics and human judgments in Fig .", "entities": [[6, 8, "MetricName", "Pearson correlation"]]}
{"text": "Instead of comparing many systems ( Sec . 4.1 , 4.2 ) ranking two systems aims to test the discriminative power of a metric , i.e. , the degree to which the metric can capture statistically significant differences between two summarization systems .", "entities": [[40, 41, "TaskName", "summarization"]]}
{"text": "Since we only have 100 annotated summaries to compare any two systems , sys 1 and sys 2 , we use paired bootstrap resampling , to test with statistical sig - nificance if sys 1 is better than sys 2 according to metric m ( Koehn , 2004 ; Dror et al , 2018 ) .", "entities": [[29, 30, "MethodName", "sig"]]}
{"text": "= 0 if the confidence is below 95 % .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "We calculate the weighted macro F1 score for all metrics and view them in Fig .", "entities": [[4, 6, "MetricName", "macro F1"]]}
{"text": "For example , top - scoring reinforcement learning based summarization systems ( B\u00f6hm et al , 2019 ) and the current state - of - the - art extractive system ( Zhong et al , 2020 ) heavily rely on summary - level reward scores to guide the optimization process .", "entities": [[9, 10, "TaskName", "summarization"]]}
{"text": "7b ( bins below y = 0 ) .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "This work is connected to the following threads of topics in text summarization .", "entities": [[11, 13, "TaskName", "text summarization"]]}
{"text": "Recently , Shapira et al ( 2019 ) proposed a lightweight and crowdsourceable version of the original Pyramid , and demonstrated it on the DUC 2005", "entities": [[24, 26, "DatasetName", "DUC 2005"]]}
{"text": "( Dang , 2005 ) and 2006 ( Dang , 2006 ) multi - document summarization datasets .", "entities": [[12, 16, "TaskName", "multi - document summarization"]]}
{"text": "In closing , we highlight some potential future directions : ( 1 ) The choice of metrics depends not only on different tasks ( e.g , summarization , translation ) but also on different datasets ( e.g. , TAC , CNNDM ) and application scenarios ( e.g , system - level , summary - level ) .", "entities": [[26, 27, "TaskName", "summarization"]]}
{"text": "( 3 ) Our collected human judgments can be used as supervision to instantiate the most recentlyproposed pretrain - then - finetune framework ( originally for machine translation ) ( Sellam et al , 2020 ) , learning a robust metric for text summarization .", "entities": [[26, 28, "TaskName", "machine translation"], [42, 44, "TaskName", "text summarization"]]}
{"text": "Building a De - identification System for Real Swedish Clinical Text Using Pseudonymised Clinical Text", "entities": [[2, 5, "TaskName", "De - identification"]]}
{"text": "Conditional Random Fields ( CFR ) and Long Short - Term Memory ( LSTM ) machine learning algorithms were used to train deidentification models .", "entities": [[7, 12, "MethodName", "Long Short - Term Memory"], [13, 14, "MethodName", "LSTM"]]}
{"text": "CRF showed better performance for some PHI information like Date Part , First Name and Last Name ; consistent with some reports in the literature .", "entities": [[0, 1, "MethodName", "CRF"]]}
{"text": "This study examines the use of pseudonymised health records as training data for de - identification tasks .", "entities": [[13, 16, "TaskName", "de - identification"]]}
{"text": "How will a de - identification system be constructed and used in a cross hospital setting without risking the privacy of patients ?", "entities": [[3, 6, "TaskName", "de - identification"]]}
{"text": "De - identification and pseudonymisation are two related concepts .", "entities": [[0, 3, "TaskName", "De - identification"]]}
{"text": "In this paper de - identification is used as a more general term to describe the process of finding personal health information to be able to conceal identifying information .", "entities": [[3, 6, "TaskName", "de - identification"]]}
{"text": "We highlight whether learning from the exist - ing , non - sensitive , pseudonymised Swedish clinical text can be useful in a new and different context ; considering the normal variations in the distribution and nature of PHI information , and potential effects of scrubbing ( Berman , 2003 ) , that is , removing and modifying PHIs that was carried out to patient records during the de - identification process .", "entities": [[68, 71, "TaskName", "de - identification"]]}
{"text": "The identification of PHI is a type of named entity recognition task where sensitive named entities specifically are identified .", "entities": [[8, 11, "TaskName", "named entity recognition"]]}
{"text": "The first study with CRF - based de - identification for Swedish was on the gold standard Stockholm EPR PHI Corpus .", "entities": [[4, 5, "MethodName", "CRF"], [7, 10, "TaskName", "de - identification"]]}
{"text": "In contrast to using only the sensitive EHR data for training , McMurry et al ( 2013 ) integrated both publicly available scientific , medical publications and private sensitive clinical notes to develop a de - identification system .", "entities": [[34, 37, "TaskName", "de - identification"]]}
{"text": "A study similar to Mc - Murry et al ( 2013 ) , by Berg and Dalianis ( 2019 ) , showed few benefits of combining non - medical public text and sensitive clinical notes to build a de - identification system for medical records .", "entities": [[38, 41, "TaskName", "de - identification"]]}
{"text": "For instance , the best system in a recent de - identification shared task was a combination of bidirectional LSTM , CRF and a rule - based subsystem ( Liu et al , 2017 ) .", "entities": [[9, 12, "TaskName", "de - identification"], [18, 20, "MethodName", "bidirectional LSTM"], [21, 22, "MethodName", "CRF"]]}
{"text": "Domain differences were cited as the reason for poor performance on psychiatric notes de - identification ( Stubbs et al , 2017 ) , compared with the previous de - identification task on general clinical narratives ( Stubbs et al , 2015 ) .", "entities": [[13, 16, "TaskName", "de - identification"], [28, 31, "TaskName", "de - identification"]]}
{"text": "In this study , machine learning approaches are used since the best de - identification systems appear to be machine learning - based ( Kushida et al , 2012 ) .", "entities": [[12, 15, "TaskName", "de - identification"]]}
{"text": "While rule - based methods such as using dictionaries and pattern - matching were previously more prevalent than machine learning methods for solving text - based de - identification problems ( Meystre et al , 2010 ) , today it is more typical to have both approaches used , since rule - based methods still yield better results for some PHI information ( Neamatullah et al , 2008b ) .", "entities": [[26, 29, "TaskName", "de - identification"]]}
{"text": "Two different data sets for de - identification were used : Stockholm EPR PHI Psuedo Corpus ( Pseudo ) as well as the Stockholm EPR PHI Cor -", "entities": [[5, 8, "TaskName", "de - identification"]]}
{"text": "Using the de - identified and pseudonymised data set , two models were trained based on two machine learning algorithms ; CRF and the deep learning algorithm LSTM .", "entities": [[21, 22, "MethodName", "CRF"], [27, 28, "MethodName", "LSTM"]]}
{"text": "In this study , the CRF algorithm implemented in CRFSuite ( Okazaki , 2007 ) is used with the sklearn - crfsuite wrapper 2 and the LSTM architecture described by Lample et al ( 2016 ) , based on an open - source implementation with Tensorflow 3 is used .", "entities": [[5, 6, "MethodName", "CRF"], [26, 27, "MethodName", "LSTM"]]}
{"text": "The CRF is based on trial - and - error experiments with feature sets described by Berg and Dalianis ( 2019 ) , and uses the same features except for section features .", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "The long short - term memory ( LSTM ) needs word embeddings as features for the training .", "entities": [[1, 6, "MethodName", "long short - term memory"], [7, 8, "MethodName", "LSTM"], [10, 12, "TaskName", "word embeddings"]]}
{"text": "Word2vec 5 was used to produce word embeddings using shallow neural networks , based on two corpora ; a clinical corpus and medical journals .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "For the training using real clinical data , word embeddings were produced using a clinical corpus of 200 million tokens that produced 300 , 824 vectors with a dimension of 300 .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "For the training with pseudo clinical data , word embeddings were produced using L\u00e4kartidningen corpus ( The Swedish scientific medical journals from 1996 to 2005 ) containing 21 million tokens that produced 118 , 662 vectors with a dimension of 300 .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "As can be observed in the figure , the CRF algorithm seems to generally outperform the LSTM algorithm on all metrics ; precision , recall and F1 measure .", "entities": [[9, 10, "MethodName", "CRF"], [16, 17, "MethodName", "LSTM"], [26, 27, "MetricName", "F1"]]}
{"text": "This result is not consistent with repeated reports in the literature , where deep learning apsklearn - crfsuite.readthedocs.io/en/ latest/ 5 word2vec , https://github.com/tmikolov/ word2vec proaches such as LSTM have been shown to out - perform most other methods , including CRF .", "entities": [[27, 28, "MethodName", "LSTM"], [40, 41, "MethodName", "CRF"]]}
{"text": "Since deep learning approaches normally require very large amounts of data , one explanation for this result could be that the word embeddings used in this study did not contain sufficient context variations required for more robust performance or an insufficient training set of annotated data .", "entities": [[21, 23, "TaskName", "word embeddings"]]}
{"text": "The ability to identify date part and age entities are similar when training on pseudonymised data and real data for the CRF .", "entities": [[21, 22, "MethodName", "CRF"]]}
{"text": "In contrast , Location , Health Care Unit and Full Date were negatively affected when using pseudonymised training data regardless of using a CRF or LSTM model .", "entities": [[23, 24, "MethodName", "CRF"], [25, 26, "MethodName", "LSTM"]]}
{"text": "Experimental results of the CRF algorithm are shown in Table 3 .", "entities": [[4, 5, "MethodName", "CRF"]]}
{"text": "The experimental results of the LSTM algorithm are shown in Table 4 and again , not presented in the table is the combination of training on real data and evaluation of pseudo data ( Real - Pseudo ) .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "For example , using the CRF algorithm on real - data training and pseudo - data testing ( Real - Pseudo ) , of the 159 instances not identified as full dates tokens , sixty contain ' - ' .", "entities": [[5, 6, "MethodName", "CRF"]]}
{"text": "There is one similar study to ours but for English by Yeniterzi et al ( 2010 ) , where the authors train their de - identification system with all combinations of pseudonymised textual data ( or what they call resynthesized records ) and real data and their results are in line with ours .", "entities": [[23, 26, "TaskName", "de - identification"]]}
{"text": "However , there are some studies on cross - domain adaptation .", "entities": [[9, 11, "TaskName", "domain adaptation"]]}
{"text": "What was a bit unexpected was the lower performance of the LSTM algorithm .", "entities": [[11, 12, "MethodName", "LSTM"]]}
{"text": "Additional sources using nonsensitive data , such as public corpora in the general domain , hold a potential to improve performance on the de - identification task , therefore this line of inquiry will be followed up on in future work .", "entities": [[23, 26, "TaskName", "de - identification"]]}
{"text": "As mentioned in the analysis the CRF algorithm rarely classifies the ' - ' in between dates as a part of the dates , and these are therefore not counted as matches despite the most identifying parts of the entity being identified .", "entities": [[6, 7, "MethodName", "CRF"]]}
{"text": "To improve the general performance , a combination of both the LSTM and CRF algorithms could be performed instead of testing them independently .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "CRF"]]}
{"text": "This problem , what we call The cross pseudo - real text adaptation problem , is an issue that could happen due to the pseudonymisation / de - identification processes on the training data due to the narrative and distributional variation as well as other differences in the nature of the PHI between the training data and the target .", "entities": [[26, 29, "TaskName", "de - identification"]]}
{"text": "IIITT at CASE 2021 Task 1 : Leveraging Pretrained Language Models for Multilingual Protest Detection", "entities": [[8, 11, "TaskName", "Pretrained Language Models"]]}
{"text": "This paper demonstrates our work on the sentence classification subtask of multilingual protest detection in CASE@ACL - IJCNLP 2021 .", "entities": [[7, 9, "TaskName", "sentence classification"]]}
{"text": "Next , section 4 gives a detailed description of the models used for the multilingual event detection .", "entities": [[15, 17, "TaskName", "event detection"]]}
{"text": "The models that were used are BERT ( Devlin et al , 2019 ) , RoBERTa ( Liu et al , 2019 ) and DistilBERT ( Sanh et al , 2019 ) .", "entities": [[6, 7, "MethodName", "BERT"], [15, 16, "MethodName", "RoBERTa"], [24, 25, "MethodName", "DistilBERT"]]}
{"text": "Bidirectional Encoder Representations from Transformers ( BERT ) ( Devlin et al , 2019 ) is a pretrained language model which was created with the objective that fine - tuning a pretrained model yields better performance .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "BERT 's pretraining phase includes two tasks .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "Firstly , Masked Language Modeling ( MLM ) is where certain words are randomly masked in a sequence .", "entities": [[2, 5, "TaskName", "Masked Language Modeling"], [6, 7, "DatasetName", "MLM"]]}
{"text": "Secondly , Next Sentence Prediction ( NSP ) , where the model has an additional loss function , NSP loss , indicates if the second sequence follows the first one .", "entities": [[15, 16, "MetricName", "loss"], [19, 20, "MetricName", "loss"]]}
{"text": "DistilBERT ( Sanh et al , 2019 ) is the distilled version of BERT .", "entities": [[0, 1, "MethodName", "DistilBERT"], [13, 14, "MethodName", "BERT"]]}
{"text": "DistilBERT employs a triple loss language modelling , where it integrates cosine distance loss with knowledge distillation .", "entities": [[0, 1, "MethodName", "DistilBERT"], [4, 5, "MetricName", "loss"], [5, 7, "TaskName", "language modelling"], [13, 14, "MetricName", "loss"], [15, 17, "MethodName", "knowledge distillation"]]}
{"text": "DistilBERT has 40 % fewer parameters than BERT but still promises 97 % of the latter 's performance .", "entities": [[0, 1, "MethodName", "DistilBERT"], [7, 8, "MethodName", "BERT"]]}
{"text": "It is also 60 % faster than BERT .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "In this system , we used a cased multilingual DistilBERT model as they are three different languages .", "entities": [[9, 10, "MethodName", "DistilBERT"]]}
{"text": "For our cause , we finetune distilbert - base - multilingual - cased , which is distilled from the mBERT checkpoint .", "entities": [[6, 7, "MethodName", "distilbert"], [19, 20, "MethodName", "mBERT"]]}
{"text": "Robustly Optimized BERT ( RoBERTa ) ( Liu et al , 2019 ) follows the same architecture of BERT while differing in the pretraining strategy .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "RoBERTa"], [18, 19, "MethodName", "BERT"]]}
{"text": "It is pretrained with MLM as its objective where the model tries to predict the masked words .", "entities": [[4, 5, "DatasetName", "MLM"]]}
{"text": "RoBERTa model is trained on the vast English Wikipedia and CC - News datasets .", "entities": [[0, 1, "MethodName", "RoBERTa"], [10, 13, "DatasetName", "CC - News"]]}
{"text": "The NSP is not employed as a pretraining strategy , and the tokens are dynamically masked , making the model slightly different to BERT .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "During tokenization , RoBERTa follows byte - pair encoding ( BPE ) ( Gall\u00e9 , 2019 ) as opposed to WordPiece employed in BERT .", "entities": [[3, 4, "MethodName", "RoBERTa"], [10, 11, "MethodName", "BPE"], [20, 21, "MethodName", "WordPiece"], [23, 24, "MethodName", "BERT"]]}
{"text": "We combine the three datasets as the number of samples for Spanish and Portuguese are quite low .", "entities": [[7, 10, "HyperparameterName", "number of samples"]]}
{"text": "The embeddings are extracted from these models to be fed as input to the LSTM layer , ( Hochreiter and Schmidhuber , 1997 ) as shown in Figure1 .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "The resulting output is fed into a global average pooling layer ( Lin et al , 2014 ) and then passed into fully connected layers , followed by a sigmoid activation function to obtain the resulting probability score for the input sentences .", "entities": [[7, 10, "MethodName", "global average pooling"], [29, 31, "MethodName", "sigmoid activation"]]}
{"text": "Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives .", "entities": [[3, 4, "DatasetName", "emotion"]]}
{"text": "However , progress on emotion detection has been hampered by the absence of large labeled datasets .", "entities": [[4, 5, "DatasetName", "emotion"]]}
{"text": "According to the Oxford English Dictionary , emotion is defined as \" [ a ] strong feeling deriving from one 's circumstances , mood , or relationships with others . \"", "entities": [[7, 8, "DatasetName", "emotion"]]}
{"text": "It is no exaggeration that humans are emotional beings : Emotions are an integral part of human life , and affect our decision making as well as our mental and physical health .", "entities": [[22, 24, "TaskName", "decision making"]]}
{"text": "As such , developing emotion detection models is important ; they have a wide array of applications , ranging from building nuanced virtual assistants that cater for the emotions of their users to detecting the emotions of social media users in order to understand their mental and/or physical health .", "entities": [[4, 5, "DatasetName", "emotion"]]}
{"text": "However , emotion detection has remained a challenging task , partly due to the limited availability of labeled data and partly due the controversial nature of what emotions themselves are ( Aaron C. Weidman and Tracy , 2017 ) .", "entities": [[2, 3, "DatasetName", "emotion"]]}
{"text": "Recent advances in machine learning for natural language processing ( NLP ) suggest that , given enough labeled data , there should be an opportunity to build better emotion detection models .", "entities": [[28, 29, "DatasetName", "emotion"]]}
{"text": "Manual labeling of data , however , is costly and so it is desirable to develop labeled emotion data without annotators .", "entities": [[17, 18, "DatasetName", "emotion"]]}
{"text": "More specifically , we harness cues in Twitter data in the form of emotion hashtags as a way to build a labeled emotion dataset that we then exploit using distant supervision ( Mintz et al , 2009 ) ( the use of hashtags as a surrogate for annotator - generated emotion labels ) to build emotion models grounded in psychology .", "entities": [[13, 14, "DatasetName", "emotion"], [22, 23, "DatasetName", "emotion"], [50, 51, "DatasetName", "emotion"], [55, 56, "DatasetName", "emotion"]]}
{"text": "We construct such a dataset and exploit it using powerful deep learning methods to build accurate , high coverage models for emotion prediction .", "entities": [[21, 22, "DatasetName", "emotion"]]}
{"text": "Key to this are methods to ensure data quality , 2 ) we validate the data collection method using human annotations , 3 ) we develop powerful deep learning models using a gated recurrent network to exploit the data , yielding new state - of - the - art on 24 fine - grained types of emotions , and 4 ) we extend the task beyond these emotion types to model Plutick 's 8 primary emotion dimensions .", "entities": [[67, 68, "DatasetName", "emotion"], [75, 76, "DatasetName", "emotion"]]}
{"text": "Our emotion modeling relies on distant supervision ( Read , 2005 ; Mintz et al , 2009 ) , the approach of using cues in data ( e.g. , hashtags or emoticons ) as a proxy for \" ground truth \" labels as we explained above .", "entities": [[1, 2, "DatasetName", "emotion"]]}
{"text": "Distant supervision has been investigated by a number of researchers for emotion detection ( Tanaka et al , 2005 ; Mohammad , 2012 ; Purver and Battersby , 2012 ; Wang et al , 2012 ; Pak and Paroubek , 2010 ;", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "Yang et al , 2007 ) and for other semantic tasks such as sentiment analysis ( Read , 2005 ; Go et al , 2009 ) and sarcasm detection ( Gonz\u00e1lez - Ib\u00e1nez et al , 2011 ) .", "entities": [[13, 15, "TaskName", "sentiment analysis"], [27, 29, "TaskName", "sarcasm detection"]]}
{"text": "We take a similar approach , using a larger collection of tweets , richer emotion definitions , and stronger filtering for tweet quality .", "entities": [[14, 15, "DatasetName", "emotion"]]}
{"text": "The SemEval - 2007 Affective Text task ( Strapparava and Mihalcea , 2007 )", "entities": [[4, 6, "DatasetName", "Affective Text"]]}
{"text": "[ SEM07 ] focused on classification of emotion and valence ( i.e. , positive and negative texts ) in news headlines .", "entities": [[7, 8, "DatasetName", "emotion"]]}
{"text": "Similarly , ( Aman and Szpakowicz , 2007 ) describe an emotion annotation task of identifying emotion category , emotion intensity and the words / phrases that indicate emotion in blog post data of 4 , 090 sentences and a system exploiting the data .", "entities": [[11, 12, "DatasetName", "emotion"], [16, 17, "DatasetName", "emotion"], [19, 20, "DatasetName", "emotion"], [28, 29, "DatasetName", "emotion"]]}
{"text": "Our work is similar to ( Mohammad , 2012 ; Mohammad and Kiritchenko , 2015 ) , ( Wang et al , 2012 ) , and ( Volkova and Bachrach , 2016 ) who use distant supervision to acquire Twitter data with emotion hashtags and report analyses and experiments to validate the utility of this approach .", "entities": [[42, 43, "DatasetName", "emotion"]]}
{"text": "For example , ( Mohammad , 2012 ) shows that by using a simple domain adaptation method to train a classifier on their data they are able to improve both precision and recall on the SemEval - 2007 ( Strapparava andMihalcea , 2007 ) dataset .", "entities": [[14, 16, "TaskName", "domain adaptation"]]}
{"text": "As the author points out , this is another premise that the selflabeled hashtags acquired from Twitter are consistent , to some degree , with the emotion labels given by the trained human judges who labeled the SemEval - 2007 data .", "entities": [[26, 27, "DatasetName", "emotion"]]}
{"text": "These include that since they are provided by the tweets ' writers , the emotion hashtags are more natural and reliable than the emotion labels traditionally assigned by annotators to data by a few annotators .", "entities": [[14, 15, "DatasetName", "emotion"], [23, 24, "DatasetName", "emotion"]]}
{"text": "Additionally , ( Volkova and Bachrach , 2016 ) follow the same distant supervision approach and find correlations of users ' emotional tone and the perceived demographics of these users ' social networks exploiting the emotion hashtag - labeled data .", "entities": [[35, 36, "DatasetName", "emotion"]]}
{"text": "Our dataset is more than an order of magnitude larger than ( Mohammad , 2012 ) and ( Volkova and Bachrach , 2016 ) and the range of emotions we target is much more fine grained than ( Mohammad , 2012 ; Wang et al , 2012 ; Volkova and Bachrach , 2016 ) since we model 24 emotion types , rather than focus on \u2264 7 basic emotions .", "entities": [[58, 59, "DatasetName", "emotion"]]}
{"text": "( Yan et al , 2016 ; Yan and Turtle , 2016a , b ) develop a dataset of 15 , 553 tweets labeled with 28 emotion types and so target a fine - grained range as we do .", "entities": [[1, 4, "DatasetName", "Yan et al"], [26, 27, "DatasetName", "emotion"]]}
{"text": "The authors instruct human annotators under lab conditions to assign any emotion they feel is expressed in the data , allowing them to assign more than one emotion to a given tweet .", "entities": [[11, 12, "DatasetName", "emotion"], [27, 28, "DatasetName", "emotion"]]}
{"text": "However , the agreement between annotators is not high and the set of assigned labels do not adhere to a specific theory of emotion .", "entities": [[23, 24, "DatasetName", "emotion"]]}
{"text": "Our work is different in that it is situated in psychological theory of emotion .", "entities": [[13, 14, "DatasetName", "emotion"]]}
{"text": "In spite of the effectiveness of feature engineering for NLP , it is a labor intensive task that also needs domain expertise .", "entities": [[6, 8, "TaskName", "feature engineering"]]}
{"text": "More importantly , feature engineering falls short of extracting and organizing all the discriminative information from data Goodfellow et", "entities": [[3, 5, "TaskName", "feature engineering"]]}
{"text": "al , 2011 ; Tang et al , 2014b , a ) aim to learn sentiment - specific word embeddings ( Bengio et al , 2003 ; from neighboring text .", "entities": [[18, 20, "TaskName", "word embeddings"]]}
{"text": "Another thread of research focuses on learning semantic composition ( Mitchell and Lapata , 2010 ) , including extensions to phrases and sentences with recursive neural networks ( a class of syntax - tree models )", "entities": [[7, 9, "TaskName", "semantic composition"]]}
{"text": "Long - short term memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) and Gated Recurrent Neural Nets ( GRNNs )", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "( Cho et al , 2014 ; Chung et al , 2015 ) , variations of recurrent neural networks ( RNNs ) , a type of networks suitable for handling time - series data like speech ( Graves et al , 2013 ) or handwriting recognition ( Graves , 2012 ; Graves and Schmidhuber , 2009 ) , have also been used successfully for sentiment analysis ( Ren et al , 2016 ;", "entities": [[44, 46, "TaskName", "handwriting recognition"], [64, 66, "TaskName", "sentiment analysis"]]}
{"text": "Convolutional neural networks ( CNNs ) have also been quite successful in NLP , and have been applied to a range of sentence classification tasks , including sentiment analysis ( Blunsom et al , 2014 ; Kim , 2014 ; Zhang et al , 2015 ) .", "entities": [[22, 24, "TaskName", "sentence classification"], [27, 29, "TaskName", "sentiment analysis"]]}
{"text": "To be able to use deep learning for modeling emotion , we needed a large dataset of labeled tweets .", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "Gonz\u00e1lez - Ib\u00e1nez et al , 2011 ; Wang et al , 2012 ) in adopting distant supervision : We collect tweets with emotion - carrying hashtags as a surrogate for emotion labels .", "entities": [[23, 24, "DatasetName", "emotion"], [31, 32, "DatasetName", "emotion"]]}
{"text": "The cone 's vertical dimension represents intensity , and the 3 circle represent degrees of similarity among the various emotion types .", "entities": [[19, 20, "DatasetName", "emotion"]]}
{"text": "The eight sectors are meant to capture that there are eight primary emotion dimensions arranged as four pairs of opposites .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "Emotions in the blank spaces are the primary emotion dyads ( i.e. , emotions that are mixtures of two of the primary emotions ) .", "entities": [[8, 9, "DatasetName", "emotion"]]}
{"text": "For each emotion type , we prepared a seed set of hashtags representing the emotion .", "entities": [[2, 3, "DatasetName", "emotion"], [14, 15, "DatasetName", "emotion"]]}
{"text": "We used Google synonyms and other online dictionaries and thesauri ( e.g. , www.thesaurus . com ) to expand the initial seed set of each emotion .", "entities": [[2, 3, "DatasetName", "Google"], [25, 26, "DatasetName", "emotion"]]}
{"text": "We acquire a total of 665 emotion hashtags across the 24 emotion types .", "entities": [[6, 7, "DatasetName", "emotion"], [11, 12, "DatasetName", "emotion"]]}
{"text": "For example , for the joy emotion , a subset of the seeds in our expanded set is { \" happy \" , \" happiness \" , \" joy \" , \" joyful \" , \" joyfully \" , \" delighted \" , \" feelingsunny \" , \" blithe \" , \" beatific \" , \" exhilarated \" , \" blissful \" , \" walkingonair \" , \" jubilant \" } .", "entities": [[6, 7, "DatasetName", "emotion"], [12, 13, "DatasetName", "seeds"]]}
{"text": "Twitter data are very noisy , not only because of use of non - standard typography ( which is less of a problem here ) but due to the many duplicate tweets and the fact that tweets often have multiple emotion hashtags .", "entities": [[40, 41, "DatasetName", "emotion"]]}
{"text": "Since our goal is to create non - overlapping categories at the level of a tweet , we first removed all tweets with hashtags belonging to more than one emotion of the 24 emotion categories .", "entities": [[29, 30, "DatasetName", "emotion"], [33, 34, "DatasetName", "emotion"]]}
{"text": "Since it was observed ( e.g. , ( Mohammad , 2012 ; Wang et al , 2012 ) ) and also confirmed by our annotation study as described in Section 4 , that hashtags in tweets with URLs are less likely to correlate with a true emotion label , we remove all tweets with URLs from our data .", "entities": [[46, 47, "DatasetName", "emotion"]]}
{"text": "Next , even though the emotion hashtags themselves are exclusively in English , we observe the data do have tweets in languages other than English .", "entities": [[5, 6, "DatasetName", "emotion"]]}
{"text": "Since the common wisdom in the literature ( e.g. , ( Mohammad , 2012 ; Wang et al , 2012 ) ) is to restrict data to hashtags occurring in final position of a tweet , we investigate correlations between a tweet 's relevance and emotion hashtag location in Section 4 and test models exclusively on data with hashtags occurring in final position .", "entities": [[45, 46, "DatasetName", "emotion"]]}
{"text": "Table 2 shows statistics of the data after applying our cleaning , filtering , language identification , and deduplication pipeline .", "entities": [[14, 16, "TaskName", "language identification"]]}
{"text": "Table 2 provides three types of relevant statistics : 1 ) counts of all tweets , 2 ) counts of tweets with at least 5 words and the emotion hashtags occurring in the last quarter of the tweet text ( based on character count ) , and 3 ) counts of tweets with at least 5 words and the emotion hashtags occurring as the final word in the tweet text .", "entities": [[28, 29, "DatasetName", "emotion"], [59, 60, "DatasetName", "emotion"]]}
{"text": "As the last column in Table 2 shows , employing our most strict criterion where an emotion hashtag must occur finally in a tweet of a minimal length 5 words , we acquire a total of 1 , 608 , 233 tweets : 205 , 125 tweets for plutchik - 1 , 790 , 059 for plutchik - 2 , and 613 , 049 for plutchik - 3 .", "entities": [[16, 17, "DatasetName", "emotion"]]}
{"text": "In their work , ( Wang et al , 2012 ) manually label a random sample of 400 tweets extracted with hash - tags in a similar way as we acquire our data and find that human annotators agree 93 % of the time with the hashtag emotion type if the hashtag occurs as the last word in the tweet .", "entities": [[47, 48, "DatasetName", "emotion"]]}
{"text": "For each of the remaining 5 , 584 tweets , the annotators assign a binary tag from the set { relevant , irrelevant } to indicate whether a tweet carries an emotion category as assigned using our distant supervision method or not .", "entities": [[31, 32, "DatasetName", "emotion"]]}
{"text": "When we limit position of the emotion hashtag to the end of a tweet , we acquire 90.57 % relevant data .", "entities": [[6, 7, "DatasetName", "emotion"]]}
{"text": "We also find that only 23.20 % ( n = 795 out of 3 , 427 ) of the emotion carrying tweets have the emotion hashtags occurring in final position , whereas 31.75 % ( n = 1 , 088 out of 3 , 427 ) of the tweets have the emotion hashtags in the last quarter of the tweet string .", "entities": [[19, 20, "DatasetName", "emotion"], [24, 25, "DatasetName", "emotion"], [51, 52, "DatasetName", "emotion"]]}
{"text": "This shows how enforcing a final hashtag location results in loss of a considerable number of emotion tweets .", "entities": [[10, 11, "MetricName", "loss"], [16, 17, "DatasetName", "emotion"]]}
{"text": "As shown in Table 2 , only 1 , 608 , 233 tweets out of a total of 6 , 851 , 955 tweets ( % = 23 , 47 ) in our bigger dataset have emotion hashtags occurring in final position .", "entities": [[36, 37, "DatasetName", "emotion"]]}
{"text": "One advantage of using distant supervision under these conditions for labeling emotion data , as ( Wang et al , 2012 ) also notes , is that the label is assigned by the writer of the tweet himself / herself rather than an annotator who could wrongly decide what category a tweet is .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "After all , emotion is a fuzzy concept and > 90 % agreement as we report here is higher than the human agreement usually acquired on many NLP tasks .", "entities": [[3, 4, "DatasetName", "emotion"]]}
{"text": "LSTM", "entities": [[0, 1, "MethodName", "LSTM"]]}
{"text": "Long short - term memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) addresses this exact problem of learning long - term dependencies by augmenting an RNN with a memory cell c t IR n at each time step .", "entities": [[0, 5, "MethodName", "Long short - term memory"], [6, 7, "MethodName", "LSTM"]]}
{"text": "As such , in addition to the input vector x t , the hiddent vector h t\u22121 , an LSTM takes a cell state vector c t\u22121 and produces h t and c t via the following calculations :", "entities": [[19, 20, "MethodName", "LSTM"]]}
{"text": "The LSTM parameters W j , U j , and b j are for j { i , f , o , g } .", "entities": [[1, 2, "MethodName", "LSTM"]]}
{"text": "GRNNs ( Cho et al , 2014 ; Chung et al , 2015 ) propose a variation of LSTM with a reset gate r t , an update state z t , and a new simpler hidden unit h t , as follows : r t = \u03c3 ( W r x t", "entities": [[18, 19, "MethodName", "LSTM"]]}
{"text": "In this set up , the hidden state is forced to ignore a previous hidden state when the reset gate is close to 0 , thus enabling the network to forget or drop irrelevant information .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "Additionally , the update gate controls how much information carries over from a previous hidden state to the current hidden state ( similar to an LSTM memory cell ) .", "entities": [[25, 26, "MethodName", "LSTM"]]}
{"text": "We use GRNNs as they are simpler and faster than LSTM .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "Online Classifiers We compare the performance of the GRNNs to four online classifiers that are capable of handling the data size : Stochastic Gradient Descent ( SGD ) , Multinomial Naive Bayes ( MNB ) , Perceptron , and the Passive Agressive Classifier ( PAC ) .", "entities": [[22, 25, "MethodName", "Stochastic Gradient Descent"], [26, 27, "MethodName", "SGD"]]}
{"text": "Settings We aim to model Plutchik 's 24 finegrained emotions as well as his 8 primary emotion dimensions where each 3 related types of emotion ( perceived as varying in intensity ) are combined in one dimension .", "entities": [[16, 17, "DatasetName", "emotion"], [24, 25, "DatasetName", "emotion"]]}
{"text": "As explained earlier , Plutchik organizes the 24 emotion types in the 3 main circles that we will refer to as plutchik - 1 , plutchik - 2 , and plutchik - 3 .", "entities": [[8, 9, "DatasetName", "emotion"]]}
{"text": "Inspired by observations from the literature and our own annotation study , we limit our data to tweets of at least 5 words with an emotional hashtag occurring at the end .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "For our loss function , we use categorical cross - entropy .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "Negative Results We experiment with aug - menting training data reported here in two ways : 1 ) For each emotion type , we concatenate the training data with training data of tweets that are more ( or less ) intense from the same sector / dimension in the wheel , and 2 ) for each emotion type , we add tweets where emotion hashtags occur in the last quarter of a tweet ( which were originally filtered out from TRAIN ) .", "entities": [[20, 21, "DatasetName", "emotion"], [56, 57, "DatasetName", "emotion"], [63, 64, "DatasetName", "emotion"]]}
{"text": "We now investigate the task of predicting each of the 8 primary emotion dimensions represented by the sectors of the wheel ( where the three degrees of intensity of a given emotion are reduced to a single emotion dimension [ e.g. , { ecstasy , joy , serenity } are reduced to the joy dimension ] ( Volkova and Bachrach , 2016 ) 's model .", "entities": [[12, 13, "DatasetName", "emotion"], [31, 32, "DatasetName", "emotion"], [37, 38, "DatasetName", "emotion"]]}
{"text": "In this paper , we built a large , automatically curated dataset for emotion detection using distant supervision and then used GRNNs to model finegrained emotion , achieving a new state - of - the - art performance .", "entities": [[13, 14, "DatasetName", "emotion"], [25, 26, "DatasetName", "emotion"]]}
{"text": "We also extended the classification to 8 primary emotion dimensions situated in psychological theory of emotion .", "entities": [[8, 9, "DatasetName", "emotion"], [15, 16, "DatasetName", "emotion"]]}
{"text": "Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis .", "entities": [[17, 18, "DatasetName", "emotion"]]}
{"text": "In order to break this bottleneck , we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language .", "entities": [[17, 18, "DatasetName", "emotion"]]}
{"text": "Our approach requires nothing but a source language emotion lexicon , a bilingual word translation model , and a target language embedding model .", "entities": [[8, 9, "DatasetName", "emotion"], [13, 15, "TaskName", "word translation"]]}
{"text": "An emotion lexicon is a lexical repository which encodes the affective meaning of individual words ( lexical entries ) .", "entities": [[1, 2, "DatasetName", "emotion"]]}
{"text": "NLP researchers , on the other hand , use them to augment the emotional loading of word embeddings ( Yu et al , 2017 ; Khosla et al , 2018 ) , as additional input to sentence - level emotion models so that the performance of even the most sophisticated neural network gets boosted ( Mohammad and Bravo - Marquez , 2017 ; De Bruyne et al , 2019 ) , or rely on them in a keyword - spotting approach when no training data is available , e.g. , for studies dealing with historical language stages .", "entities": [[16, 18, "TaskName", "word embeddings"], [39, 40, "DatasetName", "emotion"]]}
{"text": "As with any kind of manually curated resource , the availability of emotion lexicons is heavily restricted to only a few languages whose exact number varies depending on the variables under scrutiny .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "Yet , even the largest lexicons typically cover only some ten thousands of words , still leaving out major portions of the emotion - carrying vocabulary .", "entities": [[22, 23, "DatasetName", "emotion"]]}
{"text": "Finally , the diversity of emotion representation schemes adds another layer of complexity .", "entities": [[5, 6, "DatasetName", "emotion"]]}
{"text": "While psychologists and NLP researchers alike find that different sets of emotional variables are complementary to each other ( Stevenson et al , 2007 ; Pinheiro et al , 2017 ; Barnes et al , 2019 ; De Bruyne et al , 2019 ) , manually creating emotion lexicons for every language and every emotion representation scheme is virtually impossible .", "entities": [[47, 48, "DatasetName", "emotion"], [54, 55, "DatasetName", "emotion"]]}
{"text": "We here propose an approach based on crosslingual distant supervision to generate almost arbitrarily large emotion lexicons for any target language and emotional variable , provided the following requirements are met : a source language emotion lexicon covering the desired variables , a bilingual word translation model , and a target language embedding model .", "entities": [[15, 16, "DatasetName", "emotion"], [35, 36, "DatasetName", "emotion"], [44, 46, "TaskName", "word translation"]]}
{"text": "By fulfilling these preconditions , we can automatically generate emotion lexicons for 91 languages covering ratings for eight emotional variables and hundreds of thousands of lexical entries each .", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "Borrowing from distinct schools of thought in psychology , these variables can typically be subdivided into dimensional vs. discrete approaches to emotion representation ( Calvo and Mac Kim , 2013 ) .", "entities": [[21, 22, "DatasetName", "emotion"]]}
{"text": "In the first case , emotion analysis translates into a ( multi - class ) classification problem , whereas the latter turns it into a regression problem .", "entities": [[5, 6, "DatasetName", "emotion"]]}
{"text": "While our proposed methodology is agnostic towards the chosen emotion format , we will focus on the VAD and BE5 formats here , using numerical ratings ( see the examples in Table 1 ) due to the widespread availability of such data .", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "Accordingly , this paper treats word emotion prediction as a regression problem .", "entities": [[6, 7, "DatasetName", "emotion"]]}
{"text": "Usually , the ground truth for affective word ratings ( i.e. , the assignment of emotional values to a lexical item ) is acquired in a questionnaire study design where subjects ( annotators ) receive lists of words which they rate according to different emotion variables or categories .", "entities": [[44, 45, "DatasetName", "emotion"]]}
{"text": "Aggregating individual ratings of multiple annotators then results in the final emotion lexicon ( Bradley and Lang , 1999 ) .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "As a viable alternative to manual acquisition , such lexicons can also be created by automatic means ( Bestgen , 2008 ; K\u00f6per and Schulte i m Walde , 2016 ; Shaikh et al , 2016 ) , i.e. , by learning to predict emotion labels for unseen words .", "entities": [[44, 45, "DatasetName", "emotion"]]}
{"text": "More recent approaches focus heavily on word embeddings , either using semi - supervised graph - based approaches Hamilton et al , 2016 ;", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Most important for this work , Buechel and Hahn ( 2018b ) report on near - human performance using a combination of FASTTEXT vectors and a multi - task feed - forward network ( see Section 4 ) .", "entities": [[22, 23, "MethodName", "FASTTEXT"]]}
{"text": "A relatively new way of generating novel labels is emotion representation mapping ( ERM ) , an annotation projection that translates ratings from one emotion format into another , e.g. , mapping VAD labels into BE5 , or vice versa ( Hoffmann et al , 2012 ;", "entities": [[9, 10, "DatasetName", "emotion"], [24, 25, "DatasetName", "emotion"]]}
{"text": "While our work uses ERM to add additional emotion variables to the source lexicon , ERM alone can neither increase the coverage of a lexicon , nor adapt it to another language .", "entities": [[8, 9, "DatasetName", "emotion"]]}
{"text": "The approach we propose is strongly tied to the observation by Leveau et al ( 2012 ) and Warriner et al ( 2013 ) who found - comparing a large number of existing emotion lexicons of different languages - that translational equivalents of words show strong stability and adherence to their emotional value .", "entities": [[33, 34, "DatasetName", "emotion"]]}
{"text": "Also , many approaches in cross - lingual sentiment analysis ( on the sentence - level ) rely on translating polarity lexicons ( Abdalla and Hirst , 2017 ; Barnes et al , 2018 ) .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}
{"text": "Our methodology also resembles previous work which models word emotion for historical language stages ( Cook and Stevenson , 2010 ;", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "Work in this direction typically comes up with a set of seed words with assumingly temporally stable affective meaning ( our work assumes stability against translation ) and then uses distributional methods to derive emotion ratings in the target language stage .", "entities": [[34, 35, "DatasetName", "emotion"]]}
{"text": "Our methodology integrates ( 1 ) cross - lingual generation and expansion of emotion lexicons and ( 2 ) their evaluation against gold and silver standard data .", "entities": [[13, 14, "DatasetName", "emotion"]]}
{"text": "We start with a lexicon ( Source ) of arbitrary size , emotion format 1 and source language which is partitioned into train , dev , and test splits denoted by Source - train , Source - dev , and Source - test , respectively .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "Next , we leverage a bilingual word translation model between source and desired target language to build the first target - side emotion lexicon denoted as TargetMT .", "entities": [[6, 8, "TaskName", "word translation"], [22, 23, "DatasetName", "emotion"]]}
{"text": "Source words are translated according to the model , whereas target - side emotion labels are simply copied from the source to the target ( see Section 2 ) .", "entities": [[13, 14, "DatasetName", "emotion"]]}
{"text": "TargetMT is then used as the distant supervisor to train a model that predicts word emotions based on target - side word embeddings .", "entities": [[21, 23, "TaskName", "word embeddings"]]}
{"text": "Once finalized , the model is used to predict new labels for the words in TargetMT , resulting in a second target - side emotion lexicon denoted TargetPred .", "entities": [[24, 25, "DatasetName", "emotion"]]}
{"text": "Schematic view on the methodology for generating and evaluating an emotion lexicon for a given target language based on source language supervision .", "entities": [[10, 11, "DatasetName", "emotion"]]}
{"text": "Hence , it may mitigate some of the errors which were introduced in previous steps , either by machine translation or by assuming that sourceand target - side emotion are always identical .", "entities": [[18, 20, "TaskName", "machine translation"], [28, 29, "DatasetName", "emotion"]]}
{"text": "This circumstance leads to \" partial duplicates \" in TargetMT , i.e. , groups of entries with the same word type but different emotion values ( because they were derived from distinct Source entries ) .", "entities": [[23, 24, "DatasetName", "emotion"]]}
{"text": "Such overlap could do harm to the integrity of our evaluation since knowledge may \" leak \" from training to validation phase , i.e. , by testing the model on words it has already seen during training , although with distinct emotion labels .", "entities": [[41, 42, "DatasetName", "emotion"]]}
{"text": "Since partial duplicates receive the same embedding vector , the prediction model assigns the same emotion value to both , thus merging them in TargetPred .", "entities": [[15, 16, "DatasetName", "emotion"]]}
{"text": "The main advantage of the above generation method is that it allows us to create large - scale emotion lexicons for languages a subject and the response ( rating ) is recorded .", "entities": [[18, 19, "DatasetName", "emotion"]]}
{"text": "This may result in cases where the translation misaligns with the copied emotion value in TargetMT .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "We may choose to base our evaluation on the full TargetPred lexicon , including words from the training set - after all , the word emotion model does not have access to any target - side gold data .", "entities": [[25, 26, "DatasetName", "emotion"]]}
{"text": "In particular , for our silver evaluation , we intersect TargetMT - test with TargetPred - test and compute the correlation of these two sets individually for each emotion variable .", "entities": [[28, 29, "DatasetName", "emotion"]]}
{"text": "We use the English emotion lexicon from Warriner et al ( 2013 ) as first part of our Source dataset .", "entities": [[4, 5, "DatasetName", "emotion"]]}
{"text": "Since manually gathered BE5 ratings are available only for a subset of this lexicon ( Stevenson et al , 2007 ) , we add BE5 ratings from Buechel and Hahn ( 2018a ) who used emotion representation mapping ( see Section 2 ) to convert the existing VAD ratings , showing that this is about as reliable as human annotation .", "entities": [[35, 36, "DatasetName", "emotion"]]}
{"text": "Appendix A.1 gives further details on data preparation . Translation .", "entities": [[9, 10, "TaskName", "Translation"]]}
{"text": "We used the GOOGLE CLOUD TRANSLATION API 4 to produce word - to - word translation tables .", "entities": [[4, 5, "DatasetName", "CLOUD"], [14, 16, "TaskName", "word translation"]]}
{"text": "We use the fastText embedding models from Grave et al ( 2018 ) trained for 157 languages on the respective WIKIPEDIA and the respective part of COMMONCRAWL .", "entities": [[3, 4, "MethodName", "fastText"]]}
{"text": "Since our proposed methodology is agnostic towards the chosen word emotion model , we will re - use models from the literature .", "entities": [[10, 11, "DatasetName", "emotion"]]}
{"text": "In particular , we will rely on the multi - task learning feed - forward network ( MTLFFN ) worked out by Buechel and Hahn ( 2018b ) .", "entities": [[8, 12, "TaskName", "multi - task learning"]]}
{"text": "This network constitutes the current state of the art for monolingual emotion lexicon creation ( expanding an existing lexicon for a given language ) for many of the datasets in Table 2 .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "Its distinguishing feature is that hidden layer parameters are shared between the different emotion target variables , thus constituting a mild form of multi - task learning ( MTL ) .", "entities": [[13, 14, "DatasetName", "emotion"], [23, 27, "TaskName", "multi - task learning"]]}
{"text": "We apply MTL to VAD and BE5 variables individually ( but not between both groups ) , thus training two distinct emotion models per language , following the outcome of a development experiment .", "entities": [[21, 22, "DatasetName", "emotion"]]}
{"text": "Being aware of the infamous instability of neural approaches ( Reimers and Gurevych , 2017 ) , we also employ a ridge regression model , an L 2 regularized version of linear regression , as a more robust , yet al o powerful baseline ( Li et al , 2017 ) .", "entities": [[31, 33, "MethodName", "linear regression"]]}
{"text": "However , choosing the \" best \" size for an emotion lexicon necessarily translates into a quality - coverage trade - off for which there is no general solution .", "entities": [[10, 11, "DatasetName", "emotion"]]}
{"text": "This is not surprising since no ( potentially error - prone ) machine translation was performed .", "entities": [[12, 14, "TaskName", "machine translation"]]}
{"text": "Dominance and the discrete emotion variables show performance trajectories swinging between these two extremes .", "entities": [[4, 5, "DatasetName", "emotion"]]}
{"text": "We conjecture that the large size of the English Source lexicon , compared to most TargetGold lexicons , more than compensates for error - prone machine translation .", "entities": [[25, 27, "TaskName", "machine translation"]]}
{"text": "Hence , this analysis is restricted to languages where more than one gold lexicon exists per emotion format .", "entities": [[16, 17, "DatasetName", "emotion"]]}
{"text": "Translation vs. Prediction .", "entities": [[0, 1, "TaskName", "Translation"]]}
{"text": "As hypothesized , the TargetPred lexicons agree , on average , more with human judgment than the TargetMT lexicons , suggesting that the word emotion model acts as a value - adding post - processor , partly mitigating rating inconsistencies introduced by mere translation of the lexicons .", "entities": [[24, 25, "DatasetName", "emotion"]]}
{"text": "The observation holds for each individual emotion variable with particularly large benefits for Arousal , where the postprocessed TargetPred lexicons are on average 14 % - points better compared to the translation - only TargetMT lexicons .", "entities": [[6, 7, "DatasetName", "emotion"]]}
{"text": "We compute the Pearson correlation between gold and silver evaluation results across languages per emotion variable .", "entities": [[3, 5, "MetricName", "Pearson correlation"], [14, 15, "DatasetName", "emotion"]]}
{"text": "For languages where we consider multiple datasets during gold evaluation , we first average the gold evaluation results for each emotion variable .", "entities": [[20, 21, "DatasetName", "emotion"]]}
{"text": "Emotion lexicons are at the core of sentiment analysis , a rapidly flourishing field of NLP .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}
{"text": "Yet , despite large community efforts , the coverage of existing lexicons is still limited in terms of languages , size , and types of emotion variables .", "entities": [[25, 26, "DatasetName", "emotion"]]}
{"text": "While there are techniques to tackle these three forms of sparsity in isolation , we introduced a methodology which allows us to cope with them simultaneously by jointly combining emotion representation mapping , machine translation , and embedding - based lexicon expansion .", "entities": [[29, 30, "DatasetName", "emotion"], [33, 35, "TaskName", "machine translation"]]}
{"text": "We created representationally complex lexiconscomprising 8 distinct emotion variables - for 91 languages with up to 2 million entries each .", "entities": [[7, 8, "DatasetName", "emotion"]]}
{"text": "Secondly , our data suggests that embedding - based word emotion models can be used as a repair mechanism , mitigating poor target - language emotion estimates acquired by simple word - to - word translation .", "entities": [[10, 11, "DatasetName", "emotion"], [25, 26, "DatasetName", "emotion"], [34, 36, "TaskName", "word translation"]]}
{"text": "Yet , when jointly learning all eight emotion variables , the results were somewhat inconclusive .", "entities": [[7, 8, "DatasetName", "emotion"]]}
{"text": "Paraphrase Generation : A Survey of the State of the Art", "entities": [[0, 2, "TaskName", "Paraphrase Generation"]]}
{"text": "This paper focuses on paraphrase generation , which is a widely studied natural language generation task in NLP .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}
{"text": "With the development of neural models , paraphrase generation research has exhibited a gradual shift to neural methods in the recent years .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}
{"text": "This paper surveys various approaches to paraphrase generation with a main focus on neural methods .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}
{"text": "Paraphrasing can be play an important role in language understanding tasks , such as question answering ( Dong et al , 2017 ; Zhu et al , 2017 ) , machine translation ( Seraj et al , 2015 ; Thompson and Post , 2020a ) , and semantic parsing ( Berant and Liang , 2014 ; .", "entities": [[14, 16, "TaskName", "question answering"], [30, 32, "TaskName", "machine translation"], [47, 49, "TaskName", "semantic parsing"]]}
{"text": "And it is also a good way for data augmentation ( Kumar et al , 2019 ;", "entities": [[8, 10, "TaskName", "data augmentation"], [11, 12, "DatasetName", "Kumar"]]}
{"text": "Given a sentence , paraphrase generation aims to create its paraphrases that can have a different wording or different structure from the original sentence , while preserving the original meaning .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}
{"text": "The focus of paraphrase generation has exhibited a gradual shift from classical approaches to more advanced neural approaches in the recent years with the rapid development of various neural models .", "entities": [[3, 5, "TaskName", "paraphrase generation"]]}
{"text": "Neural models have changed the traditional way paraphrase generation is performed and also provided new directions and architectures for the NLP community .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}
{"text": "ation have been published ( Metzler et al , 2011 ; Gupta and Krzy\u017cak , 2020 ) , there is no thorough and comprehensive survey on neural methods for paraphrase generation .", "entities": [[29, 31, "TaskName", "paraphrase generation"]]}
{"text": "To our best knowledge , this is the first survey on neural methods for paraphrase generation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}
{"text": "Therefore , our goal in this paper is to provide a timely survey on paraphrase generation , with a main focus on neural methods .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}
{"text": "In the following section , we will first introduce the most frequently used datasets for paraphrase generation ( Section 2 ) .", "entities": [[15, 17, "TaskName", "paraphrase generation"]]}
{"text": "After introducing all the methods , we compare the performance of the different models for paraphrase generation in Section 6 .", "entities": [[15, 17, "TaskName", "paraphrase generation"]]}
{"text": "Finally , we identify some research gaps in paraphrase generation .", "entities": [[8, 10, "TaskName", "paraphrase generation"]]}
{"text": "In this section , we describe several datasets that have been extensively used for paraphrase generation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}
{"text": "MSCOCO MSCOCO ( Lin et al , 2014 )", "entities": [[0, 1, "DatasetName", "MSCOCO"], [1, 2, "DatasetName", "MSCOCO"]]}
{"text": "Two general types of evaluation metrics are commonly used to evaluate paraphrase generation : automatic evaluation and human evaluation .", "entities": [[11, 13, "TaskName", "paraphrase generation"]]}
{"text": "Automatic Evaluation Several automatic evaluation metrics are used for the evaluation of paraphrase generation .", "entities": [[12, 14, "TaskName", "paraphrase generation"]]}
{"text": "Its versions , ROUGE - N ( computing the n - gram recall ) and ROUGE - L ( focusing on the longest common subsequence ) are mostly used .", "entities": [[15, 18, "MetricName", "ROUGE - L"]]}
{"text": "( 4 ) TER ( Snover et al , 2006 ) , which was also developed to evaluate machine translation .", "entities": [[18, 20, "TaskName", "machine translation"]]}
{"text": "A TER score is a value in the range of 0 - 1 , but is frequently presented as a percentage , where lower is better .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "Rule - based paraphrase generation approaches build on hand - crafted or automatically collected paraphrase rules .", "entities": [[3, 5, "TaskName", "paraphrase generation"]]}
{"text": "This approach is based on statistical machine translation ( SMT ) and is motivated by the fact that paraphrase generation can be seen as a special case of machine translation ( i.e. , monolingual machine translation ) .", "entities": [[6, 8, "TaskName", "machine translation"], [18, 20, "TaskName", "paraphrase generation"], [28, 30, "TaskName", "machine translation"], [34, 36, "TaskName", "machine translation"]]}
{"text": "A machine translation model normally finds a best translation\u00ea of a text in language f to a text in language e by utilizing a statistical translation model", "entities": [[1, 3, "TaskName", "machine translation"]]}
{"text": "Applying this idea to paraphrase generation , such a model will find a best paraphraset of a text in the source side s to a text in the target side t obtained as , t = arg max t t *", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}
{"text": "For instance , ( Wubben et al , 2010 ) constructed a large - scale parallel corpus containing paraphrases collected from the headlines that appeared in Google News .", "entities": [[26, 27, "DatasetName", "Google"]]}
{"text": "Then they trained a Phrase - Based Machine Translation model ( PBMT ) ( Koehn et al , 2007 ) on their parallel corpus using the MOSES package .", "entities": [[7, 9, "TaskName", "Machine Translation"], [26, 27, "DatasetName", "MOSES"]]}
{"text": "Early works on paraphrasing mainly focused on template - based or statistical machine translation approaches .", "entities": [[12, 14, "TaskName", "machine translation"]]}
{"text": "With the recent advances of neural networks , especially the sequence - to - sequence framework , Seq2Seq models were first use for paraphrase generation by ( Prakash et", "entities": [[17, 18, "MethodName", "Seq2Seq"], [23, 25, "TaskName", "paraphrase generation"]]}
{"text": "Their work inspired the wide use of neural models for paraphrase generation .", "entities": [[10, 12, "TaskName", "paraphrase generation"]]}
{"text": "Below we introduce the main approaches based on neural models that are used for paraphrase generation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}
{"text": "Currently , most of the existing paraphrase generation models are based on sequence - to - sequence models consisting of an encoder and a decoder .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}
{"text": "al , 2016 ) first utilized a seq2seq model implemented as recurrent neural networks - long short term memory networks ( LSTMs ) ( Hochreiter and Schmidhuber , 1997 ) - to process long sequences .", "entities": [[7, 8, "MethodName", "seq2seq"]]}
{"text": "A nonvolutional neural network ( CNN ) has also been used to construct seq2seq models as a CNN has fewer parameters and thus is faster to train ( Vizcarra and Ochoa - Luna , 2020 ) .", "entities": [[13, 14, "MethodName", "seq2seq"]]}
{"text": "The Transformer model ( Vaswani et al , 2017 ) has shown state - of - the - art performance on multiple text generation tasks .", "entities": [[1, 2, "MethodName", "Transformer"], [22, 24, "TaskName", "text generation"]]}
{"text": "Due to the Transformer 's improved ability to capture long - range dependencies in sentences , utilized a Transformer to construct their seq2seq model .", "entities": [[3, 4, "MethodName", "Transformer"], [18, 19, "MethodName", "Transformer"], [22, 23, "MethodName", "seq2seq"]]}
{"text": "Therefore , some researchers also utilized large pretrained language models such as GPT - 2 ( Radford et al , 2019 ) and BART ( Lewis et al , 2020 ) as their encoder - decoder framework ( Witteveen and Andrews , 2019 ; Hegde and Patil , 2020 ;", "entities": [[7, 10, "TaskName", "pretrained language models"], [12, 13, "MethodName", "GPT"], [23, 24, "MethodName", "BART"]]}
{"text": "However , greedy decoding and beam search methods are both generic approaches for all text generation tasks without a specific focus on paraphrase generation .", "entities": [[14, 16, "TaskName", "text generation"], [22, 24, "TaskName", "paraphrase generation"]]}
{"text": "Encoder - Decoder Architecture The numerous attempts that have been made to improve the Encoder - Decoder architecture for paraphrase generation can be broadly categorized into two types based on their focus : A. Model - focused ; and B. Attribute - focused .", "entities": [[19, 21, "TaskName", "paraphrase generation"]]}
{"text": "Copy To counter the effect of rare and outof - vocabulary words in neural sequence models , ( Vinyals et al , 2015 ) proposed a pointer network .", "entities": [[26, 28, "MethodName", "pointer network"]]}
{"text": "A pointer network copies an element from the input sequence directly into the output .", "entities": [[1, 3, "MethodName", "pointer network"]]}
{"text": "First introduced by Gu et al ( 2016 ) for abstractive summarization , Cao et al ( 2017 ) haev also applied the copy mechanism to paraphrase generation .", "entities": [[11, 12, "TaskName", "summarization"], [26, 28, "TaskName", "paraphrase generation"]]}
{"text": "This calls for a controlled use of the copy mechanism during paraphrase generation .", "entities": [[11, 13, "TaskName", "paraphrase generation"]]}
{"text": "Therefore , minimizing the training loss might not correspond to optimizing the evaluation metric .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "RL aims to train an agent to interact with the environment with the goal of maximizing its reward .", "entities": [[5, 6, "DatasetName", "agent"]]}
{"text": "Rather than minimizing loss ( the conventional approach ) , first utilized RL to maximize the reward given by an evaluator which outputs a real value to represent the matching degree between two sentences as paraphrases of each other .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "Generative adversarial networks ( GAN ) Proposed by Goodfellow et al ( 2014 ) , GANs consist of generators and discriminators , where generators try to generate realistic outputs that match the real distribution and discriminators try to distinguish between the samples generated by generators and the samples that are real .", "entities": [[4, 5, "MethodName", "GAN"]]}
{"text": "GAN is originally trained by minimax optimization proposed in ( Goodfellow et al , 2014 ) .", "entities": [[0, 1, "MethodName", "GAN"]]}
{"text": "However , when GAN is applied in text generation , the traditional training method can not be used because generating discrete words is non - differentiable .", "entities": [[3, 4, "MethodName", "GAN"], [7, 9, "TaskName", "text generation"]]}
{"text": "Here , a model is usually trained in an adversarial way : generators and discriminators are first pretrained , then generators are trained to maximize the loss of the fixed discriminators , then generators are fixed and discriminators are again trained to minimize the loss by provided the real samples and the samples generated by the fixed generators .", "entities": [[26, 27, "MetricName", "loss"], [44, 45, "MetricName", "loss"]]}
{"text": "For the task of paraphrase generation , different discriminators are designed to distinguish between generated samples and real samples , paraphrases and non - paraphrases ( Yang et al , 2019 ; Vizcarra and Ochoa - Luna , 2020 ) .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}
{"text": "( Kumar et al , 2019 ) utilized a submodular mechanism to maximize submodular functions measuring fidelity and diversity .", "entities": [[1, 2, "DatasetName", "Kumar"]]}
{"text": "( Qian et al , 2019 ) utilized multiple generators in GAN to generate multiple diverse paraphrases .", "entities": [[11, 12, "MethodName", "GAN"]]}
{"text": "( Fu et al , 2019 ) incorporates a novel latent bag - of - word mechanism into seq2seq model for content planning , which mainly provides candidate synonyms for words in the source texts .", "entities": [[18, 19, "MethodName", "seq2seq"]]}
{"text": "Methods in the first class first encode the syntax tree of an exemplar sentence into a list of vector representations and then feed them into decoder at each timestep when decoding ( Iyyer et al , 2018 ; Chen et al , 2019 ; Goyal and Durrett , 2020 ; Kumar et al , 2020 ) .", "entities": [[50, 51, "DatasetName", "Kumar"]]}
{"text": "The second class of methods will first learn a distribution over syntax information by VAE .", "entities": [[14, 15, "MethodName", "VAE"]]}
{"text": "First , the use of attention mechanism achieves a close performance on Quora but has a better performance on MSCOCO ( row 1 and 2 ) .", "entities": [[19, 20, "DatasetName", "MSCOCO"]]}
{"text": "Similarly , the simple application of VAE also achieves a close performance on Quora but further improves the performance on MSCOCO ( row4 ) .", "entities": [[6, 7, "MethodName", "VAE"], [20, 21, "DatasetName", "MSCOCO"]]}
{"text": "With the copy mechanism , the Seq2Seq model is able to retain some words and thus yields a much better results ( row 3 ) .", "entities": [[6, 7, "MethodName", "Seq2Seq"]]}
{"text": "Transformer ( row 5 ) outperforms all the Seq2Seq - based models without copy mechanism ( row 1 , 2 , 4 , 6 ) , which shows the advantages of Transformer and meanwhile also proves the effectiveness of copy mechanism .", "entities": [[0, 1, "MethodName", "Transformer"], [8, 9, "MethodName", "Seq2Seq"], [31, 32, "MethodName", "Transformer"]]}
{"text": "Therefore , a well designed optimization goal plays an important role in the task of paraphrase generation .", "entities": [[15, 17, "TaskName", "paraphrase generation"]]}
{"text": "Third , a novel decoding algorithm based on large pretrained language models helps to generate better paraphrases at the word level ( row 8 ) because of the strength of large pretrained language models and the synonyms learned by decoding algorithm .", "entities": [[9, 12, "TaskName", "pretrained language models"], [31, 34, "TaskName", "pretrained language models"]]}
{"text": "Fourth , the attempts to improve paraphrase generation with a special focus on combining multiple granularity levels also yield good performance ( row 9 , 10 ) .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}
{"text": "When learning to generate paraphrase in word level , phrase level and sentence level at the same time , their models improve the performance on multiple metrics compared with their backbone Transformer model ( row 5 ) .", "entities": [[31, 32, "MethodName", "Transformer"]]}
{"text": "Finally , incorporating syntax control into paraphrase generation will also yield better results at word level and sentence level ( row 11 , 12 ) .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}
{"text": "However , for corpus focusing on general sentnece paraphrases , different works have different choices among MSCOCO , Twitter URL and ParaNMT .", "entities": [[16, 17, "DatasetName", "MSCOCO"]]}
{"text": "MSCOCO is more preferred for less noise compared with Twitter URL and ParaNMT .", "entities": [[0, 1, "DatasetName", "MSCOCO"]]}
{"text": "Therefore , a combination of MSCOCO and Quora is more reasonable .", "entities": [[5, 6, "DatasetName", "MSCOCO"]]}
{"text": "For evaluation metrics , BLEU is the most frequently used one .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "because of \" curse of BLEU on paraphrase evaluation \" .", "entities": [[5, 6, "MetricName", "BLEU"]]}
{"text": "Pretrained language models Virtually all recent work related to the application of pretrained language models on paraphrase generation is quite naive .", "entities": [[0, 3, "TaskName", "Pretrained language models"], [12, 15, "TaskName", "pretrained language models"], [16, 18, "TaskName", "paraphrase generation"]]}
{"text": "Therefore , we could combine the large pretrained language models with other mechanisms , for example reinforcement learning , VAE and GAN .", "entities": [[7, 10, "TaskName", "pretrained language models"], [19, 20, "MethodName", "VAE"], [21, 22, "MethodName", "GAN"]]}
{"text": "Most recent works on multi - level paraphrase generation only focus on word - level paraphrasing and phrase - level paraphrasing .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}
{"text": "Transfer learning With the goal of generating different surfaces of given sentences while preserving the meaning , text summarization , text simplification and paraphrase generation are essentially similar .", "entities": [[0, 2, "TaskName", "Transfer learning"], [17, 19, "TaskName", "text summarization"], [20, 22, "TaskName", "text simplification"], [23, 25, "TaskName", "paraphrase generation"]]}
{"text": "Therefore , one could utilize transfer learning of these three tasks to improve the performance .", "entities": [[5, 7, "TaskName", "transfer learning"]]}
{"text": "Stylistic paraphrase generation Currently , word - and phrase - substitution in paraphrase gener - ation can not be carefully controlled .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}
{"text": "One possible method is to utilize paraphrase identification in the automatic evaluation metrics to explicitly provide an evaluation of if the generated sentence and input sentence are paraphrases .", "entities": [[6, 8, "TaskName", "paraphrase identification"]]}
{"text": "We present NAKDIMON , a two - layer character - level LSTM , that performs on par with much more complicated curationdependent systems , across a diverse array of modern Hebrew sources .", "entities": [[11, 12, "MethodName", "LSTM"]]}
{"text": "Existing computational approaches to dotting are manifested as complex , multi - resourced systems which perform morphological analysis on the undotted text and look undotted words up in handcrafted dictionaries as part of the dotting process .", "entities": [[16, 18, "TaskName", "morphological analysis"]]}
{"text": "Our system , NAKDIMON , accepts the undotted character sequence as its input , consults no external resources or lexical components , and produces diacritics for each character , resulting in dotted text whose quality is comparable to that of the commercial Morfix , on both character - level and word - level accuracy .", "entities": [[53, 54, "MetricName", "accuracy"]]}
{"text": "As we are unaware of legally - obtainable dotted modern corpora , we use a combination of dotted pre - modern texts as well as automatically and semi - automatically dotted modern sources to train NAKDIMON : The PRE - MODERN portion is obtained from two main sources : A combination of late pre - modern text from Project Ben - Yehuda , mostly texts from the late 19th century and the early 20th century ; 5 rabbinical texts from the medieval period , the most important of which is Mishneh Torah ( obtained from Project Mamre ) ; 6 and 23 short stories from the short story project .", "entities": [[40, 41, "MethodName", "MODERN"]]}
{"text": "Lastly , the MODERN portion contains manually collected text in Modern Hebrew , mostly from undotted sources , which we dot using Dicta and follow up by manually fixing errors , either using Dicta 's API or via automated scripts which catch common mistakes .", "entities": [[3, 4, "MethodName", "MODERN"]]}
{"text": "Our MODERN corpus contains roughly 326 K Hebrew tokens , and is much more consistent and similar to the expectation of a native Hebrew speaker than the PRE - MODERN or the AUTOMATIC corpora , and more accurately dotted than the AU - TOMATIC corpus .", "entities": [[1, 2, "MethodName", "MODERN"], [29, 30, "MethodName", "MODERN"]]}
{"text": "We use the same technique and style for dotting this corpus as we do for the MODERN corpus ( 2.2 ) , but the documents were 3 Nakdimon NAKDIMON embeds the input characters and passes them through a two - layer Bi - LSTM ( Hochreiter and Schmidhuber , 1997 ) .", "entities": [[16, 17, "MethodName", "MODERN"], [43, 44, "MethodName", "LSTM"]]}
{"text": "The LSTM output is fed into a single linear layer , which then feeds three linear layers , one for each diacritic category ( see 2 ) .", "entities": [[1, 2, "MethodName", "LSTM"], [8, 10, "MethodName", "linear layer"]]}
{"text": "We train NAKDIMON first over PRE - MODERN , then over the AUTOMATIC corpus , and then by over the MODERN corpus .", "entities": [[7, 8, "MethodName", "MODERN"], [20, 21, "MethodName", "MODERN"]]}
{"text": "During training , the loss is the sum of the cross - entropy loss from all three categories .", "entities": [[4, 5, "MetricName", "loss"], [13, 14, "MetricName", "loss"]]}
{"text": "Metrics We report four metrics : decision accuracy ( DEC ) is computed over the entire set of individual possible decisions : dagesh / mappiq for letters that allow it , sin / shin dot for the letter \u202b , \u05e9\u202c and all other diacritics for letters that allow them ; character accuracy ( CHA ) is the portion of characters in the text that end up in their intended final form ( which may combine two or three decisions , e.g. dagesh + vowel ) ; word accuracy ( WOR ) is the portion of words with no mistakes ; and vocalization", "entities": [[7, 8, "MetricName", "accuracy"], [52, 53, "MetricName", "accuracy"], [88, 89, "MetricName", "accuracy"]]}
{"text": "Tomer ( 2012 ) designs a diacritization system for Hebrew verbs consisting of a combination of a verb inflection system , a syllable boundary detector , and an SVM model for classifying verb inflection paradigms .", "entities": [[28, 29, "MethodName", "SVM"]]}
{"text": "Dictionary - less Arabic diacritization has been attempted using a 3 - layer Bi - LSTM ( Belinkov and Glass , 2015 ) .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "Abandah et al ( 2015 ) use a Bi - LSTM where characters are assigned either one or more diacritic symbols .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "In the future , we wish to evaluate the utility of dotting as a feature for downstream tasks such as question answering , machine translation , and speech generation , taking advantage of the fact that our simplified model can be easily integrated in an end - to - end Hebrew processing system .", "entities": [[20, 22, "TaskName", "question answering"], [23, 25, "TaskName", "machine translation"]]}
{"text": "Attempted architectural modifications , including substituting a Transformer ( Vaswani et al , 2017 ) for the LSTM ; adding a CRF layer to the decoding process ; and adding a residual connection between the character LSTM layers , yielded no substantial benefits in these experiments .", "entities": [[7, 8, "MethodName", "Transformer"], [17, 18, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [31, 33, "MethodName", "residual connection"], [36, 37, "MethodName", "LSTM"]]}
{"text": "This system , NAKDIMON 0 , differs from our final variant in three main aspects : it is not trained on the Dicta portion of our training corpus ( 2.2 ) , it is not trained on the AUTOMATIC corpus , and it employs a residual connection between the two character Bi - LSTM layers .", "entities": [[4, 5, "DatasetName", "0"], [45, 47, "MethodName", "residual connection"], [53, 54, "MethodName", "LSTM"]]}
{"text": "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years .", "entities": [[0, 2, "TaskName", "Transfer learning"]]}
{"text": "In speech , a model pre - trained by self - supervised learning transfers remarkably well on multiple tasks .", "entities": [[9, 13, "TaskName", "self - supervised learning"]]}
{"text": "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing ( NLP ) ( Peters et al , 2018 ;", "entities": [[0, 2, "TaskName", "Transfer learning"]]}
{"text": "Self - supervised learning ( SSL ) is the main driver of this paradigm , an effective and scalable way to learn high - level representation of language that transfers to a variety of tasks .", "entities": [[0, 4, "TaskName", "Self - supervised learning"], [5, 6, "DatasetName", "SSL"]]}
{"text": "SSL entails learning from the input or some perturbation of it without the need for labelled data .", "entities": [[0, 1, "DatasetName", "SSL"]]}
{"text": "SSL algo - rithms in speech must be evaluated in their ability to produce representations that are useful for tasks that demand understanding of linguistic , speaker , and prosodic elements of spoken language as well as high - level semantics .", "entities": [[0, 1, "DatasetName", "SSL"]]}
{"text": "Researchers have used auto - regressive , contrastive , discriminative and multi - task learning objectives to pre - train models , and have investigated their capabilities across tasks like phoneme recognition ( van den Oord et al , 2018 ;", "entities": [[11, 15, "TaskName", "multi - task learning"]]}
{"text": "Chung et al , 2019 ) , automatic speech recognition ( ASR ) ( Liu et al , 2020b ; Schneider et al , 2019 ; Ravanelli et al , 2020 ; Hsu et al , 2021 ; Chang et al , 2021 ) , speaker verification ( Fan et al , 2020 ) , speaker identification ( Chung et al , 2019 ; Liu et al , 2020c ) , emotion recognition ( Macary et al , 2021 ) , speech translation ( Chung et al , 2019 ) , voice conversion ( Lin et al , 2020 ; Huang et al , 2021a ) , spoken language understanding ( Lai et al , 2021 ) , and text - tospeech ( \u00c1lvarez et al , 2019 ) .", "entities": [[7, 10, "TaskName", "automatic speech recognition"], [45, 47, "TaskName", "speaker verification"], [55, 57, "TaskName", "speaker identification"], [71, 73, "TaskName", "emotion recognition"], [91, 93, "TaskName", "voice conversion"], [107, 110, "TaskName", "spoken language understanding"]]}
{"text": "It studied the models ' performance in tasks focusing on linguistic ( phoneme recognition and automatic speech recognition , keyword spotting and query by example ) , shallow semantic ( intent classification and slot filling ) , speaker ( speaker identification , speaker verification and speaker diarization ) , and prosodic ( emotion recognition ) characteristics .", "entities": [[15, 18, "TaskName", "automatic speech recognition"], [19, 21, "TaskName", "keyword spotting"], [30, 32, "TaskName", "intent classification"], [33, 35, "TaskName", "slot filling"], [39, 41, "TaskName", "speaker identification"], [42, 44, "TaskName", "speaker verification"], [45, 47, "TaskName", "speaker diarization"], [52, 54, "TaskName", "emotion recognition"]]}
{"text": "In this paper , we introduce SUPERB - SG , a benchmark with 5 new tasks , which are speech translation , out - of - domain ASR , voice conversion , speech separation , and speech enhancement , with an emphasis on evaluating the semantic and generative capabilities of pre - trained models that require high - level representations to capture linguistic , semantic , and speaker characteristics .", "entities": [[29, 31, "TaskName", "voice conversion"], [32, 34, "TaskName", "speech separation"], [36, 38, "TaskName", "speech enhancement"]]}
{"text": "These tasks go beyond speech recognition by focusing on various other aspects that are essential to building intelligent speech interfaces .", "entities": [[4, 6, "TaskName", "speech recognition"]]}
{"text": "Further , we show that while SSL models achieve close to state - of - the - art performance on many tasks , there is n't one model that outperforms all others , and that a simple Log Mel - Filterbank can perform competitively on some tasks .", "entities": [[6, 7, "DatasetName", "SSL"]]}
{"text": "As more powerful SSL models are proposed with promising performance on various tasks , researchers continually try to find extensive evaluation methods to assess model performance , and wish to holistically understand the capability of the learned representations in these models .", "entities": [[3, 4, "DatasetName", "SSL"]]}
{"text": "SUPERB ( Yang et al , 2021 ) is a framework to benchmark the SSL models on 10 speech tasks by learning task - specific prediction heads on top of the frozen shared SSL models .", "entities": [[14, 15, "DatasetName", "SSL"], [33, 34, "DatasetName", "SSL"]]}
{"text": "Another recently proposed benchmark is the LeBenchmark ( Evain et al , 2021 ) , investigating the performance of SSL models trained on French data with three semantic tasks .", "entities": [[19, 20, "DatasetName", "SSL"]]}
{"text": "However , they only consider wav2vec 2.0 ( Baevski et al , 2020b ) with different architectures as their upstream models ( i.e. , networks pre - trained with SSL ) .", "entities": [[29, 30, "DatasetName", "SSL"]]}
{"text": "Here , we evaluate a diverse set of SSL models , and offer a more comprehensive analysis .", "entities": [[8, 9, "DatasetName", "SSL"]]}
{"text": "They evaluate the SSL models via zero - shot probings at four linguistic levels .", "entities": [[3, 4, "DatasetName", "SSL"]]}
{"text": "While their benchmark task is specific for certain domain , we use various tasks to evaluate different aspects of SSL models .", "entities": [[19, 20, "DatasetName", "SSL"]]}
{"text": "The HEAR 2021 Challenge 2 aims to develop general - purpose audio representation by focusing on audio tasks beyond speech that include sound event detection , speech commands and pitch & chroma classification .", "entities": [[22, 25, "TaskName", "sound event detection"], [26, 28, "DatasetName", "speech commands"]]}
{"text": "We use it to evaluate the semantic capability of SSL models , and how they benefit the translation task .", "entities": [[9, 10, "DatasetName", "SSL"]]}
{"text": "Although an ASR is included in SUPERB , it only examines SSL models on read English corpus Lib - riSpeech ( Panayotov et al , 2015 ) .", "entities": [[11, 12, "DatasetName", "SSL"]]}
{"text": "For the cross - lingual tasks , we choose the Mexican Spanish ( es ) , Mandarin ( zh ) , and Arabic ( ar ) subsets from Common Voice 7.0 ( Ardila et al , 2020 ) ( CC0 Licensed ) containing 21.5 , 31.2 , and 30.7 hours of training data respectively .", "entities": [[28, 31, "DatasetName", "Common Voice 7.0"]]}
{"text": "For evaluation , we use word error rate ( WER ) as the metric except for Mandarin which character error rate ( CER ) is used .", "entities": [[5, 11, "MetricName", "word error rate ( WER )"]]}
{"text": "During inference , we use CTC greedy decoding without language model re - scoring to simplify the process and to highlight the impact of the learned acoustic representations .", "entities": [[5, 6, "DatasetName", "CTC"]]}
{"text": "For voice conversion ( VC ) , we consider the intralingual VC task in VCC2020 ( Zhao et al , 2020 )", "entities": [[1, 3, "TaskName", "voice conversion"]]}
{"text": "For the pretraining methods , we abbreviate \" vector quantization \" as VQ , \" future \" as F , \" masked \" as M , \" generation \" as G , \" contrastive discrimination \" as C , and \" token prediction / classification \" as P. Parameters for both pretraining and inference are counted .", "entities": [[9, 10, "TaskName", "quantization"]]}
{"text": "We use the task to evaluate the speaker transferability as well as the generalizability of the SSL models .", "entities": [[16, 17, "DatasetName", "SSL"]]}
{"text": "We use the commonly used mel - cepstrum distortion ( MCD ) , word error rate ( WER ) and automatic speaker verification ( ASV ) accept rate from off - the - shelf ASR and ASV models as evaluation metrics .", "entities": [[13, 19, "MetricName", "word error rate ( WER )"], [21, 23, "TaskName", "speaker verification"]]}
{"text": "We adopted Tacotron2 ( Shen et al , 2018 ) as the downstream model , which is an autoregressive network consisting of convolutional and LSTM layers .", "entities": [[2, 3, "MethodName", "Tacotron2"], [24, 25, "MethodName", "LSTM"]]}
{"text": "For the neural vocoder , we used the Hifi - GAN ( Kong et al , 2020 ) .", "entities": [[8, 11, "MethodName", "Hifi - GAN"]]}
{"text": "Speech separation ( SS ) is the task of separating target speech from background interference .", "entities": [[0, 2, "TaskName", "Speech separation"]]}
{"text": "Speech enhancement ( SE ) is the task of removing background noise from a degraded speech signal , and it aims to improve the perceived quality and intelligibility of the signal .", "entities": [[0, 2, "TaskName", "Speech enhancement"]]}
{"text": "A 3 - layer BLSTM model similar to the speech separation task is trained to predict the spectral mask for the clean signal .", "entities": [[9, 11, "TaskName", "speech separation"]]}
{"text": "We evaluate the tasks on 15 upstream models , which are PASE+ ( Ravanelli et al , 2020 ) , APC ( Chung et al , 2019 ) , VQ - APC , NPC ( Liu et al , 2020a ) , Mockingjay ( Liu et al , 2020c ) , TERA ( Liu et al , 2020b ) , DeCoAR 2.0 ( Ling and Liu , 2020 ) , Modifile CPC ( Rivi\u00e8re et al , 2020 ) , wav2vec family ( Schneider et al , 2019 ) ( Baevski et al , 2020a ) ( Baevski et al , 2020b ) and HuBERT ( Hsu et al , 2021 ) .", "entities": [[11, 12, "MethodName", "PASE+"]]}
{"text": "Some models also use vector quantization which has an added benefit of signal compression .", "entities": [[5, 6, "TaskName", "quantization"]]}
{"text": "In speech generation tasks ( VC , SS , and SE ) , FBANK yields comparable or superior performance than some SSL models , especially for those metrics that take the quality of the output signal into account .", "entities": [[21, 22, "DatasetName", "SSL"]]}
{"text": "PASE+ , which is pre - trained with denoising tasks , performs better than half the SSL models , but this observation does n't transfer to the other tasks .", "entities": [[0, 1, "MethodName", "PASE+"], [8, 9, "TaskName", "denoising"], [16, 17, "DatasetName", "SSL"]]}
{"text": "To make our analysis more representative and generalized to all speech domains , we bring back the six tasks from SUPERB ( Yang et al , 2021 ) that are considered representative of the following four domains : ( i ) Content recognition tasks contain - ing Phoneme Recognition ( PR ) , Automatic Speech Recognition ( ASR ) ( ii ) Speaker identity tasks including Identification ( SID ) , Automatic Speaker Verification ( ASV ) ( iii ) Semantics task which is Intent Classification ( IC ) and ( iv ) Prosodic task which is Emotion Recognition ( ER ) .", "entities": [[53, 56, "TaskName", "Automatic Speech Recognition"], [68, 69, "DatasetName", "SID"], [72, 74, "TaskName", "Speaker Verification"], [84, 86, "TaskName", "Intent Classification"], [97, 99, "TaskName", "Emotion Recognition"]]}
{"text": "As a result , high - level information extracted from SSL models has little benefit for these tasks but is helpful for other tasks .", "entities": [[10, 11, "DatasetName", "SSL"]]}
{"text": "As noted earlier , there is only a small gap in performance between FBANK and SSL models .", "entities": [[15, 16, "DatasetName", "SSL"]]}
{"text": "If we leave SS and SE out , all correlation coefficients are greater than 0.58 , showing that the SSL model representations are useful for multiple domains .", "entities": [[19, 20, "DatasetName", "SSL"]]}
{"text": "For VC , the speaker information needs to be removed while the content has to be kept , similar to PR and ASR but different from SID .", "entities": [[26, 27, "DatasetName", "SID"]]}
{"text": "For the upstream models , FBANK , TERA , CPC , wav2vec 2.0 Base and HuBERT Base are used to cover different SSL algorithms .", "entities": [[22, 23, "DatasetName", "SSL"]]}
{"text": "Furthermore , when using only 1 % of training data , most of the SSL models fail on the 3 downstream tasks .", "entities": [[14, 15, "DatasetName", "SSL"]]}
{"text": "Although SSL models learn highlevel representations from the unlabeled speech signal , acquisition of task - specific knowledge such as translingual ability in ST , text - level token mapping in OOD - ASR , and mask prediction in SS , requires non - trivial supervision .", "entities": [[1, 2, "DatasetName", "SSL"]]}
{"text": "We We have open - sourced all the codes 1 and released a challenge 3 to encourage further research of SSL in speech .", "entities": [[20, 21, "DatasetName", "SSL"]]}
{"text": "Therefore , we evaluate the wav2vec 2.0 XLSR model on the OOD - ASR tasks , as shown in the last row of Table 9 .", "entities": [[7, 8, "MethodName", "XLSR"]]}
{"text": "XLSR has identical architecture as wav2vec 2.0 Large , but is trained with 56k hours of speech including 53 different languages .", "entities": [[0, 1, "MethodName", "XLSR"]]}
{"text": "The pre - training data of XLSR cover our cross - lingual tasks ' training data .", "entities": [[6, 7, "MethodName", "XLSR"]]}
{"text": "B.5.1 Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "entities": [[5, 8, "HyperparameterName", "number of parameters"]]}
{"text": "Building an end - to - end conversational agent for multi - domain task - oriented dialogues has been an open challenge for two main reasons .", "entities": [[8, 9, "DatasetName", "agent"]]}
{"text": "First , tracking dialogue states of multiple domains is non - trivial as the dialogue agent must obtain complete states from all relevant domains , some of which might have shared slots among domains as well as unique slots specifically for one domain only .", "entities": [[15, 16, "DatasetName", "agent"]]}
{"text": "Second , the dialogue agent must also process various types of information across domains , including dialogue context , dialogue states , and database , to generate natural responses to users .", "entities": [[4, 5, "DatasetName", "agent"]]}
{"text": "We conduct comprehensive experiments in dialogue state tracking , contextto - text , and end - to - end settings on the Multi - WOZ2.1 benchmark , achieving superior performance over competitive baselines .", "entities": [[5, 8, "TaskName", "dialogue state tracking"]]}
{"text": "A conventional approach to task - oriented dialogues is to solve four distinct tasks : ( 1 ) natural language understanding ( NLU ) which parses user utterance into a semantic frame , ( 2 ) dialogue state tracking ( DST ) which updates the slots and values from semantic frames to the latest values for knowledge base retrieval , ( 3 ) dialogue policy which determines an appropriate dialogue act for the next system response , and ( 4 ) response generation which generates a natural language sequence conditioned on the dialogue act .", "entities": [[18, 21, "TaskName", "natural language understanding"], [36, 39, "TaskName", "dialogue state tracking"], [81, 83, "TaskName", "response generation"]]}
{"text": "Dialogue agent : fitzbillies restaurant is an expensive british restaurant in the centre .", "entities": [[1, 2, "DatasetName", "agent"]]}
{"text": "Dialogue agent : the phone number for the pool is 01234567 , is there something else i can help you ?", "entities": [[1, 2, "DatasetName", "agent"]]}
{"text": "Dialogue state : { restaurant : { pricerange : expensive , area : centre , name = fizbillies restaurant , request= [ address ] } , attraction : { name : kings hedges learner pool , request= [ phone ] } } Dialogue acts : [ inform - phone ] Table 1 : Example of a multi - domain dialogue with two domains : restaurant and attraction . is particularly important in task - oriented dialogues as it determines the general decision towards task completion before a dialogue agent can materialize it into natural language response ( See Table 1 ) .", "entities": [[88, 89, "DatasetName", "agent"]]}
{"text": "UniConv consists of a Bi - level State Tracking ( BDST ) module which embeds natural language understanding as it can directly parse dialogue context into a structured dialogue state rather than relying on the semantic frame output from an NLU module in each dialogue turn .", "entities": [[15, 18, "TaskName", "natural language understanding"]]}
{"text": "Our dialogue state tracker disentangles slot and domain representation learning while enabling deep learning of shared representations of slots common among domains .", "entities": [[8, 10, "TaskName", "representation learning"]]}
{"text": "We evaluate our models on the large - scale Mul - tiWOZ benchmark , and compare with the existing methods in DST , context - to - text generation , and end - to - end settings .", "entities": [[27, 29, "TaskName", "text generation"]]}
{"text": "Dialogue State Tracking .", "entities": [[0, 3, "TaskName", "Dialogue State Tracking"]]}
{"text": "al , 2018 ; Lee et al , 2019 ) , which assume known slot ontology with a fixed candidate set for each slot .", "entities": [[15, 16, "MethodName", "ontology"]]}
{"text": "Different from previous Context - to - Text Generation .", "entities": [[7, 9, "TaskName", "Text Generation"]]}
{"text": "Zhao et al ( 2019 ) models action space of dialogue agent as latent variables .", "entities": [[11, 12, "DatasetName", "agent"]]}
{"text": "al ( 2019 ) ; Peng et al ( 2019 ) use multiple dialogue agents , each trained for a specific dialogue domain , and combine them through a common dialogue agent .", "entities": [[31, 32, "DatasetName", "agent"]]}
{"text": "In this task , conventional approaches combine Natural Language Understanding ( NLU ) , DST , Dialogue Policy , and NLG , into a pipeline architecture ( Wen et al , 2017 ; Bordes et al , 2016 ; Liu and Lane , 2017 ; Liu and Perez , 2017 ; Williams et al , 2017 ; Zhao et al , 2017 ; Jhunjhunwala et al , 2020 ) .", "entities": [[7, 10, "TaskName", "Natural Language Understanding"]]}
{"text": "Different from prior work such as ( Shu et al , 2019 ) , our model facilitates multi - domain state tracking and allows learning dialogue acts during response generation .", "entities": [[28, 30, "TaskName", "response generation"]]}
{"text": "To make it consistent , we encode all input with the same embedding dimension .", "entities": [[12, 14, "HyperparameterName", "embedding dimension"]]}
{"text": "The DST includes 2 modules for slot - level and domain - level representation learning .", "entities": [[13, 15, "TaskName", "representation learning"]]}
{"text": "Each module comprises attention layers to project domain or slot representations and incorporate important information from dialogue context , dialogue state of the previous turn , and current user utterance .", "entities": [[3, 5, "HyperparameterName", "attention layers"]]}
{"text": "L X is the length of sequence X and d is the embedding dimension .", "entities": [[12, 14, "HyperparameterName", "embedding dimension"]]}
{"text": "E. The final embedding is the element - wise summation between token - embedded representations and positional encoded representations with layer normalization ( Ba et al , 2016 ) : Z = LayerNorm ( Z emb + P E ( X ) )", "entities": [[20, 22, "MethodName", "layer normalization"]]}
{"text": "We adopt the Transformer attention ( Vaswani et al , 2017 ) , which consists of a dot - product attention with skip connection , to integrate dialogue contextual information into each slot representation .", "entities": [[3, 4, "MethodName", "Transformer"], [17, 21, "MethodName", "dot - product attention"]]}
{"text": "Given the k - th ( domain , slot ) pair and decoding step l , the output hidden state in each recurrent step h kl is passed through a linear transformation with softmax to obtain output distribution over vocabulary set V : P inf", "entities": [[33, 34, "MethodName", "softmax"]]}
{"text": "= Softmax ( h kl W inf )", "entities": [[1, 2, "MethodName", "Softmax"]]}
{"text": "For request slot of k - th ( domain , slot ) pair , we pass the corresponding vector Z dst vector through a linear layer with sigmoid activation to predict a value of 0 or 1 .", "entities": [[24, 26, "MethodName", "linear layer"], [27, 29, "MethodName", "sigmoid activation"], [34, 35, "DatasetName", "0"]]}
{"text": "P req k = Sigmoid ( Z dst k W req ) .", "entities": [[2, 4, "HyperparameterName", "k ="]]}
{"text": "The DST is optimized by the crossentropy loss functions of inform and request slots : L dst", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "Following , we create a one - hot vector for each domain d : x d db { 0 , 1 } 6 and 6 i x d db , i = 1 .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "Response Generation .", "entities": [[0, 2, "TaskName", "Response Generation"]]}
{"text": "These attention layers capture the information needed to generate tokens that are towards task completion and supplement the contextual cues obtained in previous attention layers .", "entities": [[1, 3, "HyperparameterName", "attention layers"], [23, 25, "HyperparameterName", "attention layers"]]}
{"text": "The final output is passed to a linear layer with softmax activation to decode system responses auto - regressively : P res = Softmax ( Z gen W gen ) R Lres\u00d7 Vres Dialogue Act Modeling .", "entities": [[7, 9, "MethodName", "linear layer"], [10, 11, "MethodName", "softmax"], [23, 24, "MethodName", "Softmax"]]}
{"text": "We couple response generation with dialogue act modeling by learning a latent variable Z act R d .", "entities": [[2, 4, "TaskName", "response generation"]]}
{"text": "We then pass this tensor to the same stacked attention layers as above .", "entities": [[9, 11, "HyperparameterName", "attention layers"]]}
{"text": "The output representation of the latent vector i.e. by domain first row in Z gen , incorporates contextual signals accumulated from all attention layers and is used to predict dialogue acts .", "entities": [[22, 24, "HyperparameterName", "attention layers"]]}
{"text": "We denote this representation as Z gen act and pass it through a linear layer to obtain a multi - hot encoded tensor .", "entities": [[13, 15, "MethodName", "linear layer"]]}
{"text": "We apply Sigmoid on this tensor to classify each dialogue act as 0 or 1 : P act = Sigmoid ( Z gen act W act )", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "The response generator is jointly trained by the cross - entropy loss functions of generated responses and dialogue acts : L gen = L res + L act = Yres l=1 \u2212 log ( P res l ( y l ) )", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "We evaluate our models with the multi - domain dialogue corpus MultiWOZ 2.0 and 2.1 ( Eric et al , 2019 ) ( The latter includes corrected state labels for the DST task ) .", "entities": [[11, 13, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "We employed dropout ( Srivastava et al , 2014 ) of 0.3 and label smoothing ( Szegedy et al , 2016 ) on target system responses during training .", "entities": [[13, 15, "MethodName", "label smoothing"]]}
{"text": "Joint Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}
{"text": "49.04 % DSTQA ( Zhou and Small , 2019 ) 51.17 % SOM - DST ( Kim et al , 2020 ) 53.01 % BDST ( Ours ) 49.55 % 71.29 % 60.96 % 18.80 TokenMoE ( Pei et al , 2019 ) 75.30 % 59.70 % 16.81 HDSA 82.90 % 68.90 % 23.60 Structured Fusion ( Mehri et al , 2019 ) 82.70 % 72.10 % 16.34 LaRL ( Zhao et al , 2019 ) 82.78 % 79.20 % 12.80 GPT2 ( Budzianowski and Vuli\u0107 , 2019 )", "entities": [[12, 13, "MethodName", "SOM"]]}
{"text": "As suggested by Liu et al ( 2016 ) , human evaluation , even though popular in dialogue research , might not be necessary in tasks with domain constraints such as MultiWOZ .", "entities": [[31, 32, "DatasetName", "MultiWOZ"]]}
{"text": "Other baseline models such as Wu et al , 2019b ) present challenges in the MultiWOZ benchmark as the models could not fully optimize due to the large scale entity memory .", "entities": [[15, 16, "DatasetName", "MultiWOZ"]]}
{"text": "We conduct a comprehensive ablation analysis with several model variants in Table 6 and have the following observations : The model variant with a single - level DST ( by considering S = DS and N dst D = 0 ) ( Row A2 ) performs worse than the Bi - level DST ( Row A1 ) .", "entities": [[39, 40, "DatasetName", "0"]]}
{"text": "In addition , using the dual architecture also improves the latency in each attention layers as typically D + S DS .", "entities": [[13, 15, "HyperparameterName", "attention layers"]]}
{"text": "We note that removing the loss function to learn the dialogue act latent variable ( Row B2 ) can hurt the generation performance , especially by the task completion metrics Inform and Success .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "This is interesting as we expect dialogue acts affect the general semantics of output sentences , indicated by BLEU score , rather than the model ability to retrieve correct entities .", "entities": [[18, 20, "MetricName", "BLEU score"]]}
{"text": "The DBs of 3 domains taxi , police , and hospital are not available as part of the benchmark .", "entities": [[17, 19, "DatasetName", "the benchmark"]]}
{"text": "We describe our baseline models in DST , contextto - text generation , and end - to - end dialogue tasks .", "entities": [[10, 12, "TaskName", "text generation"]]}
{"text": "Both models include encoder modules ( either bidirectional LSTM or hierarchical LSTM ) to encode the dialogue history .", "entities": [[7, 9, "MethodName", "bidirectional LSTM"], [11, 12, "MethodName", "LSTM"]]}
{"text": "This model considers the DST task as a reading comprehension task and predicts each slot as a span over tokens within dialogue history .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}
{"text": "The model adopts a sequence - to - sequence framework with a pointer network to generate dialogue states .", "entities": [[12, 14, "MethodName", "pointer network"]]}
{"text": "We reported the performance when the maximum length of the output dialogue state sequence L is set to 20 tokens ( original default parameter is 8 tokens but we expect longer dialogue state in MultiWOZ benchmark and selected 20 tokens ) .", "entities": [[34, 35, "DatasetName", "MultiWOZ"]]}
{"text": "The model adopts a sequence - to - sequence framework with a pointer network to generate individual slot token - by - token .", "entities": [[12, 14, "MethodName", "pointer network"]]}
{"text": "The model proposes a non - autoregressive approach for dialogue state tracking which enables learning dependencies between domain - level and slot - level representations as well as token - level representations of slot values .", "entities": [[9, 12, "TaskName", "dialogue state tracking"]]}
{"text": "The model treats dialogue state tracking as a question answering problem in which state values can be predicted through lexical spans or unique generated values .", "entities": [[3, 6, "TaskName", "dialogue state tracking"], [8, 10, "TaskName", "question answering"]]}
{"text": "SOM - DST ( Kim et al , 2020 ) .", "entities": [[0, 1, "MethodName", "SOM"]]}
{"text": "i d , address , area , internet , parking , single , double , family , name , phone , postcode , pricerange ' , takesbookings , stars , type At each dialogue turn , the mechanism involve decision making on whether to update or carryover the state values from previous turns .", "entities": [[39, 41, "TaskName", "decision making"]]}
{"text": "This is the current stateof - the - art in terms of Inform and BLEU score in the context - to - text generation setting in MultiWOZ2.0 .", "entities": [[14, 16, "MetricName", "BLEU score"], [22, 24, "TaskName", "text generation"]]}
{"text": "The graph is incorporated as an inductive bias in a self - attention network to improve the semantic quality of generated dialogue responses .", "entities": [[10, 14, "MethodName", "self - attention network"]]}
{"text": "The latent variables are learned using unsupervised learning with stochastic variational inference .", "entities": [[10, 12, "MethodName", "variational inference"]]}
{"text": "Unsupervised pre - training language models have significantly improved machine learning performance in many NLP tasks .", "entities": [[0, 4, "TaskName", "Unsupervised pre - training"]]}
{"text": "This baseline model leverages the power of a pre - trained model ( Radford et al , 2019 ) and adapts to the context - to - text generation setting in task - oriented dialogues .", "entities": [[27, 29, "TaskName", "text generation"]]}
{"text": "The sequence is used as input to a pre - trained GPT - 2 model which is then fine - tuned with MultiWOZ data .", "entities": [[11, 12, "MethodName", "GPT"], [22, 23, "DatasetName", "MultiWOZ"]]}
{"text": "This is the current state - of - the - art model for context - to - text generation task in MultiWOZ 2.1 .", "entities": [[17, 19, "TaskName", "text generation"], [21, 23, "DatasetName", "MultiWOZ 2.1"]]}
{"text": "Similar to the DST component , the response generator of TSCP also adopts a pointer network to generate tokens of the target system responses by copying tokens from source sequences .", "entities": [[14, 16, "MethodName", "pointer network"]]}
{"text": "HRED - TS ( Peng et al , 2019 ) .", "entities": [[2, 3, "MethodName", "TS"]]}
{"text": "Overall , our dialogue agent can carry a proper dialogue with the user throughout the dialogue steps .", "entities": [[4, 5, "DatasetName", "agent"]]}
{"text": "The dialogue agent can also detect some of the co - references among the domains .", "entities": [[2, 3, "DatasetName", "agent"]]}
{"text": "For example , at the 5 th turn , the dialogue agent can infer the slot area for the new domain attraction as the user mentioned ' close the restaurant ' .", "entities": [[11, 12, "DatasetName", "agent"]]}
{"text": "As we expected , the Joint Accuracy metric tends to decrease as the dialogue history extends over time .", "entities": [[6, 7, "MetricName", "Accuracy"]]}
{"text": "For response generation performance , the trend of BLEU score is less obvious .", "entities": [[1, 3, "TaskName", "response generation"], [8, 10, "MetricName", "BLEU score"]]}
{"text": "Although classical text generative models ( e.g. , sequence - tosequence model Sutskever et al , 2014 , attentionbased model , andpointer - generator networks See et al , 2017 ) have been applied to many text generation tasks , yet , in the task of the court 's view generation , such techniques can not be simply applied for the following reasons : ( 1 ) There exists \" no claim , no trial \" principle in civil legal systems : The judgment in the real court 's view is the response to the claims declared by the plaintiff , where its rationales summarize the corresponding facts .", "entities": [[36, 38, "TaskName", "text generation"]]}
{"text": "The counterfactual decoder is inspired by the backdoor adjustment in causal inference ( Pearl et al , 2016 ; Kuang et al , 2020 ) to address the confounding bias and the imbalance problem in judgment .", "entities": [[10, 12, "MethodName", "causal inference"]]}
{"text": "Charge prediction is a common task of judgment prediction , considered as a text classification problem ( Lin et al , 2012 ; Zhong et al , 2018 ;", "entities": [[13, 15, "TaskName", "text classification"]]}
{"text": "NLG has been widely studied and applied to many tasks , such as machine translation ( Wu et al , 2016 ) , question answering ( McCann et al , 2018 ; Bagchi and Wynter , 2013 ) and text summarization ( Rush et al , 2015 ) .", "entities": [[13, 15, "TaskName", "machine translation"], [23, 25, "TaskName", "question answering"], [39, 41, "TaskName", "text summarization"]]}
{"text": "Causal Inference ( Pearl , 2009 ; Kuang et al , 2020 ) is a powerful statistical modeling tool for explanatory analysis by removing confounding bias in data .", "entities": [[0, 2, "MethodName", "Causal Inference"]]}
{"text": "Recently , many methods have been proposed to remove confounding bias in the literature of causal inference , including do - operation based on structure causal model ( Pearl , 2009 ) and counterfactual outcome prediction based on potential outcome framework ( Imbens and Rubin , 2015 ) .", "entities": [[15, 17, "MethodName", "causal inference"]]}
{"text": "Court 's view ( V ) contains two main components , judgment and rationales , where the judgment is to respond the plaintiff 's claims , and the rationales are the claim - related summarization on the fact description to determine and interpret the judgment .", "entities": [[34, 35, "TaskName", "summarization"]]}
{"text": "For simplicity , we set j = 1 to denote supported judgment ( all the claims are judged to be accepted ) , and j = 0 to denote non - supported judgment .", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "Backdoor adjustment is a main de - confounding technique in causal inference ( Pearl et al , 2016 ; Pearl , 2009 ) .", "entities": [[10, 12, "MethodName", "causal inference"]]}
{"text": "= P ( V | I , j = 0 ) P ( j = 0 )", "entities": [[9, 10, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}
{"text": "Our model is conducted in a multi - task learning manner which consists of a shared encoder , a predictor , and a pair of counterfactual decoders .", "entities": [[6, 10, "TaskName", "multi - task learning"]]}
{"text": "Then the embedding sequences are fed to the Bi - LSTM , producing two sequences of hidden states h c , h f corresponding to the plaintiff 's claim and the fact description respectively .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "e i k = v T tanh", "entities": [[2, 4, "HyperparameterName", "k ="]]}
{"text": "( 4 ) q i = sof tmax ( e i ) ( 5 )", "entities": [[6, 7, "DatasetName", "sof"]]}
{"text": "i k h c k ( 6 ) After feeding to another Bi - LSTM layer , we get the claim - aware representation of fact h. Judgment Predictor Given the claim - aware representation of fact h , the judgment predictor produces the probability of support P sup through a fully connected layer and a sigmoid operation .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "The prediction result j is obtained as follow : j = 1 P sup > 0.5 0 P sup < = 0.5 ( 7 ) where 1 means support , and 0 means non - support .", "entities": [[16, 17, "DatasetName", "0"], [31, 32, "DatasetName", "0"]]}
{"text": "The context vector h * t , which can be regarded as a representation of the input for this step , is concatenated with the decode state s t and fed to linear layers to produce the vocabulary distribution p vocab : p vocab = sof tmax ( V ( V [ s t , h * t ] )", "entities": [[45, 46, "DatasetName", "sof"]]}
{"text": "Given the context h * t , the decode state s t and the decoder 's input ( the word embedding of the previous word ) x t , the generation probability p gen can be calculated : P gen = \u03c3 ( w T h * h * t + w T s s t + w T x x t + b ptr ) ( 10 )", "entities": [[64, 65, "DatasetName", "ptr"]]}
{"text": "where w h * , w s , w x and b ptr are learnable , and \u03c3 is the sigmoid function .", "entities": [[12, 13, "DatasetName", "ptr"]]}
{"text": "The final probability for a word w in time step is obtained : Training For predictor , we use cross - entropy as the loss function : P ( w ) = P gen * p vocab ( w )", "entities": [[24, 25, "MetricName", "loss"]]}
{"text": "Since we aim to make the two decoders generate two different court 's views , we take a mask operation when calculating the loss of each decoder .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "PGN Pointer Generator Networks ( See et al , 2017 ) utilizes a pointer network to solve the outof - vocabulary ( OOV ) problem , which is essential for the court 's view generation since many nouns occur there .", "entities": [[13, 15, "MethodName", "pointer network"]]}
{"text": "ROUGE - L is a Longest Common Subsequence ( LCS ) based statistics .", "entities": [[0, 3, "MetricName", "ROUGE - L"]]}
{"text": "BERT SCORE 6 ( Zhang et al , 2019 ) computes a similarity score by using contextual embedding of the tokens .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "We use Gensim ( \u0158eh\u016f\u0159ek and Sojka , 2010 ) with a large - scale generic corpus to train a language model as the pre - trained model , then use it to initialize the word embeddings , which is in the dimension of 300 . 8", "entities": [[35, 37, "TaskName", "word embeddings"]]}
{"text": "The proposed algorithm is designed for generating the court 's view draft for assisting the trial judges for decision making .", "entities": [[18, 20, "TaskName", "decision making"]]}
{"text": "Potential Error : The potential error would be as follows : a ) generating a wrong judgment and b ) generating a wrong rationale .", "entities": [[1, 2, "MetricName", "Error"]]}
{"text": "A neural attention model for abstractive sentence summarization .", "entities": [[6, 8, "TaskName", "sentence summarization"]]}
{"text": "arXiv preprint arXiv:1509.00685 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}
{"text": "Get to the point : Summarization with pointer - generator networks .", "entities": [[5, 6, "TaskName", "Summarization"]]}
{"text": "arXiv preprint arXiv:1704.04368 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}
{"text": "However , the plaintiff has the right to claim the interest to be calculated from the benchmark interest rate of the same grade of loans issued by the People 's Bank of China at the same period when the loan occurred since the date of the prosecution Rejection .", "entities": [[15, 17, "DatasetName", "the benchmark"]]}
{"text": "Opinion target extraction and opinion term extraction are two fundamental tasks in Aspect Based Sentiment Analysis ( ABSA ) .", "entities": [[5, 7, "TaskName", "term extraction"], [14, 16, "TaskName", "Sentiment Analysis"]]}
{"text": "As a case study , we also develop a Multi - Task structure named MT - TSMSA for AOPE by combining our TSMSA with an aspect and opinion term extraction module .", "entities": [[28, 30, "TaskName", "term extraction"]]}
{"text": "Experimental results indicate that TSMSA outperforms the benchmark methods on TOWE significantly ; meanwhile , the performance of MT - TSMSA is similar or even better than state - of - the - art AOPE baseline models .", "entities": [[6, 8, "DatasetName", "the benchmark"]]}
{"text": "Aspect - Based Sentiment Analysis ( ABSA )", "entities": [[0, 5, "TaskName", "Aspect - Based Sentiment Analysis"]]}
{"text": "In ABSA , aspect ( or called opinion target ) extraction and opinion term extraction are two fundamental tasks .", "entities": [[13, 15, "TaskName", "term extraction"]]}
{"text": "and opinion term extraction ( Liu et al , 2015 ;", "entities": [[2, 4, "TaskName", "term extraction"]]}
{"text": "Since aspect extraction has been fully studied and satisfactory results have been obtained , TOWE , which aims at mining the relation between aspects and opinion terms , is the key to the AOPE task .", "entities": [[1, 3, "TaskName", "aspect extraction"]]}
{"text": "For instance , Fan et al ( 2019 ) propose an Inward - Outward LSTM to pass target information to the left context and the right context of the target respectively , and then they combine the left , right , and global context to encode the sentence .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "To improve the performance of our model , we apply pre - trained language models like BERT ( Devlin et al , 2019 ) which contain a multi - head self - attention module as the encoder .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "As a case study , we integrate aspect and opinion term extraction , and TOWE into a Multi - Task architecture named MT - TSMSA to validate the effectiveness of our method on the AOPE task .", "entities": [[10, 12, "TaskName", "term extraction"]]}
{"text": "Plenty of works have been carried out for aspect extraction and opinion term extraction .", "entities": [[8, 10, "TaskName", "aspect extraction"], [12, 14, "TaskName", "term extraction"]]}
{"text": "Several works integrate aspect extraction and opinion term extraction into a co - extraction process .", "entities": [[3, 5, "TaskName", "aspect extraction"], [7, 9, "TaskName", "term extraction"]]}
{"text": "Some other works adopt the co - extraction structure in neural networks with multi - task learning ( Wang et al , 2016 ( Wang et al , , 2017Li and Lam , 2017 ) .", "entities": [[13, 17, "TaskName", "multi - task learning"]]}
{"text": "Rule - based methods ( Hu and Liu , 2004 ; Zhuang et al , 2006 ) are proposed to select corresponding opinion terms with distance rule and syntactic rule templates based on dependency parsing trees .", "entities": [[33, 35, "TaskName", "dependency parsing"]]}
{"text": "Fan et al ( 2019 ) carry out TOWE by extracting the corresponding opinion terms for a given aspect , and then utilize Inward - Outward LSTM to generate implicit representations of aspects .", "entities": [[26, 27, "MethodName", "LSTM"]]}
{"text": "Nevertheless , this approach is not capable of applying powerful pre - trained language models like BERT as the encoder to perform better .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "Our model aims to extract corresponding opinion terms of the given aspect with explicit representations , in addition to boost performance by employing BERT as the encoder .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "AOPE and PAOTE are essentially the same task with different names , and they can be split into aspect extraction and TOWE .", "entities": [[18, 20, "TaskName", "aspect extraction"]]}
{"text": "develop a span - based multi - task learning framework ( SpanMlt ) where the terms are extracted under annotated span boundaries , so as to identify the relations between every two span combinations .", "entities": [[5, 9, "TaskName", "multi - task learning"]]}
{"text": "Next , the multi - head self - attention method is applied to capture the context representations of the specific aspect explicitly , then they are passed to a projection layer and a Conditional Random Field ( CRF ) ( Lafferty et al , 2001 ) layer for sequence labeling .", "entities": [[33, 36, "MethodName", "Conditional Random Field"], [37, 38, "MethodName", "CRF"]]}
{"text": "Furthermore , the aspect and opinion words extraction ( task 0 ) as well as the target - oriented opinion words extraction ( task 1 ) are combined for multi - task learning .", "entities": [[10, 11, "DatasetName", "0"], [29, 33, "TaskName", "multi - task learning"]]}
{"text": "These two tasks share the parameters of encoder but differ in projection and CRF layers .", "entities": [[13, 14, "MethodName", "CRF"]]}
{"text": "For each attention head in the above approach , we first compute the scaled dot - product attention .", "entities": [[13, 18, "MethodName", "scaled dot - product attention"]]}
{"text": "The scaled dot - product attention is calculated as follows : Attention ( Q , K , V )", "entities": [[1, 6, "MethodName", "scaled dot - product attention"]]}
{"text": "= sof tmax ( QK T \u221a d k ) V. ( 1 )", "entities": [[1, 2, "DatasetName", "sof"]]}
{"text": "Finally , the multi - head attention is described as follows : length .", "entities": [[3, 7, "MethodName", "multi - head attention"]]}
{"text": "To start with , the input vector of each word is generated by utilizing a word embedding lookup table L w R r\u00d7dw and a positional embedding lookup table L p R n\u00d7dp , where d w is the dimension of word embeddings , r is the vocabulary size , and d p is the dimension of positional embeddings .", "entities": [[41, 43, "TaskName", "word embeddings"]]}
{"text": "For our base models ( not using a pre - trained language model ) , e i w will be projected to a low dimensional vector e i low which is calculated as follows : e i low = \u03c3 ( W e e i w ) , where W e R d low \u00d7dw ( d low < d w ) denotes the matrix of projection and \u03c3 ( ) is the activation function .", "entities": [[73, 75, "HyperparameterName", "activation function"]]}
{"text": "For a pre - trained language model like BERT ( Devlin et al , 2019 ) , t i equals the sum of e i w ,", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "Then , the input vector T is passed to multihead self - attention modules , where a feed - forward network and an add - norm network are combined in sequence to generate the context representation of each layer H = { H 1 , ... , H l } , where l is the number of multi - head attention layers and H", "entities": [[57, 61, "MethodName", "multi - head attention"]]}
{"text": "F F N i = max ( 0 ,", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "i = LN ( H i\u22121 + F F N i ) , ( 6 ) where h is the number of attention heads , H 0", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "LN ( ) is a layer normalization method applying to sequential data ( Ba et al , 2016 ) .", "entities": [[5, 7, "MethodName", "layer normalization"]]}
{"text": "Greedy decoding or CRF can be adopted in the decoding process .", "entities": [[3, 4, "MethodName", "CRF"]]}
{"text": "CRF is chosen as our decoding strategy because CRF has the ability to capture the correlations between tokens and labels and the correlations between adjacent labels simultaneously .", "entities": [[0, 1, "MethodName", "CRF"], [8, 9, "MethodName", "CRF"]]}
{"text": "Then , the linear - chain CRF is exploited to calculate the conditional probability of the predicted sequence Y as follows : p ( Y | H l )", "entities": [[6, 7, "MethodName", "CRF"]]}
{"text": "So the loss of a sentence can be calculated by the negative log likelihood as follows : L ( s )", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "By integrating aspect and opinion term extraction ( task 0 ) and TOWE ( task 1 ) into a multi - task architecture , we propose a MT - TSMSA method for AOPE .", "entities": [[5, 7, "TaskName", "term extraction"], [9, 10, "DatasetName", "0"]]}
{"text": "MT - TSMSA can be defined as using a sentence H l and a task i d { 0 , 1 } to calculate the conditional probability p ( Y | H l , i d ) .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "When the task i d equals 0 , it means aspect and opinion term extraction .", "entities": [[6, 7, "DatasetName", "0"], [13, 15, "TaskName", "term extraction"]]}
{"text": "Aiming at handling different tasks , different score functions S 0 ( H l , Y 0 ) and S 1 ( H l , Y 1 ) are defined , where S 0 ( ) and S 1 ( ) have different parameter matrices , Y 0", "entities": [[10, 11, "DatasetName", "0"], [16, 17, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [47, 48, "DatasetName", "0"]]}
{"text": "( Y 0 i { B - ASP , I - ASP , B - OP , I - OP O } ) and Y 1", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "O , [ SEP ] } ) represent the sequential labels of aspect and opinion term extraction , and TOWE , respectively .", "entities": [[15, 17, "TaskName", "term extraction"]]}
{"text": "So the conditional probabilities of the predicted sequences Y 0 and Y 1 can be calculated as follows : p ( Y 0", "entities": [[9, 10, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "| H l , i d = 0 )", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "= exp ( S 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "( H l , Y 0 ) )", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "\u1ef8 Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "all exp ( S 0 ( H l , \u1ef8 ) ) , ( 11 ) p ( Y 1 | H l , i d = 1 ) = exp ( S 1 ( H l , Y 1 ) )", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "where Y 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "all denotes the set of all possible sequential labels of task 0 and Y 1 all represents the set of all possible sequential labels of task 1 .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "The loss of a sentence is also calculated by the negative log likelihood as follows : L ( s , i d )", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "Firstly , a sentence is passed into MT - TSMSA , where aspects are extracted in task 0 .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "Accordingly , the combinations of aspects from task 0 and target - orient opinion terms from task 1 are aspect - opinion pairs .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "Fan et al ( 2019 ) have employed various baselines in TOWE , including Distance - rule ( Hu and Liu , 2004 ) , Dependency - rule ( Zhuang et al , 2006 ) , BiLSTM + Distance - rule , and TC - BiLSTM , except for BERT - based methods .", "entities": [[36, 37, "MethodName", "BiLSTM"], [45, 46, "MethodName", "BiLSTM"], [49, 50, "MethodName", "BERT"]]}
{"text": "To achieve comprehensive comparative analysis , we develop baselines of BERT + Distance - rule and Target - fused BERT ( TF - BERT ) for this task .", "entities": [[10, 11, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"], [23, 24, "MethodName", "BERT"]]}
{"text": "The former trains a sentence - level opinion term extraction model by BERT , and the target - oriented opinion term is the one nearest to each aspect .", "entities": [[8, 10, "TaskName", "term extraction"], [12, 13, "MethodName", "BERT"]]}
{"text": "The latter utilizes the average pooling of target word embeddings to represent the target information .", "entities": [[4, 6, "MethodName", "average pooling"], [8, 10, "TaskName", "word embeddings"]]}
{"text": "The word representation at each position is the addition of word embedding and target information , which is fed into BERT to extract target - oriented opinion terms .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "Besides the above methods , we also employ the following baselines : IOG ( Fan et al , 2019 ) utilizes an Inward - Outward LSTM and a Global LSTM to capture the information of aspects and global information respectively , then it combines these information for sequence labeling .", "entities": [[25, 26, "MethodName", "LSTM"], [29, 30, "MethodName", "LSTM"]]}
{"text": "SpanMlt ) is a span - based multi - task learning framework where the terms are extracted with annotated span boundaries and then the relations between combinations of every two spans are identified .", "entities": [[7, 11, "TaskName", "multi - task learning"]]}
{"text": "SDRN ( Chen et al , 2020 ) utilizes BERT as the encoder which consists of an opinion entity extraction unit , a relation detection unit , and a synchronization unit for the AOPE task .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "For the TOWE task , Fan et al ( 2019 ) utilize 300dimension GloVe ( Pennington et al , 2014 ) vectors which are pre - trained on unlabeled data of 840 billion tokens to initialize word embedding vectors in IOG .", "entities": [[13, 14, "MethodName", "GloVe"]]}
{"text": "The word embeddings are fixed at the stage of training .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "For fair comparison , we use the same fixed word embeddings in TSMSA ( Base ) .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "Pretrained language models like BERT ( Devlin et al , 2019 ) can be applied to our methods , and we adopt BERT - base 3 model , where d model is 768 and the number of attention heads and layers are both 12 .", "entities": [[0, 3, "TaskName", "Pretrained language models"], [4, 5, "MethodName", "BERT"], [22, 23, "MethodName", "BERT"]]}
{"text": "Other hyper - parameters include the learning rate of BERT and CRF , the maximal sequence length , and the number of epochs .", "entities": [[6, 8, "HyperparameterName", "learning rate"], [9, 10, "MethodName", "BERT"], [11, 12, "MethodName", "CRF"], [20, 23, "HyperparameterName", "number of epochs"]]}
{"text": "By utilizing BiL - STM or BERT as the encoder to extract opinion terms , the BiLSTM / BERT + Distance - rule perform much better than other rule - based methods .", "entities": [[6, 7, "MethodName", "BERT"], [16, 17, "MethodName", "BiLSTM"], [18, 19, "MethodName", "BERT"]]}
{"text": "Secondly , TC - BiLSTM and TF - BERT extract static word embeddings for aspects and then incorporate them into sentence representation by concatenation or addition .", "entities": [[4, 5, "MethodName", "BiLSTM"], [8, 9, "MethodName", "BERT"], [11, 13, "TaskName", "word embeddings"]]}
{"text": "Nevertheless , the results of TC - BiLSTM and TF - BERT are still over 10 % lower than IOG / TSMSA ( Base ) and SDRN / TSMSA ( BERT ) , respectively .", "entities": [[7, 8, "MethodName", "BiLSTM"], [11, 12, "MethodName", "BERT"], [30, 31, "MethodName", "BERT"]]}
{"text": "Furthermore , the pre - trained language model BERT can be applied to our basic method .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "SDRN , which also exploits BERT as the encoder , passes the information of the aspect through a synchronization unit and utilizes supervised self - attention to capture this information .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "In average , the performance of SDRN is 2 % lower than TSMSA ( BERT ) .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "As mentioned above , our method can be applied to AOPE by combining TOWE with aspect and opinion term extraction .", "entities": [[18, 20, "TaskName", "term extraction"]]}
{"text": "Moreover , SpanMlt , SDRN , and MT - TSMSA ( BERT ) use powerful pre - trained language models , which have a significant improvement in the performance on AOPE .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "We observe that SDRN and MT - TSMSA ( BERT ) perform better than Span - Mlt , showing that selecting top k spans from candidate spans as pairs might miss some correct pairs .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "Compared to SDRN , MT - TSMSA ( BERT ) performs better on three datasets and nearly the same on four datasets .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "To evaluate the impacts of different word embeddings and training strategies on our models , we conduct ablation experiments by varying the above factors .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Firstly , BERT embedding shows poor performance when compared to Glove .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "We conjecture that BERT embedding needs to cooperate with the pre - trained encoder of BERT to perform better on TOWE .", "entities": [[3, 4, "MethodName", "BERT"], [15, 16, "MethodName", "BERT"]]}
{"text": "Secondly , applying the word embedding and the encoder of BERT without fine - tuning also fails to work on TOWE .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "The reason may be that the encoder of BERT without fine - tuning can not capture the information of the specific aspect with the symbol \" [ SEP ] \" .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "Furthermore , opinion terms extracted from task 0 help to identify the corresponding opinion terms in task 1 , which means that the multi - task structure is able to achieve better results than the single - task structure on TOWE .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "In this part , we apply an open source tool 4 to visualize the attention scores of TSMSA ( BERT ) and describe two attention heads on the tenth layer in Figure 3 ( a ) and ( b ) , where attention scores less than 0.1 and unimportant words are not displayed .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "To further compare our MT - TSMSA ( BERT ) with the best - performing baseline of SDRN , we here conduct a case study by following ( Chen et al , 2020 ) .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "As shown in Table 6 , both SDRN and MT - TSMSA ( BERT ) perform well in extracting aspectopinion pairs from complicated relations .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "For example , the threshold \u03b2 in the relation synchronization mechanism of SDRN will largely affect the results of the model .", "entities": [[5, 6, "HyperparameterName", "\u03b2"]]}
{"text": "The rea - son is that task 0 of MT - TSMSA ( BERT ) fails to extract the aspect term \" log into the system \" .", "entities": [[7, 8, "DatasetName", "0"], [13, 14, "MethodName", "BERT"]]}
{"text": "Moreover , the in - depth reason is that for the aspect term extraction task , the performance of SDRN ( i.e. , 83.67 % , 89.49 % , and 74.05 % ) is better than that of MT - TSMSA ( BERT ) , i.e. , 83.11 % , 84.85 % , and 72.69 % on the datasets from ( Chen et al , 2020 ) .", "entities": [[12, 14, "TaskName", "term extraction"], [42, 43, "MethodName", "BERT"]]}
{"text": "Frame Type Frame Description Issue - Generic Economic Financial implications of an issue Policy Capacity & Resources The availability or lack of time , physical , human , or financial resources Morality & Ethics Perspectives compelled by religion or secular sense of ethics or social responsibility Fairness & Equality The ( in ) equality with which laws , punishments , rewards , resources are distributed Legality , Constitutionality & Jurisdiction Court cases and existing laws that regulate policies ; constitutional interpretation ; legal processes such as seeking asylum or obtaining citizenship ; jurisdiction", "entities": [[33, 34, "DatasetName", "Ethics"], [46, 47, "TaskName", "Fairness"]]}
{"text": "Similarly , topic models have been used to compare \" refugee crisis \" media discourses across the European countries ( Heidenreich et al , 2019 ) , and to uncover differences in attitudes towards migrants ( Hartnett , 2019 ) .", "entities": [[2, 4, "TaskName", "topic models"]]}
{"text": "Although lexicon analysis and topic models can provide insights about immigration discourse , here , we adopt a supervised approach to ground our work in framing research and to enable robust evaluation .", "entities": [[4, 6, "TaskName", "topic models"]]}
{"text": "Using this corpus , prior work has detected frames with techniques including logistic regression ( Card et al , Table 1 : List of all issue - generic policy ( Boydstun et al , 2013 ) , immigration - specific ( Benson , 2013Hovden and Mjelde , 2019 ) , and narrative ( Iyengar , 1991 ) frames with brief descriptions .", "entities": [[12, 14, "MethodName", "logistic regression"]]}
{"text": "2016 ) , recurrent neural networks ( Naderi and Hirst , 2017 ) , lexicon induction ( Field et al , 2018 ) , and fine - tuning pretrained language models ( Khanehzar et al , 2019 ;", "entities": [[28, 31, "TaskName", "pretrained language models"]]}
{"text": "Because a large dataset of unique , singlycoded documents is preferable to a small dataset of documents coded by multiple annotators for text classification ( Barbera et al , 2021 ) , we decided to increase corpus diversity in the training data by singly - annotating , at the expense of potentially noisier annotation , and to consensus code all evaluation data .", "entities": [[22, 24, "TaskName", "text classification"]]}
{"text": "Additional details are provided in Supplementary Material ( B , Figures 6 and 7 ) .", "entities": [[5, 7, "DatasetName", "Supplementary Material"]]}
{"text": "Further details about frame distributions Random LogReg RoBERTa FT RoBERTa 0.193 0.296 0.611 0.657 in our annotations can be found in Supplementary Material ( A , Figure 5 ) .", "entities": [[7, 8, "MethodName", "RoBERTa"], [9, 10, "MethodName", "RoBERTa"], [21, 23, "DatasetName", "Supplementary Material"]]}
{"text": "Experimental Setup Our proposed model is a RoBERTa model ( Liu et al , 2019b ) trained using binary cross - entropy on the CLS token .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}
{"text": "In both models , early stopping is used to avoid overfitting .", "entities": [[4, 6, "MethodName", "early stopping"]]}
{"text": "Models are compared with two baselines : random prediction , and logistic regression with unigram and bigram features .", "entities": [[11, 13, "MethodName", "logistic regression"]]}
{"text": "Each model was trained five times with different random seeds and we report bootstrapped mean performance .", "entities": [[9, 10, "DatasetName", "seeds"]]}
{"text": "Results The fine - tuned RoBERTa model significantly outperforms all baselines ( Table 2 ) .", "entities": [[5, 6, "MethodName", "RoBERTa"]]}
{"text": "RoBERTa has the most substantial gains over logistic regression for low - frequency frames ( Supplementary Material C , Figure 8 ) .", "entities": [[0, 1, "MethodName", "RoBERTa"], [7, 9, "MethodName", "logistic regression"], [15, 17, "DatasetName", "Supplementary Material"]]}
{"text": "Precision , recall , and F1 are calculated as unweighted averages over all frames belonging to each category .", "entities": [[0, 1, "MetricName", "Precision"], [5, 6, "MetricName", "F1"]]}
{"text": "Given some thematic similarities between typologies , we tested an additional model that jointly predicted frames from all three typologies using the fine - tuned RoBERTa model ; however , the resulting model offered worse performance than any single - typology model , suggesting minimal benefits of cross - typology learning .", "entities": [[25, 26, "MethodName", "RoBERTa"]]}
{"text": "Coreference resolution is often not possible and annotators avoided making assumptions to resolve ambiguities .", "entities": [[0, 2, "TaskName", "Coreference resolution"]]}
{"text": "We detect frames for all 2.6 M immigration - related tweets using the finetuned RoBERTa model with the best - performing seed on development data .", "entities": [[14, 15, "MethodName", "RoBERTa"]]}
{"text": "Using this labeled data , we estimate the effects of region and ideology by fitting separate mixed - effects logistic regression models to predict the presence or absence of each frame .", "entities": [[19, 21, "MethodName", "logistic regression"]]}
{"text": "Although we do not focus on abusive language , our topical content contains frequent instances of racism , Islamophobia , antisemitism , and personal insults .", "entities": [[6, 8, "TaskName", "abusive language"]]}
{"text": "Tables 5 - 8 and Figures 8 - 9 provide details about the fine - tuned RoBERTa models ' performance .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}
{"text": "We probe the heterogeneity in levels of abusive language in different sections of the Internet , using an annotated corpus of Wikipedia page edit comments to train a binary classifier for abuse detection .", "entities": [[7, 9, "TaskName", "abusive language"], [31, 33, "TaskName", "abuse detection"]]}
{"text": "Our test data come from the CrimeBB Corpus of hacking - related forum posts and we find that ( a ) forum interactions are rarely abusive , ( b ) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain , and tends to involve aggressive posturing rather than hate speech or threats of violence .", "entities": [[31, 33, "TaskName", "abusive language"], [59, 61, "DatasetName", "hate speech"]]}
{"text": "The automatic identification of abusive language online 1 is of growing interest and concerns have proliferated about aggressive Internet behaviours commonly known as ' trolling ' .", "entities": [[4, 6, "TaskName", "abusive language"]]}
{"text": "From an applications perspective , the accurate detection of vitriolic language is one of the clearest examples of natural language processing for social good , assuming data has been collected ethically and stored legally , and that any intervention is left to the appropriate authorities ( Kennedy et al , 2017 ; Kumar et al , 2018 ) .", "entities": [[52, 53, "DatasetName", "Kumar"]]}
{"text": "The question we address here is whether online abusive language is of one type or whether there is discernible variation in the level of abuse found in different subsections of the Internet .", "entities": [[8, 10, "TaskName", "abusive language"]]}
{"text": "We show that the type of abusive language occurring in the latter is more closely aligned with the milder levels of abuse of those found in Wikipedia discussions , and consider why this might be .", "entities": [[6, 8, "TaskName", "abusive language"]]}
{"text": "Where abusive language is found in the online hacking forum , it tends to involve profane namecalling , insults and heated disputes , rather than hate speech or threats of violence - those which have tended to be the more prominent causes for public concern .", "entities": [[1, 3, "TaskName", "abusive language"], [25, 27, "DatasetName", "hate speech"]]}
{"text": "We also distinguish aggressive language from hate speech - that which might be characterised as prejudicial diatribes to provoke action , perhaps violent , against a group or groups - and from cyberbullying - that which involves a sustained period of persecution against an individual or individuals .", "entities": [[6, 8, "DatasetName", "hate speech"]]}
{"text": "We are dealing with what we deem to be one - off instances of aggression in online communities , though if these were shown to be prejudicial against a group , or sustained against an individual , then the instances start to move into hate speech or cyberbullying behaviours .", "entities": [[44, 46, "DatasetName", "hate speech"]]}
{"text": "Instead we are interested in toxic and abusive behaviour , specifically online harassment involving abusive language , aggression and personal attacks .", "entities": [[14, 16, "TaskName", "abusive language"]]}
{"text": "There has been work on other forms of abusive behaviour , such as hate speech ( Warner and Hirschberg , 2012 ; Kwok and Wang , 2013 ; Ribeiro et al , 2018 ) and cyberbullying ( Xu et al , 2013 ; Pieschl et al , 2015 ) , and we put these aside for now as challenging , distinct topics ( though with the fuzzy edges described above ) .", "entities": [[13, 15, "DatasetName", "hate speech"]]}
{"text": "In terms of online harassment , previous work has centred around definitions , automatic detection , and dataset creation - for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpus ( Waseem and Hovy , 2016 ; Wulczyn et al , 2017 ) .", "entities": [[16, 18, "DatasetName", "and dataset"], [23, 25, "DatasetName", "Hate Speech"]]}
{"text": "Automated detection approaches have drawn on classic document classification methods for spam detection and sentiment analysis , and tend to use lexical and syntactic features ( Nobata et al , 2016 ;", "entities": [[7, 9, "TaskName", "document classification"], [11, 13, "TaskName", "spam detection"], [14, 16, "TaskName", "sentiment analysis"]]}
{"text": "Machine learning techniques range from logistic regression ( Cheng et al , 2015 ) to support vector machines ( Yin et al , 2009 ) to neural networks ( Gamb\u00e4ck and Sikdar , 2017 ) .", "entities": [[5, 7, "MethodName", "logistic regression"]]}
{"text": "In case any persuasion is needed that improved understanding , detection and action on abusive language are desirable , there is evidence that experience of online harassment leads to decreased online participation and is connected with oppression , violence and suicide ( Dinakar et al , 2011 ; Sood et", "entities": [[14, 16, "TaskName", "abusive language"]]}
{"text": "XG - Boost is known to work well with sparse matrices , which is the kind of input associated with textual data , and in NLP terms has been shown to perform competitively in sentiment analysis shared tasks ( Nasim , 2017 ;", "entities": [[34, 36, "TaskName", "sentiment analysis"]]}
{"text": "Recall that we do not compare XGBoost with other classifiers , as our focus is on the training data rather than performance .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "In future work we can investigate other models including neural networks , though logistic regression has in some cases out - performed neural nets in the detection of abusive language ( Park and Fung , 2017 ) .", "entities": [[13, 15, "MethodName", "logistic regression"], [28, 30, "TaskName", "abusive language"]]}
{"text": "Classification accuracies are shown in Table 3 5 .", "entities": [[0, 1, "TaskName", "Classification"]]}
{"text": "It is apparent that in both training data settings - controlled and non - controlled ( ' all ' ) - the accuracy of aggression identification reduces as the true / false cut - off threshold t increases .", "entities": [[22, 23, "MetricName", "accuracy"], [24, 26, "TaskName", "aggression identification"]]}
{"text": "The boxplots show medians ( the thick horizontal bars ) , first and third quar - tiles ( Q1 , Q3 , shown by the hinges ) , and whiskers extending as far as 1.5 * IQR where IQR is the inter - quartile range between Q1 and Q3 .", "entities": [[36, 37, "DatasetName", "IQR"], [38, 39, "DatasetName", "IQR"]]}
{"text": "After that , the minority labels are those which might feature in hate speech : discriminating against women , homosexuals and ethnicities .", "entities": [[12, 14, "DatasetName", "hate speech"]]}
{"text": "We have shown that abusive language in an online hacking forum is relatively mild compared to that found in Wikipedia page edit comments .", "entities": [[4, 6, "TaskName", "abusive language"]]}
{"text": "In future work we evidently need to annotate more data so that we have more than 100 examples of abusive language from CrimeBB .", "entities": [[19, 21, "TaskName", "abusive language"]]}
{"text": "Due to the low hit rate for abusive language in CrimeBB texts ( 100 in 4123 , for instance ) we can investigate automatic annotation of further chunks of the data , along with supervised sampling from those new annotations to check their quality .", "entities": [[7, 9, "TaskName", "abusive language"]]}
{"text": "One option could be to create an alert system for forum moderators , thereby offering real - world impact for our work while allowing the appropriate authorities to take action when necessary ( Kumar et al , 2018 ) .", "entities": [[33, 34, "DatasetName", "Kumar"]]}
{"text": "The ContrastMedium Algorithm : Taxonomy Induction From Noisy Knowledge Graphs With Just a Few Links", "entities": [[8, 10, "TaskName", "Knowledge Graphs"]]}
{"text": "We conduct experiments using automatically acquired knowledge graphs , as well as a SemEval benchmark , and show that our method is able to achieve high performance on the task of taxonomy induction .", "entities": [[6, 8, "TaskName", "knowledge graphs"]]}
{"text": "Web - scale open information extraction systems like NELL ( Carlson et al , 2010 ) or ReVerb have been successful in acquiring massive amounts of machine - readable knowledge by effectively tapping large amounts of text from Web pages .", "entities": [[3, 6, "TaskName", "open information extraction"], [8, 9, "DatasetName", "NELL"], [17, 18, "DatasetName", "ReVerb"]]}
{"text": "Such output , on the other hand , could be provided by the vocabulary of large - scale ontologies like DBpedia ( Bizer et al , 2009 ) or YAGO ( Hoffart et al , 2013 ) and the integration of open and closed information extraction approaches ( Dutta et al , 2014 ) .", "entities": [[20, 21, "DatasetName", "DBpedia"], [29, 30, "DatasetName", "YAGO"]]}
{"text": "That is , no resource , to date , integrates structured information from existing wide - coverage knowledge graphs with empirical evidence from text for the explicit goal of building full - fledged taxonomies consisting of a clean and fully - connected directed acyclic graph ( DAG ) .", "entities": [[17, 19, "TaskName", "knowledge graphs"]]}
{"text": "Recently , much work in Natural Language Processing focused on Knowledge Base Completion ( Nickel et al , 2016a , KBC ) , the task of enriching and refining existing KBs .", "entities": [[10, 13, "TaskName", "Knowledge Base Completion"]]}
{"text": "Finally , Open Information Extraction methods looked at ways to extract large amounts of facts from Web - scale corpora in order to acquire open - domain KBs Faruqui and Kumar , 2015 , inter alia ) ;", "entities": [[2, 5, "TaskName", "Open Information Extraction"], [30, 31, "DatasetName", "Kumar"]]}
{"text": "Here , we argue for the choice of a deterministic approach , like ours , that does not require tuning of parameters : its termination is guaranteed by the number of iterations , which we bind by the maximal diameter | E | for a graph G", "entities": [[29, 32, "HyperparameterName", "number of iterations"]]}
{"text": "We initially define the function C KB : V KB [ 0.0 \u2212 1.0 ] and assign a zero contrast medium level to all the nodes of the KB graph C KB ( x ) = 0 , x V KB ( line 1 ) .", "entities": [[36, 37, "DatasetName", "0"]]}
{"text": "When exiting a node x through out the outgoing edges ( direction = = DOWN ) we increment the level of contrast medium of the reached nodes by the observed value of x divided by number of outgoing edges of x. By converse , when we climb ( direction = = UP ) across the incoming edges of a node x we increment the CM level of the reached node by the observed CM quantity of x divided by the number of incoming edges of x. Note that the sequence UP / DOWN / UP and the specular DOWN / UP / DOWN are the only ones from the 8 possible combinations which can guarantee the contrast medium to flow on the entire graph .", "entities": [[42, 43, "DatasetName", "converse"]]}
{"text": "= s 0 , s 1 , . . .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "We first evaluate our approach when applied to large , automatically induced noisy knowledge graphs ( Section 4.1 ) and then quantify the impact it can have to further improve the quality of the output of state - ofthe - art taxonomy induction systems ( Section 4.2 ) .", "entities": [[13, 15, "TaskName", "knowledge graphs"]]}
{"text": "We first apply ContrastMedium to a variety of knowledge graphs that have been automatically acquired and linked to reference KBs like Word - Net and BabelNet using unsupervised methods ( Section 3.2 ) .", "entities": [[8, 10, "TaskName", "knowledge graphs"]]}
{"text": "These noisy knowledge graphs have been induced from large text corpora and include both taxonomic and other ( i.e. , related , topically associative ) semantic relations ( cf .", "entities": [[2, 4, "TaskName", "knowledge graphs"]]}
{"text": "Table 2 : Dimensions of the four datasets adopted as linked noisy knowledge graphs .", "entities": [[12, 14, "TaskName", "knowledge graphs"]]}
{"text": "In order to have a complete quintuple for each NKG , we selected , for the companion KB , the top KB root concept entity of the WordNet taxonomy ( SynsetID SID - 00001740 - N ) .", "entities": [[31, 32, "DatasetName", "SID"]]}
{"text": "Two annotators with previous experience in knowledge acquisition and engineering were asked to provide for each concept whether it can be classified as : i ) a root , top - level abstract concept - i.e. , any of entity , object , etc . and more in general nodes that correspond to abstract concepts that we can expect to be part of a core ontology such as , for instance , DOLCE ( Gangemi et al , 2002 ) ; ii ) a leaf terminological node ( i.e. , instances such as Lady Gaga or Porsche 911 ) ; iii ) or a middle - level concept ( e.g. , celebrity or cars , concepts not fitting into any of the previous classes ) .", "entities": [[65, 66, "MethodName", "ontology"]]}
{"text": "An adjudication procedure was used to resolve any discrepancy between the two annotators : the inter - annotator agreement after adjudication is \u03ba = 0.657 ( Fleiss , 1971 ) , with most disagreement occurring on the identification of abstract , core ontology concepts .", "entities": [[42, 43, "MethodName", "ontology"]]}
{"text": "Given ground - truth concept granularity judgements , we compute standard accuracy for each of the three classes .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "That is , we compare the system outputs against the gold standards and obtain three accuracy measures : one for the root nodes ( A R ) , one for the nodes ' in the middle ' ( A M ) and finally one for the leaf nodes ( A L ) .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "Error Reduction ( ER ) : finally , we compute the relative error reduction of ContrastMedium against other , baseline approaches as : Baseline | V G", "entities": [[0, 1, "MetricName", "Error"]]}
{"text": "Despite being less aggressive in terms of the number of edges pruned , ContrastMedium outperforms the Tarjan - based algorithm on all datasets in terms of accuracy .", "entities": [[26, 27, "MetricName", "accuracy"]]}
{"text": "We use the benchmark data from the SemEval - 15 task 17 \" Taxonomy Extraction Evaluation : TExEval \" ( Bordea et al , 2015 ) , since it provides us with gold - standard datasets and system outputs within a standard , easy - to - reproduce setting .", "entities": [[2, 4, "DatasetName", "the benchmark"]]}
{"text": "In this paper , we presented ContrastMedium , a novel algorithm that can be applied to automatically linked noisy knowledge graphs to provide an end - to - end solution for fully unsupervised taxonomy induction from scratch , i.e. , without any human effort .", "entities": [[19, 21, "TaskName", "knowledge graphs"]]}
{"text": "Our results indicate that Con - trastMedium can be successfully applied to a wide range of automatically acquired KBs , ranging from large linked noisy knowledge graphs all the way to small - scale induced taxonomies to produce high - quality isa hierarchies that achieve state - ofthe - art results on SemEval benchmarks .", "entities": [[25, 27, "TaskName", "knowledge graphs"]]}
{"text": "Using neural methods in text generation , it is possible to achieve output that is on topic and grammatically ( more or less ) correct .", "entities": [[4, 6, "TaskName", "text generation"]]}
{"text": "In the past times of rule - based text generation , argumentation synthesis was a popular task ( Zukerman et al , 2000 ) .", "entities": [[8, 10, "TaskName", "text generation"]]}
{"text": "To achieve a similar level of output control , today 's text - to - text generation models need to account for the various interdependencies between the text units to be combined .", "entities": [[15, 17, "TaskName", "text generation"]]}
{"text": "Thesis t1 German universities should on no account charge tuition fees t2 the universities in Germany should not under any circumstances charge tuition fees t3 tuition fees should not generally be charged by universities t4 universities should not charge tuition fees in Germany Con c1 one could argue that an increase in tuition fees would allow institutions to be better equipped c2 those who study later decide this early on , anyway c3 to oblige non - academics to finance others ' degrees through taxes is not just c4 unfortunately sponsoring can lead to disagreeable dependencies in some cases Pro p1 education and training are fundamental rights which the state , the society must provide p2 education must not be a question of money in a wealthy society such as Germany p3 fees result in longer durations of studies p4 funding - wise it ought to be considered how costs incurred by students from other ( federal ) states can be reimbursed p5 if a university lacks the funds , sponsors must be found p6 longer durations of studies are costly p7 studying and taking higher degrees must remain a basic right for everyone p8 there are other instruments to motivate tighter discipline while studying p9 this would impede or prevent access to those who are financially weaker p10 this would mean that only those people with wealthy parents or a previous education and a part - time job while studying would be able to apply for a degree programme in the first place p11 universities are for all citizens , independent of their finances p12 what is the good of a wonderfully outfitted university if it does n't actually allow the majority of clever people to broaden their horizons with all that great equipment Topic Should all universities in Germany charge tuition fees ?", "entities": [[72, 73, "DatasetName", "c3"], [88, 89, "DatasetName", "c4"], [131, 132, "DatasetName", "p3"]]}
{"text": "c1 one could argue that an increase in tuition fees would allow institutions to be better equipped , p3 however fees result in longer durations of studies p6 and longer durations of studies are costly .", "entities": [[18, 19, "DatasetName", "p3"]]}
{"text": "To generalize the language model beyond the covered topics , each ADU is represented using features that aim to capture general emotion - related and logic - related characteristics , accounting for the two given strategies .", "entities": [[21, 22, "DatasetName", "emotion"]]}
{"text": "( 3 ) Phrasing regression model : A linear regression model is trained which scores each ADU sequence with respect to its semantic coherence .", "entities": [[8, 10, "MethodName", "linear regression"]]}
{"text": "Affect words , e.g. , positive emotion words .", "entities": [[6, 7, "DatasetName", "emotion"]]}
{"text": "We train a linear regression model where each instance represents the features of one argument .", "entities": [[3, 5, "MethodName", "linear regression"]]}
{"text": "We report the average accuracy across all ten folds for each of the models .", "entities": [[3, 5, "MetricName", "average accuracy"]]}
{"text": "In each training / test experiment for one of the two strategies , we first abstract all ADUs across all strategy - specific topic - stance pairs by extracting the LIWC , NRC , and MPQA features , as described in Section 4.1 .", "entities": [[35, 36, "DatasetName", "MPQA"]]}
{"text": "We obtained a 300 - dimensional word embedding for each word in an ADU using the pre - trained GloVe common - crawl model ( Pennington et al , 2014 ) .", "entities": [[19, 20, "MethodName", "GloVe"]]}
{"text": "Given the ADU 2 - grams , we train a linear regression model that predicts the sum of ADU 2 - gram probabilities in each argument .", "entities": [[10, 12, "MethodName", "linear regression"]]}
{"text": "c3 to oblige non - academics to finance others ' degrees through taxes is not just .", "entities": [[0, 1, "DatasetName", "c3"]]}
{"text": "Matching intu - ition , the pathos argument appeals more to emotion , reflected in phrases such as \" wealthy society \" and \" under any circumstances \" .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "Natural language inference ( NLI ) has been widely used as a task to train and evaluate models for language understanding .", "entities": [[0, 3, "TaskName", "Natural language inference"]]}
{"text": "We use IMPLI to evaluate NLI models based on RoBERTa fine - tuned on the widely used MNLI dataset .", "entities": [[9, 10, "MethodName", "RoBERTa"], [17, 18, "DatasetName", "MNLI"]]}
{"text": "The work was done while the second author was still affiliated with the UKP Lab at TU Darmstadt .", "entities": [[13, 14, "DatasetName", "UKP"]]}
{"text": "Understanding figurative language ( i.e. , that in which the intended meaning of the utterance differs from the literal compositional meaning ) is a particularly difficult area in NLP ( Shutova , 2011 ; Veale et al , 2016 ) , but is essential for proper natural language understanding .", "entities": [[46, 49, "TaskName", "natural language understanding"]]}
{"text": "For instance , sentiment systems struggle with multiword expressions in which individual words do not directly contribute to the sentiment ( Sag et al , 2002 ) .", "entities": [[21, 22, "MethodName", "Sag"]]}
{"text": "Metaphoric understanding has been shown to be necessary for proper machine translation ( Mao et al , 2018 ; Mohammad et al , 2016 ) .", "entities": [[10, 12, "TaskName", "machine translation"]]}
{"text": "Sentiment analysis also relies critically on figurative language : irony and sarcasm can reverse the polarity of a sentence , while metaphors and idioms may make more subtle changes in the speaker meaning ( Ghosh et al , 2015 ) .", "entities": [[0, 2, "TaskName", "Sentiment analysis"]]}
{"text": "Iyer et al , 2019 ) and hate speech detection ( Lemmens et al , 2021 ) .", "entities": [[7, 10, "TaskName", "hate speech detection"]]}
{"text": "Natural language inference is the task of predicting , given two fragments of text , whether the meaning of one ( premise ) entails the other ( hypothesis ) ( Dagan et al , 2006 ) .", "entities": [[0, 3, "TaskName", "Natural language inference"]]}
{"text": "2 These are the MAGPIE Corpus ( Haagsma et al , 2020 ) , the PIE Corpus ( Adewumi et al , 2021 ) , and the SemEval 2013 Task 5 ( Korkontzelos et al , 2013 ) .", "entities": [[15, 16, "DatasetName", "PIE"], [27, 29, "DatasetName", "SemEval 2013"]]}
{"text": "We then take a portion of the Common Crawl dataset 4 , and identify sentences that contain these original MEs .", "entities": [[7, 9, "DatasetName", "Common Crawl"]]}
{"text": "For this , we identified the conjugation in the context , and used a de - lemmatization script to conjugate the replacement verb to match the original .", "entities": [[16, 17, "TaskName", "lemmatization"]]}
{"text": "With regard to syntax , we see S node roots for between 82 % and % 90 of the sentences : within the range of the SNLI performance ( 74 % - 88 % ) , and slightly behind the MNLI ( 91 % - 98 % ) .", "entities": [[26, 27, "DatasetName", "SNLI"], [40, 41, "DatasetName", "MNLI"]]}
{"text": "MNLI MNLI - MM S S l S d G G a G S", "entities": [[0, 1, "DatasetName", "MNLI"], [1, 4, "DatasetName", "MNLI - MM"]]}
{"text": "We obtain baseline NLI models by fine - tuning roberta - base and roberta - large models on the MNLI dataset ( Williams et al , 2018 ) , with entailments as the positive class and all others as the negative and evaluate them on their original test sets as well as IMPLI .", "entities": [[19, 20, "DatasetName", "MNLI"]]}
{"text": "5 Due to variance in neural model performance ( Reimers and Gurevych , 2017 ) , we take the mean score over 5 runs using different seeds .", "entities": [[26, 27, "DatasetName", "seeds"]]}
{"text": "For each task , we split the data into 10 folds by IE and incrementally incorporate these folds into the original MNLI for training , leaving one fold out for testing .", "entities": [[21, 22, "DatasetName", "MNLI"]]}
{"text": "We show that while widely used MNLI models handle entailment admirably and metaphoric expressions are relatively easy , nonentailment idiomatic relationships are more difficult .", "entities": [[6, 7, "DatasetName", "MNLI"]]}
{"text": "Additionally , we only explore this data for evaluating NLI systems : this data could also be used for other parallel data tasks such as figurative language interpretation ( Shutova , 2013 ; Su et al , 2017 ) and figurative paraphrase generation .", "entities": [[41, 43, "TaskName", "paraphrase generation"]]}
{"text": "Our data contains higher overlap than the MNLI data , with the bulk of the density falling on minimally distant pairs .", "entities": [[7, 8, "DatasetName", "MNLI"]]}
{"text": "BioNLP - OST 2019 RDoC Tasks : Multi - grain Neural Relevance Ranking Using Topics and Attention Based Query - Document - Sentence Interactions", "entities": [[2, 3, "DatasetName", "OST"]]}
{"text": "This paper presents our system details and results of participation in the RDoC Tasks of BioNLP - OST 2019 .", "entities": [[17, 18, "DatasetName", "OST"]]}
{"text": "We investigate ( 1 ) attention based supervised neural topic model and SVM for retrieval and ranking of PubMed abstracts and , further utilize BM25 and other relevance measures for re - ranking , ( 2 ) supervised and unsupervised sentence ranking models utilizing multi - view representations comprising of query - aware attention - based sentence representation ( QAR ) , bag - of - words ( BoW ) and TF - IDF .", "entities": [[12, 13, "MethodName", "SVM"]]}
{"text": "Natural Language Processing ( NLP ) techniques such as relation extraction and information retrieval have enabled us to effectively mine relevant information from a large corpus .", "entities": [[9, 11, "TaskName", "relation extraction"], [12, 14, "TaskName", "information retrieval"]]}
{"text": "Information Retrieval ( IR ) is the process of retrieving relevant information from an unstructured text corpus , which satisfies a given query / requirement , for example Google search , email search , database search etc .", "entities": [[0, 2, "TaskName", "Information Retrieval"], [28, 29, "DatasetName", "Google"]]}
{"text": "This external representation can be generated using either statistical approach i.e. , word counts or distributed semantical approach i.e. , word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "RDoC Tasks aims at exploring information retrieval ( IR ) and information extraction ( IE ) tasks on selected abstracts from PubMed dataset .", "entities": [[5, 7, "TaskName", "information retrieval"]]}
{"text": "Test data consists of abstracts without annotation and the goal is to submit a ranked lists of relevant articles for each medical domain RDoC construct .", "entities": [[21, 23, "DatasetName", "medical domain"]]}
{"text": "Our Contributions : Following are our multifold contributions in this paper : ( 1 ) RDoC - IR Task - 1 : We perform document ( or abstract ) ranking in two steps , first using supervised neural topic model and SVM .", "entities": [[41, 42, "MethodName", "SVM"]]}
{"text": "Moreover , we have introduced attentions in supervised neural topic model , along with pre - trained word embeddings from several sources .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "Word embeddings ( Mikolov et al , 2013 ; Pennington et al , 2014 ) have been successfully used in computing distributed representation of text snippets ( short or long ) .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "In ESR scheme , we employ the pre - trained word embeddings from FastText ( Bojanowski et al , 2017 ) and word2vec ( Mikolov et al , 2013 ) .", "entities": [[10, 12, "TaskName", "word embeddings"], [13, 14, "MethodName", "FastText"]]}
{"text": "where , a i , k = e ( q i ) T e ( d k )", "entities": [[5, 7, "HyperparameterName", "k ="]]}
{"text": "| for each kth word in the document", "entities": [[3, 4, "DatasetName", "kth"]]}
{"text": "Topic models ( TMs ) ( Blei et al , 2003 ) have shown to capture thematic structures , i.e. , topics appearing within the document collection .", "entities": [[0, 2, "TaskName", "Topic models"]]}
{"text": "Beyond interpretability , topic models can extract latent document representation that is used to perform document retrieval .", "entities": [[3, 5, "TaskName", "topic models"]]}
{"text": "Recently , Gupta et al ( 2019a ) and Gupta et al ( 2019b ) have shown that the neural network - based topic models ( NTM ) outperform LDA - based topic models ( Blei et al , 2003 ; Srivastava and Sutton , 2017 ) in terms of generalization , interpretability and document retrieval .", "entities": [[23, 25, "TaskName", "topic models"], [29, 30, "MethodName", "LDA"], [32, 34, "TaskName", "topic models"]]}
{"text": "In order to perform document classification and retrieval , we have employed supervised version of neural topic model with extra features and further introduced word - level attention in a neural topic model , i.e. in DocNADE ( Larochelle and Lauly , 2012 ; Gupta et al , 2019a ) .", "entities": [[4, 6, "TaskName", "document classification"]]}
{"text": "j < i W : , v j ) where , f ( ) is a non - linear activation function , W R H\u00d7Z and U R Z\u00d7H are encoding and decoding matrices , c R H and b R Z are encoding and decoding biases , H is the number of units in latent representation", "entities": [[19, 21, "HyperparameterName", "activation function"], [51, 54, "HyperparameterName", "number of units"]]}
{"text": "For a document v , the log - likelihood L ( v ) and latent representation h ( v ) are given as , L unsup ( v ) =", "entities": [[6, 9, "MetricName", "log - likelihood"]]}
{"text": "Here , we extend the unsupervised version to DocNADE with a hybrid cost L hybrid ( v ) , consisting of a ( supervised ) discriminative training cost p ( y = q | v ) along with an unsupervised generative cost p ( v ) for a given query q and associated document v : L hybrid ( v ) = L sup ( v ) + \u03bb L unsup ( v ) ( 4 ) where \u03bb [ 0 , 1 ] .", "entities": [[80, 81, "DatasetName", "0"]]}
{"text": "p ( y = q | v ) = softmax ( d + S h ( v ) )", "entities": [[9, 10, "MethodName", "softmax"]]}
{"text": "\u03b1", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "Additionally , we incorporate extra word features , such as pre - trained word embeddings from several sources : FastText ( E f ast )", "entities": [[13, 15, "TaskName", "word embeddings"], [19, 20, "MethodName", "FastText"]]}
{"text": "\u03b1 i ( E f ast : ,", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "v i + E word2vec : , v i ) ( 6 p ( q | v ) = softmax ( d + S concat ( h ( v ) , h e ( v ) ) )", "entities": [[19, 20, "MethodName", "softmax"]]}
{"text": "In doing so , we have employed supervised classifiers , especially supervised neural topic model a - supDocNADE ( section 3.2 ) for document ranking .", "entities": [[23, 25, "TaskName", "document ranking"]]}
{"text": "As demonstrated in Figure 1 ( right ) , we perform document ranking in two steps : ( 1 ) Document Relevance Ranking : We build a supervised classifier using all the training documents and their corresponding labels ( RDoC constructs ) , provided with the training set .", "entities": [[11, 13, "TaskName", "document ranking"]]}
{"text": "At the test time , we compute prediction probability score p ( CID = q |", "entities": [[12, 13, "DatasetName", "CID"]]}
{"text": "v test ( CID ) ) ) of the label = CID for each test document v test ( CID ) in the cluster , CID .", "entities": [[3, 4, "DatasetName", "CID"], [11, 12, "DatasetName", "CID"], [19, 20, "DatasetName", "CID"], [25, 26, "DatasetName", "CID"]]}
{"text": "Observe that a test document with least confidence for a cluster are ranked lower within the cluster and thus , improving mean average precision ( mAP ) .", "entities": [[22, 24, "MetricName", "average precision"], [25, 26, "MetricName", "mAP"]]}
{"text": "Beyond unsupervised ranking , we further investigate sentence ranking in supervised paradigm by introducing a distance metric between the query ( or title ) and sentence vectors .", "entities": [[15, 17, "HyperparameterName", "distance metric"]]}
{"text": "Here , the operator \u2297 performs concatenation of the projected vector with its input via residual connection .", "entities": [[15, 17, "MethodName", "residual connection"]]}
{"text": "Next , we apply a Manhattan distance metric to compute similarity ( or relevance ) scores , following : r sup = exp \u2212 | | ( \u03a6 p q ( s j ) , q p ) + \u03b2 ( \u03a6 p q ( s j ) , t p )", "entities": [[6, 8, "HyperparameterName", "distance metric"], [39, 40, "HyperparameterName", "\u03b2"]]}
{"text": "A final relevance score r sup f [ 0 , 1 ] is computed by feeding a vector [ r sup , r sup siamese , BM25 - extra ] into a supervised linear regression , which is trained end - to - end by minimizing mean squared error between the r sup f and { 0 , 1 } , i.e. , 1 when the sentence s j is relevant to query", "entities": [[8, 9, "DatasetName", "0"], [33, 35, "MethodName", "linear regression"], [56, 57, "DatasetName", "0"]]}
{"text": "Here , r sup siamese refers to a relevance Best mAP score for each model is marked in bold .", "entities": [[10, 11, "MetricName", "mAP"]]}
{"text": "( reRank # 1 : \" reRank ( BM25 - Extra ) \" ; reRank # 2 : \" reRank ( QAR ) \" ; reRank # 3 : \" reRank ( BM25 - Extra ) + reRank ( QAR ) \" ) score computed between q and s j via Siamese - LSTM .", "entities": [[53, 54, "MethodName", "LSTM"]]}
{"text": "Experimental Setup : As the training dataset labelled with RDoC constructs is very small , we use an external source of semantical knowledge by incorporating pretrained distributional word embeddings ( Zhang et al , 2019 ) from FastText model ( Bojanowski et al , 2017 ) trained on the entire corpus of PubMed and MIMIC III Clinical notes ( Johnson et al , 2016 ) .", "entities": [[27, 29, "TaskName", "word embeddings"], [37, 38, "MethodName", "FastText"]]}
{"text": "Similarly , we also use pretrained word embeddings ( Moen and Ananiadou , 2013 ) from word2vec model ( Mikolov et al , 2013 ) trained on PubMed and PMC abstracts .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "We use pretrained FastText embeddings of size 300 and pretrained word2vec embeddings of size 200 .", "entities": [[3, 4, "MethodName", "FastText"]]}
{"text": "For SVM , we use Bag - of - words ( BoW ) representation of abstracts with radial basis kernel function .", "entities": [[1, 2, "MethodName", "SVM"]]}
{"text": "RDoC Task - 2 : We use pretrained FastText embeddings to compute query - aware sentence representation of a sentence ( \u03a6 q ( s j ) ) , title ( t ) and query ( q ) representations .", "entities": [[8, 9, "MethodName", "FastText"]]}
{"text": "We also train Replicated - Siamese - LSTM model with input as sentence and query pair i.e. , ( s j , q ) and label as 1 if s j is relevant otherwise 0 .", "entities": [[7, 8, "MethodName", "LSTM"], [34, 35, "DatasetName", "0"]]}
{"text": "Table 3 shows the performance of supervised Document Ranker models i.e , a - supDocNADE and SVM , for Task - 1 .", "entities": [[16, 17, "MethodName", "SVM"]]}
{"text": "It is to be noted that the ranking mAP of the clusters using prediction probabilities is already the best possible i.e. , the intruder abstracts ( abstracts with different label ( RDoC construct ) than the cluster label ) are at the bottom of the ranked clusters .", "entities": [[8, 9, "MetricName", "mAP"]]}
{"text": "Similarly , we train a - supDocNADE model with three different settings : ( 1 ) random weight initialization , ( 2 ) incorporating FastText embeddings ( h e ( v ) ) and ( 3 ) incorporating Fast - Text and word2vec embeddings ( h e ( v ) ) .", "entities": [[24, 25, "MethodName", "FastText"]]}
{"text": "When this cluster is ranked on the basis of predic - tion probabilities ( p ( q | v ) ) , then \" Loss \" abstract is ranked third from the bottom and it degrades the mAP score of the retrieval system .", "entities": [[37, 38, "MetricName", "mAP"]]}
{"text": "But after re - ranking this cluster using reRank ( BM25 - Extra ) relevance score , the \" Loss \" abstract is ranked at the bottom , thus maximizing the mAP score .", "entities": [[31, 32, "MetricName", "mAP"]]}
{"text": "Therefore , re - ranking with BM25 - Extra on top of ranking with p ( q | v ) is , evidently , a robust abstract / document ranking technique .", "entities": [[28, 30, "TaskName", "document ranking"]]}
{"text": "Higher accuracy of version1 model suggests that title ( t ) of an abstract also contains the essential information regarding the most relevant sentence .", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "= 0 & 1 in supervised relevance score ( r sup f ) equation in section 3.5.2 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Notice that all the proposed supervised and unsupervised sentence ranking mod - - - We found that nurses experience a grieving process similar to those directly suffering from perinatal loss .", "entities": [[29, 30, "MetricName", "loss"]]}
{"text": "In conclusion , both supervised neural topic model and SVM can effectively perform ranking of PubMed abstracts in a given cluster based on the prediction probabilities .", "entities": [[9, 10, "MethodName", "SVM"]]}
{"text": "However , a further reranking using BM25 - Extra or query - aware sentence representation ( QAR ) has proven to maximize the mAP score by correctly assigning the lowest relevance score to the intruder abstracts .", "entities": [[23, 24, "MetricName", "mAP"]]}
{"text": "In future , we would like to introduce complementary feature representation via hidden vectors of LSTM jointly with topic models and would like to further investigate the interpretability ( Gupta et al , 2015 ; of the proposed neural ranking models in the sense that one can extract salient patterns determining relationship between query and text .", "entities": [[15, 16, "MethodName", "LSTM"], [18, 20, "TaskName", "topic models"]]}
{"text": "Another promising direction would be introduce abstract information , such as part - of - speech and named entity tags ( Lample et al , 2016 ;", "entities": [[11, 14, "DatasetName", "part - of"]]}
{"text": "Gupta et al , 2016 ) to augment information retrieval ( IR ) .", "entities": [[8, 10, "TaskName", "information retrieval"]]}
{"text": "The Linguistic Annotation Workshop ( LAW ) is organized annually by the Association for Computational Linguistics ' Special Interest Group for Annotation ( ACL SIGANN ) .", "entities": [[5, 6, "DatasetName", "LAW"]]}
{"text": "These proceedings include papers that were presented at LAW XIII , held in conjunction with the annual meeting of the Association for Computational Linguistics ( ACL ) in Florence , Italy , on August 1 , 2019 .", "entities": [[8, 9, "DatasetName", "LAW"], [28, 29, "MethodName", "Florence"]]}
{"text": "Since then , the LAW has been held every year , consistently drawing substantial participation ( both in terms of paper / poster submissions and participation in the actual workshop ) providing evidence that the LAW 's overall focus continues to be an important area of interest in the field .", "entities": [[4, 5, "DatasetName", "LAW"], [35, 36, "DatasetName", "LAW"]]}
{"text": "This year 's LAW has received 52 submissions , out of which 28 papers have been accepted to be presented at the workshop , 10 as talks and 18 as posters .", "entities": [[3, 4, "DatasetName", "LAW"]]}
{"text": "In addition to oral and poster paper presentations , LAW XIII also features an invited talk by Rebecca Passonneau and a discussion session .", "entities": [[9, 10, "DatasetName", "LAW"]]}
{"text": "Our thanks go to SIGANN , our organizing committee , for its continuing organization of the LAW workshops , and to the ACL 2019 workshop chairs for their support .", "entities": [[16, 17, "DatasetName", "LAW"]]}
{"text": "Also , we thank Jet Hoek , the LAW XIII publications chair , for her invaluable help with these proceedings .", "entities": [[8, 9, "DatasetName", "LAW"]]}
{"text": "This vector is passed to an RNN , such as a long shortterm memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) or gated recurrent unit ( GRU ) ( Chung et al , 2014 ) .", "entities": [[15, 16, "MethodName", "LSTM"], [25, 28, "MethodName", "gated recurrent unit"], [29, 30, "MethodName", "GRU"]]}
{"text": "The RNN computes a hidden state ( vector ) , which is retained for the next timestep ( step 8 ) , and passed to a dense layer with a softmax activation , with output dimension equal to the number of distinct system action templates ( step 9 ) .", "entities": [[30, 31, "MethodName", "softmax"]]}
{"text": "This dataset includes two end - to - end dialog learning tasks , in the restaurant domain , called task5 and task6 .", "entities": [[9, 11, "TaskName", "dialog learning"]]}
{"text": "Utterance embeddings were formed by averaging word embeddings , using a publicly available 300dimensional word embedding model trained using word2vec on web data ( Mikolov et al , 2013 ) .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "The word embeddings were static and not updated during LSTM training .", "entities": [[1, 3, "TaskName", "word embeddings"], [9, 10, "MethodName", "LSTM"]]}
{"text": "The training loss was categorical cross - entropy .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "Following past work , we report average turn accuracy - i.e. , for each turn in each dialog , present the ( true ) history of user and system actions to the network and obtain the network 's prediction as a string of characters .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "We also report dialog accuracy , which indicates if all turns in a dialog are correct .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "The addition of domain knowledge greatly simplifies the learning task and enables HCNs to also attain perfect accuracy .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "To evaluate , we observe that conventional measures like average dialog accuracy unfairly penalize the system used to collect the dialogs - in our case , the rule - based system .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "In other words , in our case , reporting dialog accuracy would favor the HCN because it would be evaluated on fewer turns than the rule - based system .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "When \u2206P > 0 , there are more dialogs in which HCNs produce longer continuous sequences of correct actions starting from the beginning of the dialog .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "In the previous sections , supervised learning ( SL ) was applied to train the LSTM to mimic dialogs provided by the system developer .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "With RL , each turn receives a measurement of goodness called a reward ; the agent explores different sequences of actions in different situations , and makes adjustments so as to maximize the expected discounted sum of rewards , which is called the return , denoted G. For optimization , we selected a policy gradient approach ( Williams , 1992 ) , which has been successfully applied to dialog systems ( Jur\u010d\u00ed\u010dek et al , 2011 ) , robotics ( Kohl and Stone , 2004 ) , and the board game Go ( Silver et al , 2016 ) .", "entities": [[15, 16, "DatasetName", "agent"]]}
{"text": "We defined the reward as being 1 for successfully completing the task , and 0 otherwise .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "A discount of 0.95 was used to incentivize the system to complete dialogs faster rather than slower , yielding return 0 for failed dialogs , and G = 0.95 T \u22121 for successful dialogs , where T is the number of system turns in the dialog .", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "For the RNN in the HCN , we again used an LSTM with AdaDelta , this time with 32 hidden units .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "AdaDelta"]]}
{"text": "We first evaluate RL by randomly initializing an LSTM , and begin RL optimization .", "entities": [[8, 9, "MethodName", "LSTM"]]}
{"text": "In addition , we also report results by initializing the LSTM using supervised learning on the training set , consisting of 1 , 2 , 5 , or 10 dialogs sampled randomly from the training set , then running RL as described above .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "Finally , we conduct a further experiment where we sample 10 training dialogs , then add one to the training set just before RL dialog 0 , 100 , 200 , ... , 900 .", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "In future work , we plan to extend HCNs by incorporating lines of existing work , such as integrating the entity extraction step into the neural network ( Dhingra et al , 2017 ) , adding richer utterance embeddings ( Socher et al , 2013 ) , and supporting text generation ( Sordoni et al , 2015 ) .", "entities": [[49, 51, "TaskName", "text generation"]]}
{"text": "We will also explore using HCNs with automatic speech recognition ( ASR ) input , for example by forming features from n - grams of the ASR n - best results ( Henderson et al , 2014b ) .", "entities": [[7, 10, "TaskName", "automatic speech recognition"]]}
{"text": "Discourse Self - Attention for Discourse Element Identification in Argumentative Student Essays", "entities": [[11, 12, "DatasetName", "Essays"]]}
{"text": "DEI can benefit automated essay scoring in many aspects : modeling organization , inferring topics and opinions or used as features for scoring systems ( Attali and Burstein , 2006 ; Burstein et al , 2001 ; Persing et al , 2010 ; Song et al , 2020 ) .", "entities": [[3, 6, "TaskName", "automated essay scoring"]]}
{"text": "Classification based methods cast DEI as a classification problem .", "entities": [[0, 1, "TaskName", "Classification"]]}
{"text": "Various classifiers have been tested , such as SVM ( Stab and Gurevych , 2014 ) , decision trees ( Burstein et al , 2003 ( Burstein et al , , 2001 and naive Bayes , maximum entropy model ( Moens et al , 2007 ; Palau and Moens , 2009 ) .", "entities": [[8, 9, "MethodName", "SVM"]]}
{"text": "Feature engineering .", "entities": [[0, 2, "TaskName", "Feature engineering"]]}
{"text": "proposed neural argumentation mining models based on sequence tagging or dependency parsing .", "entities": [[10, 12, "TaskName", "dependency parsing"]]}
{"text": "exploited CNN and LSTM for classifying sentences to identify claims from different domains .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "Mihaylov and Frank ( 2019 ) proposed a discourse - aware selfattention encoder for reading comprehension on narrative texts , where event chains , discourse relations and coreference relations are used for connecting sentences .", "entities": [[14, 16, "TaskName", "reading comprehension"]]}
{"text": "We use Hierarchical BiLSTM ( HBiLSTM ) , which is similar to ( Yang et al , 2016 ) , as the base model to model sentence and discourse level representations .", "entities": [[3, 4, "MethodName", "BiLSTM"]]}
{"text": "In this work , Long Short - Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) is used as the RNN unit and the sequence is encoded in a Bidirectional way that a hidden state h", "entities": [[4, 9, "MethodName", "Long Short - Term Memory"], [10, 11, "MethodName", "LSTM"]]}
{"text": "The summarization function \u03c6 ( ) could be based on the attention mechanism .", "entities": [[1, 2, "TaskName", "summarization"]]}
{"text": "In the discourse element layer , we feed sentence representations C = ( c 1 , ... , c n ) R d\u00d7n to a BiL - STM and use a nonlinear layer to map semantic representations to discourse element representations , D = tanh ( BiLSTM ( C ) ) .", "entities": [[46, 47, "MethodName", "BiLSTM"]]}
{"text": "Finally , we use a linear and a softmax layer to predict the discourse element of every sentence , Y = softmax ( linear ( D ) ) , ( 3 )", "entities": [[8, 9, "MethodName", "softmax"], [21, 22, "MethodName", "softmax"]]}
{"text": "The sentences in an essay are converted to sentence embeddings C through the BiLSTM encoder introduced in Section 3.1 , which are used as the input of DiSA .", "entities": [[8, 10, "TaskName", "sentence embeddings"], [13, 14, "MethodName", "BiLSTM"]]}
{"text": "The attention vectors and element representations are concatenated and fed to a linear layer and a softmax layer for prediction .", "entities": [[12, 14, "MethodName", "linear layer"], [16, 17, "MethodName", "softmax"]]}
{"text": "\u03b2 t pos t", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "i = tanh ( BiLSTM ( C i", "entities": [[4, 5, "MethodName", "BiLSTM"]]}
{"text": "An attention function is to map a query and a set of key - value pairs to an output .", "entities": [[1, 3, "HyperparameterName", "attention function"]]}
{"text": "The attention is computed as \u03b1 = Attn ( Q , K ) = softmax ( QK T \u221a d k ) .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [14, 15, "MethodName", "softmax"]]}
{"text": "K = EW K , where W Q , W K R d\u00d7d", "entities": [[0, 2, "HyperparameterName", "K ="]]}
{"text": "Instead , we use \u03b1 e = tanh ( QK T", "entities": [[4, 5, "HyperparameterName", "\u03b1"]]}
{"text": "Similarly to ElemSA , we use the sentence semantic representations C to compute the ContSA vector \u03b1 c .", "entities": [[16, 17, "HyperparameterName", "\u03b1"]]}
{"text": "To have a fixed - length attention vector , we borrow the idea of spatial pyramid pooling from image processing ( He et al , 2015 ) .", "entities": [[14, 17, "MethodName", "spatial pyramid pooling"]]}
{"text": "It can maintain relatedness information by maxpooling \u03b1 e and \u03b1 c in local bins .", "entities": [[7, 8, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b1"]]}
{"text": "We also use the English student essay dataset released by .", "entities": [[5, 7, "DatasetName", "student essay"]]}
{"text": "The Tencent pre - trained word embeddings ( Song et al , 2018 ) were used for experiments on the Chinese dataset .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "The dimension of word embeddings is 200 .", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "The dimension of all the BiLSTM hidden layers is 256 on Chinese dataset , and 128 on English dataset .", "entities": [[5, 6, "MethodName", "BiLSTM"]]}
{"text": "We use accuracy ( Acc . ) and macro - F1 as evaluation metrics .", "entities": [[2, 3, "MetricName", "accuracy"], [4, 5, "MetricName", "Acc"], [8, 11, "MetricName", "macro - F1"]]}
{"text": "We adapt features from previous feature - based methods ( Burstein et al , 2003 ; Stab and Gurevych , 2014 ; Song et al , 2015 ) to build a feature - based CRF model .", "entities": [[34, 35, "MethodName", "CRF"]]}
{"text": "The baseline described in Section 3 uses two BiLSTM layers to encode word sequences and sentences .", "entities": [[8, 9, "MethodName", "BiLSTM"]]}
{"text": "BERT .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "We fine - tune BERT on training data to train a sentence classifier , because the lengths of many Chinese essays exceed the length constraint of BERT and it is expensive to train BERT - like models at discourse level .", "entities": [[4, 5, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"], [33, 34, "MethodName", "BERT"]]}
{"text": "HBiL - STM has a low macro - F1 score , indicating that it has difficulties in identifying particular discourse elements .", "entities": [[6, 9, "MetricName", "macro - F1"]]}
{"text": "The performance of BERT is worse than HBiLSTM .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "This verifies that sequence modeling is more proper than single sentence classification for this task .", "entities": [[10, 12, "TaskName", "sentence classification"]]}
{"text": "Sinusoidal indicates the sinusoidal positional encoding which is used in Transformer ( Vaswani et al , 2017 ) .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "Perhaps recurrent neural networks such as LSTM naturally capture sequential positional information .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "We can see that both ElemSA and ContSA can make contributions , and ElemSA seems to have a larger effect on macro - F1 score .", "entities": [[21, 24, "MetricName", "macro - F1"]]}
{"text": "DiSA outperforms the baselines with a large margin on both accuracy and macro - F1 .", "entities": [[10, 11, "MetricName", "accuracy"], [12, 15, "MetricName", "macro - F1"]]}
{"text": "Less is More : Attention Supervision with Counterfactuals for Text Classification", "entities": [[9, 11, "TaskName", "Text Classification"]]}
{"text": "Our empirical results show that this machine - augmented human attention supervision is more effective than existing methods requiring a higher annotation cost , in text classification tasks , including sentiment analysis and news categorization .", "entities": [[25, 27, "TaskName", "text classification"], [30, 32, "TaskName", "sentiment analysis"]]}
{"text": "We evaluate SANA on three popular datasets , SST2 , IMDB , and 20NG .", "entities": [[8, 9, "DatasetName", "SST2"], [10, 11, "DatasetName", "IMDB"]]}
{"text": "In all of the text classification datasets , SANA achieves significant improvements over baselines , using unsupervised attention or supervised with task - or sample - level human annotations , in the following four dimensions : Models supervised by SANA predict more accurately , explain causality of attention better , and are more robust over adversarial attacks , and more tolerant of the scarcity of training samples .", "entities": [[4, 6, "TaskName", "text classification"]]}
{"text": "Text classification assumes a dataset D = {", "entities": [[0, 2, "TaskName", "Text classification"]]}
{"text": "Formally , f \u03c6 : x ( h , \u03b1 ) , where attention weights\u03b1 = { \u03b1 t } T t=1 indicate a probability distribution over the hidden states ( Zou et", "entities": [[9, 10, "HyperparameterName", "\u03b1"], [17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "Finally , the hidden representations are fed into a function g \u03b8 : ( h , \u03b1 ) \u0177 with learnable parameters \u03b8 and a softmax layer that predicts the probabilities\u0177 over classes :", "entities": [[11, 12, "HyperparameterName", "\u03b8"], [16, 17, "HyperparameterName", "\u03b1"], [22, 23, "HyperparameterName", "\u03b8"], [25, 26, "MethodName", "softmax"]]}
{"text": "whereh = h t h\u03b1t h t and Softmax ( z i )", "entities": [[8, 9, "MethodName", "Softmax"]]}
{"text": "The parameters \u03c6 and \u03b8 are trained to minimize the cross - entropy loss L task ( \u0177 , y ) between the predicted label\u0177 and the ground - truth label y.", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [13, 14, "MetricName", "loss"]]}
{"text": "Given an input sample x , let \u03b1 and\u03b1 be the attention labels ( provided by human annotators ) and the trained attention weights .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "Then , the loss for attention supervision is defined as the cross - entropy loss L att ( \u03b1 , \u03b1 ) between\u03b1 and \u03b1 .", "entities": [[3, 4, "MetricName", "loss"], [14, 15, "MetricName", "loss"], [18, 19, "HyperparameterName", "\u03b1"], [20, 21, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b1"]]}
{"text": "Formally , given an input sample x and its class label y , let A { 0 , 1 } T be a binary vector of selecting words in x , i.e. , \u2200w t x : A ( w t ) { 0 , 1 } .", "entities": [[16, 17, "DatasetName", "0"], [43, 44, "DatasetName", "0"]]}
{"text": "Then , we convert the attention annotation A into a soft distribution of target attention labels \u03b1 using softmax : \u03b1 t = exp ( \u03bb A ( w t ) )", "entities": [[16, 17, "HyperparameterName", "\u03b1"], [18, 19, "MethodName", "softmax"], [20, 21, "HyperparameterName", "\u03b1"]]}
{"text": "A = [ 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] or A = [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ] .", "entities": [[15, 16, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [21, 22, "DatasetName", "0"], [23, 24, "DatasetName", "0"], [29, 30, "DatasetName", "0"], [31, 32, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [35, 36, "DatasetName", "0"], [37, 38, "DatasetName", "0"], [41, 42, "DatasetName", "0"], [43, 44, "DatasetName", "0"], [45, 46, "DatasetName", "0"], [47, 48, "DatasetName", "0"], [49, 50, "DatasetName", "0"]]}
{"text": "Assuming the existence of the vocabulary V , the vocab - level annotation A task { 0 , 1 } | V | is a binary vector of the hard selection for words in V , i.e. , \u2200w t V : A task ( w t ) { 0 , 1 } .", "entities": [[16, 17, "DatasetName", "0"], [49, 50, "DatasetName", "0"]]}
{"text": "Formally , let\u03b1 and\u1fb1 be the original and counterfactual attention weights , respectively , and let y and\u0233 t be the original prediction and its counterfactual prediction with attention change ( i.e. , from \u03b1 t to\u1fb1 t ) on w t x , respectively .", "entities": [[34, 35, "HyperparameterName", "\u03b1"]]}
{"text": "then \u03b1 Counterfactuals ( \u03b1 , w t )", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [4, 5, "HyperparameterName", "\u03b1"]]}
{"text": "y t g \u03b8 ( h , \u1fb1 )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b3", "entities": [[0, 1, "HyperparameterName", "\u03b3"]]}
{"text": "\u03b3 \u22121", "entities": [[0, 1, "HyperparameterName", "\u03b3"]]}
{"text": "Based on \u03c6 and \u03b8 , we run the classification inference several times for an input sample : one for obtaining the original attention weights\u03b1 and the others for counterfactual attention weights \u03b1 .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b1"]]}
{"text": "For example , sentiment lexicon ( Esuli and Sebastiani , 2006 ) consists of sentiment words , which are important to the sentiment classification task , and named - entity recognizer ( NER ) ( Peters et al , 2017 ) can collect entity words commonly attended in news categorization task .", "entities": [[32, 33, "TaskName", "NER"]]}
{"text": "We empirically show that both lexicon and NER can be adequate substitutes for the manual task - level annotation .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "In an extreme scenario without any human annotator and public resources , inspired by self knowledge distillation ( Furlanello et al , 2018 ) , we report results for using the attention weights of the unsupervised model as a supervision .", "entities": [[15, 17, "MethodName", "knowledge distillation"]]}
{"text": "Note , however , this is highly unlikely in practice , but reported as a lower bound accuracy , when unsupervised attention noise is propagated through distillation supervision .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "To validate the effectiveness of SANA , we use the following three text classification datasets , which are widely used Jain and Wallace , 2019 ) and statistically diverse as well .", "entities": [[12, 14, "TaskName", "text classification"]]}
{"text": "We expect SANA in two - sentence tasks , such as SNLI and MPQA , would be promising , which we leave as future work .", "entities": [[11, 12, "DatasetName", "SNLI"], [13, 14, "DatasetName", "MPQA"]]}
{"text": "SST2 ( Socher et al , 2013 ) :", "entities": [[0, 1, "DatasetName", "SST2"]]}
{"text": "IMDB ( Maas et al , 2011 ) : IMDB Large Movie Review Corpus is a binary sentiment classification dataset containing 50 K polarized ( positive or negative ) movie reviews , split into half for training and testing .", "entities": [[0, 1, "DatasetName", "IMDB"], [9, 10, "DatasetName", "IMDB"]]}
{"text": "20NG : 20 Newsgroups 2 contains around 19 K documents evenly categorized into 20 different categories .", "entities": [[2, 4, "DatasetName", "20 Newsgroups"]]}
{"text": "Following ( Jain and Wallace , 2019 ) , we extract samples belonging to baseball and hockey classes , which we designate as 0 and 1 , deriving a binary classification task ( Hockey vs Baseball ) .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "For all datasets , we use skip - gram ( Mikolov et al , 2013 ) ( official GoogleNews - vectors - negative300 ) word embeddings with 300 dimensions .", "entities": [[24, 26, "TaskName", "word embeddings"]]}
{"text": "For attention mechanism , the size of trainable context vector is set to 100 for SST2 and 300 for IMDB and 20NG .", "entities": [[15, 16, "DatasetName", "SST2"], [19, 20, "DatasetName", "IMDB"]]}
{"text": "For attention supervision , we use the balancing coefficient \u00b5 = 1.0 for SST2 and IMDB , and \u00b5 = 2.0 for 20NG .", "entities": [[13, 14, "DatasetName", "SST2"], [15, 16, "DatasetName", "IMDB"]]}
{"text": "In our experiments , the decay ratio is not significantly correlated with the final accuracy , but correlated more with the convergence period .", "entities": [[14, 15, "MetricName", "accuracy"]]}
{"text": "Note that learning time longer than our setting does not contribute to improving the model accuracy .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "For task - level annotations ( e.g. , in SANA ) , we adopt pre - annotated task - level annotations without any additional human efforts : for the two sentiment tasks , we use SentiWordNet ( Esuli and Sebastiani , 2006 ) , and for 20NG task , we use entities recognized by AllenNLP NER ( Peters et al , 2017 ) .", "entities": [[55, 56, "TaskName", "NER"]]}
{"text": "We thus present the empirical findings for the following four research questions : RQ1 : Does SANA improve model accuracy ?", "entities": [[19, 20, "MetricName", "accuracy"]]}
{"text": "The main objective of this work is to improve attention supervisions for the purpose of better text classification .", "entities": [[16, 18, "TaskName", "text classification"]]}
{"text": "More specifically , 1 ) SANA achieves 84.35 % in SST2 dataset which is higher than the distillation only model , but lower than task - level supervised model .", "entities": [[10, 11, "DatasetName", "SST2"]]}
{"text": "Our key contribution is to show zero - cost attention supervision can improve a simple model closer to a highly sophisticated model , such as BERT ( Devlin et al , 2019 ) requiring more layers and data .", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "This motivates us to supervise attention for BERT , though understanding of BERT internals , such as ( Rogers et al , 2020 ) , is mostly observational at this stage - Intervening with attention would be an interesting future work .", "entities": [[7, 8, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"]]}
{"text": "Our experimental results show that SANA works well in diverse scenarios , but we observe that the effectiveness is reduced when the length of target text increases ( Figure 2 ) or token identifiability decreases ( e.g. , complex architecture ) : SANA more effectively works when the token identifiability is improved ( by adding residual connection between two recurrent layers ) , achieving 0.83 point gain from 89.14 % , which is larger gap than 0.47 point gain without residual connection .", "entities": [[55, 57, "MethodName", "residual connection"], [80, 82, "MethodName", "residual connection"]]}
{"text": "This experiment consists of the following steps : First , based on the original training data , we set a basic BiGRU model ( without attention mechanism ) as threat model , which an adversarial attack method aims to deceive .", "entities": [[21, 22, "MethodName", "BiGRU"], [34, 36, "TaskName", "adversarial attack"]]}
{"text": "Finally , we report the accuracy of the three attention models over both adversarial examples and their corresponding original samples , respectively .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "In the table , we can find that SANA is more robust , showing the smallest gap of the classification accuracy between the original and adversarial samples .", "entities": [[20, 21, "MetricName", "accuracy"]]}
{"text": "This section compares models over the varying amount of training samples in IMDB dataset , as a stress test for data - scarce scenarios .", "entities": [[12, 13, "DatasetName", "IMDB"]]}
{"text": "First , we randomly select 500 training samples from IMDB dataset , and ask the worker to underline the apparent rationales for the sentiment class , guided by the definition of rationale in Zhang et al ( 2016 ) .", "entities": [[9, 10, "DatasetName", "IMDB"]]}
{"text": "Existing metrics for explainability measure whether attention correlates with ( a ) class prediction or ( b ) feature importance , discussed in the next sections respectively .", "entities": [[18, 20, "TaskName", "feature importance"]]}
{"text": "In this figure , we can observe that SANA has the lowest frequency on T V D = 0 in all cases , showing the distribution skewed to larger T V D ( i.e. , right on x - axis ) compared to baselines .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "As an alternative metric of attention explainablity , ( Jain and Wallace , 2019 ) considers the relationship between attention weights and gradient - based feature importance score of each word .", "entities": [[25, 27, "TaskName", "feature importance"]]}
{"text": "We pursue the second direction , but without incurring additional human annotation , by exploring the counterfactual augmentation , originated from self - supervision signals , contributing towards both accuracy and robustness of the model .", "entities": [[29, 30, "MetricName", "accuracy"]]}
{"text": "Machine consuming attention for higher accuracy is the most classical target scenario .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "( Yang et al , 2016 ) proposes hierarchical attention for document classification , ( Chen et al , 2016 ) personalizes classification to user and product attributes .", "entities": [[11, 13, "TaskName", "document classification"]]}
{"text": "Specifically , we proposed a counterfactual signal for self - supervision , to augment task - level human annotation , into sample - level machine attention supervision , to increase both the accuracy and robustness of the model .", "entities": [[32, 33, "MetricName", "accuracy"]]}
{"text": "We hope future research to explore scenarios where human intuition is not working as well as text classification , such as graph attention ( Veli\u010dkovi\u0107 et al , 2017 ) .", "entities": [[16, 18, "TaskName", "text classification"]]}
{"text": "This work is supported by AI Graduate School Program ( 2020 - 0 - 01361 ) and IITP grant ( No.2017 - 0 - 01779 , XAI ) supervised by IITP .", "entities": [[12, 13, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "Morphological analysis is a central component in a modern corpus , and issues such as ambiguity are also always present when FST is used in this context ( \u04e6\u043d\u044c\u04e7 \u041b\u0430\u0432 , 2015 , 140 ) .", "entities": [[0, 2, "TaskName", "Morphological analysis"]]}
{"text": "As regards morphophonological descriptions , stem - final variation had to be moved one step away from the initial LEMMA + COLON + STEM + CONTINUATIONLEXICON declaration in the code .", "entities": [[19, 20, "DatasetName", "LEMMA"]]}
{"text": "Since 2017 , work has been conducted within the Universal Dependencies project to better cover Komi varieties , most recently ( Zeman et al , 2021 ) , see also Nivre et al ( 2020 ) .", "entities": [[9, 11, "DatasetName", "Universal Dependencies"]]}
{"text": "Currently this is treated at the lemma level , so that the non - standard forms are connected to the standard lemmas , with an additional tag indicating dialectal form or error .", "entities": [[6, 7, "DatasetName", "lemma"]]}
{"text": "For the NLP pipeline of Komi the most important new developments will be connected to improvements in the dependency parsing side of the analysis , ideally in connection to automatic and rule - based methods of disambiguation .", "entities": [[18, 20, "TaskName", "dependency parsing"]]}
{"text": "At the same time Komi Universal Dependencies treebanks have started to be large enough that their further modeling with deep learning starts to be an attractive and possibly fruitful task .", "entities": [[5, 7, "DatasetName", "Universal Dependencies"]]}
{"text": "Such might be machine translation , on the one hand ( Tiedemann , 2021 ) , and translation studies , on the other ( cf .", "entities": [[3, 5, "TaskName", "machine translation"]]}
{"text": "We consider entity - level sentiment analysis in Arabic , a morphologically rich language with increasing resources .", "entities": [[5, 7, "TaskName", "sentiment analysis"]]}
{"text": "Target - specific sentiment analysis has recently become a popular problem in natural language processing .", "entities": [[3, 5, "TaskName", "sentiment analysis"]]}
{"text": "If we could successfully identify the targets of sentiment , it would be valuable for a number of applications including sentiment summarization , question answering , understanding public opinion during political conflict , or assessing needs of populations during natural disasters .", "entities": [[21, 22, "TaskName", "summarization"], [23, 25, "TaskName", "question answering"]]}
{"text": "Our work also differs from much work on targeted sentiment analysis in that posts are long , complex , with many annotated targets and a lack of punctuation that is characteristic of Arabic online language .", "entities": [[9, 11, "TaskName", "sentiment analysis"]]}
{"text": "Entity - specific sentiment analysis has been frequently studied in social media and online posts .", "entities": [[3, 5, "TaskName", "sentiment analysis"]]}
{"text": "More recent work uses LSTM and RNN networks to determine sentiment toward aspects in product reviews ( Wang et al , 2016 ) and towards entities in Twitter ( Dong et al , 2014 ; Tang et al , 2015 ) .", "entities": [[4, 5, "MethodName", "LSTM"]]}
{"text": "SemEval 2016 ran two tasks on sentiment analysis ( Nakov et al , 2016 ) and stance ( Mohammad et al , 2016 ) towards pre - defined topics in Twitter , both on English data .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}
{"text": "Ruppenhofer et al ( 2008 ) discussed the challenges of identifying targets in open - domain text which can not be addressed by semantic role labeling , such as implicitly conveyed sentiment , global and local targets related to the same entity , and the need for distinguishing between entity and proposition targets .", "entities": [[23, 26, "TaskName", "semantic role labeling"]]}
{"text": "Sequence labeling models became more popular for this problem : Mitchell et al ( 2013 ) used CRF model combinations to identify named entity targets in English and Spanish , and Yang and Cardie ( 2013 ) used joint modeling to predict opinion expressions and their source and target spans in news articles , improving over several single CRF models .", "entities": [[17, 18, "MethodName", "CRF"], [58, 59, "MethodName", "CRF"]]}
{"text": "Past work in Arabic machine translation ( Habash and Sadat , 2006 ) and named entity recognition ( Benajiba et al , 2008 ) considered the tokenization of complex Arabic words as we do in our sequence labeling task .", "entities": [[4, 6, "TaskName", "machine translation"], [14, 17, "TaskName", "named entity recognition"]]}
{"text": "Analysis of such segmentation schemes has not been reported for Arabic sentiment tasks , which cover mostly sentence - level sentiment analysis and where the lemma or surface bag - of - word representations have typically been sufficient .", "entities": [[20, 22, "TaskName", "sentiment analysis"], [25, 26, "DatasetName", "lemma"]]}
{"text": "There are now many studies on sentence - level sentiment analysis in Arabic news and social media ( Abdul - Mageed and Diab , 2011 ; Mourad and Darwish , 2013 ; Refaee and Rieser , 2014 ; Salameh et al , 2015 ) .", "entities": [[9, 11, "TaskName", "sentiment analysis"]]}
{"text": "Elarnaoty et al ( 2012 ) proposed identifying sources of opinions in Arabic using a CRF with a number of patterns , lexical and subjectivity clues ; they did not discuss morphology or syntactic relations .", "entities": [[15, 16, "MethodName", "CRF"]]}
{"text": "Entity Clusters It has been shown consistently that semantic word clusters improve the performance of named entity recognition ( T\u00e4ckstr\u00f6m et al , 2012 ; Zirikly and Hagiwara , 2015 ;", "entities": [[15, 18, "TaskName", "named entity recognition"]]}
{"text": "Turian et al , 2010 ) and semantic parsing ( Saleh et al , 2014 ) ; we are not aware of such work for identifying entity targets of sentiment .", "entities": [[7, 9, "TaskName", "semantic parsing"]]}
{"text": "For modeling the data , we choose Conditional Random Fields ( CRF )", "entities": [[11, 12, "MethodName", "CRF"]]}
{"text": "( Lafferty et al , 2001 ) for the ability to engineer Arabic linguistic features and because of the success of CRF models in the past for entity identification and classification related tasks .", "entities": [[21, 22, "MethodName", "CRF"]]}
{"text": "We build two linear chain CRF models : 1 .", "entities": [[5, 6, "MethodName", "CRF"]]}
{"text": "Thus , they can be represented as separate tokens in the CRF .", "entities": [[11, 12, "MethodName", "CRF"]]}
{"text": "We consider the following two schemes : D3 : the Declitization scheme which splits off conjunction clitics , particles and prepositions , Al+ , and all the enclitics at the end .", "entities": [[7, 8, "DatasetName", "D3"]]}
{"text": "We consider the surface word and the lemma for representing the word form .", "entities": [[7, 8, "DatasetName", "lemma"]]}
{"text": "For the clitics that were split off , we use a detailed POS feature that is also extracted from the output of the analyzer and can take such forms as DET for Al+ or poss_pron_3MP for third person masculine possessive pronouns .", "entities": [[30, 31, "DatasetName", "DET"]]}
{"text": "Table 2 shows the words and part of speech for the input sentence ' so they welcomed her ' fa - istaqbalu - ha , using the lemma representation for the word form and the D3 tokenization scheme .", "entities": [[27, 28, "DatasetName", "lemma"], [35, 36, "DatasetName", "D3"]]}
{"text": "We consider three lexicons : ( 1 ) SIFAAT , a manually constructed Arabic lexicon of 3982 adjectives ( Abdul - Mageed and Diab , 2011 ) , ( 2 ) ArSenL , an Arabic lexicon developed by linking English SentiWord - Net with Arabic WordNet and an Arabic lexical database ( Badaro et al , 2014 ) , and ( 3 ) the English MPQA lexicon", "entities": [[65, 66, "DatasetName", "MPQA"]]}
{"text": "CATiB uses a number of intuitive labels specifying the token 's syntactic role : e.g SBJ , OBJ , MOD , and IDF for the Arabic idafa construct ( e.g president of government ) , as well as its part of speech role .", "entities": [[19, 20, "DatasetName", "MOD"]]}
{"text": "The morphological analyzer MADAMIRA also produces base phrase chunks ( BPC ) and named entity tags ( NER ) for each token .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "Such features can benefit the CRF where a limited amount of training data is available for target entities .", "entities": [[5, 6, "MethodName", "CRF"]]}
{"text": "Euclidean distance serves as a semantic similarity metric and has been commonly used as a distance - based measure for clustering word vectors .", "entities": [[5, 7, "TaskName", "semantic similarity"]]}
{"text": "We use Google 's word2vec tool 3 for building and clustering word vectors with dimension 200 .", "entities": [[2, 3, "DatasetName", "Google"]]}
{"text": "We run our pipelined models for all morphological representation schemes : surface word ( no token splits ) , lemma ( no clitics ) , lemma with ATB clitics ( contain all token splits except Al+ ) , and lemma with D3 clitics ( contains all token splits ) .", "entities": [[19, 20, "DatasetName", "lemma"], [25, 26, "DatasetName", "lemma"], [39, 40, "DatasetName", "lemma"], [41, 42, "DatasetName", "D3"]]}
{"text": "This evaluation is driven from the sentiment summarization perspective : we want to predict the overall opinion in the post towards an entity .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "F - all shows the overall F - measure showing the performance of correctly predicted targets with correct sentiment compared to the total number of polar targets .", "entities": [[6, 9, "MetricName", "F - measure"]]}
{"text": "We observe that the gloss - translated MPQA lexicon outperforms the two other Arabic lexicons among the sentiment baselines .", "entities": [[7, 8, "DatasetName", "MPQA"]]}
{"text": "We believe that the hit rate of MPQA is higher than that of the smaller , manually - labeled SIFAAT , and it is more precise than the automatically generated WordNet - based lexicon ArSenL.", "entities": [[7, 8, "DatasetName", "MPQA"]]}
{"text": "The performance of MPQA is , however , reliant on the availability of high - quality English glosses .", "entities": [[3, 4, "DatasetName", "MPQA"]]}
{"text": "We found MPQA to consistently outperform in the model results , so in our best - linguistic models , we only show results using the MPQA lexicon .", "entities": [[2, 3, "DatasetName", "MPQA"], [25, 26, "DatasetName", "MPQA"]]}
{"text": "Looking at table 4 , we can see that using the lemma representation easily outperforms the sparser surface word , and that adding tokenized clitics as separate tokens outperforms representations which only use the word form .", "entities": [[11, 12, "DatasetName", "lemma"]]}
{"text": "Moreover , upon using the D3 decliticization method , we observe a significant increase in recall of targets over the ATB representation .", "entities": [[5, 6, "DatasetName", "D3"]]}
{"text": "The more tokens are split off , the more targets are recalled , although this comes at the cost of a decrease in sentiment performance , where the lemma representation has the highest sentiment score and the D3 representation has the lowest af - ter surface word .", "entities": [[28, 29, "DatasetName", "lemma"], [37, 38, "DatasetName", "D3"]]}
{"text": "All models significantly improve the baselines on Fmeasure ; for Acc - sent , the surface word CRF does not significantly outperform the MPQA baseline .", "entities": [[10, 11, "MetricName", "Acc"], [17, 18, "MethodName", "CRF"], [23, 24, "DatasetName", "MPQA"]]}
{"text": "The difference in different schemes is consistent with the results of Table 4 ; the D3 representation maintains the highest recall of targets , while the opposite is true for identifying sentiment towards the targets .", "entities": [[15, 16, "DatasetName", "D3"]]}
{"text": "The ATB representation shows the best overall Fmeasure , peaking at 41.5 using k=250 ( compare with 38.2 using no clusters ) ; however , it recalls much fewer targets than the D3 representation .", "entities": [[32, 33, "DatasetName", "D3"]]}
{"text": "The effect of clusters on sentiment is less clear ; it seems to benefit the D3 and ATB schemes more than lemma ( significant boosts in sentiment accuracy ) .", "entities": [[15, 16, "DatasetName", "D3"], [21, 22, "DatasetName", "lemma"], [27, 28, "MetricName", "accuracy"]]}
{"text": "The best linguistic model is run using both ATB and D3 tokenization schemes , and then using a combined ATB+D3 scheme where we use D3 for the target model and remove the extra clitics before piping in the output to the sentiment model .", "entities": [[10, 11, "DatasetName", "D3"], [24, 25, "DatasetName", "D3"]]}
{"text": "Performance exceeds that of the simpler models which use only POS and word clusters , but it is worth noting that using only the basic model with the word clusters can achieve significant boosts in recall and F - measure bringing it closer to the rich linguistic model .", "entities": [[37, 40, "MetricName", "F - measure"]]}
{"text": "performing SemEval systems reported 70 - 80 % for sentiment given defined aspects , and ( Mitchell et al , 2013 ; Deng and Wiebe , 2015 ) for overall Fmeasure ; we note that our tasks differ as described in section 2 . Results on blind test Table 6 shows the results on unseen test data for best - linguistic using D3 , D3+ATB and with clusters using k=8000 .", "entities": [[62, 63, "DatasetName", "D3"]]}
{"text": "Output Malaysia : pos health : pos educational and social : neg financial : neg is brains , which appears as a positive subjective word in the MPQA lexicon .", "entities": [[27, 28, "DatasetName", "MPQA"]]}
{"text": "This could shed light on further research in target - specific sentiment analysis for morphologically complex languages , an area little investigated previously .", "entities": [[11, 13, "TaskName", "sentiment analysis"]]}
{"text": "This work was supported in part by grant NPRP 6 - 716 - 1 - 138 from the Qatar National Research Fund , by DARPA DEFT grant FA8750 - 12 - 2 - 0347 and by DARPA LORELEI grant HR0011 - 15 - 2 - 0041 .", "entities": [[24, 25, "DatasetName", "DARPA"], [36, 37, "DatasetName", "DARPA"]]}
{"text": "2016 Task 2 : Interpretable Semantic Textual Similarity with Distributional Semantics for Chunks", "entities": [[5, 8, "TaskName", "Semantic Textual Similarity"]]}
{"text": "We introduce a system focused on solving SemEval 2016 Task 2 - Interpretable Semantic Textual Similarity .", "entities": [[13, 16, "TaskName", "Semantic Textual Similarity"]]}
{"text": "The goal of the Interpretable Semantic Textual Similarity task is to go deeper with the assessment of semantic textual similarity of sentence pairs .", "entities": [[5, 8, "TaskName", "Semantic Textual Similarity"], [17, 20, "TaskName", "semantic textual similarity"]]}
{"text": "The best performing systems adopted various approaches , ( Banjade et al , 2015 ) relied on handcrafter rules , ( Karumuri et al , 2015 ) employed a classifier for relation types and they associated each relation with a precomputed similarity score and ( H\u00e4nig et al , 2015 ) extended their word alignment algorithm for the task .", "entities": [[53, 55, "TaskName", "word alignment"]]}
{"text": ", CH b j ) { 0 , 1 , 2 , 3 , 4 , 5 } for chunk similarity and rel ( CH a i , CH b j ) TYPE for chunk relation type .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "Lemmatization - we find lemmas with the Stanford CoreNLP tool .", "entities": [[0, 1, "TaskName", "Lemmatization"]]}
{"text": "The core of our system is based upon computing semantic similarity of sentence chunks .", "entities": [[9, 11, "TaskName", "semantic similarity"]]}
{"text": "The sim score should describe semantic similarity of a given chunk pairthe higher score the more easily both chunks can be replaced with each other without chaining the meaning of both sentences .", "entities": [[5, 7, "TaskName", "semantic similarity"]]}
{"text": "The similarity score ranges from 0 to 5 , where 0 is the lowest similarity and 5 is the highest similarity .", "entities": [[5, 6, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Our attempts to estimate the sim function are based upon estimating semantic similarity of individual words and compiling them into one number for a given chunk pair .", "entities": [[11, 13, "TaskName", "semantic similarity"]]}
{"text": "We experiment with Word2Vec ( Mikolov et al , 2013 ) and GloVe ( Pennington et al , 2014 ) for estimating similarity of words .", "entities": [[12, 13, "MethodName", "GloVe"]]}
{"text": "We compile all the word similarities in one number that reflects semantic similarity of whole chunks via the following methods : 1 ) the vector composition method and 2 ) an adapted method for constructing vectors called lexical semantic vectors .", "entities": [[11, 13, "TaskName", "semantic similarity"]]}
{"text": "The vectors are then compared with cosine distance : sim ( CH a j , CH b k ) = cos ( \u03b8 )", "entities": [[22, 23, "HyperparameterName", "\u03b8"]]}
{"text": "We do not weight words with their information content and we use methods for distributional semantics ( Word2Vec and GloVe ) rather than semantic networks .", "entities": [[19, 20, "MethodName", "GloVe"]]}
{"text": "Score classification / regression - we experiment with both classification and regression of the chunks similarity score .", "entities": [[0, 1, "MetricName", "Score"]]}
{"text": "Lexical features consist of the following features : word base form overlap , word lemma overlap , chunk length difference , word sentence positions difference .", "entities": [[14, 15, "DatasetName", "lemma"]]}
{"text": "Additionally , some members of our team participated in the STS task ( task 1 ) of the SemEval 2016 ( Brychc\u00edn and Svoboda , 2016 ) and they annotated the semantic similarity of the whole sentences with their system for us .", "entities": [[10, 11, "TaskName", "STS"], [31, 33, "TaskName", "semantic similarity"]]}
{"text": "We employ an algorithm inspired by the IBM word model II for machine translation ( Brown et al , 1993 ) .", "entities": [[12, 14, "TaskName", "machine translation"]]}
{"text": "If the similarity is 0 , then the relation type is NOALI .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "Score classification - Maximum entropy ( Brainy ) .", "entities": [[0, 1, "MetricName", "Score"]]}
{"text": "We experimented with reduced feature set ( word overlap , word positions difference , POS tags difference , semantic similarity , global semantic similarity , paraphrase database ) - run 1 and with all featuresrun 3 .", "entities": [[18, 20, "TaskName", "semantic similarity"], [22, 24, "TaskName", "semantic similarity"]]}
{"text": "All lower values are set to 0 .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "The results show F1 scores for chunk alignment ( Ali ) , determination of the relation type ( Type ) , chunk similarity score ( Score ) and combination of relation type and score similarity ( T+S ) .", "entities": [[3, 4, "MetricName", "F1"], [25, 26, "MetricName", "Score"]]}
{"text": "However , it is worth of noticing that the unsupervised alignment algorithm inspired by machine translation alignment placed quite well .", "entities": [[14, 16, "TaskName", "machine translation"]]}
{"text": "The machine learning approach with combination of methods for the distributional semantics ( Word2Vec and GloVe ) proved to be very capable of solving the advanced task of Interpretable Semantic Textual Similarity .", "entities": [[15, 16, "MethodName", "GloVe"], [29, 32, "TaskName", "Semantic Textual Similarity"]]}
{"text": "Event2Mind : Commonsense Inference on Events , Intents , and Reactions", "entities": [[0, 1, "DatasetName", "Event2Mind"]]}
{"text": "We define intent as an explanation of why the agent causes a volitional event to occur ( or \" none \" if the event phrase was unintentional ) .", "entities": [[9, 10, "DatasetName", "agent"]]}
{"text": "We define reaction as an explanation of how the mental states of the agent and other people involved in the event would change as a result .", "entities": [[13, 14, "DatasetName", "agent"]]}
{"text": "We extract phrasal events from three different corpora for broad coverage : the ROC Story training set ( Mostafazadeh et al , 2016 ) , the Google Syntactic N - grams ( Goldberg and Orwant , 2013 ) , and the Spinn3r corpus ( Gordon and Swanson , 2008 ) .", "entities": [[26, 27, "DatasetName", "Google"]]}
{"text": "For each phrase , we ask three annotators whether the agent of the event , PersonX , intentionally causes the event , and if so , to provide up to three possible textual descriptions of their intents .", "entities": [[10, 11, "DatasetName", "agent"]]}
{"text": "A majority of our events are annotated as willingly caused by the agent ( 86 % , Cohen 's \uf8ff = 0.48 ) , and 26 % involve other people ( \uf8ff = 0.41 ) .", "entities": [[12, 13, "DatasetName", "agent"]]}
{"text": "For notation purposes , we describe each event pattern E as a sequence of word embeddings he 1 , e 2 , . . .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "Lastly , we encode the event phrase with a bi - directional RNN ( specifically , a GRU ; Cho et al , 2014 ) , concatenating the final hidden states of the forward and backward cells as the encoding : h E = [ !", "entities": [[17, 18, "MethodName", "GRU"]]}
{"text": "Each decoder projects the event phrase embedding h E into a | V | dimensional vector , which is then passed through a softmax function .", "entities": [[23, 24, "MethodName", "softmax"]]}
{"text": "i = softmax ( W i h E", "entities": [[2, 3, "MethodName", "softmax"]]}
{"text": "The event phrase embedding h E is set as the initial state h dec of three decoder RNNs ( using GRU cells ) , which then output the intent / reactions one word at a time ( using beam - search at test time ) .", "entities": [[20, 21, "MethodName", "GRU"]]}
{"text": "For example , an event 's intent sequence ( v i = v ( 0 )", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "i v ( 1 ) i . . . ) is computed as follows : v ( t+1 ) i = softmax ( W i RNN ( v ( t )", "entities": [[21, 22, "MethodName", "softmax"]]}
{"text": "Further , we employ multitask learning , simultaneously minimizing the loss for all three decoders at each iteration .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "Training details We fix our input embeddings , using 300 - dimensional skip - gram word embeddings trained on Google News ( Mikolov et al , 2013 ) .", "entities": [[15, 17, "TaskName", "word embeddings"], [19, 20, "DatasetName", "Google"]]}
{"text": "Through Event2Mind inference , we can attempt to bring to the surface what is implied about people 's behavior and mental states .", "entities": [[1, 2, "DatasetName", "Event2Mind"]]}
{"text": "The intent and reaction categories are then ( 1990 , bottom ) , augmented with Event2mind inferences on the characters ' intents and reactions .", "entities": [[15, 16, "DatasetName", "Event2mind"]]}
{"text": "E.g. , our model infers that the event PersonX sits on PersonX 's bed , lost in thought implies that the agent , Vivian , is sad or worried .", "entities": [[21, 22, "DatasetName", "agent"]]}
{"text": "We compute correlations with gender for each category of intent or reaction using a logistic regression model , testing significance while using Holm 's correction for multiple comparisons ( Holm , 1979 ) .", "entities": [[14, 16, "MethodName", "logistic regression"]]}
{"text": "Intents and Reactions Our Event2Mind inferences automate portrayal analyses that previously required manual annotations ( Behm - Morawitz and Mastro , 2008 ; Prentice and Carranza , 2002 ; England et al , 2011 ) .", "entities": [[4, 5, "DatasetName", "Event2Mind"]]}
{"text": "Previous work in natural language inference has focused on linguistic entailment ( Bowman et al , 2015 ; Bos and Markert , 2005 ) while ours focuses on commonsense - based inference .", "entities": [[3, 6, "TaskName", "natural language inference"]]}
{"text": "For instance , ConceptNet contains only 25 % of our events , and only 12 % have relations that resemble intent and reaction .", "entities": [[3, 4, "DatasetName", "ConceptNet"]]}
{"text": "We present a more detailed comparison with ConceptNet in Appendix C.", "entities": [[7, 8, "DatasetName", "ConceptNet"]]}
{"text": "This work was supported in part by National Science Foundation Graduate Research Fellowship Program under grant DGE - 1256082 , NSF grant IIS - 1714566 , and the DARPA CwC program through ARO ( W911NF - 15 - 1 - 0543 ) .", "entities": [[28, 29, "DatasetName", "DARPA"]]}
{"text": "Learning to Generalize to More : Continuous Semantic Augmentation for Neural Machine Translation", "entities": [[11, 13, "TaskName", "Machine Translation"]]}
{"text": "The principal task in supervised neural machine translation ( NMT ) is to learn to generate target sentences conditioned on the source inputs from a set of parallel sentence pairs , and thus produce a model capable of generalizing to unseen instances .", "entities": [[6, 8, "TaskName", "machine translation"]]}
{"text": "Although data augmentation is widely used to enrich the training data , conventional methods with discrete manipulations fail to generate diverse and faithful training samples .", "entities": [[1, 3, "TaskName", "data augmentation"]]}
{"text": "In this paper , we present a novel data augmentation paradigm termed Continuous Semantic Augmentation ( CSANMT ) , which augments each training instance with an adjacency semantic region that could cover adequate variants of literal expression under the same meaning .", "entities": [[8, 10, "TaskName", "data augmentation"]]}
{"text": "We conduct extensive experiments on both rich - resource and low - resource settings involving various language pairs , including WMT14 English { German , French } , NIST Chinese English and multiple low - resource IWSLT translation tasks .", "entities": [[20, 21, "DatasetName", "WMT14"]]}
{"text": "Neural machine translation ( NMT ) is one of the core topics in natural language processing , which aims to generate sequences of words in the target language conditioned on the source inputs ( Sutskever et al , 2014 ;", "entities": [[1, 3, "TaskName", "machine translation"]]}
{"text": "x ; \u0398 ) with the usage of parallel data .", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "For example , backtranslation ( BT ) ( Sennrich et al , 2016a ) makes use of the monolingual data on the target side to synthesize large scale pseudo parallel data , which is further combined with the real parallel corpus in machine translation task .", "entities": [[42, 44, "TaskName", "machine translation"]]}
{"text": "The above two search strategies are approximate algorithms to identify the maximum a - posteriori ( MAP ) output , and thus favor the most frequent one in case of ambiguity .", "entities": [[16, 17, "DatasetName", "MAP"]]}
{"text": "In this paper , we propose Continuous Semantic Augmentation ( CSANMT ) , a novel data augmentation paradigm for NMT , to alleviate both limitations mentioned above .", "entities": [[15, 17, "TaskName", "data augmentation"]]}
{"text": "We evaluate our framework on a variety of machine translation tasks , including WMT14 English - German / French , NIST Chinese - English and multiple IWSLT tasks .", "entities": [[8, 10, "TaskName", "machine translation"], [13, 14, "DatasetName", "WMT14"]]}
{"text": "A sequence - tosequence model is usually applied to neural machine translation , which aims to learn a transformation from the source space to the target space X Y :", "entities": [[10, 12, "TaskName", "machine translation"]]}
{"text": "x ; \u0398 ) with the usage of parallel data .", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "Formally , given a set of observed sentence pairs C = { ( x ( n ) , y ( n ) ) } N n=1 , the training objective is to maximize the log - likelihood : J mle ( \u0398 ) =", "entities": [[34, 37, "MetricName", "log - likelihood"], [41, 42, "HyperparameterName", "\u0398"]]}
{"text": "| x ; \u0398 ) .", "entities": [[3, 4, "HyperparameterName", "\u0398"]]}
{"text": "x ; \u0398 )", "entities": [[2, 3, "HyperparameterName", "\u0398"]]}
{"text": "( y t | y < t , x ; \u0398 ) , where \u0398 is a set of trainable parameters and y < t is a partial sequence before time - step t.", "entities": [[10, 11, "HyperparameterName", "\u0398"], [14, 15, "HyperparameterName", "\u0398"]]}
{"text": "However , there is a major problem in the common supervised setting for neural machine translation , that is the number of training instances is very limited because of the cost in acquiring parallel data .", "entities": [[14, 16, "TaskName", "machine translation"]]}
{"text": "Traditional data augmentation methods generate more training samples by applying discrete manipulations to unlabeled ( or labeled ) data , such as back - translation or randomly replacing a word with another one , which usually suffer from the problems of semantic deviation and the lack of diversity .", "entities": [[1, 3, "TaskName", "data augmentation"]]}
{"text": "We propose a novel data augmentation paradigm for neural machine translation , termed continuous semantic augmentation ( CSANMT ) , to better generalize the model 's capability to unseen instances .", "entities": [[4, 6, "TaskName", "data augmentation"], [9, 11, "TaskName", "machine translation"]]}
{"text": "We adopt the Transformer ( Vaswani et al , 2017 ) model as a backbone , and the framework is shown in Figure 1 .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "( x ; \u0398 \u2032 ) and r y = \u03c8 ( y ; \u0398 \u2032 )", "entities": [[3, 4, "HyperparameterName", "\u0398"], [14, 15, "HyperparameterName", "\u0398"]]}
{"text": "respectively , where \u03c8 ( ; \u0398 \u2032 ) is the forward function of the semantic encoder parameterized by \u0398 \u2032 ( parameters other than \u0398 ) . \u2200", "entities": [[6, 7, "HyperparameterName", "\u0398"], [19, 20, "HyperparameterName", "\u0398"], [25, 26, "HyperparameterName", "\u0398"]]}
{"text": "x , r ( k ) ; \u0398 ) .", "entities": [[7, 8, "HyperparameterName", "\u0398"]]}
{"text": "Tangential Contrastive Learning We start from analyzing the geometric interpretation of adjacency semantic regions .", "entities": [[1, 3, "MethodName", "Contrastive Learning"]]}
{"text": "The training objective is formulated as : J ctl ( \u0398 \u2032 )", "entities": [[10, 11, "HyperparameterName", "\u0398"]]}
{"text": "p ( \u03c9 | \u03c9 ( 1 ) , \u03c9 ( 2 ) , ... , \u03c9 ( k\u22121 ) ) , p = \u03b7N 0 , diag ( W 2 r ) + ( 1.0 \u2212 \u03b7 )", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "Thus N ( 0 , diag ( W 2 r ) ) limits the range of sampling to a subspace of the adjacency semantic region , and rejects to conduct sampling from the uninformative dimensions .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "The training objective in our approach is a combination of J mle ( \u0398 ) in Eq .", "entities": [[13, 14, "HyperparameterName", "\u0398"]]}
{"text": "Firstly , we train the semantic encoder from scratch using the task - specific data , i.e. \u0398 \u2032 * = argmax \u0398 \u2032", "entities": [[17, 18, "HyperparameterName", "\u0398"], [22, 23, "HyperparameterName", "\u0398"]]}
{"text": "J ctl ( \u0398 \u2032 ) .", "entities": [[3, 4, "HyperparameterName", "\u0398"]]}
{"text": "Secondly , we optimize the encoder - decoder model by maximizing the log - likelihood , i.e. \u0398 * = argmax \u0398 J mle ( \u0398 ) , and fine - tune the semantic encoder with a small learning rate at the same time .", "entities": [[12, 15, "MetricName", "log - likelihood"], [17, 18, "HyperparameterName", "\u0398"], [21, 22, "HyperparameterName", "\u0398"], [25, 26, "HyperparameterName", "\u0398"], [38, 40, "HyperparameterName", "learning rate"]]}
{"text": "During inference , the sequence of target words is generated auto - regressively , which is almost the same as the vanilla Transformer ( Vaswani et al , 2017 ) .", "entities": [[22, 23, "MethodName", "Transformer"]]}
{"text": "A major difference is that our method involves the semantic vector of the input sequence for generation : y * t = argmax yt P ( | y < t , x , r x ; \u0398 ) , where r x = \u03c8 ( x ; \u0398 \u2032 ) .", "entities": [[36, 37, "HyperparameterName", "\u0398"], [47, 48, "HyperparameterName", "\u0398"]]}
{"text": "We first apply CSANMT to NIST Chinese - English ( Zh En ) , WMT14 English - German ( En De ) and English - French ( En Fr ) tasks , and conduct extensive analyses for better understanding the proposed method .", "entities": [[14, 15, "DatasetName", "WMT14"]]}
{"text": "We implement our approach on top of the Transformer ( Vaswani et al , 2017 ) .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "Moreover , compared to the vanilla Transformer , our approach consistently achieves promising improvements on five test sets .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "Meanwhile , CSANMT shows a better capability on preserving the semantics of the generated translations than Transformer .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "We intuitively attribute the significantly increases of BLEU scores on all datasets to these two factors .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "( 2 ) We replace the default 4 - layer semantic encoder with a large pre - trained model ( PTM ) ( i.e. , XLM - R ( Conneau et al , 2020 ) ) .", "entities": [[25, 26, "MethodName", "XLM"]]}
{"text": "Effect of MGRC sampling and tangential contrastive learning .", "entities": [[6, 8, "MethodName", "contrastive learning"]]}
{"text": "To better understand the effectiveness of the MGRC sampling and the tangential contrastive learning , we conduct detailed ablation studies in Table 5 .", "entities": [[12, 14, "MethodName", "contrastive learning"]]}
{"text": "We also have tried the training objectives with other forms , such as variational inference and cosine similarity , to optimize the semantic encoder .", "entities": [[13, 15, "MethodName", "variational inference"]]}
{"text": "However , the BLEU score drops significantly .", "entities": [[3, 5, "MetricName", "BLEU score"]]}
{"text": "shows the evolution of BLEU scores during training .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "It is obvious that our method performs consistently better than both the vanilla Transformer and the back - translation method at each iteration ( except for the first 10 K warm - up iterations , where the former one has access to less unique training data than the latter two due to the K times over - sampling ) .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "In other words , CSANMT spends 44 % more training costs than the vanilla Transformer , due to the longer time to make the NMT model converge with augmented training instances .", "entities": [[14, 15, "MethodName", "Transformer"]]}
{"text": "Word prediction accuracy .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "In contrast to the vanilla Transformer , CSANMT involves with approximate 20 % additional parameters .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "We further generalize the capability of the proposed CSANMT to various low - resource machine translation tasks , including IWSLT14 English - German and IWSLT17 English - French .", "entities": [[14, 16, "TaskName", "machine translation"]]}
{"text": "Data Augmentation ( DA ) Kobayashi , 2018 ; Gao et al , 2019 ; Khayrallah et al , 2020 ; Pham et al , 2021 ) has been widely used in neural machine translation .", "entities": [[0, 2, "TaskName", "Data Augmentation"], [33, 35, "TaskName", "machine translation"]]}
{"text": "( Chapelle et al , 2000 ) is another principle of data augmentation , in which DA is formalized as extracting additional pseudo samples from the vicinal distribution of observed instances .", "entities": [[11, 13, "TaskName", "data augmentation"]]}
{"text": "Typically the vicinity of a training example is defined using datasetdependent heuristics , such as color ( scale , mixup ) augmentation ( Simonyan and Zisserman , 2014 ; Krizhevsky et al , 2012 ; Zhang et al , 2018 ) in computer vision and adversarial augmentation with manifold neighborhoods ( Ng et al , 2020 ; Cheng et al , 2021 ) in NLP .", "entities": [[19, 20, "MethodName", "mixup"]]}
{"text": "Sentence Representation Learning is a well investigated area with dozens of methods ( Kiros et al , 2015 ; .", "entities": [[1, 3, "TaskName", "Representation Learning"]]}
{"text": "In this context , contrastive learning has become a popular paradigm in NLP ( Kong et al , 2020 ;", "entities": [[4, 6, "MethodName", "contrastive learning"]]}
{"text": "We propose a novel data augmentation paradigm CSANMT , which involves with an adjacency semantic region as the vicinity manifold for each training instance .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "The main components of CSANMT consists of the tangential contrastive learning and the Mixed Gaussian Recurrent Chain ( MGRC ) sampling .", "entities": [[9, 11, "MethodName", "contrastive learning"]]}
{"text": "Experiments on both rich - and low - resource machine translation tasks demonstrate the effectiveness of our method .", "entities": [[9, 11, "TaskName", "machine translation"]]}
{"text": "In the future work , we would like to further study the vicinal risk minimization with the combination of multi - lingual aligned scenarios and large - scale monolingual data , and development it as a pure data augmentator merged into the vanilla Transformer .", "entities": [[43, 44, "MethodName", "Transformer"]]}
{"text": "We use the Stanford segmenter ( Tseng et al , 2005 ) for Chinese word segmentation and apply the script tokenizer.pl of Moses ( Koehn et al , 2007 ) for English , German and French tokenization .", "entities": [[13, 16, "TaskName", "Chinese word segmentation"]]}
{"text": "The case - insensitive BLEU is reported on the Zh En task .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "We use a joint source and target vocabulary with 10k byte - pair - encoding ( BPE ) types ( Sennrich et al , 2016b ) for above two tasks .", "entities": [[16, 17, "MethodName", "BPE"]]}
{"text": "( k ) \u223c \u03b7N 0 , diag ( W 2 r )", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "+ ( 1.0 \u2212 \u03b7 ) N 0 ,", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "al , 2013 ) on new - stest2014 for the WMT14 English - German task .", "entities": [[10, 11, "DatasetName", "WMT14"]]}
{"text": "Inspired by ( Gao et al , 2019 ) , we construct noisy test sets via several strategies described as follows : Original : the original testset without any manipulations ; WS : word swap , randomly swap words in nearby positions within a window size 3 ( Artetxe et al , 2018 ; Lample et al , 2018b ) ; WD : word dropout , randomly drop words with a ratio of 15 % ( Iyyer et al , 2015 ; Lample et al , 2018b ) ; WR : word replace , randomly replace word tokens with a placeholder token ( e.g. , [ UNK ] )", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "( Xie et al , 2017 ) or with a relevant ( measured by the similarity of word embeddings ) alternative .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "For the En De task , we employ the popular WMT14 dataset , which consists of approximately 4.5 M sentence pairs for training .", "entities": [[10, 11, "DatasetName", "WMT14"]]}
{"text": "For the En Fr task , we use the significantly larger WMT14 dataset consisting of 36 M sentence pairs .", "entities": [[11, 12, "DatasetName", "WMT14"]]}
{"text": "The combination of { newstest2012 , 2013 } was used for model selection and the experimental results were reported on newstest2014 .", "entities": [[11, 13, "TaskName", "model selection"]]}
{"text": "By representing semantic content as a sequence of tokens , we evaluated the semantic content prediction model using BLEU .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "The results demonstrated that the semantic content produced by the proposed method was closer to the ground truth than the semantic content transformed from the output text generated by the retrieval model and GPT - 2 .", "entities": [[33, 34, "MethodName", "GPT"]]}
{"text": "Further , we present some examples of dialogue generation by applying model outputs to template - based sentence generation .", "entities": [[7, 9, "TaskName", "dialogue generation"]]}
{"text": "In task - oriented dialogue systems , such as an airline ticket reservation system ( Hemphill et al , 1990 ) , eliciting specific information from the user , such as the date , time , and destination of the flight , is an important functionality for completing the task .", "entities": [[1, 6, "TaskName", "task - oriented dialogue systems"]]}
{"text": "However , in non - task - oriented dialogue systems , the system does not have a clear goal of eliciting information from the user , and the content of the dialogue is free .", "entities": [[5, 10, "TaskName", "task - oriented dialogue systems"]]}
{"text": "One possible approach for achieving the requirements discussed above is end - to - end neural network , where dialogue generation is the task of predicting the next utterance using dialogue history as input ( Vinyals and Le , 2015 ; .", "entities": [[19, 21, "TaskName", "dialogue generation"]]}
{"text": "Previous studies on dialogue generation have proposed different techniques to generate task - and non - task - oriented dialogue .", "entities": [[3, 5, "TaskName", "dialogue generation"]]}
{"text": "In traditional task - oriented dialogue systems , the information required to achieve the dialogue goals is limited to the task domain .", "entities": [[2, 7, "TaskName", "task - oriented dialogue systems"]]}
{"text": "To train the SCG model , we used a pre - trained Japanese language model 2 of the Transformerbased GPT - 2 model ( Radford et al , 2019 ) , which is commonly used for conversation generation and fine - tuned it using our own small dataset described in Section 3.1 .", "entities": [[19, 20, "MethodName", "GPT"]]}
{"text": "2 japanese - gpt2 - small : https://huggingface.co/rinna/japanese - gpt2 - small sequence is concatenated with the semantic content of the prediction target ( the interviewer 's sentence ) and fed to GPT - 2 .", "entities": [[32, 33, "MethodName", "GPT"]]}
{"text": "We used the BERT ( Devlin et al , 2019 ) Japanese pre - trained model 3 .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "We compared the proposed models with two baseline models : the retrieval model and text generation model .", "entities": [[14, 16, "TaskName", "text generation"]]}
{"text": "Retrieval Model : We simply applied a technique used in information retrieval to a response selection , as proposed in ( Ritter et al , 2011 ; Sordoni et al , 2015b ) .", "entities": [[10, 12, "TaskName", "information retrieval"]]}
{"text": "The sentence vector was a hidden representation of the [ CLS ] token obtained from BERT , and cosine similarity was used to calculate the sentence similarity .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "Text Generation Model : A GPT - 2 language model was trained using pairs of dialogue context and the next interviewer 's sentence .", "entities": [[0, 2, "TaskName", "Text Generation"], [5, 6, "MethodName", "GPT"]]}
{"text": "Numbers in parentheses indicate the length of the dialogue history in the best model using the validation dataset .", "entities": [[16, 18, "DatasetName", "validation dataset"]]}
{"text": "Text Generation ( N=4 ) 13.0 ( 22.3 ) SCG ( Proposed ) ( N=3 ) 17.3 ( 24.7 ) the semantic content of the sentence .", "entities": [[0, 2, "TaskName", "Text Generation"]]}
{"text": "To evaluate the output produced by the models , we conducted an automated evaluation using the BLEU with respect to the semantic content .", "entities": [[16, 17, "MetricName", "BLEU"]]}
{"text": "For the SCG model , the BLEU score was calculated by comparing the generated semantic content with the ground truth .", "entities": [[6, 8, "MetricName", "BLEU score"]]}
{"text": "For the text generation model , the semantic content was assigned by annotating the generated message and comparing it with the ground truth to calculate the BLEU score .", "entities": [[2, 4, "TaskName", "text generation"], [26, 28, "MetricName", "BLEU score"]]}
{"text": "The proposed model achieved the highest BLEU score .", "entities": [[6, 8, "MetricName", "BLEU score"]]}
{"text": "\" Retrieval , \" \" Text Generation , \" and \" Proposed \" are the outputs by the methods examined in our experiment .", "entities": [[5, 7, "TaskName", "Text Generation"]]}
{"text": "In Dialogue - 2 in Table 4 , all three models failed to generate an utterance about the current topic focus ( cheese ) , but the retrieval and text generation models still successfully generated a natural response .", "entities": [[29, 31, "TaskName", "text generation"]]}
{"text": "The outputs of these two models were then applied to template - based response generation to produce a response .", "entities": [[13, 15, "TaskName", "response generation"]]}
{"text": "As future work , we will improve the response generation mechanism to generate a variety of expressions because the current template - based response generation may not be sufficient in its expressiveness .", "entities": [[8, 10, "TaskName", "response generation"], [23, 25, "TaskName", "response generation"]]}
{"text": "Our results demonstrate that positive knowledge transfer via context - specific shared representations of a flexible cross - stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings .", "entities": [[37, 39, "TaskName", "emotion classification"]]}
{"text": "We show how for more domain - specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act ( refutation , justification , allegation ) classification , homogeneous multitask learning is helpful , whereas for more general tasks such as stance and hate speech detection , heterogeneous multitask learning with emotion classification works better .", "entities": [[45, 48, "TaskName", "hate speech detection"], [53, 55, "TaskName", "emotion classification"]]}
{"text": "This work is the first attempt at joint modeling of narratives related to sexual abuse disclosures and emotion classification to learn the patterns of their interaction via parameter sharing techniques offered by Multitask Learning ( MTL ) .", "entities": [[17, 19, "TaskName", "emotion classification"]]}
{"text": "More specifically , we formulate an MTL framework for multi - label classification of narratives related to sexual abuse disclosures ( stance , hate - speech , sarcasm , dialogue acts ) and emotional classification in the context of the # MeToo movement .", "entities": [[9, 13, "TaskName", "multi - label classification"]]}
{"text": "Heterogeneous MTL - cross - domain MTL between pairs of tasks in emotion classification and narratives of sexual abuse disclosure ( Section 5.2 ) .", "entities": [[12, 14, "TaskName", "emotion classification"]]}
{"text": "Jha and Mamidi ( 2017 ) experimented with algorithms such as SVM and BiLSTM along with fastText to categorize hostility of sexist posts .", "entities": [[11, 12, "MethodName", "SVM"], [13, 14, "MethodName", "BiLSTM"], [16, 17, "MethodName", "fastText"]]}
{"text": "MTL has been utilized for name error recognition ( Cheng et al , 2015 ) , tagging - chunking ( Collobert et al , 2011 ) , machine translation ( Luong et al , 2015 ) and relation extraction ( Gupta et al , 2016 ) .", "entities": [[18, 19, "TaskName", "chunking"], [27, 29, "TaskName", "machine translation"], [37, 39, "TaskName", "relation extraction"]]}
{"text": "Liu et al ( 2017 ) used shared and private latent features leveraging multitask learning for different text classification tasks .", "entities": [[17, 19, "TaskName", "text classification"]]}
{"text": "al ( 2020 ) ; Duong et al ( 2016 ) ; Liu et al ( 2016 ) proposed a joint framework for modeling abuse and emotion detection and showed improvements over STL and transfer learning .", "entities": [[26, 27, "DatasetName", "emotion"], [34, 36, "TaskName", "transfer learning"]]}
{"text": "al ( 2018 ) proposed a multitask ensemble architecture for jointly modeling emotion , sentiment , and intensity , which gave improvements over single - label classification .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "Stance Detection : Determining the opinion of the author of a tweet , regarding a particular target of interest ( Augenstein et al , 2016 ) .", "entities": [[0, 2, "TaskName", "Stance Detection"]]}
{"text": "Stance detection is categorized into three classes : Support for when the author favors the # MeToo movement or it 's cause ; Opposition , representing opposing stance or indifference towards the movement ; or Neither , when the text does not have a clear viewpoint ( Mohammad and Turney , 2013 ) .", "entities": [[0, 2, "TaskName", "Stance detection"]]}
{"text": "Hate Speech Identification : Detection of hate speech involves labeling the tweets as Directed Hate if the comment is targeted towards an individual or an entity , Generalized Hate if it is targeted towards a community or a section of people or Neither otherwise ( Basile et al , 2019 ) .", "entities": [[0, 2, "DatasetName", "Hate Speech"], [6, 8, "DatasetName", "hate speech"]]}
{"text": "Sarcasm Detection : Given a tweet t i , we aim to map it to either be Sarcastic or Not Sarcastic based on the presence of implicit sarcastic tone of the post ( Bamman and Smith , 2015 ) .", "entities": [[0, 2, "TaskName", "Sarcasm Detection"]]}
{"text": "Dialogue Act Classification : These are a function of a speaker 's utterance during a conversation , for example , question , answer , suggestion , etc . , and are classified into three classes , namely Allegation ( when the author intends to allege an individual or group of sexual misconduct )", "entities": [[0, 3, "TaskName", "Dialogue Act Classification"]]}
{"text": "Modeling Settings To validate MTL 's performance across different domains , we also experiment with emotion detection as the auxiliary task .", "entities": [[15, 16, "DatasetName", "emotion"]]}
{"text": "Simultaneous optimization of a pair selected from the four tasks associated with the sexual abuse disclosure posts , and ( iii ) Heterogeneous Multitask Learning : Classification of narratives associated with sexual abuse disclosure as the primary task and emotion detection as the auxiliary task .", "entities": [[26, 27, "TaskName", "Classification"], [39, 40, "DatasetName", "emotion"]]}
{"text": "BERTweet has been trained with the same training procedure as RoBERTa ( Liu et al , 2019 ) and has the same model configuration as the BERT base architecture ( Devlin et al , 2019 ) .", "entities": [[10, 11, "MethodName", "RoBERTa"], [26, 27, "MethodName", "BERT"]]}
{"text": "The key component in transformer - based models is the token level selfattention ( Vaswani et al , 2017 ) that enables them to generate dynamic contextualized embeddings as opposed to static embeddings of GloVe ( Pennington et al , 2014 ) .", "entities": [[34, 35, "MethodName", "GloVe"]]}
{"text": "Embedding for each tweet is of dimension m \u00d7 k , where k represents the dimension size of BERT based model and m represents the maximum length for the tweets .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "e i = BERT weet ( t i ) ( 1 ) These representations from Equation 1 are passed through a stacked BiLSTM encoder .", "entities": [[3, 4, "MethodName", "BERT"], [22, 23, "MethodName", "BiLSTM"]]}
{"text": "Dropout is then applied to these encoded representations h ( t ) ( Equation 4represents general formulation for both the tasks ) .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "These are then passed to a BiLSTM decoder , followed by a dropout layer and then a linear output layer to get output o ( p ) ( p representing primary task ) or o ( a ) ( a representing auxiliary task ) .", "entities": [[6, 7, "MethodName", "BiLSTM"]]}
{"text": "We treat the task of categorizing narratives related to sexual abuse disclosure - Stance , Hate Speech , 3 Implementation used for BERTweet is available here Sarcasm and Dialogue Acts , independently .", "entities": [[15, 17, "DatasetName", "Hate Speech"]]}
{"text": "Within the proposed tasks for classifying sexual abuse disclosure narrative for the tweets related to the # MeToo movement ( Section 3 ) , we use sigmoid activation for Sarcasm detection ( whose classification outputs are binary ) and softmax activation for all other tasks for the final output layer .", "entities": [[26, 28, "MethodName", "sigmoid activation"], [29, 31, "TaskName", "Sarcasm detection"], [39, 40, "MethodName", "softmax"]]}
{"text": "Given a sample class i containing n i samples in total , it adds a weighting factor of ( 1\u2212\u03b2 ) ( 1\u2212\u03b2 n i ) with parameters \u03b2", "entities": [[28, 29, "HyperparameterName", "\u03b2"]]}
{"text": "p represents predicted class probabilities and L represents the choice of the loss function ( binary cross entropy for Sarcasm and categorical cross entropy for others ) .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "As for the multilabel emotion classification task , the unnormalized output ( assuming one or more of 11 different emotions ) is subjected to a Sigmoid activation , and the network is optimized using binary cross - entropy ( BCE ) as : LBCE = \u2212 1 N N i=1", "entities": [[4, 6, "TaskName", "emotion classification"], [25, 27, "MethodName", "Sigmoid activation"]]}
{"text": "For our MTL approach , we use two optimization objectives : one for the primary task , which can be any of the proposed tasks for classifying tweets related to # MeToo movement ( Section 3 ) , and other for the auxiliary task , which can be either a task related to classifying sexual abuse disclosure for # MeToo movement ( Homogeneous MTL ) or emotion classification task ( Heterogeneous MTL ) .", "entities": [[66, 68, "TaskName", "emotion classification"]]}
{"text": "For each training pass of the primary task , the input representation e ( p ) is passed through ( a ) stacked BiLSTM encoder and ( b ) stacked shared BiLSTM encoder .", "entities": [[23, 24, "MethodName", "BiLSTM"], [31, 32, "MethodName", "BiLSTM"]]}
{"text": "We calculate the weighted summation of these two representations - h ( p ) , using two learnable parameters , \u03b1 ( p ) and \u03b1 ( s ) ( where \u03b1 ( p )", "entities": [[20, 21, "HyperparameterName", "\u03b1"], [25, 26, "HyperparameterName", "\u03b1"], [31, 32, "HyperparameterName", "\u03b1"]]}
{"text": "These output representations ( in the case of both Homogeneous and Heterogeneous experiments ) are passed through respective BiLSTM decoders and dropout layers to get the final representation m ( p ) and m ( a ) , respectively for both the tasks .", "entities": [[18, 19, "MethodName", "BiLSTM"]]}
{"text": "The auxiliary network branch is optimized using either Equation 5 ( Class Balanced Focal Loss ) or Equation 6 ( Binary Cross Entropy ) , depending upon whether the auxiliary task is associated with identifying sexual abuse disclosure narratives or emotions .", "entities": [[13, 15, "MethodName", "Focal Loss"]]}
{"text": "Sigmoid activation function is used for Sarcasm detection and the emotion classification task , and Softmax activation for others .", "entities": [[0, 2, "MethodName", "Sigmoid activation"], [6, 8, "TaskName", "Sarcasm detection"], [10, 12, "TaskName", "emotion classification"], [15, 16, "MethodName", "Softmax"]]}
{"text": "Two controllable parameters \u03b1 ( p ) and \u03b1 ( s ) are used to control information flow from task - specific and shared encoder respectively , for the primary task .", "entities": [[3, 4, "HyperparameterName", "\u03b1"], [8, 9, "HyperparameterName", "\u03b1"]]}
{"text": "Sexual Abuse Disclosures - # MeTooMA This dataset 4 has 9 , 973 tweets and covers different mutually non - exclusive linguistic annotations related to the # MeToo movement .", "entities": [[4, 6, "DatasetName", "# MeTooMA"]]}
{"text": "This dataset 5 has been taken from SemEval - 2018 Task - 1 ( Mohammad et al , 2018 ) and covers emotion - specific labels representing the mental state of the authors of the tweets .", "entities": [[22, 23, "DatasetName", "emotion"]]}
{"text": "It consists of 10 , 986 tweets distributed across 11 emotion labels - ( anger , disgust , anticipation , fear , joy , love , optimism , pessimism , sadness , surprise and trust ) , each being a binary label to indicate the presence of a particular emotion .", "entities": [[10, 11, "DatasetName", "emotion"], [49, 50, "DatasetName", "emotion"]]}
{"text": "Single Task Learning STL experiments optimize each of the tasks associated with identifying narratives related to sexual abuse disclosures within # MeToo movement ( Section 3 ) and emotion 4 The publicly available dataset can be found at https : //doi.org/10.7910 / DVN / JN4EYU .", "entities": [[28, 29, "DatasetName", "emotion"]]}
{"text": "We experiment with two distinct embedding spaces - GloVe - Twitter and BERTweet .", "entities": [[8, 9, "MethodName", "GloVe"]]}
{"text": "Based on the superior performance of BERTweet with respect to GloVe - Twitter , we preferred it for further experimentation and studies .", "entities": [[10, 11, "MethodName", "GloVe"]]}
{"text": "Heterogeneous Multitask Learning In these sets of experiments , we evaluate the positive transfer of representations across datasets by considering the identification of narratives associated with sexual abuse disclosure as the primary task and emotion detection as the auxiliary task .", "entities": [[34, 35, "DatasetName", "emotion"]]}
{"text": "For the MTL experiments , \u03b1 p and \u03b1 s are learnable and tuned on the validation loss .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [8, 9, "HyperparameterName", "\u03b1"], [17, 18, "MetricName", "loss"]]}
{"text": "The encoders consist of two stacked BiLSTM 's with hidden size = 128 .", "entities": [[6, 7, "MethodName", "BiLSTM"]]}
{"text": "Dropout is set to 0.3 .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "For our MTL experiments , the training process involves alternating between primary and auxiliary task steps , with each task having its own loss function .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "As per our hypothesis and preliminary results on STL experiments on the # MeTooMA dataset , models trained using BERTweet embeddings perform far better than GloVe - Twitter .", "entities": [[12, 14, "DatasetName", "# MeTooMA"], [25, 26, "MethodName", "GloVe"]]}
{"text": "Learning the affective states in the # MeTooMA dataset is challenging due to the inherently subjective nature of the tweets coupled with limitations on the data 's size .", "entities": [[6, 8, "DatasetName", "# MeTooMA"]]}
{"text": "Multitask learning achieves significant performance gains in terms of macro F1 score , as shown in Homogeneous MTL with row identifying primary task and columns denoting auxiliary task .", "entities": [[9, 11, "MetricName", "macro F1"]]}
{"text": "Stance detection is strongly coupled with Sarcasm labeling , and the same is seen to be true for Hate Speech classification and Stance identification .", "entities": [[0, 2, "TaskName", "Stance detection"], [18, 20, "DatasetName", "Hate Speech"]]}
{"text": "Results in highly correlated to emotion recognition .", "entities": [[5, 7, "TaskName", "emotion recognition"]]}
{"text": "Learning effective features across the joint formulation of pair - wise tasks in Homogeneous MTL is evident from T 4 , where BERT 's self - attention allots a higher weight to words such as ideology , stigma , and forward in line with the actual label as Support .", "entities": [[22, 23, "MethodName", "BERT"]]}
{"text": "On the other hand , due to positive knowledge transfer from the emotion recognition task , Heterogeneous MTL obtains better performance in several cases .", "entities": [[12, 14, "TaskName", "emotion recognition"]]}
{"text": "Words such as grave , mistake and swindling in T 2 connoted a negative emotion , hence accordingly being identified as belonging to the Oppose category .", "entities": [[14, 15, "DatasetName", "emotion"]]}
{"text": "Similarly , terms such as hope and pain were given higher token - level attention in T 1 emphasizing a positive emotion and thus can be correlated with belonging to the Support category .", "entities": [[21, 22, "DatasetName", "emotion"]]}
{"text": "Qualitative and quantitative results demonstrate how joint optimization of Stance detection and Sarcasm identification benefit each other , indicating their relatedness and dependence on each other .", "entities": [[9, 11, "TaskName", "Stance detection"]]}
{"text": "Similarly , we observe that tasks like Hate - Speech classification and Stance labeling benefit from each other and from emotion detection , thus reinforcing the benefit of joint linguistic learning between the related tasks .", "entities": [[20, 21, "DatasetName", "emotion"]]}
{"text": "In the future , we aim to explore how this joint learning paradigm can be effectively leveraged for improving performance on downstream tasks like emotion analysis , identifying suicidal tendencies among abuse survivors .", "entities": [[24, 25, "DatasetName", "emotion"]]}
{"text": "Application from this work also has utility for problems such as identification of patterns of reported sexual harassment narratives , hate speech detection , the spread of rumors and fake news , and entity extraction for digital vigilantism ( Yuce et al , 2014 ; Hosterman et al , 2018 ) .", "entities": [[20, 23, "TaskName", "hate speech detection"]]}
{"text": "The Power of Prompt Tuning for Low - Resource Semantic Parsing", "entities": [[9, 11, "TaskName", "Semantic Parsing"]]}
{"text": "In this paper , we investigate prompt tuning for semantic parsing - the task of mapping natural language utterances onto formal meaning representations .", "entities": [[9, 11, "TaskName", "semantic parsing"]]}
{"text": "On the low - resource splits of Overnight and TOPv2 , we find that a prompt tuned T5 - xl significantly outperforms its fine - tuned counterpart , as well as strong GPT - 3 and BART baselines .", "entities": [[9, 10, "DatasetName", "TOPv2"], [17, 18, "MethodName", "T5"], [32, 33, "MethodName", "GPT"], [36, 37, "MethodName", "BART"]]}
{"text": "We also conduct ablation studies across different model scales and target representations , finding that , with increasing model scale , prompt tuned T5 models improve at generating target representations that are far from the pre - training distribution .", "entities": [[23, 24, "MethodName", "T5"]]}
{"text": "On a set of language understanding tasks , Lester et al ( 2021 ) show that prompt tuning becomes competitive with finetuning for the largest pre - trained T5 models ( Raffel et al , 2020 ) .", "entities": [[28, 29, "MethodName", "T5"]]}
{"text": "In this paper , we investigate prompt tuning for semantic parsing .", "entities": [[9, 11, "TaskName", "semantic parsing"]]}
{"text": "In particular , we focus on the low - resource setup because examples for semantic parsing are difficult and expensive to collect ( Wang et al , 2015 ; Marzoev et al , 2020 ) .", "entities": [[14, 16, "TaskName", "semantic parsing"]]}
{"text": "We therefore evaluate prompt tuning on two datasets : the 200 - shot version of Overnight ( Wang et al , 2015 ; Shin et al , 2021 ) and the low - resource splits TOPv2 ( Chen et al , 2020 ) .", "entities": [[35, 36, "DatasetName", "TOPv2"]]}
{"text": "On both datasets , we compare prompt tuning T5 against fine - tuning and investigate the effect of canonicalizing the meaning representation , i.e. to what extent naturalizing the logical forms influences performance .", "entities": [[8, 9, "MethodName", "T5"]]}
{"text": "In addition , we study the effect of T5 model scale on Overnight as well as varying data regimes on TOPv2 .", "entities": [[8, 9, "MethodName", "T5"], [20, 21, "DatasetName", "TOPv2"]]}
{"text": "Our main findings can be summarized as follows : For large T5 models , prompt tuning significantly outperforms fine - tuning in the low - data regime , resulting in an absolute improvement of 6 % and 15 % on Overnight and TOPv2 , respectively .", "entities": [[11, 12, "MethodName", "T5"], [42, 43, "DatasetName", "TOPv2"]]}
{"text": "With growing model size , prompt tuned T5 models are increasingly capable of outputting diverse target representations ( see Figure 1 ) .", "entities": [[7, 8, "MethodName", "T5"]]}
{"text": "On Overnight , we find that the disparity between canonical and meaning representations shrinks from 17 % to 4 % for T5 - small and T5 - xl , respectively .", "entities": [[21, 22, "MethodName", "T5"], [25, 26, "MethodName", "T5"]]}
{"text": "On TOPv2 , prompt tuned T5 - large models are much better at generating out - of - vocabulary tokens than T5 - small .", "entities": [[1, 2, "DatasetName", "TOPv2"], [5, 6, "MethodName", "T5"], [21, 22, "MethodName", "T5"]]}
{"text": "Our work is related to recent work on semantic parsing and prompt tuning , which we briefly describe below .", "entities": [[8, 10, "TaskName", "semantic parsing"]]}
{"text": "Semantic parsing is the task of converting a natural language utterance u = ( u 1 , . . .", "entities": [[0, 2, "TaskName", "Semantic parsing"]]}
{"text": "In recent years , neural sequence - to - sequence models have become the dominant approach for semantic parsing tasks ( Dong and Lapata , 2016 ) .", "entities": [[17, 19, "TaskName", "semantic parsing"]]}
{"text": "Canonicalization A common simplification step in semantic parsing is to canonicalize the meaning representations .", "entities": [[6, 8, "TaskName", "semantic parsing"]]}
{"text": "Examples of the meaning and canonical representation for Overnight and TOPv2 ( Wang et al , 2015 ; Chen et al , 2020 ) can be found in Fig .", "entities": [[10, 11, "DatasetName", "TOPv2"]]}
{"text": "2 . When canonical representations are available , Berant and Liang ( 2014 ) argue that semantic parsing can be seen as a paraphrase task .", "entities": [[16, 18, "TaskName", "semantic parsing"]]}
{"text": "Marzoev et al ( 2020 ) extends this work by showing that pre - trained language models like BERT can be effective paraphrasers .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "While Berant and Liang ( 2014 ) ; Marzoev et al ( 2020 ) use models to score canonical utterances , Shin et al ( 2021 ) propose to constrain the generation process of autoregressive models like BART and GPT - 3 .", "entities": [[37, 38, "MethodName", "BART"], [39, 40, "MethodName", "GPT"]]}
{"text": "On a number of few - shot semantic parsing tasks , they demonstrate the benefit of generating canonical representations over meaning representations .", "entities": [[7, 9, "TaskName", "semantic parsing"]]}
{"text": "Lester et al ( 2021 ) evaluates prompt tuning on SuperGLUE , a benchmark consisting of eight language understanding tasks .", "entities": [[10, 11, "DatasetName", "SuperGLUE"]]}
{"text": "They find that prompt tuning becomes competitive with fine - tuning for the largest T5 model .", "entities": [[14, 15, "MethodName", "T5"]]}
{"text": "Li and Liang ( 2021 ) propose prefix - tuning to adapt BART and GPT - 2 for natural language generation tasks .", "entities": [[12, 13, "MethodName", "BART"], [14, 15, "MethodName", "GPT"]]}
{"text": "Related to our work are also other few - shot adaptation techniques like PET ( Schick and Sch\u00fctze , 2021 ) .", "entities": [[13, 14, "DatasetName", "PET"]]}
{"text": "To evaluate low - resource prompt tuning , we compare against fine - tuned variants of the same model on two semantic parsing datasets with canonical representations available .", "entities": [[21, 23, "TaskName", "semantic parsing"]]}
{"text": "We compare both large and small variants of the T5 architecture on these datasets and experiment with various canonicalized representations .", "entities": [[9, 10, "MethodName", "T5"]]}
{"text": "Overnight The Overnight semantic parsing dataset ( Wang et al , 2015 ) consists of 13 , 682 natural utterance , canonical form , meaning representation triples split across eight domains .", "entities": [[3, 5, "TaskName", "semantic parsing"]]}
{"text": "TOPv2 Chen et al ( 2020 ) introduce the TOPv2 dataset , a task - oriented semantic parsing dataset with eight domains , two of which come with predefined low - resource splits .", "entities": [[0, 1, "DatasetName", "TOPv2"], [9, 10, "DatasetName", "TOPv2"], [16, 18, "TaskName", "semantic parsing"]]}
{"text": "The authors propose a principled way of constructing low - resource training sets , samples per intent and slot ( SPIS ) , intended to ensure equal exposure to ontology labels across domains of varying complexity .", "entities": [[29, 30, "MethodName", "ontology"]]}
{"text": "Chen et al apply a set of simple modifications to the TOPv2 meaning representations to arrive at a canonical form used in all their experiments .", "entities": [[11, 12, "DatasetName", "TOPv2"]]}
{"text": "We adopt all of these canonicalization steps ( except for lexicographic sorting of the semantic parse tree ) and add an ontology label shortening step .", "entities": [[21, 22, "MethodName", "ontology"]]}
{"text": "before feeding it to a language model with parameters \u03b8 .", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": ", p K ) exclusively , keeping the language model parameters \u03b8 and the pretrained vocabulary embeddings fixed .", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "In Table 4 , we summarize the results of the canonicalization ablation study for TOPv2 .", "entities": [[14, 15, "DatasetName", "TOPv2"]]}
{"text": "This performance disparity lessens when training data increases ; however , prompt tuned T5 - large continues to beat its finetuned counterpart by 5 points at 500 SPIS and the BART - CopyPtr model by 1.4 points .", "entities": [[13, 14, "MethodName", "T5"], [30, 31, "MethodName", "BART"]]}
{"text": "On the 25 SPIS split of TOPv2 , we see an average improvement of more than 5 points compared to the BART - CopyPTR of Chen et al ( 2020 ) .", "entities": [[6, 7, "DatasetName", "TOPv2"], [21, 22, "MethodName", "BART"]]}
{"text": "Our main finding is that prompt tuned T5 models become better at generating meaning representations with increased model size .", "entities": [[7, 8, "MethodName", "T5"]]}
{"text": "On Overnight , we see the absolute difference between canonical and meaning representations shrink from 17.5 points for T5 - small to 3.4 points for T5 - xl", "entities": [[18, 19, "MethodName", "T5"], [25, 26, "MethodName", "T5"]]}
{"text": "This gap shrinks another 18 % to 2.8 points when we apply constrained decoding to T5 - xl", "entities": [[15, 16, "MethodName", "T5"]]}
{"text": "By contrast , Shin et al ( 2021 ) reports an 11.7 point difference when prompting GPT - 3 .", "entities": [[16, 17, "MethodName", "GPT"]]}
{"text": "For our finetuning baselines , we observe a small performance gap of 4 points across target representations for BART and T5 - xl , while we observe no gap for T5 - small , T5 - base , and T5 - large models .", "entities": [[18, 19, "MethodName", "BART"], [20, 21, "MethodName", "T5"], [30, 31, "MethodName", "T5"], [34, 35, "MethodName", "T5"], [39, 40, "MethodName", "T5"]]}
{"text": "In our TOPv2 experiments we find similar evidence of large T5 model flexibility for generating sequences far from the training distribution .", "entities": [[2, 3, "DatasetName", "TOPv2"], [10, 11, "MethodName", "T5"]]}
{"text": "In particular , for our most intrusive canonicalization scheme Out - of - Vocab , which adds novel tokens to the vocabulary and leaves these embeddings un - trained , we find no significant reduction in performance for T5 - large across all data resource levels .", "entities": [[38, 39, "MethodName", "T5"]]}
{"text": "T5 - small , in comparison , sees almost a 50 % drop in performance relative to no canonicalization ( None ) at the 10 SPIS level and continues to underperform by 33 % at the 500 SPIS level .", "entities": [[0, 1, "MethodName", "T5"]]}
{"text": "Interestingly , we find that In - Vocab drastically reduces performance for T5 - small at the 10 SPIS level - 30.9 % vs. 43.4 % for None - but slightly outperforms it at 500 SPIS .", "entities": [[12, 13, "MethodName", "T5"]]}
{"text": "We find that prompt tuning is an effective method for adapting language models to the semantic parsing task .", "entities": [[15, 17, "TaskName", "semantic parsing"]]}
{"text": "We furthermore find that while canonicalizing meaning representations can slightly improve performance , the disparity between target representations decreases when prompt tuning larger T5 models .", "entities": [[23, 24, "MethodName", "T5"]]}
{"text": "We experiment with BART and T5 ( Lewis et al , 2020 ; Raffel et al , 2020 ) , two large pre - trained encoder - decoder language models .", "entities": [[3, 4, "MethodName", "BART"], [5, 6, "MethodName", "T5"]]}
{"text": "BART is trained on the same 160 GB text dataset used to train RoBERTa ( Lewis et al , 2020 )", "entities": [[0, 1, "MethodName", "BART"], [13, 14, "MethodName", "RoBERTa"]]}
{"text": "Fine - tuning baseline We compare against baselines that fine - tune all parameters of BART and T5 .", "entities": [[15, 16, "MethodName", "BART"], [17, 18, "MethodName", "T5"]]}
{"text": "We train the T5 models with AdaFactor ( Shazeer and Stern , 2018 ) and BART with Adam ( Lewis et al , 2020 ; Kingma and Ba , 2015 ) .", "entities": [[3, 4, "MethodName", "T5"], [6, 7, "MethodName", "AdaFactor"], [15, 16, "MethodName", "BART"], [17, 18, "MethodName", "Adam"]]}
{"text": "On both datasets , we train for 5000 epochs and perform model selection by early stopping on the validation set .", "entities": [[11, 13, "TaskName", "model selection"], [14, 16, "MethodName", "early stopping"]]}
{"text": "Prompt tuning We follow the prompt tuning procedure proposed by Lester et al for T5 .", "entities": [[14, 15, "MethodName", "T5"]]}
{"text": "Like the fine - tuned baseline , we perform model selection with best exact match accuracy on the validation set .", "entities": [[9, 11, "TaskName", "model selection"], [13, 15, "MetricName", "exact match"], [15, 16, "MetricName", "accuracy"]]}
{"text": "We apply the same method to BART and found that it did not converge under a number of hyperparameter configurations .", "entities": [[6, 7, "MethodName", "BART"]]}
{"text": "We therefore exclude prompt tuned BART models from our results 1 .", "entities": [[5, 6, "MethodName", "BART"]]}
{"text": "Prompt tuned parameter efficiency comes at a cost : we find that prompt tuning takes significantly longer to train with early stopping than does finetuning .", "entities": [[20, 22, "MethodName", "early stopping"]]}
{"text": "BART , GPT - 2 , and GPT - 3 results results are included from Shin et al ( 2021 )", "entities": [[0, 1, "MethodName", "BART"], [2, 3, "MethodName", "GPT"], [7, 8, "MethodName", "GPT"]]}
{"text": "In this paper we focus on a popular class of learning problems , sequence prediction applied to several sentiment analysis tasks , and suggest a modular learning approach in which different sub - tasks are learned using separate functional modules , combined to perform the final task while sharing information .", "entities": [[18, 20, "TaskName", "sentiment analysis"]]}
{"text": "Given the popularity of sequence labeling tasks in NLP , we demonstrate the strength of this approach over several sentiment analysis tasks , adapted for sequence prediction .", "entities": [[19, 21, "TaskName", "sentiment analysis"]]}
{"text": "To ensure the broad applicability of our approach to other problems , we extend the popular LSTM - CRF ( Lample et al , 2016 ) model that was applied to many sequence labeling tasks 1 .", "entities": [[16, 17, "MethodName", "LSTM"], [18, 19, "MethodName", "CRF"]]}
{"text": "The modular learning process corresponds to a task decomposition , in which the prediction label , y , is deconstructed into a set of partial labels { y 0 , .. , y k } , each defining a sub - task , capturing a different aspect of the original task .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "Our approach is related to multi - task learning ( Caruana , 1997 ) , which has been extensively applied in NLP ( Toshniwal et al , 2017 ;", "entities": [[5, 9, "TaskName", "multi - task learning"]]}
{"text": "To answer the first question , we conduct experiments over several sequence prediction tasks , and compare our approach to several recent models for deep structured prediction ( Lample et al , 2016 ; Ma and Hovy , 2016 ; Liu et al , 2018 ) , and when available , previously published results ( Mitchell et al , 2013 ; Zhang et al , 2015 ; Li and Lu , 2017 ;", "entities": [[25, 27, "TaskName", "structured prediction"]]}
{"text": "While we focus on sentiment analysis task , the framework is broadly applicable to many other tagging tasks , for example , NER ( Carreras et al , 2002 ; Lample et al , 2016 ) and SRL ( Zhou and Xu , 2015 ) , to name a few .", "entities": [[4, 6, "TaskName", "sentiment analysis"], [22, 23, "TaskName", "NER"]]}
{"text": "( 3 ) We evaluated our proposed model , in both the fullysupervised and weakly supervised scenarios , over several sentiment analysis tasks .", "entities": [[20, 22, "TaskName", "sentiment analysis"]]}
{"text": "From a technical perspective , our task decomposition approach is related to multi - task learning ( Caruana , 1997 ) , specifically , when the tasks share information using a shared deep representation ( Collobert et al , 2011 ; Luong , 2016 ) .", "entities": [[12, 16, "TaskName", "multi - task learning"]]}
{"text": "To help ensure the broad applicability of our framework , we provide a general modular network formulation for sequence labeling tasks by adapting a neural - CRF to capture the task structure .", "entities": [[26, 27, "MethodName", "CRF"]]}
{"text": "This family of models , combining structured prediction with deep learning showed promising results ( Gillick et al , 2015 ; Lample et al , 2016 ; Ma and Hovy , 2016 ;", "entities": [[6, 8, "TaskName", "structured prediction"]]}
{"text": "Using neural networks to generate emission potentials in CRFs was applied successfully in several sequence prediction tasks , such as word segmentation ( Chen et al , 2017 ) , NER ( Ma and Hovy , 2016 ; Lample et al , 2016 ) , chunking and PoS tagging ( Liu et al , 2018 ; Zhang et al , 2017 ) .", "entities": [[30, 31, "TaskName", "NER"], [45, 46, "TaskName", "chunking"]]}
{"text": "Despite the difference in tasks , these models follow a similar general architecture : ( 1 ) Characterlevel information , such as prefix , suffix and capitalization , is represented through a character embedding layer learned using a bi - directional LSTM ( BiLSTM ) .", "entities": [[41, 42, "MethodName", "LSTM"], [43, 44, "MethodName", "BiLSTM"]]}
{"text": "( 3 ) The two representations are concatenated to represent an input token , used as input to a word - level BiLSTM which generates the emission potentials for a succeeding CRF .", "entities": [[22, 23, "MethodName", "BiLSTM"], [31, 32, "MethodName", "CRF"]]}
{"text": "( 4 ) The CRF is used as an inference layer to generate the globally - normalized probability of possible tag sequences .", "entities": [[4, 5, "MethodName", "CRF"]]}
{"text": "A CRF model describes the probability of predicted labels y , given a sequence x as input , as , \u1ef9 ) is the partition function that marginalize over all possible assignments to the predicted labels of the sequence , and \u03a6 ( x , y ) is the scoring function , which is defined as : P \u039b (", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "In the Neural CRF model , \u03c6 ( x , y t ) is generated by the aforementioned Bi - LSTM while \u03c8 ( y t\u22121 , y t ) by a transition matrix .", "entities": [[3, 4, "MethodName", "CRF"], [20, 21, "MethodName", "LSTM"]]}
{"text": "Partial Labels and Task Decomposition : Given a learning task , defined over an output space y Y , where Y is the set of all possible tags , each specific label y is decomposed into a set of partial labels , { y 0 , .. , y k } .", "entities": [[44, 45, "DatasetName", "0"]]}
{"text": "Text ABC News ' President Tag B - neu", "entities": [[1, 2, "MethodName", "ABC"]]}
{"text": "O O Christiane Amanpour Exclusive Interview with", "entities": [[5, 6, "DatasetName", "Interview"]]}
{"text": "Model 1 , denoted Twofold Modular , LSTM - CRF - T , is similar in spirit to multi - task learning ( Collobert et al , 2011 ) with three separate modules .", "entities": [[7, 8, "MethodName", "LSTM"], [9, 10, "MethodName", "CRF"], [18, 22, "TaskName", "multi - task learning"]]}
{"text": "Model 2 , denoted Twofold modular Infusion , ( LSTM - CRF - TI ) and Model 3 , denoted Twofold modular Infusion with guided gating , ( LSTM - CRF - TI ( g ) )", "entities": [[9, 10, "MethodName", "LSTM"], [11, 12, "MethodName", "CRF"], [28, 29, "MethodName", "LSTM"], [30, 31, "MethodName", "CRF"]]}
{"text": "In all of these models , underlying neural architecture are used for the emission potentials when CRF inference layers are applied on top .", "entities": [[16, 17, "MethodName", "CRF"]]}
{"text": "The twofold modular model enhances the original monolithic model by using multi - task learning with shared underlying representations .", "entities": [[11, 15, "TaskName", "multi - task learning"]]}
{"text": "Since the information above the embedding level is independent , the LSTM layers in the different modules do not share information , so we refer to these layers of each module as private .", "entities": [[11, 12, "MethodName", "LSTM"]]}
{"text": "The segmentation module predicts the segmentation BIO labels at position t of the sequence by using the representations extracted from its private word level bi - directional LSTM ( denoted as H seg ) as emission for a individual CRF :", "entities": [[27, 28, "MethodName", "LSTM"], [39, 40, "MethodName", "CRF"]]}
{"text": "= W seg h seg t + b seg , where W seg and b seg denote the parameters of the segmentation module emission layer , and H seg denotes its private LSTM layer .", "entities": [[32, 33, "MethodName", "LSTM"]]}
{"text": "By using representations from the its own private LSTM layers , the type module predicts the sentiment ( entity ) type at position t of the sequence : h typ t = H typ ( e t , \u2212 h typ t\u22121 , \u2212 h typ t+1 ) , \u03c6 ( x , y typ t )", "entities": [[8, 9, "MethodName", "LSTM"]]}
{"text": "+ b , where S t is the shared final emission potential to the CRF layer in the decision module , and ; is the Figure 2 : Three modular models for task decomposition .", "entities": [[14, 15, "MethodName", "CRF"]]}
{"text": "The loss function is a linear combination of the negative log probability of each sub - tasks , together with the decision module :", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "\u03b1 log P ( y seg ( i ) |", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "+ \u03b2 log P ( y typ ( i ) |", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "If the training example is fully labeled with both segmentation and type annotated , training is straightforward ; if the training example is partially labeled , e.g. , only with segmentation but without type , we can set the log probability of the type module and the decision module 0 and only train the segmentation module .", "entities": [[49, 50, "DatasetName", "0"]]}
{"text": "To answer this question , we compare our modular architecture to the traditional neural - CRF model and several recent competitive models for sequence labeling combining inference and deep learning .", "entities": [[15, 16, "MethodName", "CRF"]]}
{"text": "We evaluated our models over three different sentiment analysis tasks adapted for sequence prediction .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}
{"text": "We included additional results for multilingual NER in the Appendix for reference .", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "We adapted the SemEval 2013 Task 2 subtask A as another task to evaluate our model .", "entities": [[3, 5, "DatasetName", "SemEval 2013"]]}
{"text": "Following previous studies ( Ma and Hovy , 2016 ; Liu et al , 2018 ) showing that the word embedding choice can significantly influence performance , we used the pre - trained GloVe 100 dimension Twitter embeddings only for all tasks in the main text .", "entities": [[33, 34, "MethodName", "GloVe"]]}
{"text": "Our models were deployed with minimal hyper parameters tuning , and can be briefly summarized as : the character embeddings has dimension 30 , the hidden layer dimension of the character level LSTM is 25 , and the hidden layer of the word level LSTM has dimension 300 .", "entities": [[32, 33, "MethodName", "LSTM"], [44, 45, "MethodName", "LSTM"]]}
{"text": "Similar to Liu et al ( 2018 ) , we also applied highway networks ( Srivastava et al , 2015 ) from the character level LSTM to the word level LSTM .", "entities": [[12, 14, "MethodName", "highway networks"], [25, 26, "MethodName", "LSTM"], [30, 31, "MethodName", "LSTM"]]}
{"text": "In our pilot study , we shrank the number of parameters in our modular architectures to around one third such that the total number of parameter is similar as that in the LSTM - CRF model , but we did not observe a significant performance change so we kept them as denoted .", "entities": [[8, 11, "HyperparameterName", "number of parameters"], [32, 33, "MethodName", "LSTM"], [34, 35, "MethodName", "CRF"]]}
{"text": "AT improves robustness to small worst - case perturbations by computing the gradients of a loss function w.r.t .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "In all three tasks , we compare with strong sequence prediction models , including LSTM - CRF ( Lample et al , 2016 ) , which is directly equivalent to our baseline model ( i.e. , final task decision without the modules ) , and LSTM - CNN - CRF ( Ma and Hovy , 2016 ) and", "entities": [[14, 15, "MethodName", "LSTM"], [16, 17, "MethodName", "CRF"], [45, 46, "MethodName", "LSTM"], [49, 50, "MethodName", "CRF"]]}
{"text": "LSTM - CRF - LM ( Liu et al , 2018 ) which use a richer latent representation for scoring the emission potentials .", "entities": [[0, 1, "MethodName", "LSTM"], [2, 3, "MethodName", "CRF"]]}
{"text": "Subjective Phrase Identification and Classification This dataset contains tweets annotated with sentiment phrases , used for training the models .", "entities": [[4, 5, "TaskName", "Classification"]]}
{"text": "We experiment with several sentiment analysis tasks .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}
{"text": "In Figure 6 , we show an example of task decomposition for standard NER .", "entities": [[13, 14, "TaskName", "NER"]]}
{"text": "Our LSTM - CRF - TI ( g ) model outperforms all the other competing models in Precision , Recall and the F1 score .", "entities": [[1, 2, "MethodName", "LSTM"], [3, 4, "MethodName", "CRF"], [17, 18, "MetricName", "Precision"], [19, 20, "MetricName", "Recall"], [22, 24, "MetricName", "F1 score"]]}
{"text": "NER datasets We evaluated our models on three NER datasets , the English , Dutch and Spanish parts of the 2002 and 2003 CoNLL shared tasks ( Sang and F. , 2002 ; Sang et al , 2003 ) .", "entities": [[0, 1, "TaskName", "NER"], [8, 9, "TaskName", "NER"]]}
{"text": "Results on NER We compared our models with the state - of - the - art systems on English 6 , Dutch and Spanish .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "Figure 8 shows the results for Dutch and Spanish NER datasets , while Figure 9 shows the results for the Subjective Polarity Disambiguation Datasets using the in - domain data .", "entities": [[9, 10, "TaskName", "NER"]]}
{"text": "We compare between our LSTM - CRF - TI ( g ) model and recent published top models on the English NER dataset in Figure 10 and on the subjec - tive polarity disambiguation datasets in Figure 11 .", "entities": [[4, 5, "MethodName", "LSTM"], [6, 7, "MethodName", "CRF"], [21, 22, "TaskName", "NER"]]}
{"text": "Our LSTM - CRF - TI ( g ) model has a much faster convergence rate compared to the other models .", "entities": [[1, 2, "MethodName", "LSTM"], [3, 4, "MethodName", "CRF"]]}
{"text": "The x - axis is number of epochs and the y - axis is the F1 - score .", "entities": [[5, 8, "HyperparameterName", "number of epochs"], [15, 18, "MetricName", "F1 - score"]]}
{"text": "This work was partially funded by a Google Gift .", "entities": [[7, 8, "DatasetName", "Google"]]}
{"text": "Identifying Aggression and Toxicity in Comments using Capsule Network", "entities": [[7, 9, "MethodName", "Capsule Network"]]}
{"text": "Aggression and related activities like trolling , hate speech etc . involve toxic comments in various forms .", "entities": [[7, 9, "DatasetName", "hate speech"]]}
{"text": "In this paper , we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment .", "entities": [[9, 11, "MethodName", "capsule network"], [12, 14, "MethodName", "focal loss"]]}
{"text": "Our model achieves competitive results over other strong baseline methods , which show its effectiveness and that focal loss exhibits significant improvement in such cases where class imbalance is a regular issue .", "entities": [[17, 19, "MethodName", "focal loss"]]}
{"text": "Additionally , we show that the problem of extensive data preprocessing , data augmentation can be tackled by capsule networks implicitly .", "entities": [[12, 14, "TaskName", "data augmentation"]]}
{"text": "We use randomly initialised word embeddings in such a case and show how they can be trained during model training procedure such that it results in clusters of OOV words which have similar meaning in Hindi .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "Early works in automated detection of abusive language made use of basic machine learning like Tf - Idf ( Yin et al , 2009 ) , SVM ( Warner and Hirschberg , 2012 ) , Naive Bayes , random forests , or logistic regression over a bag - of - ngrams and achieved limited success .", "entities": [[6, 8, "TaskName", "abusive language"], [26, 27, "MethodName", "SVM"], [42, 44, "MethodName", "logistic regression"]]}
{"text": "Recently Capsule Network ( Sabour et al , 2017 ) has been used in text classification ( Zhao et al , 2018 ) .It makes use of the dynamic routing process to alleviate the disturbance of some noise capsules which may contain background information such as stop words and words that are unrelated to specific categories and show that capsule networks achieves significant improvement over strong baseline methods .", "entities": [[1, 3, "MethodName", "Capsule Network"], [14, 16, "TaskName", "text classification"]]}
{"text": "We use focal loss ( Lin et al , 2017 ) to tackle it as it prevents the vast number of easy negatives from overwhelming the detector during training .", "entities": [[2, 4, "MethodName", "focal loss"]]}
{"text": "In our experiments we have compared performances of CNNs and RNNs as feature extractors and found that sentence representation obtained from RNNs performs better than representations obtained after applying convolution operation , although CNNs tends to perform better on short texts .", "entities": [[29, 30, "MethodName", "convolution"]]}
{"text": "Suppose we have\u00ea number of feature extractors , then the input to the Primary capsule layer will be Z R n\u00d7\u00ea ( where n is the number of timesteps in RNNs ) .", "entities": [[26, 29, "HyperparameterName", "number of timesteps"]]}
{"text": "Here , g is the nonlinear squash function which shrinks the small vectors to around 0 and large vectors around 1 .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "We seek to answer the following questions : ( 1 ) Is combination of Capsules and focal loss the new apotheosis for toxic comment classification problems ?", "entities": [[16, 18, "MethodName", "focal loss"], [22, 25, "TaskName", "toxic comment classification"]]}
{"text": "Recently , Kaggle hosted a competition named Toxic Comment Classification .", "entities": [[7, 10, "TaskName", "Toxic Comment Classification"]]}
{"text": "This dataset has been contributed by Conversation AI , which is a research initiative founded by Jigsaw and Google .", "entities": [[16, 17, "MethodName", "Jigsaw"], [18, 19, "DatasetName", "Google"]]}
{"text": "The task was comprised of calculating the log - likelihood of a sentence for the six classes , i.e. , given a sentence calculate the probability of it belonging to six classes .", "entities": [[7, 10, "MetricName", "log - likelihood"]]}
{"text": "\" First Shared Task on Aggression Identification \" released a dataset for Aggression Identification .", "entities": [[5, 7, "TaskName", "Aggression Identification"], [12, 14, "TaskName", "Aggression Identification"]]}
{"text": "We evaluate and compare our model with several strong baseline methods including : LSTM with Maxpool ( Lai et al , 2015 ) , Attention networks ( Raffel and Ellis , 2015 ) , Pre - trained LSTMs ( Dai and Le , 2015 ) , Hierarchical ConvNet ( Conneau et al , 2017a ) , Bi - LSTM with Skip - connections , variation of CNN - LSTM ( Wang et al , 2016 ) , CNN - multifilter ( Kim , 2014 ) , Bi - LSTM with xgboost and logistic regression .", "entities": [[13, 14, "MethodName", "LSTM"], [58, 59, "MethodName", "LSTM"], [68, 69, "MethodName", "LSTM"], [88, 89, "MethodName", "LSTM"], [92, 94, "MethodName", "logistic regression"]]}
{"text": "The models were first evaluated on Kaggle competition for Toxic Comment Classification .", "entities": [[9, 12, "TaskName", "Toxic Comment Classification"]]}
{"text": "After experimentation , fasttext embeddings with dimension of 300 were found to perform better than rest of the initialization process .", "entities": [[3, 4, "MethodName", "fasttext"]]}
{"text": "For CNNs , number of Kernels was chosen from the range [ 128 , 256 , 512 ] and the LSTM units were selected from the range [ 128 , 256 ] .", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "The proposed CapsNet architecture was able to beat other strong baseline algorithms with reasonable difference in accuracies with minimal preprocessing .", "entities": [[2, 3, "MethodName", "CapsNet"]]}
{"text": "All of our experiments were performed on NVIDIA Quadro M1200 4096 MB GPU system with 32 GB RAM and Intel i7 processor .", "entities": [[17, 18, "MethodName", "RAM"]]}
{"text": "For example , the second best performing model , which uses Pre - trained LSTM embeddings takes more than a day for the autoencoder to train and further 39 + minutes for each epoch .", "entities": [[14, 15, "MethodName", "LSTM"], [23, 24, "MethodName", "autoencoder"]]}
{"text": "For this we used TRAC shared dataset , initialised the word embeddings randomly and trained the model for classification process .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "We show the capability of our model to tackle the problem of overfitting , as observed during training we see the model to have comparatively lower difference in training and validation loss than other models .", "entities": [[31, 32, "MetricName", "loss"]]}
{"text": "Same can be seen from Fig : 2a the loss margin difference does n't change .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "In this work , we did basic preprocessing of the data , however in future we intend to explore more preprocessing techniques for the dataset , like data augmentation using translation approaches and methods to deal with mispelled words .", "entities": [[27, 29, "TaskName", "data augmentation"]]}
{"text": "Also , we would like to examine the usage of focal loss with the rest of the baseline models .", "entities": [[10, 12, "MethodName", "focal loss"]]}
{"text": "The intuition behind this was that , by passing most relevant features along with summary of the input to the softmax layer may enhance the clasification process .", "entities": [[20, 21, "MethodName", "softmax"]]}
{"text": "The authors reported improvenemt in error rates by good margin in many tasks like , text classification on 20 Newsgroup , IMDB etc .", "entities": [[15, 17, "TaskName", "text classification"], [21, 22, "DatasetName", "IMDB"]]}
{"text": "For our experiments we gathered many related datasets like all of Wikimedia datasets ( Wulczyn et al , 2017 ) , TRAC shared dataset , IMDB movie reviews dataset .", "entities": [[25, 28, "DatasetName", "IMDB movie reviews"]]}
{"text": "An autoencoder was trained on these datasets and the LSTMs from the encoder part were extracted and used in the classifcation task .", "entities": [[1, 2, "MethodName", "autoencoder"]]}
{"text": "The idea of applying CNNs for text classification was proposed in ( Kim , 2014 ) , where authors applied filters of different length to extract N - gram features from text .", "entities": [[6, 8, "TaskName", "text classification"]]}
{"text": "For activations we used Leaky ReLU , and performed Batch Normalization to stablize the data .", "entities": [[4, 6, "MethodName", "Leaky ReLU"], [9, 11, "MethodName", "Batch Normalization"]]}
{"text": "In our experiment , we again used Leaky ReLU for CNNs activations , filter size of 3 was fixed for the experiments to decide the dropout values and other hyperparameters tuning .", "entities": [[7, 9, "MethodName", "Leaky ReLU"]]}
{"text": "In our experiments , we fixed LSTM units to be 51 , and rest of the parameters were decided on the basis of validation - data experiments .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis", "entities": [[0, 1, "MethodName", "BERT"], [6, 8, "TaskName", "Reading Comprehension"], [9, 14, "TaskName", "Aspect - based Sentiment Analysis"]]}
{"text": "Question - answering plays an important role in e - commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making .", "entities": [[29, 31, "TaskName", "decision making"]]}
{"text": "Inspired by the recent success of machine reading comprehension ( MRC ) on formal documents , this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions .", "entities": [[0, 1, "DatasetName", "Inspired"], [6, 9, "TaskName", "machine reading comprehension"]]}
{"text": "We call this problem Review Reading Comprehension ( RRC ) .", "entities": [[5, 7, "TaskName", "Reading Comprehension"]]}
{"text": "In this work , we first build an RRC dataset called ReviewRC based on a popular benchmark for aspectbased sentiment analysis .", "entities": [[19, 21, "TaskName", "sentiment analysis"]]}
{"text": "Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC .", "entities": [[12, 17, "TaskName", "aspect - based sentiment analysis"], [33, 34, "MethodName", "BERT"], [43, 44, "MethodName", "BERT"]]}
{"text": "To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .", "entities": [[25, 27, "TaskName", "aspect extraction"], [32, 37, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "Many intelligent personal assistants ( such as Amazon Alexa and Google Assistant ) support online shopping by allowing the user to speak directly to the assistants .", "entities": [[10, 11, "DatasetName", "Google"]]}
{"text": "One major hindrance for this mode of shopping is that such systems have limited capability to answer user questions about products ( or services ) , which are vital for customer decision making .", "entities": [[31, 33, "TaskName", "decision making"]]}
{"text": "As such , an intelligent agent that can automatically answer customers ' questions is very important for the success of online businesses .", "entities": [[5, 6, "DatasetName", "agent"]]}
{"text": "Although there are existing studies that have used information retrieval ( IR ) techniques ( McAuley and Yang , 2016 ; Yu and Lam , 2018 ) to find a whole review as the response to a user question , giving the whole review to the user is undesirable as it is quite time - consuming for the user to read it .", "entities": [[8, 10, "TaskName", "information retrieval"]]}
{"text": "Inspired by the success of Machine Reading Comphrenesions ( MRC ) ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 , we propose a novel task called Review Reading Comprehension ( RRC ) as following .", "entities": [[0, 1, "DatasetName", "Inspired"], [32, 34, "TaskName", "Reading Comprehension"]]}
{"text": "Table 1 : An example of review reading comprehension : we show 3 questions and their corresponding answer spans from a review .", "entities": [[7, 9, "TaskName", "reading comprehension"]]}
{"text": "Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA )", "entities": [[10, 15, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "This work adopts BERT ( Devlin et al , 2018 ) as the base model as it achieves the state - ofthe - art performance on MRC ( Rajpurkar et al , 2016 ( Rajpurkar et", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Although BERT aims to learn contextualized representations across a wide range of NLP tasks ( to be task - agnostic ) , leveraging BERT alone still leaves the domain challenges un - 2 http://alt.qcri.org/semeval2016/ task5/. We choose these review datasets to align RRC with existing research on sentiment analysis .", "entities": [[1, 2, "MethodName", "BERT"], [23, 24, "MethodName", "BERT"], [47, 49, "TaskName", "sentiment analysis"]]}
{"text": "resolved ( as BERT is trained on Wikipedia articles and has almost no understanding of opinion text ) , and it also introduces another challenge of task - awareness ( the RRC task ) , called the task challenge .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "This challenge arises when the taskagnostic BERT meets the limited number of finetuning examples in ReviewRC ( see Sec . 5 ) for RRC , which is insufficient to fine - tune BERT to ensure full task - awareness of the system 3 .", "entities": [[6, 7, "MethodName", "BERT"], [32, 33, "MethodName", "BERT"]]}
{"text": "To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre - trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC .", "entities": [[18, 19, "MethodName", "BERT"], [34, 35, "MethodName", "BERT"]]}
{"text": "As a general - purpose approach , we show that the proposed method can also benefit ABSA tasks such as aspect extraction ( AE ) and aspect sentiment classification ( ASC ) .", "entities": [[20, 22, "TaskName", "aspect extraction"], [23, 24, "MethodName", "AE"]]}
{"text": "( 1 ) It proposes the new problem of review reading comprehension ( RRC ) .", "entities": [[10, 12, "TaskName", "reading comprehension"]]}
{"text": "( 3 ) It proposes a general - purpose posttraining approach to improve RRC , AE , and ASC .", "entities": [[15, 16, "MethodName", "AE"]]}
{"text": "Many datasets have been created for MRC from formally written and objective texts , e.g. , Wikipedia ( WikiReading ( Hewlett et al , 2016 ) , SQuAD ( Rajpurkar", "entities": [[18, 19, "DatasetName", "WikiReading"], [27, 28, "DatasetName", "SQuAD"]]}
{"text": "( Rajpurkar et al , , 2018 , Wiki - Hop ( Welbl et al , 2018 ) , DRCD ( Shao et al , 2018 ) , QuAC ( Choi et al , 2018 ) , HotpotQA ) news and other articles ( CNN / Daily Mail ( Hermann et al , 2015 ) , NewsQA ( Trischler et al , 2016 ) , RACE ( Lai et al , 2017 ) ) , fictional stories ( MCTest ( Richardson et al , 2013 ) , CBT ( Hill et al , 2015 ) , NarrativeQA ( Ko\u010disk\u1ef3 et al , 2018 ) ) , and general Web documents ( MS MARCO", "entities": [[19, 20, "DatasetName", "DRCD"], [28, 29, "DatasetName", "QuAC"], [37, 38, "DatasetName", "HotpotQA"], [44, 48, "DatasetName", "CNN / Daily Mail"], [56, 57, "DatasetName", "NewsQA"], [65, 66, "DatasetName", "RACE"], [78, 79, "DatasetName", "MCTest"], [87, 88, "DatasetName", "CBT"], [96, 97, "DatasetName", "NarrativeQA"], [111, 113, "DatasetName", "MS MARCO"]]}
{"text": "( Nguyen et al , 2016 ) , TriviaQA ( Joshi et al , 2017 ) , SearchQA ( Dunn et al , 2017 ) ) .", "entities": [[8, 9, "DatasetName", "TriviaQA"], [17, 18, "DatasetName", "SearchQA"]]}
{"text": "Also , CoQA ( Reddy et al , 2018 ) is built from multiple sources , such as Wikipedia , Reddit , News , Mid / High School Exams , Literature , etc .", "entities": [[2, 3, "DatasetName", "CoQA"], [20, 21, "DatasetName", "Reddit"]]}
{"text": "Answers from ReviewRC are extractive ( similar to SQuAD ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 ) rather than abstractive ( or generative ) ( such as in MS MARCO", "entities": [[8, 9, "DatasetName", "SQuAD"], [34, 36, "DatasetName", "MS MARCO"]]}
{"text": "( Nguyen et al , 2016 ) and CoQA ( Reddy et al , 2018 ) ) .", "entities": [[8, 9, "DatasetName", "CoQA"]]}
{"text": "This is crucial because online businesses are typically cost - sensitive and extractive answers written by humans can avoid generating incorrect answers beyond the contents in reviews by an AI agent .", "entities": [[30, 31, "DatasetName", "agent"]]}
{"text": "Although there exist researches that align reviews to questions as an information retrieval task ( McAuley and Yang , 2016 ; Yu and Lam , 2018 ) , giving a whole review to the user to read is time - consuming and not suitable for customer service settings that require interactive responses .", "entities": [[11, 13, "TaskName", "information retrieval"]]}
{"text": "al , 2016 ; Yao and Van Durme , 2014 ) or DBpedia ( Lopez et al , 2010 ; Unger et al , 2012 ) ) have been used for question answering ( Yu and Lam , 2018 ) .", "entities": [[12, 13, "DatasetName", "DBpedia"], [31, 33, "TaskName", "question answering"]]}
{"text": "Reviews also serve as a rich resource for sentiment analysis ( Pang et al , 2002 ; Hu and Liu , 2004 ; Liu , 2012Liu , , 2015 .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}
{"text": "Although documentlevel ( review ) sentiment classification may be considered as a solved problem ( given ratings are largely available ) , aspect - based sentiment analysis ( ABSA ) is still an open challenge , where alleviating the cost of human annotation is also a major issue .", "entities": [[22, 27, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "Two important tasks in ABSA are aspect extraction ( AE ) and aspect sentiment classification ( ASC ) ( Hu and Liu , 2004 ) , where the former aims to extract aspects ( e.g. , \" battery \" ) and the latter targets to identify the polarity for a given aspect ( e.g. , positive for battery ) .", "entities": [[6, 8, "TaskName", "aspect extraction"], [9, 10, "MethodName", "AE"]]}
{"text": "Although these approaches may achieve better performances by manually injecting human knowledge into the model , human baby - sat models may not be intelligent enough 6 and automated representation learning from review corpora is always preferred ( Xu et al , 2018a ; .", "entities": [[29, 31, "TaskName", "representation learning"]]}
{"text": "Although it is practical to train domain word embeddings from scratch on large - scale review corpora ( Xu et al , 2018a ) , it is impractical to train language models from scratch with limited computational resources .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "In this section , we briefly review BERT and derive its fine - tuning formulation on three ( 3 ) reviewbased end tasks .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "BERT is one of the key innovations in the recent progress of contextualized representation learning ( Peters et al , 2018 ; Howard and Ruder , 2018 ;", "entities": [[0, 1, "MethodName", "BERT"], [13, 15, "TaskName", "representation learning"]]}
{"text": "Unlike ELMo ( Peters et al , 2018 ) and ULMFiT Figure 1 : Overview of BERT settings for review reading comprehension ( RRC ) , aspect extraction ( AE ) and aspect sentiment classification ( ASC ) .", "entities": [[1, 2, "MethodName", "ELMo"], [10, 11, "MethodName", "ULMFiT"], [16, 17, "MethodName", "BERT"], [20, 22, "TaskName", "reading comprehension"], [26, 28, "TaskName", "aspect extraction"], [29, 30, "MethodName", "AE"]]}
{"text": "( Howard and Ruder , 2018 ) that are intended to provide additional features for a particular architecture that bears human 's understanding of the end task , BERT adopts a fine - tuning approach that requires almost no specific architecture for each end task .", "entities": [[28, 29, "MethodName", "BERT"]]}
{"text": "This is desired as an intelligent agent should minimize the use of prior human knowledge in the model design .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "We focus on three ( 3 ) review - based tasks : review reading comprehension ( RRC ) , aspect extraction ( AE ) and aspect sentiment classification ( ASC ) .", "entities": [[13, 15, "TaskName", "reading comprehension"], [19, 21, "TaskName", "aspect extraction"], [22, 23, "MethodName", "AE"]]}
{"text": "Following the success of SQuAD ( Rajpurkar et al , 2016 ) and BERT 's SQuAD implementation , we design review reading comprehension as follows .", "entities": [[4, 5, "DatasetName", "SQuAD"], [13, 14, "MethodName", "BERT"], [15, 16, "DatasetName", "SQuAD"], [21, 23, "TaskName", "reading comprehension"]]}
{"text": ", d n , [ SEP ] ) , where [ CLS ] is a dummy token not used for RRC and [ SEP ] is intended to separate q and d. Let BERT ( ) be the pre - trained ( or posttrained as in the next section ) BERT model .", "entities": [[33, 34, "MethodName", "BERT"], [50, 51, "MethodName", "BERT"]]}
{"text": "We first obtain the hidden representation as h = BERT ( x ) R r h * |", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "And BERT severely lacks two kinds of prior knowledge : ( 1 ) large - scale domain knowledge ( e.g. , about a specific product category ) , and ( 2 ) task - awareness knowledge ( MRC / RRC in this case ) .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "As a core task in ABSA , aspect extraction ( AE ) aims to find aspects that reviewers have expressed opinions on ( Hu and Liu , 2004 ) .", "entities": [[7, 9, "TaskName", "aspect extraction"], [10, 11, "MethodName", "AE"]]}
{"text": "After h = BERT ( x ) , we apply a dense layer and a softmax for each position of the sequence : l 3 = softmax ( W 3 h+b 3 ) , where W 3 R 3 * r h and b 3 R 3 ( 3 is the total number of labels ( BIO ) ) .", "entities": [[3, 4, "MethodName", "BERT"], [15, 16, "MethodName", "softmax"], [26, 27, "MethodName", "softmax"]]}
{"text": "Softmax is applied along the dimension of labels for each position and l 3 [ 0 , 1 ] 3 * | x | .", "entities": [[0, 1, "MethodName", "Softmax"], [15, 16, "DatasetName", "0"]]}
{"text": "AE is a task that requires intensive domain knowledge ( e.g. , knowing that \" screen \" is a part of a laptop ) .", "entities": [[0, 1, "MethodName", "AE"]]}
{"text": "Previous study ( Xu et al , 2018a ) has shown that incorporating domain word embeddings greatly improve the performance .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "Adapting BERT 's general language models to domain reviews is crucial for AE , as shown in Sec . 5 .", "entities": [[1, 2, "MethodName", "BERT"], [12, 13, "MethodName", "AE"]]}
{"text": "As a subsequent task of AE , aspect sentiment classification ( ASC ) aims to classify the sentiment polarity ( positive , negative , or neutral ) expressed on an aspect extracted from a review sentence .", "entities": [[5, 6, "MethodName", "AE"]]}
{"text": "Al - though BERT 's pre - trained weights strongly boost the performance of many other NLP tasks on formal texts , we observe in Sec . 5 that BERT 's weights only result in limited gain or worse performance compared with existing baselines .", "entities": [[3, 4, "MethodName", "BERT"], [29, 30, "MethodName", "BERT"]]}
{"text": "As discussed in the introduction , fine - tuning BERT directly on the end task that has limited tuning data faces both domain challenges and taskawareness challenge .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "To enhance the performance of RRC ( and also AE and ASC ) , we may need to reduce the bias introduced by non - review knowledge ( e.g. , from Wikipedia corpora ) and fuse domain knowledge ( DK ) ( from unsupervised domain data ) and task knowledge ( from supervised MRC task but out - of - domain data ) .", "entities": [[9, 10, "MethodName", "AE"]]}
{"text": "Given MRC is a general task with answers of questions covering almost all document contents , a large - scale MRC supervised corpus may also benefit AE and ASC .", "entities": [[26, 27, "MethodName", "AE"]]}
{"text": "To post - train on domain knowledge , we leverage the two novel pre - training objectives from BERT : masked language model ( MLM ) and next sentence 7 prediction ( NSP ) .", "entities": [[18, 19, "MethodName", "BERT"], [24, 25, "DatasetName", "MLM"]]}
{"text": "MLM is crucial for injecting review domain knowledge and for alleviating the bias of the knowledge from Wikipedia .", "entities": [[0, 1, "DatasetName", "MLM"]]}
{"text": "For example , in the Wikipedia domain , BERT may learn to guess the [ MASK ] in \" The [ MASK ] is bright \" as \" sun \" .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "Further , if the [ MASK ] ed word is an opinion word in \" The touch screen is [ MASK ] \" , this objective challenges BERT to learn the representations for fine - grained opinion words like \" great \" or \" terrible \" for [ MASK ] .", "entities": [[27, 28, "MethodName", "BERT"]]}
{"text": "The objective of NSP further encourages BERT to learn contextual representation beyond word - level .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "We let the loss function of MLM be L MLM and the loss function of next text piece prediction be L NSP , the total loss of the domain knowledge posttraining is L DK = L MLM + L NSP .", "entities": [[3, 4, "MetricName", "loss"], [6, 7, "DatasetName", "MLM"], [9, 10, "DatasetName", "MLM"], [12, 13, "MetricName", "loss"], [25, 26, "MetricName", "loss"], [36, 37, "DatasetName", "MLM"]]}
{"text": "To post - train BERT on task - aware knowledge , we use SQuAD ( 1.1 ) , which is a popular largescale MRC dataset .", "entities": [[4, 5, "MethodName", "BERT"], [13, 14, "DatasetName", "SQuAD"]]}
{"text": "Although BERT gains great success on SQuAD , this success is based on the huge amount of training examples of SQuAD ( 100 , 000 + ) .", "entities": [[1, 2, "MethodName", "BERT"], [6, 7, "DatasetName", "SQuAD"], [20, 21, "DatasetName", "SQuAD"]]}
{"text": "This amount is large enough to ameliorate the flaws of BERT that has almost no questions on the left side and no textual span predictions based on both the question and the document on the right side .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "However , a small amount of finetuning examples is not sufficient to turn BERT to be more task - aware , as shown in Sec . 5 .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "We let the loss on SQuAD be L MRC , which is in a similar setting as the loss L RRC for RRC .", "entities": [[3, 4, "MetricName", "loss"], [5, 6, "DatasetName", "SQuAD"], [18, 19, "MetricName", "loss"]]}
{"text": "As a result , the joint loss of post - training is defined as L = L DK + L MRC .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "One major issue of post - training on such a loss is the prohibitive cost of GPU memory usage .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis , we adopt the laptop and restaurant reviews of SemEval 2016", "entities": [[16, 18, "TaskName", "sentiment analysis"]]}
{"text": "We keep the split of training and testing of the SemEval 2016 Task 5 datasets and annotate multiple QAs for each review following the way of constructing QAs for the SQuAD 1.1 datasets ( Rajpurkar et al , 2016 ) .", "entities": [[30, 31, "DatasetName", "SQuAD"]]}
{"text": "Note that the annotations for sentiment analysis tasks are not exposed to annotators to avoid biased annotation on RRC .", "entities": [[5, 7, "TaskName", "sentiment analysis"]]}
{"text": "The annotated data is in the format of SQuAD 1.1 ( Rajpurkar et al , 2016 ) to ensure compatibility with existing implementations of MRC models .", "entities": [[8, 9, "DatasetName", "SQuAD"]]}
{"text": "Statistics of datasets for AE and ASC are given in Table 3 .", "entities": [[4, 5, "MethodName", "AE"]]}
{"text": "For AE , we choose SemEval 2014 Task 4 for laptop and SemEval -", "entities": [[1, 2, "MethodName", "AE"]]}
{"text": "For MRC task - awareness post - training , we leverage SQuAD 1.1 ( Rajpurkar et al , 2016 ) that come with 87 , 599 training examples from 442 Wikipedia articles .", "entities": [[11, 12, "DatasetName", "SQuAD"]]}
{"text": "We adopt BERT BASE ( uncased ) as the basis for all experiments 10 .", "entities": [[2, 3, "MethodName", "BERT"], [3, 4, "MethodName", "BASE"]]}
{"text": "Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "As BERT outperforms existing open source MRC baselines by a large margin , we do not intend to exhaust existing implementations but focus on variants of BERT introduced in this paper .", "entities": [[1, 2, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"]]}
{"text": "DrQA+MRC is derived from the above baseline with official pre - trained weights on SQuAD .", "entities": [[14, 15, "DatasetName", "SQuAD"]]}
{"text": "For AE and ASC , we summarize the scores of the state - of - the - arts on SemEval ( based the best of our knowledge ) for brevity .", "entities": [[1, 2, "MethodName", "AE"]]}
{"text": "al , 2018a ) reaches the state - ofthe - arts for AE by leveraging domain embeddings .", "entities": [[12, 13, "MethodName", "AE"]]}
{"text": "Lastly , to answer RQ1 , RQ2 , and RQ3 , we have the following BERT variants .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "BERT leverages the vanilla BERT pre - trained weights and fine - tunes on all 3 end tasks .", "entities": [[0, 1, "MethodName", "BERT"], [4, 5, "MethodName", "BERT"]]}
{"text": "We use this baseline to answer RQ2 and show that BERT 's pre - trained weights alone have limited performance gains on review - based tasks .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "BERT - DK post - trains BERT 's weights only on domain knowledge ( reviews ) and fine - tunes on the 3 end tasks", "entities": [[0, 1, "MethodName", "BERT"], [6, 7, "MethodName", "BERT"]]}
{"text": "We use BERT - DK and the following BERT - MRC to answer RQ3 .", "entities": [[2, 3, "MethodName", "BERT"], [8, 9, "MethodName", "BERT"]]}
{"text": "BERT - MRC post - trains BERT 's weights on SQuAD 1.1 and then fine - tunes on the 3 end tasks", "entities": [[0, 1, "MethodName", "BERT"], [6, 7, "MethodName", "BERT"], [10, 11, "DatasetName", "SQuAD"]]}
{"text": "BERT - PT ( proposed method ) post - trains BERT 's weights using the joint post - training algorithm in Section 4 and then fine - tunes on the 3 end tasks .", "entities": [[0, 1, "MethodName", "BERT"], [10, 11, "MethodName", "BERT"]]}
{"text": "EM requires the answers to have exact string match with human annotated answer spans .", "entities": [[0, 1, "MetricName", "EM"]]}
{"text": "F1 score is the averaged F1 scores of individual answers , which is typically higher than EM and is the major metric .", "entities": [[0, 2, "MetricName", "F1 score"], [5, 6, "MetricName", "F1"], [16, 17, "MetricName", "EM"]]}
{"text": "Each individual F1 score is the harmonic mean of individual precision and recall computed based on the number of overlapped words between the predicted answer and human annotated answers .", "entities": [[2, 4, "MetricName", "F1 score"]]}
{"text": "For AE , we use the standard evaluation scripts come with the SemEval datasets and report the F1 score .", "entities": [[1, 2, "MethodName", "AE"], [17, 19, "MetricName", "F1 score"]]}
{"text": "Results are reported as averages of 9 runs ( 9 different random seeds for random batch generation ) .", "entities": [[12, 13, "DatasetName", "seeds"]]}
{"text": "The results of RRC , AE and ASC are shown in Tables 4 , 5 and 6 , respectively .", "entities": [[5, 6, "MethodName", "AE"]]}
{"text": "To answer RQ1 , we observed that the proposed joint post - training ( BERT - PT ) has the best performance over all tasks in all domains , which show the benefits of having two types of knowledge .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "To answer RQ2 , to our surprise we found that the vanilla pre - trained weights of BERT do not work well for review - based tasks , although it achieves state - of - the - art results on many other NLP tasks ( Devlin et al , 2018 ) .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "This justifies the need to adapt BERT to review - based tasks .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "For RRC , we found that the performance gain of BERT - PT mostly comes from task - awareness ( MRC ) post - training ( as indicated by BERT - MRC ) .", "entities": [[10, 11, "MethodName", "BERT"], [29, 30, "MethodName", "BERT"]]}
{"text": "We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE .", "entities": [[1, 2, "MethodName", "AE"], [28, 29, "MethodName", "AE"]]}
{"text": "BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "We suspect that the improvements on laptop come from the fact that many answer spans in SQuAD are noun terms , which bear a closer relationship with laptop aspects .", "entities": [[16, 17, "DatasetName", "SQuAD"]]}
{"text": "MRC training data may help BERT to understand the input format of ASC given their closer input formulation .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "We further investigated the errors from BERT - PT over the 3 tasks .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "For AE , errors mostly come from annotation inconsistency and boundaries of aspects ( e.g. , apple OS is predicted as OS ) .", "entities": [[1, 2, "MethodName", "AE"]]}
{"text": "Also , BERT - PT has the problem of dealing with one sentence with two opposite opinions ( \" The screen is good but not for windows . \" ) .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "We proposed a new task called review reading comprehension ( RRC ) and investigated the possibility of turning reviews as a valuable resource for answering user questions .", "entities": [[7, 9, "TaskName", "reading comprehension"]]}
{"text": "We adopted BERT as our base model and proposed a joint post - training approach to enhancing both the domain and task knowledge .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "We further explored the use of this approach in two other review - based tasks : aspect extraction and aspect sentiment classification .", "entities": [[16, 18, "TaskName", "aspect extraction"]]}
{"text": "The framework employs reinforced neural sequence to sequence learning and information retrieval and is trained only using unlabeled non - sarcastic and sarcastic opinions .", "entities": [[5, 8, "MethodName", "sequence to sequence"], [10, 12, "TaskName", "information retrieval"]]}
{"text": "Qualitative and quantitative performance analyses on the data reveal our system 's superiority over baselines , built using known unsupervised statistical and neural machine translation and style transfer techniques .", "entities": [[23, 25, "TaskName", "machine translation"], [26, 28, "TaskName", "style transfer"]]}
{"text": "In the broader area of style transformation of texts , most of the existing works have focused narrowly on transformations at lexical and syntax levels , i.e. , text simplification ( Siddharthan , 2014 ) , text formalization ( Jain et al , 2018 ) , sentiment style transfer ( Shen et al , 2017 ;", "entities": [[28, 30, "TaskName", "text simplification"], [47, 49, "TaskName", "style transfer"]]}
{"text": "As baselines , we consider some of our simplistic model variants and existing systems for unsupervised machine translation and style transfer .", "entities": [[15, 18, "TaskName", "unsupervised machine translation"], [19, 21, "TaskName", "style transfer"]]}
{"text": "The embeddings are then passed through a layer of recurrent units such as Long Short Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) .", "entities": [[18, 19, "MethodName", "LSTM"]]}
{"text": "The output of the LSTM layers are then sent to a self - attention layer before passing through a softmax based classifier .", "entities": [[4, 5, "MethodName", "LSTM"], [19, 20, "MethodName", "softmax"]]}
{"text": "where \u03b1 i is the attention weight for the i th word , \u00b5 and \u03c3 are the mean and standard deviation for the attention vector .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "For each word in the input , if the discretized attention weight is 0 , it is filtered out .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "For this , we use a traditional sequence to sequence pipeline ( Bahdanau et al , 2014 ) with attention and copy mechanisms ( Gulcehre et al , 2016 ) .", "entities": [[7, 10, "MethodName", "sequence to sequence"]]}
{"text": "These are transformed into embeddings and are then encoded with the help of LSTM layers .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "This is a standard technique , typically used in neural machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}
{"text": "Hence , a sequence to sequence module analogous to Section 5 - grams : getting up for school facts , getting yelled at by people , trying to schedule my classes , feeling like every single person , walking to class in pouring , making people who already hate , working on my last day , spending countless hours at doctors , getting overdraft statements in mail 4 - grams : talking about world politics , stuck in a generation , sitting in class wondering , canceled at short notice , distancing myself from certain , wipe my own tears 3 - grams : born not breathing , paid to sleep , scared those faces , taking a shower , starting your monday , accused of everything , worrying about someone , fight jealousy arguments , license to trill , awarded literature prize 2 - grams : scratching itchy , looking chair , getting hiv , shot first , collecting death , lost respect 1 - gram : canceled , sleeping , trying , buying , stapling ( Riloff et al , 2013 ) 3.2 may not be very useful .", "entities": [[3, 6, "MethodName", "sequence to sequence"]]}
{"text": "We implement an information retrieval system based on PyLucene .", "entities": [[3, 5, "TaskName", "information retrieval"]]}
{"text": "To prepare the input , we implement a keyword extraction technique based on POS tagging .", "entities": [[8, 10, "TaskName", "keyword extraction"]]}
{"text": "For training the neural network , the crossentropy loss is back propagated .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "In other words , the gradient of the negative cross - entropy loss is considered to update the model parameters .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "The gradient is given by : \u2207 \u03b8 L ( \u03b8 ) = \u2207 \u03b8 T i=1", "entities": [[7, 8, "HyperparameterName", "\u03b8"], [10, 11, "HyperparameterName", "\u03b8"], [14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "i log P M \u03b8 ( \u0177 i |", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "For instance , the decoder may incur insignificant cross - entropy loss after generating a sentence like absolutely loved it , as this sentence has considerable overlap with the reference sarcastic text that provides supervision .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "However , the sarcasm scorer may be external to the sequence - tosequence pipeline , and the scoring function may not be differentiable with respect to the model M \u03b8 .", "entities": [[29, 30, "HyperparameterName", "\u03b8"]]}
{"text": "The generator , operating with a policy of P M \u03b8 ( \u0177 i | x i ) , producing an output\u0177 i with an expected reward score computed using a scorer , will thus have the following gradient : \u2207 \u03b8 RL ( \u03b8 )", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [41, 42, "HyperparameterName", "\u03b8"], [44, 45, "HyperparameterName", "\u03b8"]]}
{"text": "= \u2207 \u03b8", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "E\u0177 i \u223cP M \u03b8 ( \u0177 i |", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "= E [ R ( \u0177 i ) \u2207 \u03b8 log ( P M \u03b8 ( \u0177 i |", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "In practice , the expected reward is computed by ( a ) sampling candidate outputs from the policy P M \u03b8 , ( b ) computing the reward score for each candidate and ( c ) averaging the rewards so obtained .", "entities": [[20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "However , in our case , since some supervision is already available in the form of target sarcastic sentences , we pretrain the model with the loss minimization objective given in Eq .", "entities": [[26, 27, "MetricName", "loss"]]}
{"text": "The classifier is based on embedding , LSTM and softmax layers .", "entities": [[7, 8, "MethodName", "LSTM"], [9, 10, "MethodName", "softmax"]]}
{"text": "Since the input to the system is a list of words , it may seem that the sarcasm synthesis module may not require sequence to sequence learning , and a much simpler approach like bag - of - words to sequence generation could have been used .", "entities": [[23, 26, "MethodName", "sequence to sequence"]]}
{"text": "This makes sequence to sequence model an intuitive choice .", "entities": [[2, 5, "MethodName", "sequence to sequence"]]}
{"text": "Tweets are normalized by removing hashtags , usernames , and performing spell checking and lexical normalization using NLTK ( Loper and Bird , 2002 ) .", "entities": [[14, 16, "TaskName", "lexical normalization"]]}
{"text": "For the unlabelled sarcasm corpus S , we relied on popular datasets used for sarcasm detection tasks such as the ones by Ghosh and Veale ( 2016 ) , Riloff et al ( 2013 ) , and the Reddit Sarcasm Corpus 4 .", "entities": [[14, 16, "TaskName", "sarcasm detection"], [38, 39, "DatasetName", "Reddit"]]}
{"text": "Both the encoder and decoder have only one layer of LSTM , with a hidden dimension of 500 .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "The positive sentiment induction module , at the end of the training , produces a bigram BLEU", "entities": [[16, 17, "MetricName", "BLEU"]]}
{"text": "For optimization , cross entropy loss criterion is used .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "For evaluation , we still use the popular translation and summarization evaluation metrics METEOR ( Banerjee and Lavie , 2005 ) and ROUGE ( Lin , 2004 ) .", "entities": [[10, 11, "TaskName", "summarization"], [13, 14, "DatasetName", "METEOR"]]}
{"text": "Note that using BLEU ( Papineni et", "entities": [[3, 4, "MetricName", "BLEU"]]}
{"text": "For human evaluation , we consider only the 30 sentences randomly picked from the benchmark ( test ) dataset .", "entities": [[13, 15, "DatasetName", "the benchmark"]]}
{"text": "This is an open - sourced sarcasm generation chatbot released by Joshi et al ( 2015a ) .", "entities": [[8, 9, "TaskName", "chatbot"]]}
{"text": "Monoses : This is similar to UNMT but based on unsupervised Statistical Machine Translation ( Artetxe et al , 2018 ) .", "entities": [[12, 14, "TaskName", "Machine Translation"]]}
{"text": "The root verb is determined along with its tense and aspects with the help of its part - of - speech tags 7 .", "entities": [[16, 19, "DatasetName", "part - of"]]}
{"text": "Deep neural networks based solutions for sarcasm detection include ( Ghosh and Veale , 2016 ) who uses a combination of RNNs and CNNs for sarcasm detection , and ( Tay et al , 2018 ) , who propose a variant of CNN for extracting features related to context incongruity .", "entities": [[6, 8, "TaskName", "sarcasm detection"], [25, 27, "TaskName", "sarcasm detection"]]}
{"text": "From the perspective of language style transfer .", "entities": [[5, 7, "TaskName", "style transfer"]]}
{"text": "Shen et al ( 2017 ) propose an unsupervised scheme to learn latent content distribution across different text corpora and use it for sentiment style transfer .", "entities": [[24, 26, "TaskName", "style transfer"]]}
{"text": "propose a style transfer technique based on unsupervised MT inspired by Artetxe et al ( 2017 ) .", "entities": [[2, 4, "TaskName", "style transfer"]]}
{"text": "al ( 2018 ) have recently proposed an unsupervised statistical machine translation scheme .", "entities": [[10, 12, "TaskName", "machine translation"]]}
{"text": "Through qualitative and quantitative anal - ysis of the system 's performance on the benchmark dataset , we observed that our system often generates better sarcastic sentences compared to some of our trivial model variants , and unsupervised systems used for machine translation and sentiment style transfer .", "entities": [[13, 15, "DatasetName", "the benchmark"], [41, 43, "TaskName", "machine translation"], [45, 47, "TaskName", "style transfer"]]}
{"text": "Detection of Adverse Drug Reaction mentions in tweets using ELMo", "entities": [[9, 10, "MethodName", "ELMo"]]}
{"text": "This paper describes the models used by our team in SMM4H 2019 shared task ( Weissenbacher et al , 2019 ) .", "entities": [[10, 11, "DatasetName", "SMM4H"]]}
{"text": "For task 1 which aims to detect tweets with Adverse Drug Reaction ( ADR ) mentions we used ELMo embeddings which is a deep contextualized word representation able to capture both syntactic and semantic characteristics .", "entities": [[18, 19, "MethodName", "ELMo"]]}
{"text": "The output of this layer is fed to a series of 6 convolutional neural network layers ( CNNs ) with ReLU activation .", "entities": [[20, 21, "MethodName", "ReLU"]]}
{"text": "Max pooling with size 3 was used for the first two and last CNNs .", "entities": [[0, 2, "MethodName", "Max pooling"]]}
{"text": "The CNNs ' output was fed into a bidirectional LSTM ( Bi - LSTM ) with 2 * 200 units , whose output was flattened to feed into two dense layers .", "entities": [[8, 10, "MethodName", "bidirectional LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "We used two fully connected layers with 1024 units each , ReLU activation , and dropout of 0.5 .", "entities": [[11, 12, "MethodName", "ReLU"]]}
{"text": "Finally , we used a dense layer with size two and softmax activation .", "entities": [[11, 12, "MethodName", "softmax"]]}
{"text": "We used Adam as the optimizer and binary cross - entropy as the loss function .", "entities": [[2, 3, "MethodName", "Adam"], [5, 6, "HyperparameterName", "optimizer"], [13, 14, "MetricName", "loss"]]}
{"text": "We combined the Bi - LSTM output of the first and second models and then applied dense layers as before .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "We used a multi - head self - attention with an attention width of 15 and ReLU activation .", "entities": [[16, 17, "MethodName", "ReLU"]]}
{"text": "In our final model , we used ELMo ( Peters et al , 2018 )", "entities": [[7, 8, "MethodName", "ELMo"]]}
{"text": "In contrast to traditional word embeddings such as GloVe and word2vec , ELMo assigns each word to a vector as a function of the entire sentence containing that word .", "entities": [[4, 6, "TaskName", "word embeddings"], [8, 9, "MethodName", "GloVe"], [12, 13, "MethodName", "ELMo"]]}
{"text": "Since ELMo already captures character - level information under the hood , we decided to encircle the complexity inside the embedding layer and used only two additional dense layers with 256 and 2 units , using ReLU and softmax activations , respectively .", "entities": [[1, 2, "MethodName", "ELMo"], [36, 37, "MethodName", "ReLU"], [38, 39, "MethodName", "softmax"]]}
{"text": "Therefore , we only submitted ELMo results with 5 , 10 , and 15 epochs .", "entities": [[5, 6, "MethodName", "ELMo"]]}
{"text": "Since ADR phrases and tweets do not always lexically match , approaches such as named entity recognition ( NER ) might perform better .", "entities": [[14, 17, "TaskName", "named entity recognition"], [18, 19, "TaskName", "NER"]]}
{"text": "Other approaches to improve performance : Task 1 : Try other embeddings such as BERT I would also like to show my gratitude to Peter Leimbigler for comments that greatly improved the manuscript .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "Solving Aspect Category Sentiment Analysis as a Text Generation Task", "entities": [[3, 5, "TaskName", "Sentiment Analysis"], [7, 9, "TaskName", "Text Generation"]]}
{"text": "Aspect category sentiment analysis has attracted increasing research attention .", "entities": [[2, 4, "TaskName", "sentiment analysis"]]}
{"text": "Our method allows more direct use of pre - trained knowledge in seq2seq language models by directly following the task setting during pre - training .", "entities": [[12, 13, "MethodName", "seq2seq"]]}
{"text": "Aspect - based sentiment analysis ( ABSA ) is a finegrained sentiment analysis task that includes a number of subtasks , two of which are aspect category sentiment analysis ( ACSA ) and aspect category detection ( ACD ) .", "entities": [[0, 5, "TaskName", "Aspect - based sentiment analysis"], [11, 13, "TaskName", "sentiment analysis"], [27, 29, "TaskName", "sentiment analysis"], [33, 36, "TaskName", "aspect category detection"]]}
{"text": "The main idea is to make use of pre - trained models such as BERT ( Devlin et al , 2019a ) for representing an aspect - specific form of the input ( e.g. , by concatenating the aspect category to the end of the input sentence ( Figure 3 ( a ) ) ) , which provides useful semantic features for ACSA and ACD classifiers .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "We investigate the above potentials by casting the sentiment classification tasks into language modelling tasks .", "entities": [[12, 14, "TaskName", "language modelling"]]}
{"text": "In particular , as shown in Figure 2 , both ACSA and ACD are transformed into sequence - to - sequence ( seq2seq ) tasks , where the encoder takes the input sentence and the decoder generates a natural language sentence .", "entities": [[22, 23, "MethodName", "seq2seq"]]}
{"text": "The setting corresponds closely to the denoising auto -", "entities": [[6, 7, "TaskName", "denoising"]]}
{"text": "The price category is not discussed ( scoring : 0.1 ) encoder training scheme of BART ( Lewis et al , 2020 ) , which we use as the pre - trained model .", "entities": [[15, 16, "MethodName", "BART"]]}
{"text": "In addition to classification - based methods , we take masked language models ( MLM ) as a baseline also , for which a natural counterpart of our method is a mask - refilling task .", "entities": [[14, 15, "DatasetName", "MLM"]]}
{"text": "This MLM task corresponds closely to BERT ( Devlin et al , 2019a ) pre - training .", "entities": [[1, 2, "DatasetName", "MLM"], [6, 7, "MethodName", "BERT"]]}
{"text": "In comparison to this MLM method , a generation method can better learn the correlation between the input and output template as two related sequences , which has been demonstrated by the strong performance of BART for abstractive text summarization ( Lewis et al , 2020 ) .", "entities": [[4, 5, "DatasetName", "MLM"], [35, 36, "MethodName", "BART"], [37, 40, "TaskName", "abstractive text summarization"]]}
{"text": "Experimental results on three standard benchmarks datasets show that both generation and MLM methods outperform classification methods using the same pre - trained language models .", "entities": [[12, 13, "DatasetName", "MLM"]]}
{"text": "Finally , generation methods give stronger performances than MLM methods , outperforming the previous stateof - the - art methods by a large margin .", "entities": [[8, 9, "DatasetName", "MLM"]]}
{"text": "Aspect Category Sentiment Analysis Wang et al ( 2016 ) propose an attention - based LSTM network , which can concentrate on different parts of a sentence when different aspect categories are taken as input .", "entities": [[2, 4, "TaskName", "Sentiment Analysis"], [15, 16, "MethodName", "LSTM"]]}
{"text": "Ruder et al ( 2016 ) model the interdependencies of sentences in a text with a hierarchical bidirectional LSTM .", "entities": [[17, 19, "MethodName", "bidirectional LSTM"]]}
{"text": "Schmitt et al ( 2018 ) propose two joint models : end - to - end LSTM and end - to - end CNN , which produce all the aspect categories and their corresponding sentiment polarities at once .", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "Masked Language Model Methods There is a line of work using the masked language model ( MLM ) for natural language understanding tasks .", "entities": [[16, 17, "DatasetName", "MLM"], [19, 22, "TaskName", "natural language understanding"]]}
{"text": "The basic idea is to leverage information from pre - trained models by defining specific sentence prompt in a language modelling task .", "entities": [[19, 21, "TaskName", "language modelling"]]}
{"text": "al ( 2020 ) use prompt for few - shot learning in text classification tasks .", "entities": [[7, 11, "TaskName", "few - shot learning"], [12, 14, "TaskName", "text classification"]]}
{"text": "rephrase inputs as cloze questions for text classification .", "entities": [[6, 8, "TaskName", "text classification"]]}
{"text": "Petroni et al ( 2019 ) extract relation between entities from BERT by constructing cloze - style templates .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "Different from these template - based models , our final model uses BART for text generation , which better models the correlations between the input sentence and the output sentence compared with BERT .", "entities": [[12, 13, "MethodName", "BART"], [14, 16, "TaskName", "text generation"], [32, 33, "MethodName", "BERT"]]}
{"text": "solve the entity - relation extraction task as a multi - turn question answering generation method .", "entities": [[4, 6, "TaskName", "relation extraction"], [12, 14, "TaskName", "question answering"]]}
{"text": "Different from the above methods , our goal is to make the most of pre - trained knowledge in BART for ACSA .", "entities": [[19, 20, "MethodName", "BART"]]}
{"text": "We introduce relevant pre - trained language models in 3.1 , classification methods in Section 3.2 , MLM methods in Section 3.3 , and our generation method in Section 3.4 .", "entities": [[17, 18, "DatasetName", "MLM"]]}
{"text": "We take BERT ( Devlin et al , 2019a ) and BART ( Lewis et al , 2020 ) as the pre - trained language models .", "entities": [[2, 3, "MethodName", "BERT"], [11, 12, "MethodName", "BART"]]}
{"text": "Both are built on the Transformer ( Vaswani et al , 2017 ) architecture .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "BERT ( Devlin et al , 2019a ) is an encoder stack of Transformer for masked text filling , where a model uses the context words to predict masked words .", "entities": [[0, 1, "MethodName", "BERT"], [13, 14, "MethodName", "Transformer"]]}
{"text": "BART ( Lewis et al , 2020 ) is a denoising auto - encoder seq2seq model pre - training for natural language generation .", "entities": [[0, 1, "MethodName", "BART"], [10, 11, "TaskName", "denoising"], [14, 15, "MethodName", "seq2seq"]]}
{"text": "BART is trained to reconstruct the original text .", "entities": [[0, 1, "MethodName", "BART"]]}
{"text": "Both BERT and BART are considered as the encoders .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "MethodName", "BART"]]}
{"text": "BERT Classification BERT adopts \" [ CLS ] in - put sentence [ SEP ]", "entities": [[0, 1, "MethodName", "BERT"], [1, 2, "TaskName", "Classification"], [2, 3, "MethodName", "BERT"]]}
{"text": "BART Classification BART adopts \" S input sentence /S", "entities": [[0, 1, "MethodName", "BART"], [1, 2, "TaskName", "Classification"], [2, 3, "MethodName", "BART"]]}
{"text": "Formally , suppose that the query category is a , x 0 = S , x n+1", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "= /S , then the input to BART is x 0 : n+3", "entities": [[7, 8, "MethodName", "BART"], [10, 11, "DatasetName", "0"]]}
{"text": "The output hidden vec - tors obtained by the BART encoder ( ENCODER ) and BART decoder ( DECODER ) are : h enc = ENCODER ( x 0 : n+3 ) h 0 . . .", "entities": [[9, 10, "MethodName", "BART"], [15, 16, "MethodName", "BART"], [28, 29, "DatasetName", "0"], [33, 34, "DatasetName", "0"]]}
{"text": "h n+3 = DECODER ( h enc ; x 0 : n+3 )", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "Masked language models ( MLM ) ( Devlin et al , 2019a ) complete a given prompt by filling missing tokens .", "entities": [[4, 5, "DatasetName", "MLM"]]}
{"text": "For sentiment analysis tasks , BERT MLM adopts the input sentence and the prompt as the model input and predicts the sentiment polarity label word towards the given category .", "entities": [[1, 3, "TaskName", "sentiment analysis"], [5, 6, "MethodName", "BERT"], [6, 7, "DatasetName", "MLM"]]}
{"text": "For BART MLM , the same input is fed into the encoder and decoder , and the highest decoder prediction from label words of the MASK token is the predicted polarity label ( see Figure 3 ( b ) ) .", "entities": [[1, 2, "MethodName", "BART"], [2, 3, "DatasetName", "MLM"]]}
{"text": "We use the same template in the MLM method and generation method , following the template creation method in section 3.4.1 .", "entities": [[7, 8, "DatasetName", "MLM"]]}
{"text": "We take both ACSA and ACD as language model ranking problems under a seq2seq framework ( see Figure 3 ( c ) ) .", "entities": [[13, 14, "MethodName", "seq2seq"]]}
{"text": ", p | L | } , | L | is the polarity type size ( e.g. , p k = \" positive \" ) , and use words to define templates T", "entities": [[19, 21, "HyperparameterName", "k ="]]}
{"text": "( tc | t1 : c\u22121 , X ) ( 1 ) We calculate a score f ( T a i , p k ) for each possible polarity by employing the pre - trained generative language model ( i.e. , BART ) to score the templates , and then choose the polarity of category a i with the largest score .", "entities": [[41, 42, "MethodName", "BART"]]}
{"text": "= { ( X , T + ) \u222a ( X , T \u2212 ) } Given a sequence pair ( X , T ) , we feed the input X = x 1 : n to the BART encoder , obtaining hidden representations of the sentence : h enc = ENCODER ( x1 : n ) ( 2 )", "entities": [[38, 39, "MethodName", "BART"]]}
{"text": "= SOFTMAX ( h dec c W lm + b lm ) , ( 4 ) where W lm R d h \u00d7 | V | and", "entities": [[1, 2, "MethodName", "SOFTMAX"]]}
{"text": "b lm R | V | , | V | represents the vocab size of pre - trained BART .", "entities": [[18, 19, "MethodName", "BART"]]}
{"text": "We use the pre - trained BERT - base 1 and BARTbase 2 models for task fine - tuning .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "We compare our generation method with classification and MLM baselines ( Figure 3 ) using the same encoder .", "entities": [[8, 9, "DatasetName", "MLM"]]}
{"text": "In particular , BART generation ( i.e. , Figure 3 ( c ) ) is compared with BART classification ( Figure 3 ( a ) ) and BART MLM ( Figure 3 ( b ) ) , as well as BERT classification and BERT MLM .", "entities": [[3, 4, "MethodName", "BART"], [17, 18, "MethodName", "BART"], [27, 28, "MethodName", "BART"], [28, 29, "DatasetName", "MLM"], [40, 41, "MethodName", "BERT"], [43, 44, "MethodName", "BERT"], [44, 45, "DatasetName", "MLM"]]}
{"text": "( 1 ) non - BERT models : GCAE ( Xue and Li , 2018 ) , As - capsule and CapsNet ( Jiang et al , 2019 ) ; ( 2 ) BERT ( Devlin et al , 2019b ) based models : BERT - pair - QA - B , CapsNet - BERT ( Jiang et al , 2019 ) and AC - MIMLLN - BERT ( Li et al , 2020b ) .", "entities": [[5, 6, "MethodName", "BERT"], [21, 22, "MethodName", "CapsNet"], [33, 34, "MethodName", "BERT"], [44, 45, "MethodName", "BERT"], [52, 53, "MethodName", "CapsNet"], [54, 55, "MethodName", "BERT"], [67, 68, "MethodName", "BERT"]]}
{"text": "( 1 ) non - BERT models : LSTM ( Tang et al , 2015 ) , HAN ( Yang et al , 2016 )", "entities": [[5, 6, "MethodName", "BERT"], [8, 9, "MethodName", "LSTM"]]}
{"text": "For ACSA , we investigate the impact of manual templates using the MAMS development set .", "entities": [[12, 13, "DatasetName", "MAMS"]]}
{"text": "We can see that , first , the performance of BERT MLM and BART MLM is better than BERT classification and BART classification , respectively .", "entities": [[10, 11, "MethodName", "BERT"], [11, 12, "DatasetName", "MLM"], [13, 14, "MethodName", "BART"], [14, 15, "DatasetName", "MLM"], [18, 19, "MethodName", "BERT"], [21, 22, "MethodName", "BART"]]}
{"text": "In particular , BERT MLM gives a strong baseline , outperforming all non - BERT and BERT classification baselines .", "entities": [[3, 4, "MethodName", "BERT"], [4, 5, "DatasetName", "MLM"], [14, 15, "MethodName", "BERT"], [16, 17, "MethodName", "BERT"]]}
{"text": "Also , the BART MLM and classification models perform better than the corresponding BERT models .", "entities": [[3, 4, "MethodName", "BART"], [4, 5, "DatasetName", "MLM"], [13, 14, "MethodName", "BERT"]]}
{"text": "Second , BART generation outperforms all baselines on all three datasets , which indicates that our model can better detect multiple sentiment polarities in one sentence toward different aspect categories .", "entities": [[2, 3, "MethodName", "BART"]]}
{"text": "This shows the strength of BART pre - training for generating semantically related content , which was also reflected by the strong performance of BART on abstractive sum - We use the results reported in XRCE ( Brun et al , 2014 ) , NRC - Canada ( Kiritchenko et al , 2014 ) , BERT - pair - NLI - B and CNE - net ( Dai et al , 2020 ) .", "entities": [[5, 6, "MethodName", "BART"], [24, 25, "MethodName", "BART"], [55, 56, "MethodName", "BERT"]]}
{"text": "In contrast , the MLM method concatenates the input and output into one sequence , and thus fails to model their correlation in encoder - decoder pre - trainng .", "entities": [[4, 5, "DatasetName", "MLM"]]}
{"text": "Compared with LSTM , HAN and MR , BERT classification and BART classification outperform all baselines , which shows the effectiveness of pre - training .", "entities": [[2, 3, "MethodName", "LSTM"], [6, 7, "DatasetName", "MR"], [8, 9, "MethodName", "BERT"], [11, 12, "MethodName", "BART"]]}
{"text": "BERT MLM and BART MLM surpass BERT classification and BART classification , respectively .", "entities": [[0, 1, "MethodName", "BERT"], [1, 2, "DatasetName", "MLM"], [3, 4, "MethodName", "BART"], [4, 5, "DatasetName", "MLM"], [6, 7, "MethodName", "BERT"], [9, 10, "MethodName", "BART"]]}
{"text": "Our BART generation model achieves improvements of 1.15 % and 0.70 % over BART MLM on TripAdvisor and BeerAdvocate , respectively , demonstrating that the generation method can more effectively make use of BART for ACSA .", "entities": [[1, 2, "MethodName", "BART"], [13, 14, "MethodName", "BART"], [14, 15, "DatasetName", "MLM"], [18, 19, "DatasetName", "BeerAdvocate"], [33, 34, "MethodName", "BART"]]}
{"text": "Following Pontiki et al ( 2014b ) , we use Micro - F1 for evaluating .", "entities": [[10, 13, "MetricName", "Micro - F1"]]}
{"text": "Again BART generation achieves better results than BART classification and BART MLM .", "entities": [[1, 2, "MethodName", "BART"], [7, 8, "MethodName", "BART"], [10, 11, "MethodName", "BART"], [11, 12, "DatasetName", "MLM"]]}
{"text": "We also investigate the performance on the MAMS dataset , which consists of at least two unique aspect categories with different sentiment polarities in each input sentence .", "entities": [[7, 8, "DatasetName", "MAMS"]]}
{"text": "Table 7 shows that BART generation outperforms all baselines , indicating better ability of our model to detect multiple aspect categories in one sentence .", "entities": [[4, 5, "MethodName", "BART"]]}
{"text": "The results on Rest - 14 and MAMS are presented in Table 6 .", "entities": [[7, 8, "DatasetName", "MAMS"]]}
{"text": "We find that joint BART generation achieves better results on this task with improvements over pipeline BART generation .", "entities": [[4, 5, "MethodName", "BART"], [16, 17, "MethodName", "BART"]]}
{"text": "Joint BART generation outperforms all baselines on precision , recall and F - 1 score , which shows the advantage of joint learning .", "entities": [[1, 2, "MethodName", "BART"]]}
{"text": "( 10 , 20 , 50 , 100 , 200 , 500 instances per category type for Rest14 and MAMS ) .", "entities": [[19, 20, "DatasetName", "MAMS"]]}
{"text": "The results are shown in Figure 4 , where the methods of BERT classification , BART classification and BART MLM are also compared .", "entities": [[12, 13, "MethodName", "BERT"], [15, 16, "MethodName", "BART"], [18, 19, "MethodName", "BART"], [19, 20, "DatasetName", "MLM"]]}
{"text": "It can be seen that on all the datasets , our model outperforms BERT classification , BART classification and BART MLM , especially when the number of training instances is small .", "entities": [[13, 14, "MethodName", "BERT"], [16, 17, "MethodName", "BART"], [19, 20, "MethodName", "BART"], [20, 21, "DatasetName", "MLM"]]}
{"text": "When the number of instances grows as large as 500 , our model gives 2.24 % and 2.65 % better accuracies than BART MLM on Rest14 and MAMS , respectively .", "entities": [[22, 23, "MethodName", "BART"], [23, 24, "DatasetName", "MLM"], [27, 28, "DatasetName", "MAMS"]]}
{"text": "One possible reason is that our method makes more use of direct sentiment knowledge in the pre - trained language model by directly adopting the original structure of BART mentioned earlier .", "entities": [[28, 29, "MethodName", "BART"]]}
{"text": "The results of our zero - shot learning experiments are in Table 8 .", "entities": [[4, 8, "TaskName", "zero - shot learning"]]}
{"text": "In particular , the model trained on MAMS has a better performance on Rest14 than the reverse zero - shot setting , which proves that the MAMS dataset has a higher challenge .", "entities": [[7, 8, "DatasetName", "MAMS"], [26, 27, "DatasetName", "MAMS"]]}
{"text": "To explore the correlation between ACSA accuracy and the occurrence frequency of a given category , we split the eight categories in the MAMS test set into four subsets based on the occurrence frequency .", "entities": [[6, 7, "MetricName", "accuracy"], [23, 24, "DatasetName", "MAMS"]]}
{"text": "As the category occurrence frequency decreases , the relative gap of accuracy between the two models increases .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "Figure 6 shows typical examples from the test set which can not be inferred by the BART classification model .", "entities": [[16, 17, "MethodName", "BART"]]}
{"text": "The last instance ( c ) has conditional reasoning which is difficult for BART classification .", "entities": [[13, 14, "MethodName", "BART"]]}
{"text": "In contrast , BART generation gives the correct label by correctly recognizing the negativity in \" if there was ... would be a bit more inviting \" .", "entities": [[3, 4, "MethodName", "BART"]]}
{"text": "This is likely because our method makes use of pre - trained knowledge to infer the inter - sentential correlations between the input and the output sequences , which the BART classification model failed to achieve due to the indirect use of BART in the additional classification network .", "entities": [[30, 31, "MethodName", "BART"], [42, 43, "MethodName", "BART"]]}
{"text": "We investigated a generation method for aspect category detection ( ACD ) and aspect category sentiment analysis ( ACSA ) , which can make better use of BART 's advantages in making semantic level summaries to the input by not introducing additional model parameters .", "entities": [[6, 9, "TaskName", "aspect category detection"], [15, 17, "TaskName", "sentiment analysis"], [27, 28, "MethodName", "BART"]]}
{"text": "Experiments show that our proposed method obtains superior performance over the baseline models for both sentence - level and document - level aspect sentiment analysis .", "entities": [[23, 25, "TaskName", "sentiment analysis"]]}
{"text": "TripAdvisor ( Wang et al , 2010 ) and BeerAdvocate ( McAuley et al , 2012 ; Lei et al , 2016 ) contain seven aspects ( value , room , location , cleanliness , check in / front desk , service , and business service ) and four aspects ( feel , look , smell , and taste ) respectively .", "entities": [[9, 10, "DatasetName", "BeerAdvocate"]]}
{"text": "We also apply early stopping in training , which means that the training will stop if the performance on validation set does not improve in 5 epochs .", "entities": [[3, 5, "MethodName", "early stopping"]]}
{"text": "License details : http:// creativecommons.org/licenses/by/4.0/. 1 http://hdl.handle.net/11234/1 - 3367 2 VMWEs are represented as multisets ( i.e. bags of elements with repetition allowed ) , since the same lemma and/or POS can occur twice , as in appeler un chat un chat ' to call a cat a cat'\u21d2'to call a spade a spade ' .", "entities": [[28, 29, "DatasetName", "lemma"], [51, 52, "MethodName", "spade"], [53, 54, "MethodName", "spade"]]}
{"text": "Seen2Seen extracts lemma combinations of VMWEs seen in Train , looking for the same combinations ( within one sentence ) in Test , with an expected high recall .", "entities": [[2, 3, "DatasetName", "lemma"]]}
{"text": "It outperformed 6 other open track systems , notably those using complex neural architectures and contextual word embeddings .", "entities": [[16, 18, "TaskName", "word embeddings"]]}
{"text": "7 http://hdl.handle.net/11234/1 - 1989 8 Googletrans : https://pypi.org/project/googletrans , implementing the Google Translate API .", "entities": [[11, 12, "DatasetName", "Google"]]}
{"text": "This set is used in the following steps : 2 Translation : By translating seen VMWE types in one language we obtain a list of VMWE type candidates in another language : T RAN S L is built only for French and Italian , and is empty for other languages .", "entities": [[10, 11, "TaskName", "Translation"], [33, 34, "MethodName", "RAN"]]}
{"text": "T RAN S F R ( resp .", "entities": [[1, 2, "MethodName", "RAN"]]}
{"text": "T RAN S IT ) contains automatic translations of each VMWE in SeenV N L , with L = FR ( resp .", "entities": [[1, 2, "MethodName", "RAN"]]}
{"text": "P L Dist ( i ) is the ratio of VMWE tokens in SeenV N L in which the number of words inserted between the verb and the noun is i. For instance , P F R Dist ( 0 )", "entities": [[39, 40, "DatasetName", "0"]]}
{"text": "Then , P ( c ) = 21 ) 0 ( 0 ) 0 ( 0 ) 0 ( 0 ) SIM L 0 ( 0 ) 0 ( 0 ) 0.45 ( 11 ) 0.17 ( 6 ) 0 ( 0 ) 0 ( 0 ) 0 ( 0 ) RAN K L 0.19 ( 101 AM I ( c ) is the augmented mutual information of c 's type in the CoNLL - ST corpus .", "entities": [[9, 10, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [23, 24, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [29, 30, "DatasetName", "0"], [39, 40, "DatasetName", "0"], [41, 42, "DatasetName", "0"], [43, 44, "DatasetName", "0"], [45, 46, "DatasetName", "0"], [47, 48, "DatasetName", "0"], [49, 50, "DatasetName", "0"], [51, 52, "MethodName", "RAN"]]}
{"text": "Cand L is then ranked by RR ( c ) .", "entities": [[6, 7, "DatasetName", "RR"]]}
{"text": "12 This n - best list is called RAN K L n .", "entities": [[8, 9, "MethodName", "RAN"]]}
{"text": "Therefore , every c in Cand L is annotated as an LVC if c belongs to RAN K L n or if c 's type belongs to M IX L \u222a SIM L \u222a T RAN S L .", "entities": [[16, 17, "MethodName", "RAN"], [35, 36, "MethodName", "RAN"]]}
{"text": "RR ( c )", "entities": [[0, 1, "DatasetName", "RR"]]}
{"text": "13 Sec . 3.1 analyses the impact of M IX L , SIM L and RAN K L n , while Sec . 3.2 discusses T RAN S L for French .", "entities": [[15, 16, "MethodName", "RAN"], [26, 27, "MethodName", "RAN"]]}
{"text": "3.1 Impact of M IX L , SIM L and RAN K L n As shown in Table 1 , using M IX L alone leads to precision values above 0.29 for 7 languages out of 14 .", "entities": [[10, 11, "MethodName", "RAN"]]}
{"text": "Conversely , RAN K L alone mostly leads to values below 0.22 ( except for Hindi with P = 0.46 ) .", "entities": [[2, 3, "MethodName", "RAN"]]}
{"text": "In French , by dividing n by 4 in RAN K F R n , the precision would have increased from 0.19 to 0.45 ( 18 VMWEs over 40 candidates ) .", "entities": [[9, 10, "MethodName", "RAN"]]}
{"text": "In other words , using RAN K L n in step 4 can slightly increase recall but causes a drop in precision , unless n is low .", "entities": [[5, 6, "MethodName", "RAN"]]}
{"text": "Hindi appears as an exception : no negative impact is observed with RAN K HI n due to a bias in the corpora ( compound mentioned in the dependency label ) .", "entities": [[12, 13, "MethodName", "RAN"]]}
{"text": "3.2 Impact of T RAN S L : ( IT ) Traduttore , traditore ' translator , traitor ' ? With translational equivalences , we hypothesized that T RAN S L would lead to situations such as : exact matches : ( PT ) cometer crime ' commit a crime ' ( FR ) commettre crime , partial matches leading to VMWEs nonetheless : ( PT ) causar problema 'cause problem ' ( FR ) causer ennui , instead of causer probl\u00e8me , no match , but another VMWE : ( PT ) ter destaque ' highlight ' ( FR ) mettre en \u00e9vidence .", "entities": [[4, 5, "MethodName", "RAN"], [28, 29, "MethodName", "RAN"]]}
{"text": "jeter la serviette instead of jeter l'\u00e9ponge ' throw the sponge ' , non - existing VMWEs in the target language : ( TR ) el atma ( FR ) lancer main ' throw hand ' We focus on French due to the high number of candidates in T RAN S F R .", "entities": [[49, 50, "MethodName", "RAN"]]}
{"text": "In Test - FR , among the 44 annotated verb - noun candidates using T RAN S F R alone , 18 are actually VMWEs and 3 partially correspond to VMWEs due to omitted determiners , yielding an unseen MWE - based precision of 0.41 and an unseen token - based precision value of 0.48 .", "entities": [[15, 16, "MethodName", "RAN"]]}
{"text": "These 21 candidates are mainly provided by Greek ( 10 vs. 6 from PT and 0 from IT or RO ) .", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "Consequently , translation may be a clue to discover unseen VMWEs , since 78 % of Cand F R \u2229 T RAN S F R are VMWEs out of context , but barely half of them were manually annotated in context .", "entities": [[21, 22, "MethodName", "RAN"]]}
{"text": "As highlighted above , a restriction to the most frequent VMWE syntactic relations could help filter out coincidental occurrences corresponding to 39 % of false positives ( e.g. lancer la balle \u00e0 la main OBL : MOD ' throw the ball with the hand ' ) .", "entities": [[36, 37, "DatasetName", "MOD"]]}
{"text": "Using contextual rather than non - contextual word embeddings might also be helpful , even if computationally more intensive .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "We could also combine T RAN S L and M IX L \u222a SIM L by applying lexical substitution to the translated VMWEs .", "entities": [[5, 6, "MethodName", "RAN"]]}
{"text": "Deep learning ( DL ) is being used extensively for text classification .", "entities": [[10, 12, "TaskName", "text classification"]]}
{"text": "Additionally , Sample Shielding maintains near original accuracy when applied to original texts .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "However , this type of reconfiguration will not be possible if a third party classifier ( e.g. Google Perspective ) is leveraged .", "entities": [[17, 18, "DatasetName", "Google"]]}
{"text": "For example , BERT - Attack ( Li et al , 2020 ) only perturbs up to 16 % of text , and often far less ( e.g. 1.1 % ) for some datasets .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Two have binary class labels ( Yelp , IMDB ) and the third has multi class labels ( AG News ) .", "entities": [[8, 9, "DatasetName", "IMDB"], [18, 20, "DatasetName", "AG News"]]}
{"text": "1 . IMDB - Movie review dataset for binary sentiment classification .", "entities": [[2, 3, "DatasetName", "IMDB"]]}
{"text": "3 . AG News - News articles from over 2000 news sources annotated by type of news : Sports , World , Business , and Science / Tech .", "entities": [[2, 4, "DatasetName", "AG News"]]}
{"text": "TextFooler leverages word embeddings for word replacements , Bert - Attack leverages BERT itself by masking words and using BERT suggestions , PWWS selects and weights word replacements from WordNet .", "entities": [[2, 4, "TaskName", "word embeddings"], [12, 13, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"]]}
{"text": "CNN - A word based CNN ( Kim , 2014 ) , with three window sizes ( 3 , 4 , 5 ) , 100 filters per window with dropout of 0.3 and Glove embeddings .", "entities": [[33, 35, "MethodName", "Glove embeddings"]]}
{"text": "2 . LSTM - A word based bidirectional LSTM with 150 hidden units .", "entities": [[2, 3, "MethodName", "LSTM"], [7, 9, "MethodName", "bidirectional LSTM"]]}
{"text": "As with the CNN a dropout of 0.3 is used and Glove embeddings are leveraged .", "entities": [[11, 13, "MethodName", "Glove embeddings"]]}
{"text": "3 . BERT - The 12 layer BERT base model which has been fine - tuned on the corresponding dataset .", "entities": [[2, 3, "MethodName", "BERT"], [7, 8, "MethodName", "BERT"]]}
{"text": "For all attacks , we leverage TextAttack framework 7 which provides classification algorithms and adversarial text generation algorithms implemented as specified in respective papers ( Morris et al , 2020 ) .", "entities": [[14, 16, "TaskName", "adversarial text"]]}
{"text": "\u2212 Attacked Acc .", "entities": [[2, 3, "MetricName", "Acc"]]}
{"text": "Original Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}
{"text": "TextFooler and Bert - Attack are the most successful , dropping accuracies to 0 - 5 % generally .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "As in the previous condition , classifiers perform well on original texts ( Table 1 ) with BERT often achieving the highest accuracies .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "For example , on the IMDB - TextFooler combination , attack success rate drops from 100 to 5 for LSTM , 100 to 1 for CNN , and 99 to 6 against BERT .", "entities": [[5, 6, "DatasetName", "IMDB"], [19, 20, "MethodName", "LSTM"], [32, 33, "MethodName", "BERT"]]}
{"text": "The largest protection provided by Sample Shielding ( 100 % ) is for TextBugger vs CNN in IMDB .", "entities": [[17, 18, "DatasetName", "IMDB"]]}
{"text": "The smallest is for 85 % ( PWWS vs LSTM ) .", "entities": [[9, 10, "MethodName", "LSTM"]]}
{"text": "The recovered accuracies are only 13 to 0 percent away from the originals .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "Optimal p. Figure 4 shows the results for all com - binations of attacks against LSTM on IMDB with word shielding as the defense , k fixed at 100 .", "entities": [[15, 16, "MethodName", "LSTM"], [17, 18, "DatasetName", "IMDB"]]}
{"text": "As we increase p , we see a continued drop in accuracy which is consistent with the idea that a higher p is more likely to capture perturbed text .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "We also examined the same combination on AG News ( Figure 5 ) since it 's texts are considerably shorter and found consistent results .", "entities": [[7, 9, "DatasetName", "AG News"]]}
{"text": "Optimal k. Figure 6 shows results for all attacks against LSTM on IMDB with word sampling as the defense , p fixed at 0.3 .", "entities": [[10, 11, "MethodName", "LSTM"], [12, 13, "DatasetName", "IMDB"]]}
{"text": "We also found similar results when examining the same combination on AG News ( Figure 7 ) , however , k stabilized lower ( about 50 ) .", "entities": [[11, 13, "DatasetName", "AG News"]]}
{"text": "To address this , we ran Sample Shielding 100 times on the IMDB attacked texts from Table 3 against BERT classifier .", "entities": [[12, 13, "DatasetName", "IMDB"], [19, 20, "MethodName", "BERT"]]}
{"text": "We calculate accuracies right after the final perturbed text is generated using W \u2032 eliminating a followup round of W with Sample Shielding . 3 ) .", "entities": [[16, 17, "DatasetName", "followup"]]}
{"text": "We have not compared with them because these two papers appeared very recently , one last revised in July ( Zeng et al , 2021 ) and the other appeared in arXiv in September 2021 ( Wang et al , 2021a ) .", "entities": [[31, 32, "DatasetName", "arXiv"]]}
{"text": "Second , the neural net summarizer leverages a simple linear layer .", "entities": [[9, 11, "MethodName", "linear layer"]]}
{"text": "Other networks , e.g. , LSTM , maybe better at finding patterns in sequential data .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "Another limitation of our current method is that we do not measure Sample Shielding 's effectiveness on other common text tasks including Natural Language Understanding .", "entities": [[22, 25, "TaskName", "Natural Language Understanding"]]}
{"text": "If the samples vote for a different label than the label produced by the unsampled input , then the text is labeled as an adversarial text .", "entities": [[24, 26, "TaskName", "adversarial text"]]}
{"text": "For example , Rodriguez and Galeano ( 2018 ) defend Perspective ( Google 's toxicity classification model ) by neutralizing adversarial inputs via a negated predicates list .", "entities": [[12, 13, "DatasetName", "Google"]]}
{"text": "Thus , we expect Sample Shielding to cause ripples in future adversarial attack strategies while providing text classifiers with a definite advantage .", "entities": [[11, 13, "TaskName", "adversarial attack"]]}
{"text": "IIE 's Neural Machine Translation Systems for WMT20", "entities": [[3, 5, "TaskName", "Machine Translation"]]}
{"text": "Our systems are based on the Transformer architecture with some effective improvements .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "Multiscale collaborative deep architecture , data selection , back translation , knowledge distillation , domain adaptation , model ensemble and re - ranking are employed and proven effective in our experiments .", "entities": [[11, 13, "MethodName", "knowledge distillation"], [14, 16, "TaskName", "domain adaptation"]]}
{"text": "Our methods are based on techniques and approaches used in submissions from past years ( Deng et al , 2018 ; Ng et al , 2019 ; Sun et al , 2019 ; Li et al , 2019 ; Xia et al , 2019 ) , including the use of subword models ( Sennrich et al , 2016 ) , iterative back - translation , knowledge distillation , model ensembling and several techniques we proposed recently ( Wei et al , 2020b , a ) .", "entities": [[15, 18, "DatasetName", "Deng et al"], [65, 67, "MethodName", "knowledge distillation"]]}
{"text": "For our submissions of two language directions , we adopt the deep transformer architectures ( 48layer ) based on multiscale collaboration mechanism ( Wei et al , 2020b ) as our baseline , which outperformed the standard Transformer - Big as well as shallower models significantly in terms of translation quality .", "entities": [[37, 38, "MethodName", "Transformer"]]}
{"text": "Moreover , the knowledge distillation ( Freitag et al , 2017 ) is employed to leverage the source - side monolingual data .", "entities": [[3, 5, "MethodName", "knowledge distillation"]]}
{"text": "= G ( B n\u22121 d , B n e ; \u0398 n d ) + B n\u22121 d .", "entities": [[11, 12, "HyperparameterName", "\u0398"]]}
{"text": "To model long - term spatial dependencies and reuse global representations , we define a GRU cell Q ( c , x ) , which maps a hidden state c and an additional inputx into a new hidden state : C n = Q", "entities": [[15, 16, "MethodName", "GRU"]]}
{"text": "n [ 1 , N ] C 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "( H n , l\u22121 e , C n\u22121 ; \u0398 n , l e )", "entities": [[10, 11, "HyperparameterName", "\u0398"]]}
{"text": "+ H n , l\u22121 e , H n , 0 e = B n\u22121 e .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "= G ( B n\u22121 d , B n e , C n ; \u0398 n d ) + B n\u22121 d .", "entities": [[14, 15, "HyperparameterName", "\u0398"]]}
{"text": "Back - translation ( BT ) is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system .", "entities": [[12, 14, "TaskName", "data augmentation"]]}
{"text": "The process can be summarized as below : step 1 : we train both a source - to - target model ( M 0 x y ) and a target - to - source model ( M 0 y x ) using the human translated data . step 2 : we use M t", "entities": [[23, 24, "DatasetName", "0"], [37, 38, "DatasetName", "0"]]}
{"text": "y x to translate target - side monolingual data to source language , where t starts from 0 . step 3 : we combine both the human translated data and pseudo data synthesized in step 2 to further optimize the two NMT models respectively .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "The early adoption of knowledge distillation ( KD ) ( Kim and Rush , 2016 ) is for model compression .", "entities": [[4, 6, "MethodName", "knowledge distillation"], [18, 20, "TaskName", "model compression"]]}
{"text": "We first select 100 K sentence - pairs from the bilingual as well as pseudo - generated data according to the filter method in Deng et al ( 2018 ) and continue to train the model on the filtered data .", "entities": [[24, 27, "DatasetName", "Deng et al"]]}
{"text": "The weights \u03bb 1 and \u03bb 2 are determined by tuning them with a random search on a validation set and selecting the weights that give the best performance .", "entities": [[14, 16, "MethodName", "random search"]]}
{"text": "We share a vocabulary for the two languages and apply BPE for word segmentation with 32 K merge operations .", "entities": [[10, 11, "MethodName", "BPE"]]}
{"text": "We use the PyTorch implementation of Transformer 2 .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "We choose the Transformer base setting , in which the encoder and decoder are of 48 and 6 layers , respectively .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "It addresses IsA relations from their formal definition ; through practical choices made in their representation within the larger and more widely - used of the available knowledge resources ; to their automatic acquisition from document repositories , as opposed to their manual compilation by human contributors ; to their impact in text analysis and information retrieval .", "entities": [[55, 57, "TaskName", "information retrieval"]]}
{"text": "The tutorial teaches the audience about definitions , assumptions and practical choices related to modeling and representing IsA relations in existing , human - compiled resources of instances , concepts and resulting conceptual hierarchies ; methods for automatically extracting sets of instances within unlabeled or labeled concepts , where the concepts may be considered as a flat set or organized hierarchically ; and applications of IsA relations in information retrieval .", "entities": [[68, 70, "TaskName", "information retrieval"]]}
{"text": "Marius Pa\u015fca is a research scientist at Google .", "entities": [[7, 8, "DatasetName", "Google"]]}
{"text": "The use of word embeddings is an important NLP technique for extracting meaningful conclusions from corpora of human text .", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "One important question that has been raised about word embeddings is the degree of gender bias learned from corpora .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "al ( 2016 ) proposed an important technique for quantifying gender bias in word embeddings that , at its heart , is lexically based and relies on sets of highly gendered word pairs ( e.g. , mother / father and madam / sir ) and a list of professions words ( e.g. , doctor and nurse ) .", "entities": [[13, 15, "TaskName", "word embeddings"]]}
{"text": "Natural Language Processing ( NLP ) plays a significant role in many powerful applications such as speech recognition , text translation , and autocomplete and is at the heart of many critical automated decision systems making crucial recommendations about our future world .", "entities": [[16, 18, "TaskName", "speech recognition"]]}
{"text": "Systems such as Word2Vec ( Mikolov et al , 2013 ) , GloVe ( Pennington et al , 2014 ) , and BERT ( Devlin et al , 2018 ) ingest large corpora of human text and can be used to learn semantic and syntactic relationships between words .", "entities": [[12, 13, "MethodName", "GloVe"], [22, 23, "MethodName", "BERT"]]}
{"text": "In a widely cited paper , Bolukbasi et al ( 2016 ) demonstrated that a system trained with a corpora of Google News would complete the word comparison \" man is to computer programmer as woman is to what ? \" with the response \" homemaker \" suggesting an alarming level of gender bias when used in tasks such as sorting resumes for computer programming jobs .", "entities": [[21, 22, "DatasetName", "Google"]]}
{"text": "Chen et al ( 2021 ) extended these techniques beyond English to eight other languages ( Chinese , Spanish , Arabic , German , French , Farsi , Urdu , and Wolof ) and applied them to Wikipedia corpora in each of these languages .", "entities": [[28, 29, "DatasetName", "Urdu"]]}
{"text": "NLP research often uses large , modern datasets like Google News and Wikipedia .", "entities": [[9, 10, "DatasetName", "Google"]]}
{"text": "Investments in multilingual NLP and processing of diachronic corpora are essential if we want our NLP - based automated decision making systems to more widely reflect foundational cultural norms and identity from around the world .", "entities": [[2, 4, "TaskName", "multilingual NLP"], [19, 21, "TaskName", "decision making"]]}
{"text": "In Section 4 , we discuss changes in some defining set words in Chinese using the Google Ngram Viewer .", "entities": [[16, 17, "DatasetName", "Google"]]}
{"text": "They then apply Principal Component Analysis ( PCA ) to the matrix of these distances .", "entities": [[7, 8, "MethodName", "PCA"]]}
{"text": "PCA is an approach that compresses multiple dimensions into fewer dimensions , ideally in a way that the information within the original data is not lost .", "entities": [[0, 1, "MethodName", "PCA"]]}
{"text": "al ( 2016 ) used the first eigenvalue from the PCA matrix ( i.e. the one that is larger than the rest ) .", "entities": [[10, 11, "MethodName", "PCA"]]}
{"text": "Chen et al ( 2021 ) extended the Bolukbasi et al ' method to eight languages besides English - Chinese , Spanish , Arabic , German , French , Farsi , Urdu , and Wolof .", "entities": [[31, 32, "DatasetName", "Urdu"]]}
{"text": "Second , they observed that the Bolukbasi et al 's method can not be applied directly to languages such as Spanish , Arabic , German , French , and Urdu that primarily use grammatically gendered nouns ( e.g. , escritor / escritora in Spanish vs. writer in English ) .", "entities": [[29, 30, "DatasetName", "Urdu"]]}
{"text": "Wevers ( 2019 ) also used word embeddings to examine gender bias over time .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "They used a collection of Dutch Newspaper articles spanning over four eras , training four embedding models per newspaper , one per era , using the Gensim implementation of Word2Vec to demonstrate how word embeddings can be used to examine historical language change .", "entities": [[33, 35, "TaskName", "word embeddings"]]}
{"text": "They used three datasets of the U.S. Senate speeches from 1858 to 2009 , the history of computer science ACM abstracts from 1951 to 2014 , and machine learning papers on the ArXiv from 2007 to 2015 .", "entities": [[19, 20, "DatasetName", "ACM"], [32, 33, "DatasetName", "ArXiv"]]}
{"text": "We did not train a GloVe model on the unknown books alone or the duplicate books and therefore are not reporting vocab size and token size .", "entities": [[5, 6, "MethodName", "GloVe"]]}
{"text": "We then produced a total of 16 GloVe models ( Pennington et al , 2014 ) from the three time periods of Shamela , the 11 eras of APCD , all Shamela , and all APCD .", "entities": [[7, 8, "MethodName", "GloVe"]]}
{"text": "2 Each GloVe model is a contextindependent model that produces a one - word vector ( word embedding ) for each word even if that word appears in the context a few times unlike BERT and ELMo ( Devlin et al , 2018 ; Peters et al , 2018 ) .", "entities": [[2, 3, "MethodName", "GloVe"], [34, 35, "MethodName", "BERT"], [36, 37, "MethodName", "ELMo"]]}
{"text": "Each GloVe model provides vocabulary size , token size , and word vectors .", "entities": [[1, 2, "MethodName", "GloVe"]]}
{"text": "It is important to note that before training GloVe models , it was necessary to preprocess the two datasets using Linux / Unix command - line utilities like tr ( for translating or deleting characters ) , sed ( for filtering and transforming text ) , iconv ( for converting between encoding schemes ) , and awk ( for pattern scanning and language processing ) , along with CAMeL tools ( Obeid et al , 2020 ) , an open - source python toolkit for Arabic NLP , to dediacritize the Arabic diacritical marks and remove unnecessary characters .", "entities": [[8, 9, "MethodName", "GloVe"], [20, 21, "DatasetName", "Linux"]]}
{"text": "A word cluster of chosen GloVe 's most similar words of the female profession trader ( ) in Shamela Library dataset in the time period after 1900 , illustrating that a new related - trading activity word joining the profession word cluster , ( trade/ ) 4 the time period after 1900 , showing that this male profession / position is on its way to extinction .", "entities": [[5, 6, "MethodName", "GloVe"]]}
{"text": "For example , the word ( ) for female teacher also means a school building ( ) , another word ( ) for a female pilot also means an airplane 4 English translations of the word clusters are automatically generated using Google Translator API that is included in the deep - translator Python model ( https:// deep - translator.readthedocs.io ) .", "entities": [[41, 42, "DatasetName", "Google"]]}
{"text": "It is a difficult tradeoff for Arabic NLP that other researchers are attempting to tackle with advanced techniques , such as stemming and lemmatization ( Kadri and Nie , 2006 ; Mubarak , 2017 ) .", "entities": [[23, 24, "TaskName", "lemmatization"]]}
{"text": "In all these cases , this complicates the use of both word counts and word embeddings in tracking the relative uses of profession words over time .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "To investigate the semantic meaning of related words to the trading activity , we studied GloVe 's most similar words ( calculated based on the cosine similarity between two word vectors ) for this profession word in two time periods of the Shamela Library dataset : before 1900 and after 1900 .", "entities": [[15, 16, "MethodName", "GloVe"]]}
{"text": "However , in Figure 1b , after 1900 , we see a word related to trading activity ( trade/ ) appear in the most similar words of GloVe model .", "entities": [[27, 28, "MethodName", "GloVe"]]}
{"text": "The word \" \u5973\u5b50 \" was popularly used in an - Figure 3 : A timeline of word frequencies of different translations of word ' woman ' : \" \u5973\u5b50 \" , \" \u5973\u4eba \" , and \" \u5987 \u5973 \" that were found in multi - sources printed between 1500 and 2019 using Google Books Ngram Viewer .", "entities": [[54, 55, "DatasetName", "Google"]]}
{"text": "In Figure 3 , we used Google Books Ngram Viewer to chart the word frequencies of the different translations of the word ' woman ' : \" \u5973\u5b50 \" , \" \u5973 \u4eba \" , and \" \u5987\u5973 \" found in sources printed between 1500 and 2019 in Google 's Books corpora in English , Chinese , French , German , Hebrew , Italian , Russian , or Spanish ( Karch , 2021 ) .", "entities": [[6, 7, "DatasetName", "Google"], [48, 49, "DatasetName", "Google"]]}
{"text": "Besides using Google Books Ngram Viewer , we also assembled a small collection of works that might be considered \" classics \" in Chinese spanning the period 475 BC - 1992 , for example \u53f8 \u9a6c\u8fc1 ( Records of the Grand Historian ) by Qian Sima , \u8427\u7ea2 ( Tales of Hulan River ) by Hong Xiao , and \u8bba\u8bed ( The Analects ) .", "entities": [[2, 3, "DatasetName", "Google"]]}
{"text": "Interestingly , Google Books Ngram Viewer showed that the word ' madam ' was used very frequently between 1905 and 1910 , but our small classics corpora did not include texts written in that time period .", "entities": [[2, 3, "DatasetName", "Google"]]}
{"text": "In this paper , we have focused mostly on identifying the problems with techniques applied successfully to measure gender bias in modern corpora like Google News or Wikipedia .", "entities": [[24, 25, "DatasetName", "Google"]]}
{"text": "We would also like to experiment with different advanced Arabic NLP techniques like stemming and lemmatization ( Kadri and Nie , 2006 ; Mubarak , 2017 ) and see how applying such techniques could improve the results and reduce Arabic 's orthographical ambiguity or even other Arabic NLP - related current issues like correcting spelling errors , especially in Arabic dialects , where there are no official orthography rules ( Habash et al , 2018 ) .", "entities": [[15, 16, "TaskName", "lemmatization"]]}
{"text": "Our evaluations on part - of - speech tagging , universal dependency parsing , and named entity recognition in nine diverse low - resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low - resource settings .", "entities": [[3, 9, "TaskName", "part - of - speech tagging"], [11, 13, "TaskName", "dependency parsing"], [15, 18, "TaskName", "named entity recognition"]]}
{"text": "Following Chau et al ( 2020 ) , we consider how to apply the pretrained multilingual BERT model ( MBERT ; Devlin et al , 2019 ) to a target lowresource language , for which both labeled and unlabeled data is scarce .", "entities": [[16, 17, "MethodName", "BERT"], [19, 20, "MethodName", "MBERT"]]}
{"text": "MBERT covers the languages with the 104 largest Wikipedias , and it uses this data to con - struct a wordpiece vocabulary ( Wu et al , 2016 ) and train its transformer - based architecture ( Vaswani et", "entities": [[0, 1, "MethodName", "MBERT"], [20, 21, "MethodName", "wordpiece"]]}
{"text": "Chau et al ( 2020 ) note that target low - resource languages fall into three categories with respect to MBERT 's pretraining data : the lowest - resource languages in the data ( Type 1 ) , completely unseen low - resource languages ( Type 2 ) , and low - resouce languages with more representation ( Type 0 ) .", "entities": [[20, 21, "MethodName", "MBERT"], [59, 60, "DatasetName", "0"]]}
{"text": "4 Due to their poor representation in the vocabulary , Type 1 and Type 2 languages achieve suboptimal tokenization and higher rates of the \" unknown \" wordpiece 5 when using MBERT out of the box .", "entities": [[27, 28, "MethodName", "wordpiece"], [31, 32, "MethodName", "MBERT"]]}
{"text": "MBERT 's vocabulary is heavily Latin - centric ( \u00c1cs , 2019 ; Muller et al , 2021 ) , resulting in a significantly larger portion of non - Latin scripts being represented with \" unknown \" tokens ( Pfeiffer et al , 2021b ) and further limiting the model 's ability to generalize .", "entities": [[0, 1, "MethodName", "MBERT"]]}
{"text": "In effect , MBERT 's low initial performance on such languages can be attributed to its inability to represent the script itself .", "entities": [[3, 4, "MethodName", "MBERT"]]}
{"text": "To alleviate the problem of poor tokenization , Chau et al ( 2020 ) propose to specialize MBERT using Vocabulary Augmentation ( VA ) .", "entities": [[17, 18, "MethodName", "MBERT"]]}
{"text": "Given unlabeled data in the target language , they train a new wordpiece vocabulary on the data , then select the 99 most common wordpieces in the new vocabulary that replace \" unknown \" tokens under the original vocabulary .", "entities": [[12, 13, "MethodName", "wordpiece"]]}
{"text": "They then add these 99 wordpieces to the original vocabulary and continue pretraining MBERT on the unlabeled data for additional steps .", "entities": [[13, 14, "MethodName", "MBERT"]]}
{"text": "VA yields strong gains over unadapted multilingual language models on dependency parsing in four low - resource languages with Latin scripts .", "entities": [[10, 12, "TaskName", "dependency parsing"]]}
{"text": "We can view VA and TVA as an instantation of a more general framework of vocabulary augmentation , shared by other approaches to using MBERT in low - resource settings .", "entities": [[24, 25, "MethodName", "MBERT"]]}
{"text": "Given a new vocabulary V , number of wordpieces n , and learning rate multiplier a , the n most common wordpieces in V are added to the original vocabulary .", "entities": [[12, 14, "HyperparameterName", "learning rate"]]}
{"text": "Additional pretraining is then performed , with the embeddings of the n wordpieces taking on a learning rate a times greater than the overall learning rate .", "entities": [[16, 18, "HyperparameterName", "learning rate"], [24, 26, "HyperparameterName", "learning rate"]]}
{"text": "The related E - MBERT method of sets", "entities": [[4, 5, "MethodName", "MBERT"]]}
{"text": "We expand on the dependency parsing evaluations of Chau et al ( 2020 ) by additionally considering named entity recognition and part - of - speech tagging .", "entities": [[4, 6, "TaskName", "dependency parsing"], [17, 20, "TaskName", "named entity recognition"], [21, 27, "TaskName", "part - of - speech tagging"]]}
{"text": "We follow Kondratyuk and Straka ( 2019 ) and compute the CWR for each token as a weighted sum of the activations at each MBERT layer .", "entities": [[24, 25, "MethodName", "MBERT"]]}
{"text": "For dependency parsing , we follow the setup of Chau et al ( 2020 ) and Muller et al ( 2021 ) and use the CWRs as input to the graph - based dependency parser of Dozat and Manning ( 2017 ) .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "For named entity recognition , the CWRs are used as input to a CRF layer , while part - of - speech tagging uses a linear projection atop the representations .", "entities": [[1, 4, "TaskName", "named entity recognition"], [13, 14, "MethodName", "CRF"], [17, 23, "TaskName", "part - of - speech tagging"]]}
{"text": "We train models on five different random seeds and report average scores and standard errors .", "entities": [[7, 8, "DatasetName", "seeds"]]}
{"text": "Of the lan - guages seen by MBERT , all selected Type 0 languages are within the 45 largest Wikipedias , while the remaining Type 1 languages are within the top 100 .", "entities": [[7, 8, "MethodName", "MBERT"], [12, 13, "DatasetName", "0"]]}
{"text": "The Type 2 languages , which are excluded from MBERT , are all outside of the top 150 .", "entities": [[9, 10, "MethodName", "MBERT"]]}
{"text": "Labeled Datasets For dependency parsing and part - of - speech tagging , we use datasets and train / test splits from Universal Dependencies ( Nivre et al , 2020 ) , version 2.5 ( Zeman et al , 2019 ) .", "entities": [[3, 5, "TaskName", "dependency parsing"], [6, 12, "TaskName", "part - of - speech tagging"], [22, 24, "DatasetName", "Universal Dependencies"]]}
{"text": "The Belarusian treebank lacks XPOS tags for certain examples , so we use universal part - of - speech tags instead .", "entities": [[14, 17, "DatasetName", "part - of"]]}
{"text": "Dependency parsers are trained with gold word segmentation and no part - of - speech features .", "entities": [[10, 13, "DatasetName", "part - of"]]}
{"text": "Experiments with named entity recognition use the WikiAnn dataset ( Pan et al , 2017 ) , following past work ( Muller et al , 2021 ; Pfeiffer et al , 2020 ;", "entities": [[2, 5, "TaskName", "named entity recognition"], [7, 8, "DatasetName", "WikiAnn"]]}
{"text": "We note that UD datasets were unavailable for Meadow Mari , and partitioned WikiAnn datasets were missing for Wolof .", "entities": [[3, 4, "DatasetName", "UD"], [13, 14, "DatasetName", "WikiAnn"]]}
{"text": "To measure the effectiveness of VA , we benchmark it against unadapted MBERT , as well as directly pretraining MBERT on the unlabeled data without modifying the vocabulary ( Chau et al , 2020 ; Muller et al , 2021 ; Pfeiffer et al , 2020 ) .", "entities": [[12, 13, "MethodName", "MBERT"], [19, 20, "MethodName", "MBERT"]]}
{"text": "We also evaluate two monolingual baselines that are trained on our unlabeled data : fastText embeddings ( FASTT ; Bojanowski et al , 2017 ) , which represent a static word vector approach ; and a BERT model trained from scratch ( BERT ) .", "entities": [[14, 15, "MethodName", "fastText"], [36, 37, "MethodName", "BERT"], [42, 43, "MethodName", "BERT"]]}
{"text": "For ( Liu et al , 2019 ) with a language - specific SentencePiece tokenizer ( Kudo and Richardson , 2018 ) .", "entities": [[13, 14, "MethodName", "SentencePiece"]]}
{"text": "We train with a fixed linear warmup of 1000 steps .", "entities": [[5, 7, "MethodName", "linear warmup"]]}
{"text": "To pretrain BERT models , we use the HuggingFace Transformers library ( Wolf et al , 2020 ) .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "Following Muller et al ( 2021 ) , we train a half - sized RoBERTa model with six layers and 12 attention heads .", "entities": [[14, 15, "MethodName", "RoBERTa"]]}
{"text": "We use a byte - pair vocabulary of size 52000 and a linear warmup of 1 epoch .", "entities": [[12, 14, "MethodName", "linear warmup"]]}
{"text": "Models are trained with AllenNLP , version 2.1.0 , for up to 200 epochs with early stopping based on validation performance .", "entities": [[15, 17, "MethodName", "early stopping"]]}
{"text": "Tab . 2 presents performance of the different input representations on POS tagging , dependency parsing , and named entity recognition .", "entities": [[14, 16, "TaskName", "dependency parsing"], [18, 21, "TaskName", "named entity recognition"]]}
{"text": "VA achieves strong results across all languages and tasks and is the top performer in the majority of them , suggesting that augmenting the vocabulary addresses MBERT 's limited vocabulary coverage of the target language and is beneficial during continued pretraining .", "entities": [[26, 27, "MethodName", "MBERT"]]}
{"text": "For instance , in Vietnamese , which is a Type 0 Latin script language , the improvements from VA are marginal at best , reflecting the Latindominated pretraining data of MBERT .", "entities": [[10, 11, "DatasetName", "0"], [30, 31, "MethodName", "MBERT"]]}
{"text": "However , Type 0 languages in Cyrillic and Arabic scripts , which are less represented in MBERT 's pretraining data , are more receptive to VA , with VA even outperforming all other methods for Urdu .", "entities": [[3, 4, "DatasetName", "0"], [16, 17, "MethodName", "MBERT"], [35, 36, "DatasetName", "Urdu"]]}
{"text": "Prior to specialization , MBERT is especially poorly equipped to handle unseen lowresource languages and languages in non - Latin scripts due to its inability to model the script itself .", "entities": [[4, 5, "MethodName", "MBERT"]]}
{"text": "In such cases , specialization via VA is beneficial , providing MBERT with explicit signal about the target language and script while maintaining its language - agnostic insights .", "entities": [[11, 12, "MethodName", "MBERT"]]}
{"text": "Inspired by \u00c1cs ( 2019 ) , Chau et al ( 2020 ) , and Rust et al ( 2021 ) , we quantify tokenizer coverage using the percentage of tokens in the raw text that yield unknown wordpieces when tokenized with a given vocabulary ( \" UNK token percentage \" ) .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "These are tokens whose representations contain at least partial ambiguity due to the inclusion of the unknown wordpiece .", "entities": [[17, 18, "MethodName", "wordpiece"]]}
{"text": "3 presents the UNK token percentage for each dataset using the MBERT vocabulary , averaged over each script and language type .", "entities": [[11, 12, "MethodName", "MBERT"]]}
{"text": "We also include the change in the UNK token percentage between the MBERT and VA vocabularies , which quantifies the coverage improvement .", "entities": [[12, 13, "MethodName", "MBERT"]]}
{"text": "We observe that off - the - shelf MBERT already at - tains relatively high vocabulary coverage for Type 0 and 1 languages , as well as languages written in Latin and Cyrillic scripts .", "entities": [[8, 9, "MethodName", "MBERT"], [19, 20, "DatasetName", "0"]]}
{"text": "On the other hand , up to one - fifth of the tokens in Arabic languages and one - sixth of those in Type 2 languages yield an unknown wordpiece .", "entities": [[29, 30, "MethodName", "wordpiece"]]}
{"text": "It is notable that VA does not always eliminate the issue of unknown wordpieces , even in languages for which MBERT attains high vocabulary coverage .", "entities": [[20, 21, "MethodName", "MBERT"]]}
{"text": "The resulting \u03c1 - values - 0.29 for NER , 0.56 for POS tagging , and 0.81 for UD parsing - suggest that this set of factors is meaningful for some tasks , though additional and more fine - grained analysis in future work should give a more complete explanation .", "entities": [[8, 9, "TaskName", "NER"], [18, 19, "DatasetName", "UD"]]}
{"text": "3 Mix - in Specialization : VA and Transliteration We now expand on the observation made in 2.3 regarding the difficulties that MBERT encounters when faced with unseen low - resource languages in non - Latin scripts because of its inability to model the script .", "entities": [[8, 9, "TaskName", "Transliteration"], [22, 23, "MethodName", "MBERT"]]}
{"text": "They hypothesize that the increased similarity in the languages ' writing systems , combined with MBERT 's overall Latin - centricity , provides increased opportunity for crosslingual transfer .", "entities": [[15, 16, "MethodName", "MBERT"]]}
{"text": "Furthermore , the transliteration step is performed prior to pretraining MBERT on additional unlabeled data in the target language , the same stage at which VA is performed .", "entities": [[10, 11, "MethodName", "MBERT"]]}
{"text": "These mix - ins target different components of the experimental pipeline , which naturally raises our second research question : RQ2 : How do the VA and transliteration mix - ins for MBERT compare and interact ?", "entities": [[32, 33, "MethodName", "MBERT"]]}
{"text": "For the MBERT - based models , both VA and transliteration provide strong improvements over their respective baselines .", "entities": [[2, 3, "MethodName", "MBERT"]]}
{"text": "Although VA with transliteration i m - proves over plain VA for Uyghur POS tagging and dependency parsing , it still slightly underperforms LAPT with transliteration for the latter .", "entities": [[16, 18, "TaskName", "dependency parsing"]]}
{"text": "For the two NER experiments , VA with transliteration lags both methods independently .", "entities": [[3, 4, "TaskName", "NER"]]}
{"text": "Our work follows a long line of studies investigating the performance of multilingual language models like MBERT in various settings .", "entities": [[16, 17, "MethodName", "MBERT"]]}
{"text": "The exact source of such models ' crosslingual ability is contested : early studies attributed MBERT 's success to vocabulary overlap between languages ( Cao et al , 2020 ;", "entities": [[15, 16, "MethodName", "MBERT"]]}
{"text": "Indeed , Wu and Dredze ( 2020 ) find that MBERT is unable to outperform baselines in the lowest - resource seen languages .", "entities": [[10, 11, "MethodName", "MBERT"]]}
{"text": "Our experiments build off these insights , which motivate the development of methods for adapting MBERT to target low - resource languages .", "entities": [[15, 16, "MethodName", "MBERT"]]}
{"text": "Muller et al , 2021 ; Pfeiffer et al , 2020 ) , which in turn builds off similar approaches for domain adaptation ( Gururangan et al , 2020 ; Han and Eisenstein , 2019 ) .", "entities": [[21, 23, "TaskName", "domain adaptation"]]}
{"text": "al ( 2020 ) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer , while Rust et al ( 2021 ) find that MBERT 's tokenization scheme for many languages is subpar .", "entities": [[27, 28, "MethodName", "MBERT"]]}
{"text": "Pfeiffer et al ( 2021b ) further observe that for languages with unseen scripts , a large proportion of the language is mapped to the generic \" unknown \" wordpiece , and they propose a matrix factorization - based approach to improve script representation .", "entities": [[29, 30, "MethodName", "wordpiece"]]}
{"text": "Wang et al ( 2020 ) extend MBERT 's vocabulary with an entire new vocabulary in the target language to facilitate zero - shot transfer to low - resource languages from English .", "entities": [[7, 8, "MethodName", "MBERT"]]}
{"text": "The present study most closely derives from Chau et al ( 2020 ) , who select 99 wordpieces with the greatest amount of coverage to augment MBERT 's vocabulary while preserving the remainder ; and Muller et al ( 2021 ) , who transliterate target language data into Latin script to improve vocabulary coverage .", "entities": [[26, 27, "MethodName", "MBERT"]]}
{"text": "In particular , our results in this work unify the separate findings of past works , which use MBERT as a case study ; a natural continuation would extend these methods to a broader set of multilingual models , such as mT5 ( Xue et al , 2021 ) and XLM - R ( Conneau et al , 2020a ) , in order to obtain a clearer understanding of the factors behind specialization methods ' patterns of success .", "entities": [[18, 19, "MethodName", "MBERT"], [41, 42, "MethodName", "mT5"], [50, 51, "MethodName", "XLM"]]}
{"text": "Finally , future work might shed light on the interaction between different configurations of the adaptations studied here ( e.g. , the number of wordpiece types used in vocabulary augmentation ) .", "entities": [[24, 25, "MethodName", "wordpiece"]]}
{"text": "We show that our model successfully predicts denotational probabilities for unseen phrases , and that its predictions are useful for textual entailment datasets such as SICK and SNLI .", "entities": [[25, 26, "DatasetName", "SICK"], [27, 28, "DatasetName", "SNLI"]]}
{"text": "Due in part to the Recognizing Textual Entailment ( RTE ) challenges ( Dagan et al , 2006 ) , the task of textual entailment recognition has received a lot of attention in recent years .", "entities": [[9, 10, "DatasetName", "RTE"]]}
{"text": "Although full entailment recognition systems typically require a complete NLP pipeline , including coreference resolution , etc . , this paper considers a simplified variant of this task in which the premise and hypothesis are each a single sentence .", "entities": [[13, 15, "TaskName", "coreference resolution"]]}
{"text": "This version of the textual entailment task has been popularized by two datasets , the Sentences Involving Compositional Knowl - edge ( SICK ) dataset ( Marelli et al , 2014 ) and", "entities": [[22, 23, "DatasetName", "SICK"]]}
{"text": "the Stanford Natural Language Inference ( SNLI ) corpus ( Bowman et al , 2015 ) , both of which involve a 3 - way classification for textual entailment .", "entities": [[2, 5, "TaskName", "Natural Language Inference"], [6, 7, "DatasetName", "SNLI"]]}
{"text": "SICK was created for SemEval 2014 based on image caption data and video descriptions .", "entities": [[0, 1, "DatasetName", "SICK"]]}
{"text": "Most approaches to SICK involve hand - engineered features or large collections of entailment rules ( Beltagy et al , 2015 ) .", "entities": [[3, 4, "DatasetName", "SICK"]]}
{"text": "SNLI is the largest textual entailment dataset by several orders of magnitude .", "entities": [[0, 1, "DatasetName", "SNLI"]]}
{"text": "The premises in SNLI are captions from the FLICKR30 K corpus ( Young et al , 2014 ) .", "entities": [[3, 4, "DatasetName", "SNLI"]]}
{"text": "Bowman et al ( 2015 ) initially illustrated the effectiveness of LSTMs ( Hochreiter and Schmidhuber , 1997 ) on SNLI , and recent approaches have focused on improvements in neural network architectures .", "entities": [[20, 21, "DatasetName", "SNLI"]]}
{"text": "These include sentence embedding models ( Liu et al , 2016 ; Munkhdalai and Yu , 2017a ) , neural attention models ( Rockt\u00e4schel et al , 2016 ; Parikh et al , 2016 ) , and neural tree - based models ( Munkhdalai and Yu , 2017b ; Chen et al , 2016 ) .", "entities": [[2, 4, "TaskName", "sentence embedding"]]}
{"text": "We demonstrate that the results of the LSTM model of Bowman et al ( 2015 ) can be improved by adding a single feature based on our predicted denotational probabilities .", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "They evaluate the resulting representation on lexical entailment tasks and on sentence entailment in SICK , but they re - strict SICK to a binary task and their sentence vectors result from simple composition functions ( e.g. addition ) over their word representations .", "entities": [[6, 8, "TaskName", "lexical entailment"], [14, 15, "DatasetName", "SICK"], [21, 22, "DatasetName", "SICK"]]}
{"text": "In order to compute visual denotations from the corpus , they define a set of normalization and reduction rules ( e.g. lemmatization , dropping modifiers , replacing nouns with their hypernyms , dropping PPs , extracting NPs ) that augment the original FLICKR30 K captions with a large number of shorter , more generic phrases that are each associated with a subset of the FLICKR30 K images .", "entities": [[21, 22, "TaskName", "lemmatization"]]}
{"text": "Conversely , if h contradicts p , then the conditional probability P ( h | p ) is close to 0 .", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "The origin ( the zero vector ) therefore has probability exp ( 0 ) = 1 .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "Any other vector x that does not lie on the origin ( i.e. i x i > 0 ) has probability less than 1 , and a vector x that is farther from the origin than a vector y represents a phrase x that has a smaller denotational probability than phrase y. We can visualize this as each phrase vector occupying a region in the embedding space that is proportional to the denotational probability of the phrase .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "We also can not represent two phrases that have completely disjoint denotations : in Figure 2 , the P ( X ) and P ( Y ) regions will always intersect and therefore the P ( X , Y ) region will always have an area greater than 0 .", "entities": [[48, 49, "DatasetName", "0"]]}
{"text": "We note also that our model \u2026 \u2026 w 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "w i w i+1 w N Glove embeddings LSTM RNN 512D", "entities": [[6, 8, "MethodName", "Glove embeddings"], [8, 9, "MethodName", "LSTM"]]}
{"text": "FF p ( x ) LSTM is run once per phrase x ,", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "Predictions trained with Cross Entropy Each phrase is a sequence of word embeddings that is passed through an LSTM to produce a 512d vector representation for the premise and the hypothesis .", "entities": [[11, 13, "TaskName", "word embeddings"], [18, 19, "MethodName", "LSTM"]]}
{"text": "( x | y ) for phrases x and y. This model consists of an LSTM that outputs a 512d vector which is passed through an additional 512d layer .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "We use 300d GloVe vectors ( Pennington et al , 2014 ) trained on 840B tokens as the word embedding input to the LSTM .", "entities": [[3, 4, "MethodName", "GloVe"], [23, 24, "MethodName", "LSTM"]]}
{"text": "Thus , we pass the sequence of word embeddings for phrase x through the model to get x , and we do the same for phrase y to get y.", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "Our per - example loss is the sum of the cross entropy losses for P ( x ) , P ( y ) , and P", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "x i \u2264 0 .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "This element - wise minimum is a standard pooling operation ( we take the minimum instead of the more common max pooling ) .", "entities": [[20, 22, "MethodName", "max pooling"]]}
{"text": "i is updated with respect to the P ( x | y ) loss .", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "i will always be updated with respect to the P ( x ) and P ( y ) components of the loss .", "entities": [[21, 22, "MetricName", "loss"]]}
{"text": "0 .", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "= 0 , we include phrase pairs x , y that have no images in common", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "This yields 2 million pairs where P ( x | y ) = 0 .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "= 0 as a confident prediction of contradiction .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "= 0 ( top ) and gold P ( h", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "To improve our model 's performance on longer sentences , we add the SNLI training data ( which has a mean sentence length of 11 words ) to our training data .", "entities": [[13, 14, "DatasetName", "SNLI"]]}
{"text": "We train a new model from scratch on a corpus consisting of the previously described 42 million phrase pairs and the 550 , 000 SNLI training sentence pairs ( lemmatized to match our phrase pairs ) .", "entities": [[24, 25, "DatasetName", "SNLI"]]}
{"text": "We do not train on SICK because the corpus is much smaller and has a different distribution of phenomena , including explicit negation .", "entities": [[5, 6, "DatasetName", "SICK"]]}
{"text": "We augment the SNLI data with approximate gold denotational probabilities by assigning a probability P ( S )", "entities": [[3, 4, "DatasetName", "SNLI"]]}
{"text": "Figure 6 shows the predicted probabilities on the SNLI test data when our model is trained on different distributions of data .", "entities": [[8, 9, "DatasetName", "SNLI"]]}
{"text": "The bottom row shows that when our model is trained on both denotational phrases and SNLI sentence pairs with approximate conditional probabilities , its probability predictions for longer sentences improve .", "entities": [[15, 16, "DatasetName", "SNLI"]]}
{"text": "Entailing sentence pairs have high conditional probabilities ( median 0.72 ) , neutral sentence pairs have mid - range conditional probabilities ( median 0.46 ) , and contradictory sentence pairs have conditional probabilities approaching 0", "entities": [[34, 35, "DatasetName", "0"]]}
{"text": "In Section 7.2 , we trained our probability model on both short phrase pairs for which we had gold probabilities and longer SNLI sentence pairs for which we estimated probabilities .", "entities": [[22, 23, "DatasetName", "SNLI"]]}
{"text": "We now evaluate the effectiveness of this model for textual entailment , and demonstrate that these predicted probabilities are informative features for predicting entailment on both SICK and SNLI .", "entities": [[26, 27, "DatasetName", "SICK"], [28, 29, "DatasetName", "SNLI"]]}
{"text": "It takes GloVe word vectors as input and produces 100d sentence vectors for the premise and hypothesis .", "entities": [[2, 3, "MethodName", "GloVe"]]}
{"text": "200d tanh layers and a softmax layer for 3 - class entailment classification .", "entities": [[5, 6, "MethodName", "softmax"]]}
{"text": "Next , we take the output vector produced by the LSTM for each sentence pair and append our predicted P ( h | p ) value ( the probability of the hypothesis given the premise ) .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "We train another classifier that passes this 201d vector through two tanh layers with a dropout rate of 0.5 and a final 3 - class softmax classification layer .", "entities": [[25, 26, "MethodName", "softmax"]]}
{"text": "Results Table 2 contains our results on SNLI .", "entities": [[7, 8, "DatasetName", "SNLI"]]}
{"text": "We use the same approach to evaluate the effectiveness of our predictions on SICK ( Table 3 ) .", "entities": [[13, 14, "DatasetName", "SICK"]]}
{"text": "SICK does not have enough data to train an LSTM , so we combine the SICK and SNLI training data to train both the LSTM and the final model .", "entities": [[0, 1, "DatasetName", "SICK"], [9, 10, "MethodName", "LSTM"], [15, 16, "DatasetName", "SICK"], [17, 18, "DatasetName", "SNLI"], [24, 25, "MethodName", "LSTM"]]}
{"text": "This approach outperforms the transfer learning approach of Bowman et al ( 2015 ) , which was also trained on both SICK and SNLI .", "entities": [[4, 6, "TaskName", "transfer learning"], [21, 22, "DatasetName", "SICK"], [23, 24, "DatasetName", "SNLI"]]}
{"text": "Table 6 has examples of predicted conditional probabilities for sentence pairs from the SNLI development data .", "entities": [[13, 14, "DatasetName", "SNLI"]]}
{"text": "This work was supported by NSF Grants 1563727 , 1405883 , and 1053856 , and by a Google Research Award .", "entities": [[17, 18, "DatasetName", "Google"]]}
{"text": "Action knowledge ( McDermott et al , 1998 ; Jiang et al , 2019 ) has been used to reason about action sequences and help an RL agent explore only the states that can potentially contribute to achieving the ultimate goal ( Leonetti et al , 2016 ) .", "entities": [[27, 28, "DatasetName", "agent"]]}
{"text": "Relational RL ( RRL ) combines RL with relational reasoning ( D\u017eeroski et al , 2001 ) .", "entities": [[8, 10, "TaskName", "relational reasoning"]]}
{"text": "[ 0 , 1 ] specifies the state transition probabilities .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "An agent can then use planning methods to calculate an action policy ( Sutton , 1990 ; Kocsis and Szepesv\u00e1ri , 2006 ) .", "entities": [[1, 2, "DatasetName", "agent"]]}
{"text": "This \" maximum - reward \" strategy automatically enables the agent to balance the exploration of unknown states and exploitation .", "entities": [[10, 11, "DatasetName", "agent"]]}
{"text": "We use R - Max in this work , though KRR - RL practitioners can use supervised machine learning methods , e.g. , imitation learning ( Osa et al , 2018 ) , to build the model learning component .", "entities": [[23, 25, "TaskName", "imitation learning"]]}
{"text": "= v. where B is a collection of literals or their default negations ; a is a random variable ; t is a vector of terms ( a term is a constant or a variable ) ; y is a term ; and v [ 0 , 1 ] .", "entities": [[45, 46, "DatasetName", "0"]]}
{"text": "Reasoning with an ASP program generates a set of possible worlds : { W 0 , W 1 , } .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "Consider an indoor robot navigation domain , where a robot wants to maximize the success rate of moving to goal positions through navigation actions .", "entities": [[3, 5, "TaskName", "robot navigation"]]}
{"text": "In this work , we aim at a framework that retains both model scalability and computational feasibility , i.e. , the agent is able to learn within relatively little memory while computing action policies accounting for a large number of domain variables .", "entities": [[21, 22, "DatasetName", "agent"]]}
{"text": "In factored spaces , state variables V = { V 0 , V 1 , ... , V n\u22121 } can be split into two categories , namely endogenous variables V en and exogenous variables V ex ( Chermack , 2004 ) , where V en = { V en 0 , V en 1 , ... , V en p\u22121 } and V ex = { V ex 0 , V ex 1 , ... , V ex q\u22121 } .", "entities": [[10, 11, "DatasetName", "0"], [50, 51, "DatasetName", "0"], [69, 70, "DatasetName", "0"]]}
{"text": "When a task arrives , the KRR component uses probabilistic rules to generate a task - oriented Markov decision process ( MDP ) ( Puterman , 1994 ) , which only contains a subset of V that are relevant to the current task , Procedure 1 Learning in KRR - RL Framework Require : Logical rules \u03a0 L ; probabilistic rules \u03a0 P ; random variables V = { V 0 , V 1 , ... , V n\u22121 } ; task selector \u2206 ; and guidance functions ( from human knowledge ) of f V ( V , \u03c4 ) and f A ( \u03c4 ) 1 : while Robot has no task do 2 : \u03c4 \u2206", "entities": [[70, 71, "DatasetName", "0"]]}
{"text": "( V , \u03c4 ) , and V ex V \\ V en 4 : A f A ( \u03c4 ) 5 : M Procedure - 2 ( \u03a0 L , \u03a0 P , V en , V ex , A ) 6 : Initialize agent : agent R - Max ( M )", "entities": [[45, 46, "DatasetName", "agent"], [47, 48, "DatasetName", "agent"]]}
{"text": "7 : RL agent repeatedly works on task \u03c4 , and keeps maintaining task model M , until policy convergence 8 : end while 9 : Use M to update \u03a0 P i.e. , V en , and their transition probabilities .", "entities": [[3, 4, "DatasetName", "agent"]]}
{"text": "Procedures 1 and 2 focus on how our KRR - RL agent learns by interacting with an environment when there is no task assigned .", "entities": [[11, 12, "DatasetName", "agent"]]}
{"text": "The challenges come from unreliable human language understanding ( e.g. , speech recognition ) and unforeseen obstacles that probabilistically block the robot in navigation .", "entities": [[11, 13, "TaskName", "speech recognition"]]}
{"text": "i V en , i in [ 0 , , | V en | \u22121 ] do 2 : for each possible value v in range ( V i ) do 3 : for each a A do 4 : for each possible value v in range ( V i ) do 5 :", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "We use an MDP to model this robot navigation task , where the states and actions are specified using sorts cell and move .", "entities": [[7, 9, "TaskName", "robot navigation"]]}
{"text": "The agent first initializes an MDP , from which it uses R - Max to learn the partial world model ( of navigation tasks ) .", "entities": [[1, 2, "DatasetName", "agent"]]}
{"text": "For instance , a robot supposed to deliver to a near ( distant ) location being wrongly directed to a distant ( near ) location , due to a failure in the dialog subtask , will produce a higher ( lower ) penalty to the dialog agent .", "entities": [[46, 47, "DatasetName", "agent"]]}
{"text": "Human walkers in the blocking areas ( \" BA \" ) can probabilistically impede the robot , resulting in different success rates in navigation tasks .", "entities": [[8, 9, "DatasetName", "BA"]]}
{"text": "We use the Speech Application Programming Interface ( SAPI ) package ( http://www.iflytek.com/en ) for speech recognition .", "entities": [[15, 17, "TaskName", "speech recognition"]]}
{"text": "Since the robot is aware of its unreliable speech recognition , it asked about the item , \" Which item is it ? \"", "entities": [[8, 10, "TaskName", "speech recognition"]]}
{"text": "Learning to Dialog and Navigate from Navigation Tasks Robot delivering objects requires both tasks : dialog management for specifying service request ( under unreliable speech recognition ) and navigation for physically delivering objects ( under unforeseen obstacles ) .", "entities": [[4, 5, "TaskName", "Navigate"], [24, 26, "TaskName", "speech recognition"]]}
{"text": "General questions and confirming questions cost 2.0 and 1.5 respectively .", "entities": [[0, 1, "DatasetName", "General"]]}
{"text": "The cost / bonus / penalty values are heuristically set in this work , following guidelines based on studies from the literature on dialog agent behaviors ( Zhang and Stone , 2015 ) .", "entities": [[24, 25, "DatasetName", "agent"]]}
{"text": "For example , an agent does not know the current time is morning or noon , there are two possible values for variable \" time \" .", "entities": [[4, 5, "DatasetName", "agent"]]}
{"text": "Consider that our agent has learned world dynamics under the times of morning and noon .", "entities": [[3, 4, "DatasetName", "agent"]]}
{"text": "Without our framework , an agent would have to randomly select one between the \" morning \" and \" noon \" policies .", "entities": [[5, 6, "DatasetName", "agent"]]}
{"text": "To evaluate our policies dynamically constructed via KRR , we let an agent learn three controllers under three different environment settings - the navigation actions have decreasing success rates under the settings .", "entities": [[12, 13, "DatasetName", "agent"]]}
{"text": "Experimental results show that the baseline agent achieved an average of 26.8 % success rate in navigation tasks , whereas our KRR - RL agent achieved 83.8 % success rate on average .", "entities": [[6, 7, "DatasetName", "agent"], [24, 25, "DatasetName", "agent"]]}
{"text": "In comparison , the KRR - RL agent needs to compute only | T | N world models , which dramatically reduces the number of parameters that must be learned through RL while retaining policy quality .", "entities": [[7, 8, "DatasetName", "agent"], [23, 26, "HyperparameterName", "number of parameters"]]}
{"text": "Our KRR - RL agent learns world dynamics via modelbased RL , and then incorporates the learned dynamics into the logical - probabilistic reasoning module , which is used for dynamic construction of efficient run - time task - specific planning models .", "entities": [[4, 5, "DatasetName", "agent"]]}
{"text": "Partner Personas Generation for Dialogue Response Generation", "entities": [[5, 7, "TaskName", "Response Generation"]]}
{"text": "Incorporating personas information allows diverse and engaging responses in dialogue response generation .", "entities": [[10, 12, "TaskName", "response generation"]]}
{"text": "This paper attempts to tackle these issues by offering a novel framework that leverages automatic partner personas generation to enhance the succeeding dialogue response generation .", "entities": [[23, 25, "TaskName", "response generation"]]}
{"text": "This enhances the succeeding dialogue response generation , which surpasses our competitive baselines that condition on the ground truth partner personas .", "entities": [[5, 7, "TaskName", "response generation"]]}
{"text": "To our knowledge , this is the first attempt to formulate partner personas generation for improved performance on the downstream dialogue response generation .", "entities": [[21, 23, "TaskName", "response generation"]]}
{"text": "Automatic and human evaluation results support the hypothesis and indicate that generated personas are even more interesting than the ground truth , which improves the downstream dialogue response generation .", "entities": [[27, 29, "TaskName", "response generation"]]}
{"text": "This paper thus paves the way to exploit partner personas generation ( PPG ) for dialogue response generation ( DRG ) .", "entities": [[16, 18, "TaskName", "response generation"]]}
{"text": "Interestingly , we observe that the generative counterpart proposed in our framework generates relevant , informative and coherent partner personas , which further improves the succeeding dialogue response generation .", "entities": [[27, 29, "TaskName", "response generation"]]}
{"text": "One close work to ours is a multi - task framework for meta - learning ( Lee et al , 2021 ) that uses personas reconstruction as an auxiliary task to improve response consistency .", "entities": [[12, 15, "TaskName", "meta - learning"]]}
{"text": "Very recently , formulates personas generation as a Seq2Seq task for improved downstream response generation via multi - task learning .", "entities": [[8, 9, "MethodName", "Seq2Seq"], [13, 15, "TaskName", "response generation"], [16, 20, "TaskName", "multi - task learning"]]}
{"text": "The most wellknown multi - turn dialogue dataset conditioned on personal profiles is PERSONACHAT , in which two crowdsourcers converse and find more about each other .", "entities": [[19, 20, "DatasetName", "converse"]]}
{"text": "Madotto et al ( 2019 ) leverages meta - learning with several dialogues of the current speakers to enhance response personality .", "entities": [[7, 10, "TaskName", "meta - learning"]]}
{"text": "Lee et al ( 2021 ) uses multi - task learning for improved personality consistency in the meta - learning scenario .", "entities": [[7, 11, "TaskName", "multi - task learning"], [17, 20, "TaskName", "meta - learning"]]}
{"text": "proposes to categorize the profile extraction task into two different difficulties , namely ' extraction ' and ' inference ' , and they leverage a GPT - based generator to extract user profiles .", "entities": [[25, 26, "MethodName", "GPT"]]}
{"text": "In contrast , we propose to formulate personas generation to be conditioned dialogue input to be jointly trained with response generation .", "entities": [[19, 21, "TaskName", "response generation"]]}
{"text": "Reinforcement learning ( RL ) , or specifically , policy gradient methods ( Williams , 1992 ) , have been frequently adopted to both task - oriented dialogue agents ( Roman Roman et al , 2020 ; Deng et al , 2021 ) or open - domain chitchat agents ( Li et al , 2016c ; Saleh et al , 2020 ) .", "entities": [[9, 12, "TaskName", "policy gradient methods"], [37, 40, "DatasetName", "Deng et al"]]}
{"text": "It can either propagate non - differentiable loss ( Cai et al , 2019a ) or optimize an expert reward such as ease of answering ( Li et al , 2016c ) .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "It also adopts a scenario where a user simulator and a dialogue agent interact , and an Figure 1 : An example of the inference flow that shows the generated partner personas and the incorporation of partner personas generation into response generation .", "entities": [[12, 13, "DatasetName", "agent"], [40, 42, "TaskName", "response generation"]]}
{"text": "After the supervised learning stage , the second stage is a reinforcement learning stage which jointly optimizes both partner personas generator and dialogue response generator as depicted in Figure 2 to train the partner personas generator under the reward signal that is relevant to dialogue response generation as well as fine - tuning dialogue response generator trained on the generated partner personas .", "entities": [[45, 47, "TaskName", "response generation"]]}
{"text": "A Seq2Seq neural network ( Sutskever et al , 2014 ) is adopted as our partner personas generator for the task of partner personas generation ( PPG ) .", "entities": [[1, 2, "MethodName", "Seq2Seq"]]}
{"text": "We also adopt a Seq2Seq neural network for the task of dialogue response generation ( DRG ) .", "entities": [[4, 5, "MethodName", "Seq2Seq"], [12, 14, "TaskName", "response generation"]]}
{"text": "Then we can derive two negative samples as : ( s A , r B , L = 0 ) and ( s B , r A , L = 0 ) .", "entities": [[18, 19, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}
{"text": "Thereafter , we fine - tune on a binary classifier to be used as our critic in RL on the training partition by minimizing the binary cross - entropy loss : \u2212Llog ( P ( L | s , r ) )", "entities": [[29, 30, "MetricName", "loss"]]}
{"text": "The predicted binary labelL is then converted to a reward R. R is a positive reward whenL = 1 , and R is a negative reward whenL = 0 .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "\u2206\u03b8 PPG = \u2212R \u25bd \u03b8 PPG log P", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "for the partner personas generator ( PPG ) , and for the dialogue response generator ( DRG ) : \u2206\u03b8 DRG = \u2212R \u25bd \u03b8 DRG log P ( r | s , p , c )", "entities": [[24, 25, "HyperparameterName", "\u03b8"]]}
{"text": "By formulating a reward that measures the relevance between generated partner personas and generated dialogue response , we are motivated by the following objectives : Further fine - tune the partner personas generator to generate personas that benefits the downstream dialogue response generation .", "entities": [[41, 43, "TaskName", "response generation"]]}
{"text": "However , some of them can be irrelevant and unhelpful for the next - turn dialogue response generation .", "entities": [[16, 18, "TaskName", "response generation"]]}
{"text": "Therefore , we design such a reward to train the personas generator to learn to generate a set of personas that is more helpful for the downstream dialogue response generation .", "entities": [[28, 30, "TaskName", "response generation"]]}
{"text": "The previous work from Cai et al ( 2019a ) employed critic network for RL loss backpropagation .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "In contrast , we aim for improved response generation with a classifier conditioning on both the generated personas and the generated response .", "entities": [[7, 9, "TaskName", "response generation"]]}
{"text": "Retrieving Top - 3 relevant partner personas using BM25 ( Robertson and Walker , 1994 ) yields the best performance on the original personas . GPT - 2", "entities": [[25, 26, "MethodName", "GPT"]]}
{"text": "This is a comparison model fine - tuned on GPT - 2 ( Radford et al , 2019 ) .", "entities": [[9, 10, "MethodName", "GPT"]]}
{"text": "We build the same three E2E systems described above , and the best model is selected , the third one .", "entities": [[5, 6, "DatasetName", "E2E"]]}
{"text": "A comparison model built with a Transformer - based model pre - trained on gen - eral domain corpus , which is then fine - tuned on PERSONACHAT .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "This is a comparison model that employs a memory - augmented architecture incorporated with conditional variational autoencoder that exploits persona information .", "entities": [[15, 17, "MethodName", "variational autoencoder"]]}
{"text": "MTL w/ Personas Reconstruction", "entities": [[3, 4, "TaskName", "Reconstruction"]]}
{"text": "\u03b1 is weight tuned over the validation set , and both tasks condition on dialogue context and self personas and share the same model parameters .", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "More implementation details can be found in Appendx B. The dialogue response generation results are presented in Table 1 .", "entities": [[11, 13, "TaskName", "response generation"]]}
{"text": "This supports the usefulness of our framework , which generates reasonable personas and effectively enhances the succeeding dialogue response generation , through the use of RL .", "entities": [[18, 20, "TaskName", "response generation"]]}
{"text": "Although TRANSFERTRANSFO attains a better score on the PPL than the fine - tuned GPT - 2 , GPT - 2 have better extrinsic scores than TRANSFER - TRANSFO .", "entities": [[14, 15, "MethodName", "GPT"], [18, 19, "MethodName", "GPT"]]}
{"text": "GPT - 2 also has better overall scores than the E2E baselines without the complete partner personas .", "entities": [[0, 1, "MethodName", "GPT"], [10, 11, "DatasetName", "E2E"]]}
{"text": "However , it is surpassed by the E2E baseline with the complete partner personas during training and inference .", "entities": [[7, 8, "DatasetName", "E2E"]]}
{"text": "The E2E baseline with the complete ground truth partner personas attains better scores on all of the metrics than our remaining baselines .", "entities": [[1, 2, "DatasetName", "E2E"]]}
{"text": "The multi - task learning comparison model ( Lee et al , 2021 ) produces less promising results .", "entities": [[1, 5, "TaskName", "multi - task learning"]]}
{"text": "The remaining metrics also report reasonable scores , suggesting the plausbility to formulate personas generation as a Seq2Seq task .", "entities": [[17, 18, "MethodName", "Seq2Seq"]]}
{"text": "As in Zou et al ( 2021 ) and ACUTE - Evals ( Li et al , 2020 ) , annotators follow the criteria which we present in Appendix D. trained under RL surpasses the E2E model that leverages both training and inference ground truth partner personas from all the aspects .", "entities": [[35, 36, "DatasetName", "E2E"]]}
{"text": "Further scaling deteriorates the quality of response generation .", "entities": [[6, 8, "TaskName", "response generation"]]}
{"text": "Our novel framework incorporates partner personas generation into dialogue response generation .", "entities": [[9, 11, "TaskName", "response generation"]]}
{"text": "We employ reinforcement learning with a dedicatedly designed critic network that boosts the response generation by conditioning on the generated personas .", "entities": [[13, 15, "TaskName", "response generation"]]}
{"text": "DistilBERT is used to initialize the model parameters for the critic network .", "entities": [[0, 1, "MethodName", "DistilBERT"]]}
{"text": "We set Adam as our optimizer , with hyperparameters \u03b7", "entities": [[2, 3, "MethodName", "Adam"], [5, 6, "HyperparameterName", "optimizer"]]}
{"text": "We observe that they improve D Human Evaluation Criteria ( Appropriateness ) : \" Who is more appropriate given the previous dialogue context ? \" ( Informativeness ) : \" Who is more diverse instead of null answers such as I do not know ? \" ( Engagingness ) : \" Who would you prefer to talk with for a long conversation ? \" ( Human - likeness ) : \" Which speaker do you think sounds more like a real person ? \" ( Coherence ) : \" Which persona contains traits that are more coherent to each other ? \" ( Interestingness ) : \" Which persona is more interesting and diverse ? \"", "entities": [[103, 104, "DatasetName", "Interestingness"]]}
{"text": "Our work uses an off - the - shelf persona - based conversational dataset PERSONACHAT , which is collected and built by crowdsourcing to converse based on a fake set of discrete traits .", "entities": [[24, 25, "DatasetName", "converse"]]}
{"text": "However , their main focus is to investigate the impact of personas on empathetic dialogue generation .", "entities": [[14, 16, "TaskName", "dialogue generation"]]}
{"text": "We report averaged results from 3 runs for our dialogue response generation and partner personas generation results reported in Table 1 , Table 4 and Table 7 .", "entities": [[10, 12, "TaskName", "response generation"]]}
{"text": "Active Learning via Membership Query Synthesis for Semi - supervised Sentence Classification", "entities": [[0, 2, "TaskName", "Active Learning"], [10, 12, "TaskName", "Sentence Classification"]]}
{"text": "Active learning ( AL ) is a technique for reducing manual annotation effort during the annotation of training data for machine learning classifiers .", "entities": [[0, 2, "TaskName", "Active learning"]]}
{"text": "We present the first successful attempt to use Membership Query Synthesis for generating AL queries for natural language processing , using Variational Autoencoders for query generation .", "entities": [[22, 23, "MethodName", "Autoencoders"]]}
{"text": "We evaluate our approach in a text classification task and demonstrate that query synthesis shows competitive performance to pool - based AL strategies while substantially reducing annotation time .", "entities": [[6, 8, "TaskName", "text classification"]]}
{"text": "Active learning ( AL ) has the potential to substantially reduce the amount of labeled instances needed to reach a certain classifier performance in supervised machine learning .", "entities": [[0, 2, "TaskName", "Active learning"]]}
{"text": "We provide proof of concept that generating highly informative artificial training instances for text classification is feasible .", "entities": [[13, 15, "TaskName", "text classification"]]}
{"text": "We use Variational Autoencoders ( VAE ) ( Kingma and Welling , 2013 ) to learn representations from unlabeled text in an unsupervised fashion by encoding individual sentences as low - dimensional vectors in latent space .", "entities": [[3, 4, "MethodName", "Autoencoders"], [5, 6, "MethodName", "VAE"]]}
{"text": "In addition to mapping input sequences into latent space , the VAE can also learn to generate new instances from this space .", "entities": [[11, 12, "MethodName", "VAE"]]}
{"text": "We utilize these abilities to generate new examples for active learning from a region in latent space where the classifier is most uncertain , and hand them over to the annotator who then provides labels for the newly created instances .", "entities": [[9, 11, "TaskName", "active learning"]]}
{"text": "We test our approach in a text classification setup with a real human annotator in the loop .", "entities": [[6, 8, "TaskName", "text classification"]]}
{"text": "Sentence representation learning ( Kiros et al , 2015 ; Conneau et al , 2017 ; Subramanian et al , 2018 ; in combination with new methods for semi - supervised learning Hu et al , 2017 ; Xu et", "entities": [[1, 3, "TaskName", "representation learning"]]}
{"text": "Mehrjou et al ( 2018 ) use VAEs to learn structural information from unlabeled data and use it as an additional criterion in conventional active learning to make it more robust against outliers and noise .", "entities": [[24, 26, "TaskName", "active learning"]]}
{"text": "To our best knowledge , our work is the first to present positive results for Membership Query Synthesis for text classification .", "entities": [[19, 21, "TaskName", "text classification"]]}
{"text": "To identify uncertain points along the separating hyperplane of an SVM the following approach is proposed .", "entities": [[10, 11, "MethodName", "SVM"]]}
{"text": "The Variational Autoencoder is a generative model first introduced by Kingma and Welling ( 2013 ) .", "entities": [[1, 3, "MethodName", "Variational Autoencoder"]]}
{"text": "Like other autoencoders , VAEs learn a mapping q \u03b8 ( z | x ) from high dimensional input x to a low dimensional latent variable z.", "entities": [[2, 3, "MethodName", "autoencoders"], [9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "The decoder p \u03b8 ( x | z ) , also referred to as dec ( z ) , is trained to reconstruct the input x based on the latent variable z.", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b8 ( \u00b5 , \u03c3 | x ) : z", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "where \u223c N ( 0 , I ) and is the element - wise product .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "= \u2212KL ( q \u03b8 ( z | x )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "+ E q \u03b8 ( z | x )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "[ logp \u03b8 ( x | z ) ] .", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "We train a Variational Autoencoder on an unlabeled corpus of sentences .", "entities": [[3, 5, "MethodName", "Variational Autoencoder"]]}
{"text": "The text classification task is performed on a binary sentiment dataset split into training , development and test set .", "entities": [[1, 3, "TaskName", "text classification"]]}
{"text": "As depicted in Figure 2 , the sentences in the classification dataset are vectorized using the VAE encoder which generates the latent variable z for each sentence x.", "entities": [[16, 17, "MethodName", "VAE"]]}
{"text": "One important parameter for active learning determines how many new instances are to be selected in each AL iteration .", "entities": [[4, 6, "TaskName", "active learning"]]}
{"text": "In this section we want to explore how the ability to generate human readable sentences from arbitrary points in the feature space affects active learning performance .", "entities": [[23, 25, "TaskName", "active learning"]]}
{"text": "We start the active learning process with two utterances in the seed set , namely ' good movie ' and ' bad movie ' .", "entities": [[3, 5, "TaskName", "active learning"]]}
{"text": "The data used in our experiments comes from two sources , ( i ) the SST2 ( Socher et al , 2013 ) and ( ii ) SAR14", "entities": [[15, 16, "DatasetName", "SST2"]]}
{"text": "Input embeddings are also of size 512 , which allows us to share the embed - ding weights with the softmax weights of the output layer ( Press and Wolf , 2016 ) .", "entities": [[20, 21, "MethodName", "softmax"]]}
{"text": "Once the KL term weight is close to 1 , the learning weight is linearly decreased to 0 .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "Learner The Learner is an SVM 1 with linear kernel .", "entities": [[5, 6, "MethodName", "SVM"]]}
{"text": "Each instance is represented as the latent variable z learned by the autoencoder .", "entities": [[12, 13, "MethodName", "autoencoder"]]}
{"text": "The latent variable is a vector with 50 dimensions and the SVM is trained on this representation .", "entities": [[11, 12, "MethodName", "SVM"]]}
{"text": "We calculate classification performance on the reduced SST2 test set and report F1 - scores .", "entities": [[7, 8, "DatasetName", "SST2"], [12, 13, "MetricName", "F1"]]}
{"text": "Generator The generator is the decoder of the VAE described above .", "entities": [[8, 9, "MethodName", "VAE"]]}
{"text": "We compare our approach to Membership Query Synthesis for text classification to four baselines .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "6 Results and Analysis 6.1 Classification Performance F - scores as a function of annotated instances Figure 3 shows learning curves for the different AL strategies and baselines as a function of the number of annotation instances added to the training data .", "entities": [[5, 6, "TaskName", "Classification"]]}
{"text": "Overall , gen uniform is competitive with respect to F1 - scores and shows that sentences generated from points in the feature space are informative and useful for training a text classifier .", "entities": [[9, 10, "MetricName", "F1"]]}
{"text": "These are indicators that the selected points are close to the hyperplane and the VAE is able to generate coherent and highly informative sentences from them .", "entities": [[14, 15, "MethodName", "VAE"]]}
{"text": "a complete mess 0 5 . nothing spectacular 0 6 . absolutely terrible !", "entities": [[3, 4, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "the UNK is a disappointment 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Label 1 for positive and 0 for negative class .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "To further analyze the behavior of the different AL strategies , we apply dimensionality reduction and visualize the instances in latent space ( Figure 6 ) .", "entities": [[13, 15, "TaskName", "dimensionality reduction"]]}
{"text": "The two largest absolute coefficients of the trained SVM 's linear kernel identify the most important dimensions .", "entities": [[8, 9, "MethodName", "SVM"]]}
{"text": "Figure 6 plots the points , represented by theses two dimensions , selected by different active learning schedules .", "entities": [[15, 17, "TaskName", "active learning"]]}
{"text": "In a perfect VAE z and\u1e91 should be nearly identical .", "entities": [[3, 4, "MethodName", "VAE"]]}
{"text": "The sampled point z \u223c N ( 0 , I ) is decoded to query sequence x , labeled and subsequently re - encoded to\u1e91 = enc ( x ) .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "With the same amount of instances , this model performs much worse than gen uniform , indicating that point\u1e91 still preserves some of the informativeness of z. We thus assume that the closer\u1e91 is to selected point z , the better the generation based active learning schedules will work .", "entities": [[44, 46, "TaskName", "active learning"]]}
{"text": "al ( 2016 ) create artificial training instances for machine translation , using monolingual data paired with automatic back - translations .", "entities": [[9, 11, "TaskName", "machine translation"]]}
{"text": "Alberti et al ( 2019 ) use a large number of synthetic instances to pre - train a Question Answering ( QA ) model that is then fine - tuned on the target QA dataset .", "entities": [[18, 20, "TaskName", "Question Answering"]]}
{"text": "Due to the practical constraints of the active learning process , we need to keep the training time short enough so that the human annotator does not have to wait for the next set of instances to annotate .", "entities": [[7, 9, "TaskName", "active learning"]]}
{"text": "Given that we use an SVM for classification , we do not expect a strong effect for adding large numbers of additional training instances , given that the majority of those data points will not be positioned close to the decision boundary .", "entities": [[5, 6, "MethodName", "SVM"]]}
{"text": "One of the main drawbacks of our work is its limitation to binary sentence classification .", "entities": [[13, 15, "TaskName", "sentence classification"]]}
{"text": "However , multi - class classification in an one - vs - rest schema is compatible with our method and worth further exploration .", "entities": [[2, 6, "TaskName", "multi - class classification"]]}
{"text": "Another interesting direction for future work is the synthesis of data for more complex tasks like Natural Language Inference ( NLI ) or QA .", "entities": [[16, 19, "TaskName", "Natural Language Inference"]]}
{"text": "This , however , requires modifications to the structure of the autoencoder and exceeds the scope of this work .", "entities": [[11, 12, "MethodName", "autoencoder"]]}
{"text": "One example that comes to mind is the detection of offensive language or ' hate speech ' , where we have to deal with highly unbalanced training sets with only a small number of positive instances , and attempts to increase this number have been shown to result in systematically biased datasets ( Davidson et al , 2019 ;", "entities": [[14, 16, "DatasetName", "hate speech"]]}
{"text": "Our approach uses a Variational Autoencoder as a representation learner and generates informative ac - tive learning queries from latent space .", "entities": [[4, 6, "MethodName", "Variational Autoencoder"]]}
{"text": "The classification performance for the generated instances is competitive with pool - based active learning strategies and outperforms other AL strategies with regard to annotation cost ( time ) and computational complexity .", "entities": [[13, 15, "TaskName", "active learning"]]}
{"text": "The main advantage of Membership Query Synthesis for active learning is that it allows us to target specific points along the separating hyperplane and thus to provide the classifier with information on specific areas of uncertainty in the data space .", "entities": [[8, 10, "TaskName", "active learning"]]}
{"text": "While pool - based active learning has the same objective , Membership Query Synthesis gives us a more precise tool to explore the data space and to generate exactly those instances that we need , making MQS a promising approach for future work in active learning .", "entities": [[4, 6, "TaskName", "active learning"], [44, 46, "TaskName", "active learning"]]}
{"text": "DAMO - NLP at SemEval - 2022 Task 11 : A Knowledge - based System for Multilingual Named Entity Recognition", "entities": [[16, 20, "TaskName", "Multilingual Named Entity Recognition"]]}
{"text": "The MultiCoNER shared task aims at detecting semantically ambiguous and complex named entities in short and low - context settings for multiple languages .", "entities": [[1, 2, "DatasetName", "MultiCoNER"]]}
{"text": "To alleviate this issue , our team DAMO - NLP proposes a knowledge - based system , where we build a multilingual knowledge base based on Wikipedia to provide related context information to the named entity recognition ( NER ) model .", "entities": [[34, 37, "TaskName", "named entity recognition"], [38, 39, "TaskName", "NER"]]}
{"text": "Our system wins 10 out of 13 tracks in the MultiCoNER shared task .", "entities": [[10, 11, "DatasetName", "MultiCoNER"]]}
{"text": "The MultiCoNER shared task ( Malmasi et al , 2022b ) aims at building Named Entity Recognition ( NER ) systems for 11 languages , including English , Spanish , Dutch , Russian , Turkish , Korean , Farsi , German , Chinese , Hindi , and Bangla .", "entities": [[1, 2, "DatasetName", "MultiCoNER"], [14, 17, "TaskName", "Named Entity Recognition"], [18, 19, "TaskName", "NER"]]}
{"text": "The multilingual track requires training multilingual NER models that are able to handle all languages .", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "Therefore , we believe retrieving related knowledge can help the NER model to disambiguate hard samples in the shared task as well .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "In this paper , we propose a general knowledgebased system for the MultiCoNER shared task .", "entities": [[12, 13, "DatasetName", "MultiCoNER"]]}
{"text": "We then feed the input sentence and the related documents into the NER model .", "entities": [[12, 13, "TaskName", "NER"]]}
{"text": "We first train a multilingual model so that the NER model can learn from all annotations .", "entities": [[9, 10, "TaskName", "NER"]]}
{"text": "Next , we train the monolingual models ( one for each language ) and a code - mixed model by using the fine - tuned XLM - RoBERTa ( XLM - R ) ( Conneau et al , 2020 ) embeddings in the multilingual model as initialization to further boost model performance on monolingual and code - mixed tracks .", "entities": [[25, 26, "MethodName", "XLM"], [27, 28, "MethodName", "RoBERTa"], [29, 30, "MethodName", "XLM"]]}
{"text": "For each track , we train multiple models with different random seeds and use majority voting to form the final predictions .", "entities": [[11, 12, "DatasetName", "seeds"]]}
{"text": "Comparing with other model variants we have tried , our NER model enjoys a good balance between model performance and speed .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "NER ( Sundheim , 1995 ) is a fundamental task in natural language processing .", "entities": [[0, 1, "TaskName", "NER"]]}
{"text": "The task has a lot of applications in various domains such as social media ( Derczynski et al , 2017 ) , news ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ) , Ecommerce ( Fetahu et al , 2021 ;", "entities": [[15, 16, "DatasetName", "Derczynski"]]}
{"text": "Recently , pretrained contextual embeddings such as BERT ( Devlin et al , 2019 ) , XLM - R and LUKE ( Yamada et al , 2020 ) have significantly improved the NER performance .", "entities": [[7, 8, "MethodName", "BERT"], [16, 17, "MethodName", "XLM"], [32, 33, "TaskName", "NER"]]}
{"text": "Recently , a lot of work shows that utilizing the document - level contexts in the CoNLL NER datasets can significantly improve token representations and achieves state - of - the - art performance ( Yu et al , 2020 ; Luoma and Pyysalo , 2020 ; Yamada et al , 2020 ; Wang et al , 2021a ) .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "However , the lack of context in the MultiCoNER datasets means the embeddings can not take advantage of long - range dependencies for entity disambiguation .", "entities": [[8, 9, "DatasetName", "MultiCoNER"], [23, 25, "TaskName", "entity disambiguation"]]}
{"text": "Recently , Wang et al ( 2021b ) use Google search to retrieve external contexts of the input sentence and successfully achieve state - of - the - art performance across multiple domains .", "entities": [[9, 10, "DatasetName", "Google"]]}
{"text": "We extend these ideas as multi - stage fine - tuning , which improves the accuracy of monolingual models that use finetuned multilingual embeddings as initialization in training .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "We introduce how our knowledge - based NER system works in this section .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "The output token representations of the input sentence are fed into a linear - chain conditional random field ( CRF ) ( Lafferty et al , 2001 ) layer and the CRF layer produces the label predictions .", "entities": [[15, 18, "MethodName", "conditional random field"], [19, 20, "MethodName", "CRF"], [31, 32, "MethodName", "CRF"]]}
{"text": "Given the label predictions of multiple NER models with different random seeds , the ensemble module uses a voting strategy to decide the final prediction\u015d y = { \u0177 1 , , \u0177 n } of the sentence .", "entities": [[6, 7, "TaskName", "NER"], [11, 12, "DatasetName", "seeds"]]}
{"text": "Retrieval - augmented context is effective for named entity recognition tasks ( Wang et al , 2021b ) , as external relevant contexts can provide auxiliary information for disambiguating complex named entities .", "entities": [[7, 10, "TaskName", "named entity recognition"]]}
{"text": "These retrieved documents act as contexts and are fed into the NER module .", "entities": [[11, 12, "TaskName", "NER"]]}
{"text": "To enhance the retrieval quality , we further designed an iterative retrieval approach , which incorporates predicted entities of NER models into the search query .", "entities": [[19, 20, "TaskName", "NER"]]}
{"text": "For the NER task , these anchors provide useful clues on where the entities are to the model .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "Iterative Entity Retrieval", "entities": [[1, 3, "TaskName", "Entity Retrieval"]]}
{"text": "The core of the NER task lies in the entities , while retrieval at the sentence level overlooks the key entities in the sentences .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "On the test set , we first perform the sentence retrieval and then use the entity mentions 6 predicted by the model for entity retrieval .", "entities": [[23, 25, "TaskName", "entity retrieval"]]}
{"text": "In our system , we use XLM - R large as the embedding for all the tracks .", "entities": [[6, 7, "MethodName", "XLM"]]}
{"text": "Given the input sentence x and the retrieved contexts { x 1 , , x k } , we add the separator token ( i.e. , \" < /s > \" in XLM - R ) between them and concatenated them together to form the inputx of the NER module .", "entities": [[32, 33, "MethodName", "XLM"], [48, 49, "TaskName", "NER"]]}
{"text": "We chunk retrieved texts to avoid the amount of subtoken in the sequence exceeding the maximum subtoken length in XLM - R ( i.e. , 512 in XLM - R ) .", "entities": [[19, 20, "MethodName", "XLM"], [27, 28, "MethodName", "XLM"]]}
{"text": "Our system regards the NER task as a sequence labeling problem .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "The embedding layer in the NER module encode the concatenated sequenc\u1ebd x and output the corresponding token representa - tions { v 1 , ,", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "i , v i ) where \u03b8 represents the model parameters and Y ( x ) denotes the set of all possible label sequences given x.", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "During training , the negative log - likelihood loss L NLL ( \u03b8 ) = \u2212 log p", "entities": [[5, 8, "MetricName", "log - likelihood"], [8, 9, "MetricName", "loss"], [10, 11, "MetricName", "NLL"], [12, 13, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b8 ( y * | x ) for the concatenated input sequence with gold labels", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "During inference , the model prediction\u0177 \u03b8 is given by Viterbi decoding .", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "( Nguyen et al , 2016 ) containing a lot of natural language questions ; ORCAS ( Search Query NER ) contains user queries from Microsoft Bing ( Craswell et al , 2020 ) .", "entities": [[15, 16, "DatasetName", "ORCAS"], [19, 20, "TaskName", "NER"]]}
{"text": "The MSQ and ORCAS samples are taken as out - ofdomain data in the shared task .", "entities": [[3, 4, "DatasetName", "ORCAS"]]}
{"text": "The test set , however , contains much more MSQ and ORCAS samples to assess the out - of - domain performance .", "entities": [[11, 12, "DatasetName", "ORCAS"]]}
{"text": "The results of the shared task are evaluated with the entity - level macro F1 scores , which treat all the labels equally .", "entities": [[13, 15, "MetricName", "macro F1"]]}
{"text": "NER Model Training Before building the final system , we compare a lot of variants of the system .", "entities": [[0, 1, "TaskName", "NER"]]}
{"text": "We train these variant models on the training set for 3 times each with different random seeds and compare the averaged performance of the models .", "entities": [[16, 17, "DatasetName", "seeds"]]}
{"text": "Our final NER models are trained on the combined dataset including both the training and development sets on each track to fully utilize the labeled data .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "For models trained on the training set , we use the best macro F1 on the development set during training to select the best model checkpoint .", "entities": [[12, 14, "MetricName", "macro F1"]]}
{"text": "For models trained on the combined dataset , Continue Pretraining To make XLM - R learn the data distribution of the shared task , we combine the training and development sets on the monolingual tracks to build a corpus to continue pretrain", "entities": [[12, 13, "MethodName", "XLM"]]}
{"text": "XLM - R.", "entities": [[0, 1, "MethodName", "XLM"]]}
{"text": "Specifically , we collocate all sentences according to their languages , then cut the text into chunks of fixed length , and train the model on these text chunks using the Masked Language Modeling objective .", "entities": [[31, 34, "TaskName", "Masked Language Modeling"]]}
{"text": "We continue pretrain XLM - R for 5 epochs .", "entities": [[3, 4, "MethodName", "XLM"]]}
{"text": "We use the continue pretrained XLM - R model as the initialization of the multilingual 7 Please refer to Appendix A for detailed settings .", "entities": [[5, 6, "MethodName", "XLM"]]}
{"text": "According to the baseline performance over the three domains , the ORCAS domain has the lowest score , which shows the challenges in recognizing named entities in user queries .", "entities": [[11, 12, "DatasetName", "ORCAS"]]}
{"text": "To evaluate the relevance of the retrieval results to the query , we define a character - level relevance metric , which calculates the Intersectionover - Union ( IoU ) between the characters of query and result .", "entities": [[28, 29, "MetricName", "IoU"]]}
{"text": "Online Search Engine In the early stage , we tried to use the knowledge retrieved from Google Search , which can retrieve related knowledge from a large scale of webs and is believed to be a strong multilingual search engine .", "entities": [[16, 17, "DatasetName", "Google"]]}
{"text": "Entity Retrieval with Gold Entities We use gold entities on the development set to see whether the model performance can be improved .", "entities": [[0, 2, "TaskName", "Entity Retrieval"]]}
{"text": "For the three context options , PARA is the best option for EN , ES , NL , RU , TR , KO , FA , MIX and MULTI .", "entities": [[24, 25, "MethodName", "FA"]]}
{"text": "As a result , we choose SENT for the two languages since we believe the wiki anchors from the Wikipedia can help model performance ; 2 ) Comparing with the baseline , the knowledge from Google Search can improve model performance .", "entities": [[35, 36, "DatasetName", "Google"]]}
{"text": "Based on the best context option of each track , the knowledge from Wikipedia is better than the online search engine ; 3 ) For ITER G , we can find that the context can further Iterative Entity Retrieval with Predicted Entities Based on the results in Table 3 , we further analyze how the predicted entity mentions can improve the retrieval quality .", "entities": [[37, 39, "TaskName", "Entity Retrieval"]]}
{"text": "We denote the iterative entity retrieval with predicted mentions as ITER P .", "entities": [[4, 6, "TaskName", "entity retrieval"]]}
{"text": "Since iterative entity retrieval uses predicted mentions as a part of retrieval query , the performance of mention detection directly affects the retrieval quality .", "entities": [[2, 4, "TaskName", "entity retrieval"]]}
{"text": "For comparison with mention detection performance of NER models , we additionally train mention detection models by discarding the entity labels during training .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "Therefore , we train the ITER models only for the code - mixed track and use the NER models with sentence retrieval to predict mentions .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "In the table , we also show that the retrieval speed of our local KB is significantly faster than that of Google Search .", "entities": [[21, 22, "DatasetName", "Google"]]}
{"text": "The bottleneck of the system speed is the NER module rather than the knowledge retrieval module .", "entities": [[8, 9, "TaskName", "NER"]]}
{"text": "The main reason for the slow speed of the NER module is that the input length of the knowledge - based system is significantly longer than the original input .", "entities": [[9, 10, "TaskName", "NER"]]}
{"text": "The longer inputs slow down the encoding at XLM - R embeddings .", "entities": [[8, 9, "MethodName", "XLM"]]}
{"text": ") CE is one of the usual approaches to NER , which concatenates different kinds of embeddings to improve the token representations .", "entities": [[9, 10, "TaskName", "NER"]]}
{"text": "In the early stage of our system building , we compare CE with only using the XLM - R embeddings based on the knowledge retrieved from the Google Search .", "entities": [[16, 17, "MethodName", "XLM"], [27, 28, "DatasetName", "Google"]]}
{"text": "Results in Table 7 show that CE models are stronger than the models using XLM - R embeddings only in all the cases , which show the effectiveness of CE .", "entities": [[14, 15, "MethodName", "XLM"]]}
{"text": "We experiment on EN , ES , NL , RU , TR , KO and FA , which are strong with PARA contexts .", "entities": [[15, 16, "MethodName", "FA"]]}
{"text": "In Table 9 , we further compare ACE with ensemble XLM - R models .", "entities": [[10, 11, "MethodName", "XLM"]]}
{"text": "However , as we have shown in Section 5.5 , the prediction speed is quite slow with the single XLM - R embeddings .", "entities": [[19, 20, "MethodName", "XLM"]]}
{"text": "In this paper , we describe our knowledge - based system for the MultiCoNER shared task , which wins 10 out of 13 tracks in the shared task .", "entities": [[13, 14, "DatasetName", "MultiCoNER"]]}
{"text": "We show that the NER models can use the retrieved knowledge to facilitate complex entity prediction , significantly improving both the in - domain and out - of - domain performance .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "We believe this system can be widely applied to other domains for the task of NER .", "entities": [[15, 16, "TaskName", "NER"]]}
{"text": "For iterative entity retrieval , we set T = 2 .", "entities": [[2, 4, "TaskName", "entity retrieval"]]}
{"text": "Each NER model built by our system can be trained and evaluated on a single Tesla V100 GPU with 16 GB memory .", "entities": [[1, 2, "TaskName", "NER"]]}
{"text": "A. ( Akbik et al , 2018 ) , ELMo embeddings ( Peters et al , 2018 ; Che et al , 2018 ) , XLM - R embeddings fine - tuned on the whole training data and XLM - R embeddings fine - tuned on the language data by multi - stage finetuning .", "entities": [[9, 10, "MethodName", "ELMo"], [25, 26, "MethodName", "XLM"], [38, 39, "MethodName", "XLM"]]}
{"text": "We only feed the knowledge - based input into XLM - R embeddings and feed the original input into other embeddings because it is hard for the other embeddings ( especially for LSTM - based embeddings such as Flair and ELMo ) to encode such a long input .", "entities": [[9, 10, "MethodName", "XLM"], [32, 33, "MethodName", "LSTM"], [40, 41, "MethodName", "ELMo"]]}
{"text": "We use Bi - LSTM encoder to encode the concatenated embeddings with a hidden state of 1 , 000 and then feed the output token representations into the CRF layer .", "entities": [[4, 5, "MethodName", "LSTM"], [28, 29, "MethodName", "CRF"]]}
{"text": "Moreover , we compare the majority voting ensemble and CRF level ensemble in Table 12 .", "entities": [[9, 10, "MethodName", "CRF"]]}
{"text": "The CRF level ensemble averages the emission and transition scores in the Eq . 1 predicted by the candidate models and uses the Viterbi algorithm to get the prediction .", "entities": [[1, 2, "MethodName", "CRF"]]}
{"text": "The results show that CRF level ensemble performs inferior to the majority voting ensemble .", "entities": [[4, 5, "MethodName", "CRF"]]}
{"text": "The possible reason is that training with different random seeds may lead to different emission transition scores at different Avg . scales .", "entities": [[9, 10, "DatasetName", "seeds"]]}
{"text": "The detailed statistics of the MultiCoNER dataset are listed in Table 10 and the statistics of our KBs ares shown in Table 11 .", "entities": [[5, 6, "DatasetName", "MultiCoNER"]]}
{"text": "English - Portuguese Biomedical Translation Task Using a Genuine Phrase - Based Statistical Machine Translation Approach", "entities": [[4, 5, "TaskName", "Translation"], [13, 15, "TaskName", "Machine Translation"]]}
{"text": "Our approach to produce translations for the ACL - 2016 Biomedical Translation Task on the English - Portuguese language pair , in both directions , is described .", "entities": [[11, 12, "TaskName", "Translation"]]}
{"text": "This paper shows how we obtained our results using our patented Machine Translation system to produce translations for the English - Portuguese language pair from the Biomedical Translation Task .", "entities": [[11, 13, "TaskName", "Machine Translation"], [27, 28, "TaskName", "Translation"]]}
{"text": "Our approach differs from common Statistical Machine Translation approaches like Moses ( Koehn et al , 2007 ) in several aspects : phrases are not analyzed at their word level in any model ; the language model depends on the target al ernatives of given adjacent sources and does not try to avoid null scores to phrases that do not occur ; the translation score is not log - linear , but instead a tuned weighted average between the translation model and the language model , and so no smoothing techniques are required ; several models can be used with different relevances or weights ; and instead of simply relying on statistics , we include human validation and correction on several stages of the system , namely for validating extracted term translations , to improve the quality of the source data used in the automatically produced translations .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}
{"text": "In order to produce translations , our system ( like any other Statistical Machine Translation system ) requires a translation model and a language model to support the translation decoding stage .", "entities": [[13, 15, "TaskName", "Machine Translation"]]}
{"text": "The automatically extracted word and phrasal translations were automatically classified , prior to human validation , using an SVM classifier trained on previously validated translations as described by Mahesh et al ( 2015 ) .", "entities": [[18, 19, "MethodName", "SVM"]]}
{"text": "However , there are a few reasons we can think of for these values , namely the way the BLEU measure has been calculated ( case sensitivity and synonyms penalty - translating \" home \" instead of \" house \" might be perfectly fine ) , the differences between European Portuguese and Brazilian Portuguese , and the presence of several spelling and alignment errors in the training data .", "entities": [[19, 20, "MetricName", "BLEU"]]}
{"text": "Nonetheless , we can still take several actions to improve our system : namely testing both parallel corpora , health and biology , with identical weights : using Europarl and eventually EMEA corpus ; the refinement of our phrase translation extraction ; the extraction of specific bilingual terminology , additionally to the use of cognaticity ; subsentence realignment after the bilingual terminology extraction , and a more efficient implementation of the patterns ( comparable to a hierarchical translation ) application .", "entities": [[31, 32, "MethodName", "EMEA"]]}
{"text": "Improving Distantly - supervised Entity Typing with Compact Latent Space Clustering", "entities": [[4, 6, "TaskName", "Entity Typing"]]}
{"text": "Recently , distant supervision has gained great success on Fine - grained Entity Typing ( FET ) .", "entities": [[12, 14, "TaskName", "Entity Typing"]]}
{"text": "Existing works alleviated this issue with partial - label loss , but usually suffer from confirmation bias , which means the classifier fit a pseudo data distribution given by itself .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "Extensive experiments on standard benchmarks show that our CLSC model consistently outperforms state - of - the - art distantly supervised entity typing systems by a significant margin .", "entities": [[21, 23, "TaskName", "entity typing"]]}
{"text": "Recent years have seen a surge of interests in fine - grained entity typing ( FET ) as it serves as an important cornerstone of several nature language processing tasks including relation extraction ( Mintz et al , 2009 ) , entity linking ( Raiman and Raiman , 2018 ) , and knowledge base completion ( Dong et al , 2014 ) .", "entities": [[12, 14, "TaskName", "entity typing"], [31, 33, "TaskName", "relation extraction"], [41, 43, "TaskName", "entity linking"], [52, 55, "TaskName", "knowledge base completion"]]}
{"text": "In contrast , the other thread of works try to incorporate such imperfect annotation by partiallabel loss ( PLL ) .", "entities": [[16, 17, "MetricName", "loss"]]}
{"text": "In this paper , we propose a new method for distantly supervised fine - grained entity typing .", "entities": [[15, 17, "TaskName", "entity typing"]]}
{"text": "Fine - grained entity typing takes a corpus and an external knowledge base ( KB ) with a type hierarchy Y as input .", "entities": [[3, 5, "TaskName", "entity typing"]]}
{"text": "Formally , given a batch of samples { ( m i , c i , Y t i ) } B i=1 , we first convert each sample ( m i , c i ) into a real - valued vector z i via a feature extractor z ( ( m i , c i ) ; \u03b8 z ) parameterized by \u03b8 z .", "entities": [[57, 58, "HyperparameterName", "\u03b8"], [62, 63, "HyperparameterName", "\u03b8"]]}
{"text": "Then a type classifier g ( z i ; \u03b8 g ) parameterized by \u03b8 g gives the posterior P ( y", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "z i ; \u03b8 g ) .", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "Mention Encoder : To capture lexical level information of mentions , an averaging mention encoder and a LSTM mention encoder ( Hochreiter and Schmidhuber , 1997 ) is applied to encode mentions .", "entities": [[17, 18, "MethodName", "LSTM"]]}
{"text": "By applying a LSTM over an extended mention ( w s\u22121 , w s , w s+1 , , w e , w e+1 ) , we get a sequence ( h s\u22121 , h s , h s+1 , , h e , h e+1 ) .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "We use h e+1 as LSTM mention representation r l i R d l .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "Context Encoder : A bidirectional LSTM with d l hidden units is employed to encode embedding se - quence ( w E s\u2212W , w E s\u2212W+1 , , w E e+W ) :", "entities": [[4, 6, "MethodName", "bidirectional LSTM"]]}
{"text": "Then , the word - level attention mechanism computes a score \u03b2", "entities": [[11, 12, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b1", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "j = w T tanh ( h j ) \u03b2", "entities": [[9, 10, "HyperparameterName", "\u03b2"]]}
{"text": "q has n layers with h n hidden units and use ReLu activation .", "entities": [[11, 12, "MethodName", "ReLu"]]}
{"text": "The overview of CLSC regularization is exhibited in Figure 4 , which includes three steps : dynamic graph construction ( Figure 4c ) , label propagation ( Figure 4d , e ) and Markov chains ( Figure 4 g ) .", "entities": [[17, 19, "TaskName", "graph construction"]]}
{"text": "Dynamic Graph Construction : We start by creating a fully connected graph G over the batch of samples Z = { z i } B i=1 , as shown in Figure 4c 1 .", "entities": [[1, 3, "TaskName", "Graph Construction"]]}
{"text": "Repeat from step 1 until \u03a6 converges In this work \u03a6 ( 0 ) is randomly initialized 2 .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "The loss L 1\u2212step has largely described the regularization we use in z ( ( m i , c i ) ; \u03b8 z ) for compression clustering .", "entities": [[1, 2, "MetricName", "loss"], [22, 23, "HyperparameterName", "\u03b8"]]}
{"text": "Given the representation of a mention , the type posterior is given by a standard softmax classifier parameterized by \u03b8 g : P ( \u0177 i |", "entities": [[15, 16, "MethodName", "softmax"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "z i ; \u03b8 g )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "= sof tmax ( W c", "entities": [[1, 2, "DatasetName", "sof"]]}
{"text": "z i ; \u03b8 g ) .", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "Our loss function consists of two parts .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "z i ; \u03b8 g ) )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "We evaluate our method on two standard benchmarks : OntoNotes and BBN : OntoNotes : The OntoNotes dataset is composed of sentences from the Newswire part of OntoNotes corpus ( Weischedel et al , 2013 ) .", "entities": [[9, 10, "DatasetName", "OntoNotes"], [13, 14, "DatasetName", "OntoNotes"], [16, 17, "DatasetName", "OntoNotes"], [27, 28, "DatasetName", "OntoNotes"]]}
{"text": "( Gillick et al , 2014 ) annotated the training part with the aid of DBpedia spotlight ( Daiber et al , 2013 ) , while the test data is manually annotated .", "entities": [[15, 16, "DatasetName", "DBpedia"]]}
{"text": "We compare the performance of the NFETC model with two different loss functions : partial - label loss and PLL+hierarchical loss .", "entities": [[11, 12, "MetricName", "loss"], [17, 18, "MetricName", "loss"], [20, 21, "MetricName", "loss"]]}
{"text": "We search the hyper parameter of Ontonotes and BBN respectively via Hyperopt proposed by ( Bergstra et al , 2013 ) .", "entities": [[6, 7, "DatasetName", "Ontonotes"]]}
{"text": "We optimize the model via Adam Optimizer .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "Optimizer"]]}
{"text": "The full hyper parameters includes the learning rate lr , the dimension d p of word position embedding , the dimension d l of the mention encoder 's output ( equal to the dimension of the context encoder 's ourput ) , the input dropout keep probability p", "entities": [[6, 8, "HyperparameterName", "learning rate"]]}
{"text": "BN ( whether using Batch normalization ) , the max step S lp of the label propagation , the max length S m of Markov chain , the influence parameter \u03bb clsc of CLSC , the batch size B , the number n of hidden layers in q", "entities": [[4, 6, "MethodName", "Batch normalization"], [36, 38, "HyperparameterName", "batch size"]]}
{"text": "By applying CLSC regularization on the basic NFETC model , we observe consistent and significant performance boost ; Hierarchical - aware loss shows significant advantage on the OntoNotes dataset , while showing insignificant performance boost on the BBN dataset .", "entities": [[21, 22, "MetricName", "loss"], [27, 28, "DatasetName", "OntoNotes"]]}
{"text": "The proportion of terminal types of the test set is 69 % for the BBN dataset , while is only 33 % on the OntoNotes dataset .", "entities": [[24, 25, "DatasetName", "OntoNotes"]]}
{"text": "Thus , applying hierarchical - aware loss on the BBN dataset brings little improvement ; Both algorithms are able to utilize noisy data to improve performance , so we would like to further study their performance in different noisy scenarios in following discussions .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "Named entity Recognition ( NER ) has been excavated for a long time ( Collins and Singer , 1999 ; , which classifies coarsegrained types ( e.g. person , location ) .", "entities": [[0, 3, "TaskName", "Named entity Recognition"], [4, 5, "TaskName", "NER"]]}
{"text": "( Xin et al , 2018a ) used the TransE entity embeddings ( Bordes et al , 2013 ) as the query vector of attention .", "entities": [[9, 10, "MethodName", "TransE"], [10, 12, "TaskName", "entity embeddings"]]}
{"text": "To utilize noisy data , ( Ren et al , 2016a ) distinguished the loss function of noisy data from clean data via partial label loss ( PLL ) .", "entities": [[14, 15, "MetricName", "loss"], [25, 26, "MetricName", "loss"]]}
{"text": "In this paper , we propose a new method for distantly supervised fine - grained entity typing , which leverages imperfect annotations as model regularization via Compact Latent Space Clustering ( CLSC ) .", "entities": [[15, 17, "TaskName", "entity typing"]]}
{"text": "As a part of future investigation , we plan to apply the approach to other distantly supervised tasks , such as relation extraction .", "entities": [[21, 23, "TaskName", "relation extraction"]]}
{"text": "Complementary Evidence Identification in Open - Domain Question Answering", "entities": [[4, 9, "TaskName", "Open - Domain Question Answering"]]}
{"text": "This paper proposes a new problem of complementary evidence identification for opendomain question answering ( QA ) .", "entities": [[12, 14, "TaskName", "question answering"]]}
{"text": "Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "In recent years , significant progress has been made in the field of open - domain question answering ( Chen et al , 2017 ; Wang et al , 2017Wang et al , , 2018Min et al , 2018 ; Asai et al , 2019 ) .", "entities": [[13, 18, "TaskName", "open - domain question answering"]]}
{"text": "Lin et al , 2018 ) , the solutions to the first problem still rely on traditional or neural information retrieval ( IR ) approaches , which solely measure the relevance between the question and each individual paragraph , and will highly possibly put the wrong evidence to the top .", "entities": [[19, 21, "TaskName", "information retrieval"]]}
{"text": "With these three defined properties , we hope to both improve the selective accuracy and encourage the interpretability of the evidence identification .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "Note that complementary evidence identification in QA is different from Search Result Diversification ( SRD ) in IR on their requirement of compactness .", "entities": [[14, 15, "DatasetName", "SRD"]]}
{"text": "The size of the selected set is constrained in QA tasks by the capability of downstream reasoning models and practically needs to be a small value , whereas it is not the case in SRD .", "entities": [[34, 35, "DatasetName", "SRD"]]}
{"text": "This is especially critical when we use heavy models like ELMo ( Peters et al , 2018 ) and BERT ( Devlin et al , 2018 ) , where passage encoding is time and memory consuming .", "entities": [[10, 11, "MethodName", "ELMo"], [19, 20, "MethodName", "BERT"]]}
{"text": "By properly training the passage encoder with a loss function derived by the above terms , we expect the resulted vector space satisfies the property that the complementary evidence passages lead to large scores .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "To evaluate the proposed method , we use the multi - hop QA dataset HotpotQA ( the full wiki setting ) since the ground - truth of evidence passages are provided .", "entities": [[14, 15, "DatasetName", "HotpotQA"]]}
{"text": "Experiments show that our method significantly improves the accuracy of complementary evidence selection .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Vector Space Modeling We apply BERT model to estimate the likelihood of a paragraph p being the supporting evidence to the question q , denoted as P ( p | q ) .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "We feed q and the concatenation of q and p i into the BERT model , and use the hidden states of the last layer to represent q and p i in vector space , denoted as q and p i respectively .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "A fully connected layer f ( ) followed by sigmoid activation is added to the end of the BERT model , and outputs a scalar P ( p i | q ) to estimate how relevant the paragraph", "entities": [[9, 11, "MethodName", "sigmoid activation"], [18, 19, "MethodName", "BERT"]]}
{"text": "We propose a new supervised training objective to learn the BERT encoder for QA that optimizes the previous conditions .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "and y p i = 0", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Score Function During inference , we use the following score function to find the best paragraph combination : g ( P sel ; q ; { pi } )", "entities": [[0, 1, "MetricName", "Score"]]}
{"text": "\u03b1 cos ( p i pi , q ) + \u03b2", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b2"]]}
{"text": "Note that our approach requires to encode each passage in P only once for each question , resulting in an O ( K ) time complexity of encoding ( K = | P | ) ; and the subset selection is performed in the vector space , which is much more efficient than selecting subsets before encoding .", "entities": [[29, 31, "HyperparameterName", "K ="]]}
{"text": "Datasets Considering the prerequisite of sentence - level evidence annotations , we evaluate our approach on two datasets , a synthetic dataset MNLI - 12 and a real application HotpotQA - 50 .", "entities": [[22, 23, "DatasetName", "MNLI"], [29, 30, "DatasetName", "HotpotQA"]]}
{"text": "Data sampling is detailed in Appendix B. MNLI - 12 is constructed based on the textual entailment dataset MNLI", "entities": [[7, 8, "DatasetName", "MNLI"], [18, 19, "DatasetName", "MNLI"]]}
{"text": "In original MNLI , each premise sentence corresponds to three hypotheses sentences : entailment , neutral and contradiction .", "entities": [[2, 3, "DatasetName", "MNLI"]]}
{"text": "The original train / dev / test splits from MNLI are used .", "entities": [[9, 10, "DatasetName", "MNLI"]]}
{"text": "HotpotQA - 50 is based on the open - domain setting of the multi - hop QA benchmark HotpotQA ( Yang et al , 2018 ) .", "entities": [[0, 1, "DatasetName", "HotpotQA"], [18, 19, "DatasetName", "HotpotQA"]]}
{"text": "We use the original development set from HotpotQA as our test set and randomly split a subset from the original training set as our development set .", "entities": [[7, 8, "DatasetName", "HotpotQA"]]}
{"text": "Baseline We compare with the BERT passage ranker ( Nie et al , 2019 ) that is commonly used on open - domain QA including HotpotQA .", "entities": [[5, 6, "MethodName", "BERT"], [25, 26, "DatasetName", "HotpotQA"]]}
{"text": "We also compare the DRN model from ( Harel et al , 2019 ) which is designed for the SRD task .", "entities": [[19, 20, "DatasetName", "SRD"]]}
{"text": "It is worth mentioning that we replace their LSTM encoder with BERT encoder for fair comparison .", "entities": [[8, 9, "MethodName", "LSTM"], [11, 12, "MethodName", "BERT"]]}
{"text": "The final performance is evaluated by exact match ( EM ) , i.e. , whether both true evidence passages are covered , and the F1 score on the test sets .", "entities": [[6, 8, "MetricName", "exact match"], [9, 10, "MetricName", "EM"], [24, 26, "MetricName", "F1 score"]]}
{"text": "In the experiments , we have M = 3 , N = 4 for MNLI - 12 and M = 4 , N = 5 for HotpotQA - 50 with our method .", "entities": [[14, 15, "DatasetName", "MNLI"], [26, 27, "DatasetName", "HotpotQA"]]}
{"text": "For HotpotQA dataset , we use a bi - gram BM25 ranker to collect top 50 relevant passages and build the basis for the experiments 4 , which inevitably leads some of the true evidences to be filtered out and makes its upper - bound less than 100 % .", "entities": [[1, 2, "DatasetName", "HotpotQA"]]}
{"text": "For the artificial MNLI - 12 dataset , all the true evidences are guaranteed to be included .", "entities": [[3, 4, "DatasetName", "MNLI"]]}
{"text": "Therefore it is more challenging in matching the q with the p i s. Specifically , both our method and the BERT baseline surpass the DRN model on all datasets and metrics , which results from our question - conditioned passage encoding approach .", "entities": [[21, 22, "MethodName", "BERT"]]}
{"text": "The ablation study of our loss function further illustrates that the diversity and the compactness terms efficiently bring additional 20%/30 % increase in EM score on two datasets and consequently raise the F1 score by about 8/6 absolute points .", "entities": [[5, 6, "MetricName", "loss"], [23, 24, "MetricName", "EM"], [32, 34, "MetricName", "F1 score"]]}
{"text": "The BERT baseline selects two incorrect passages that cover identical part of facts required by the question and similarly the DRN baseline select a relevant evidence and an irrelevant evidence , while our method scores lower the second passage that does not bring new information , and reaches a supporting selection .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "A similar situation contributes to the majority of improvement on one - supporting - evidence data sample in HotpotQA - 50 .", "entities": [[18, 19, "DatasetName", "HotpotQA"]]}
{"text": "On HotpotQA - 50 , it takes 1 , 990 milliseconds ( ms ) on average to obtain the embeddings of all passages for one data sample whereas our vector - based complementary selection only adds an extra 2 ms which can be negligible compared to the encoding time .", "entities": [[1, 2, "DatasetName", "HotpotQA"]]}
{"text": "We further design an algorithm and a loss function to support efficient training and inference for complementary evidence selection .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "In both examples , the DRN baseline first finds the most relevant evidence to the question ( left ) and then select a diverse one ( right ) ; the BERT baseline model selected the top - 2 most relevant passages ( P1 , P2 ) to the question regardless of their complementation ; whereas our model made the selection ( P1 , P3 ) with consideration of both relevance and evidence sufficiency .", "entities": [[30, 31, "MethodName", "BERT"], [63, 64, "DatasetName", "P3"]]}
{"text": "Predict the probability P ( p i ) of being a supporting passage for each passage B Data Sampling In original MNLI , each premise sentence P corresponds to one entailment EP , one neutral NP and one contradiction CP .", "entities": [[21, 22, "DatasetName", "MNLI"]]}
{"text": "HotpotQA In HotpotQA , the true supporting paragraphs of each question q are given .", "entities": [[0, 1, "DatasetName", "HotpotQA"], [2, 3, "DatasetName", "HotpotQA"]]}
{"text": "Domain adaptation in practice : Lessons from a real - world information extraction pipeline", "entities": [[0, 2, "TaskName", "Domain adaptation"]]}
{"text": "Advances in transfer learning and domain adaptation have raised hopes that oncechallenging NLP tasks are ready to be put to use for sophisticated information extraction needs .", "entities": [[2, 4, "TaskName", "transfer learning"], [5, 7, "TaskName", "domain adaptation"]]}
{"text": "In this work , we describe an effort to do just that - combining state - of - the - art neural methods for negation detection , document time relation extraction , and aspectual link prediction , with the eventual goal of extracting drug timelines from electronic health record text .", "entities": [[24, 26, "TaskName", "negation detection"], [29, 31, "TaskName", "relation extraction"], [34, 36, "TaskName", "link prediction"]]}
{"text": "Although domain adaptation shows improvements on each individual system , the model selection problem is a barrier to improving overall pipeline performance .", "entities": [[1, 3, "TaskName", "domain adaptation"], [11, 13, "TaskName", "model selection"]]}
{"text": "Unsupervised domain adaptation algorithms ( e.g. , Ziser and Reichart ( 2019 ) ) have made it possible to reduce performance degradation when applying trained models to new domains .", "entities": [[0, 3, "TaskName", "Unsupervised domain adaptation"]]}
{"text": "For example , if the goal is to extract normalized concepts with assertion status , the concept error can come from normalization error , negation detection error , uncertainty detection error , etc , and the errors may not be correlated .", "entities": [[24, 26, "TaskName", "negation detection"]]}
{"text": "Successfully solving this task is beneficial for understanding patient treatment course , and enabling more causal understanding in important tasks such as adverse drug event detection or relating medication courses to outcomes .", "entities": [[24, 26, "TaskName", "event detection"]]}
{"text": "This is the case even though we use a metric , accuracy , that is forgiving to the worst - performing individual model .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "It is both formally and empirically understood that classifiers can suffer performance loss when the test data is drawn from a different distribution than the training data ( sometimes called domain shift ) .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "As a result , domain adaptation approaches have been applied to multiple tasks in clinical NLP ( Miller et al , 2017 ; Liu et al , 2018 ; Hur et al , 2020 ) .", "entities": [[4, 6, "TaskName", "domain adaptation"]]}
{"text": "Recent work in the general domain has made use of transfer learning , which can attack the problem of domain shift , but by a different mechanism than domain adaptation ; by training on massive corpora , large pre - trained models both learn general features , and are able to learn from smaller new datasets without overfitting .", "entities": [[10, 12, "TaskName", "transfer learning"], [28, 30, "TaskName", "domain adaptation"]]}
{"text": "BERT ( Devlin et al , 2019 ) uses a transformer encoder , and has shown that pre - training with massive amounts of text on a language modeling task , then fine - tuning on a supervised task of interest , achieves large performance gains in multiple NLP tasks .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "During fine - tuning for sentence classification tasks , a classification head with randomly initialized weights is attached to a special sentenceinitial token .", "entities": [[5, 7, "TaskName", "sentence classification"]]}
{"text": "We began this work by developing several NLP components necessary to extract drug temporality signatures , including negation detection , relation to document creation time ( DocTimeRel ) , and aspectual link extraction ( ALINK ) , all detailed below .", "entities": [[17, 19, "TaskName", "negation detection"]]}
{"text": "The first is a classical feature - based support vector machine that is the default model of Apache cTAKES ( Savova et al , 2010 ) .", "entities": [[8, 11, "MethodName", "support vector machine"]]}
{"text": "For comparison we train a RoBERTa - based system , where the input representation is the sentence with special tokens indicating the event to be classified .", "entities": [[5, 6, "MethodName", "RoBERTa"]]}
{"text": "Hyperparameters such as learning rate and number of training epochs are optimized on the THYME colon development set .", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}
{"text": "The feature - based approach again uses the default cTAKES SVM - based implementation ( Lin et al , 2016 ) , with features based on bags of words in and around the event , and verb tense information for verbs on either side of the event .", "entities": [[10, 11, "MethodName", "SVM"]]}
{"text": "We train a separate RoBERTa - based model with the same architecture as the negation model , with the only difference being that the output layer is a softmax over the four categories rather than a sigmoid .", "entities": [[4, 5, "MethodName", "RoBERTa"], [28, 29, "MethodName", "softmax"]]}
{"text": "We are not aware of any existing open - source models for this task , so for our feature - based baseline we train a model with the same SVM classification approach and feature set as the DocTimeRel model in cTAKES .", "entities": [[29, 30, "MethodName", "SVM"]]}
{"text": "We did not perform extensive feature engineering for this task , so further gains in the SVM system are probably possible .", "entities": [[5, 7, "TaskName", "feature engineering"], [16, 17, "MethodName", "SVM"]]}
{"text": "For the RoBERTa - based model , we used the same architecture as both systems above , with a softmax over the 5 categories - the 4 ALINK categories above as well as NONE , indicating a drug mention does not participate in any ALINK relation .", "entities": [[2, 3, "MethodName", "RoBERTa"], [19, 20, "MethodName", "softmax"]]}
{"text": "To adapt to the target domain , we use unsupervised domain adaptation methods , where we have access to only unlabeled target examples .", "entities": [[9, 12, "TaskName", "unsupervised domain adaptation"]]}
{"text": "Since large pre - trained transformer models have arrived , they have been shown to be quite robust to out - of - distribution examples ( Hendrycks et al , 2020 ) , including on clinical tasks ( Lin et al , 2020 ) , where it was shown that adding domain adaptation layers on top of BERT was no better than BERT itself for negation detection .", "entities": [[51, 53, "TaskName", "domain adaptation"], [57, 58, "MethodName", "BERT"], [62, 63, "MethodName", "BERT"], [65, 67, "TaskName", "negation detection"]]}
{"text": "Specifically , to use this method , we run additional masked language model training steps on the target training data from the RoBERTa - base checkpoint , before fine - tuning on the labeled colon cancer data , and then testing on target test data .", "entities": [[22, 23, "MethodName", "RoBERTa"]]}
{"text": "We tune the learning rate for the language model pre - training on target development set data , optimizing for perplexity .", "entities": [[3, 5, "HyperparameterName", "learning rate"], [20, 21, "MetricName", "perplexity"]]}
{"text": "We measure performance on negation with F1score , on DocTimeRel with accuracy ( because the classes are relatively balanced ) , and on ALINK extraction with the average F1 score of all categories , macro - F1 ( because the high frequency NONE label makes accuracy uninformative ) .", "entities": [[11, 12, "MetricName", "accuracy"], [27, 29, "MetricName", "average F1"], [34, 37, "MetricName", "macro - F1"], [45, 46, "MetricName", "accuracy"]]}
{"text": "In addition to system - level performance , we report an evaluation of mention - level accuracy : an event is counted as correct if all three systems made the correct prediction , and we report the percentage of events that were correct .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "RoBERTa performance is stronger than the SVM on all three tasks .", "entities": [[0, 1, "MethodName", "RoBERTa"], [6, 7, "MethodName", "SVM"]]}
{"text": "It is likely this is a more difficult task , in particular because the RE - INITIATES category has relatively few examples and whose low performance skews the averaging of the macro - F1 .", "entities": [[31, 34, "MetricName", "macro - F1"]]}
{"text": "On THYME brain cancer data , RoBERTa again out - performs SVM substantially on all sub - tasks , but surprisingly the SVM performs better on PH data for negation and DocTimeRel .", "entities": [[6, 7, "MethodName", "RoBERTa"], [11, 12, "MethodName", "SVM"], [22, 23, "MethodName", "SVM"]]}
{"text": "Adapting the RoBERTa model ( RoBERTa+LM ) by performing additional language modeling in the target domain before fine - tuning on colon cancer data leads to gains only on DocTimeRel for the PH data and on ALINK for both corpora .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}
{"text": "However , the improvement to DocTimeRel from adapting RoBERTa still leaves it worse off than the SVM .", "entities": [[8, 9, "MethodName", "RoBERTa"], [16, 17, "MethodName", "SVM"]]}
{"text": "Mention level accuracy ( \" All \" column ) is good for the in - domain data ( THYME colon cancer ) , but drops off substantially even for the THYME brain cancer corpus from the same institution , created with the same guidelines and using the same annotators .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "This accuracy means that roughly one of every two drug mentions will have at least one of its attributes classified incorrectly .", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "In particular , our experiments raise questions about real - world use of domain adaptation .", "entities": [[13, 15, "TaskName", "domain adaptation"]]}
{"text": "If we treated THYME colon and brain sets as representative in - domain and out - of - domain datasets we would select RoBERTa or RoBERTa+LM for everything .", "entities": [[23, 24, "MethodName", "RoBERTa"]]}
{"text": "But an oracle optimizing PH performance would tell us to use the SVM for negation and DocTimeRel and RoBERTa+LM for ALINK .", "entities": [[12, 13, "MethodName", "SVM"]]}
{"text": "One of the difficulties in even studying domain adaptation is model selection - if labeled target data is not available , standard practices like tuning on held out data are impossible .", "entities": [[7, 9, "TaskName", "domain adaptation"], [10, 12, "TaskName", "model selection"]]}
{"text": "Domain adaptation should leverage \" BERTology \" and interpretability research to help understand how different aspects of domains contribute to performance differences .", "entities": [[0, 2, "TaskName", "Domain adaptation"]]}
{"text": "Future work will explore this direction to develop unsupervised model selection algorithms that better predict target domain performance .", "entities": [[9, 11, "TaskName", "model selection"]]}
{"text": "XGLUE :", "entities": [[0, 1, "DatasetName", "XGLUE"]]}
{"text": "In this paper , we introduce XGLUE , a new benchmark dataset that can be used to train large - scale cross - lingual pre - trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross - lingual tasks .", "entities": [[6, 7, "DatasetName", "XGLUE"]]}
{"text": "Comparing to GLUE ( Wang et al , 2019 ) , which is labeled in English for natural language understanding tasks only , XGLUE has two main advantages : ( 1 ) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios ; ( 2 ) for each task , it provides labeled data in multiple languages .", "entities": [[2, 3, "DatasetName", "GLUE"], [17, 20, "TaskName", "natural language understanding"], [23, 24, "DatasetName", "XGLUE"], [40, 43, "TaskName", "natural language understanding"]]}
{"text": "We extend a recent cross - lingual pre - trained model Unicoder to cover both understanding and generation tasks , which is evaluated on XGLUE as a strong baseline .", "entities": [[24, 25, "DatasetName", "XGLUE"]]}
{"text": "We also evaluate the base versions ( 12 - layer ) of Multilingual BERT , XLM and XLM - R for comparison .", "entities": [[13, 14, "MethodName", "BERT"], [15, 16, "MethodName", "XLM"], [17, 18, "MethodName", "XLM"]]}
{"text": "Pre - training + Fine - tuning has become a new NLP paradigm , where the general knowledge are firstly learnt from large - scale corpus by self - supervised learning and then transferred to downstream tasks by task - specific fine - tuning .", "entities": [[16, 18, "TaskName", "general knowledge"], [27, 31, "TaskName", "self - supervised learning"]]}
{"text": "In order to further advance the development of cross - lingual pre - trained models for various downstream tasks in different languages , this paper introduces XGLUE , a new benchmark dataset that can be used to : ( i ) train large - scale cross - lingual pre - trained models using multilingual and bilingual corpora , ( ii ) evaluate generalization capabilities of the cross - lingual pre - trained models across a diverse set of cross - lingual tasks .", "entities": [[26, 27, "DatasetName", "XGLUE"]]}
{"text": "The contribution of XGLUE is two - fold .", "entities": [[3, 4, "DatasetName", "XGLUE"]]}
{"text": "XTREME ) is a concurrent work of XGLUE .", "entities": [[0, 1, "DatasetName", "XTREME"], [7, 8, "DatasetName", "XGLUE"]]}
{"text": "Besides , XGLUE introduces 6 new tasks selected from Search , Ads and News scenarios , which makes XGLUE have more practical values .", "entities": [[2, 3, "DatasetName", "XGLUE"], [18, 19, "DatasetName", "XGLUE"]]}
{"text": "Second , an extended version of Unicoder is described and evaluated as a strong cross - lingual pre - trained model baseline on XGLUE for both understanding and generation tasks .", "entities": [[23, 24, "DatasetName", "XGLUE"]]}
{"text": "We also evaluate the base versions ( 12 - layer ) of Multilingual BERT ( Devlin et al , 2019 ) , XLM ( Conneau and Lample , 2019 ) and XLM - R for comparison .", "entities": [[13, 14, "MethodName", "BERT"], [22, 23, "MethodName", "XLM"], [31, 32, "MethodName", "XLM"]]}
{"text": "Bilingual Corpus We use an in - house pipeline to extract bilingual sentence pairs from the Web , which leads to a 146 G bilingual corpus covering 27 languages , including Arabic , Bulgarian , Danish , German , Greek , English , Spanish , Finnish , French , Hebrew , Hindi , Hungarian , Indonesian , Italian , Japanese , Korean , Dutch , Polish , Portuguese , Russian , Swedish , Swahili , Thai , Turkish , Urdu , Vietnamese and Chinese .", "entities": [[79, 80, "DatasetName", "Urdu"]]}
{"text": "Multilingual Corpus Following , we construct a clean version of Common Crawl ( CC ) 2 as the multilingual corpus .", "entities": [[10, 12, "DatasetName", "Common Crawl"]]}
{"text": "First , we use a language identification model trained based on Wikipedia to classify the language of each page in CC .", "entities": [[5, 7, "TaskName", "language identification"]]}
{"text": "We will add CCMatrix ( Schwenk et al , 2019 ) in the future .", "entities": [[3, 4, "DatasetName", "CCMatrix"]]}
{"text": "We select 11 cross - lingual tasks in XGLUE , which are categorized into 3 groups : single - input understanding tasks , pair - input understanding tasks , and generation tasks .", "entities": [[8, 9, "DatasetName", "XGLUE"]]}
{"text": "In order to obtain a good performance on XGLUE , a model should be able to learn how to do a task well using its English training set , and then transfer this ability to test sets in other languages .", "entities": [[8, 9, "DatasetName", "XGLUE"]]}
{"text": "NER We select a subset of the following two NER tasks , CoNLL - 2002NER ( Sang , 2002 and CoNLL - 2003 NER ( Sang andDe Meulder , 2003 ) , to form this cross - lingual NER dataset .", "entities": [[0, 1, "TaskName", "NER"], [9, 10, "TaskName", "NER"], [23, 24, "TaskName", "NER"], [35, 39, "TaskName", "cross - lingual NER"]]}
{"text": "It covers 4 languages , including English , German , Spanish and Dutch , and 4 types of named entities , including Person , Location , Organization and Miscellaneous entities that do not belong to the previous three types .", "entities": [[28, 29, "TaskName", "Miscellaneous"]]}
{"text": "F1 score is used as the metric .", "entities": [[0, 2, "MetricName", "F1 score"]]}
{"text": "Following ( Kim et al , 2017 ) , we select a subset of Universal Dependencies ( UD ) Treebanks ( v2.5 ) ( Zeman et al , 2019 ) , which covers 18 languages .", "entities": [[14, 16, "DatasetName", "Universal Dependencies"], [17, 18, "DatasetName", "UD"]]}
{"text": "Accuracy ( ACC ) of the predicted POS tags is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}
{"text": "News Classification ( NC )", "entities": [[0, 2, "TaskName", "News Classification"]]}
{"text": "Accuracy ( ACC ) of the multi - class classification is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"], [6, 10, "TaskName", "multi - class classification"]]}
{"text": "MLQA The MLQA ( Lewis et al , 2019b ) is a multilingual machine reading comprehension task , which contains QA annotations labeled in 7 languages , including English , Arabic , German , Spanish , Hindi , Vietnamese and Chinese .", "entities": [[0, 1, "DatasetName", "MLQA"], [2, 3, "DatasetName", "MLQA"], [13, 16, "TaskName", "machine reading comprehension"]]}
{"text": "F1 score of the predicted answers is used as the metric .", "entities": [[0, 2, "MetricName", "F1 score"]]}
{"text": "XNLI", "entities": [[0, 1, "DatasetName", "XNLI"]]}
{"text": "We reuse the original XNLI dataset ( Conneau et al , 2018 ) in XGLUE .", "entities": [[4, 5, "DatasetName", "XNLI"], [14, 15, "DatasetName", "XGLUE"]]}
{"text": "PAWS - X The PAWS - X ( Yang et al , 2019a ) is a paraphrase identification dataset , which extends the Wikipedia portion of the PAWS (", "entities": [[0, 3, "DatasetName", "PAWS - X"], [4, 7, "DatasetName", "PAWS - X"], [16, 18, "TaskName", "paraphrase identification"], [27, 28, "DatasetName", "PAWS"]]}
{"text": "We select 4 languages , including English , Spanish , French and German , from the original dataset and use them in XGLUE .", "entities": [[22, 23, "DatasetName", "XGLUE"]]}
{"text": "Accuracy ( ACC ) of the binary classification is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}
{"text": "Accuracy ( ACC ) of the binary classification is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}
{"text": "The relevance label contains 5 ratings : Perfect ( 4 ) , Excellent ( 3 ) , Good ( 2 ) , Fair ( 1 ) and Bad ( 0 ) .", "entities": [[29, 30, "DatasetName", "0"]]}
{"text": "The label indicates whether the passage is the answer of the question ( 1 ) , or not ( 0 ) .", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "Accuracy ( ACC ) of the binary classification is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}
{"text": "Question Generation ( QG )", "entities": [[0, 2, "TaskName", "Question Generation"]]}
{"text": "The original Unicoder includes more pre - training tasks besides MLM and TLM .", "entities": [[10, 11, "DatasetName", "MLM"]]}
{"text": "But to keep the baseline pre - trained model simple and to reduce the experimental cost , we just use MLM and TLM in this paper .", "entities": [[20, 21, "DatasetName", "MLM"]]}
{"text": "It means for understanding tasks , Unicoder is almost equal to XLM , except some hyper - parameter differences .", "entities": [[11, 12, "MethodName", "XLM"]]}
{"text": "\u03b1 l i /", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "Following Conneau and Lample ( 2019 ) , this task extends the MLM task to bilingual corpus .", "entities": [[12, 13, "DatasetName", "MLM"]]}
{"text": "Given a bilingual sentence pair , TLM first concatenates them into a single sentence , and then masks words using the same strategy of MLM .", "entities": [[24, 25, "DatasetName", "MLM"]]}
{"text": "Motivated by BART ( Lewis et al , 2019a ) , xDAE aims to predict the original text X = ( x 1 , x 2 , ... , x | X | )", "entities": [[2, 3, "MethodName", "BART"]]}
{"text": "Here , 0 - length spans correspond to the insertion of [ MASK ] tokens .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "We train Unicoder using this task by maximizing the following loss function L xDAE : L", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "Motivated by ProphetNet ( Yan et al , 2020 ) , xFNP introduces a future n - gram prediction mechanism to natural language generation .", "entities": [[2, 3, "MethodName", "ProphetNet"], [4, 7, "DatasetName", "Yan et al"]]}
{"text": "Following Yan et al ( 2020 ) , we set n = 2 .", "entities": [[1, 4, "DatasetName", "Yan et al"]]}
{"text": "We train Unicoder using this task by maximizing the following loss function L xF", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "Understanding Tasks The hyper - parameters are set as follows : 768 hidden units , 12 heads , GELU activation , a dropout rate of 0.1 , 512 max input length , 12 layers in encoder .", "entities": [[18, 19, "MethodName", "GELU"]]}
{"text": "For all sentence classification tasks , we finetune 10 epochs .", "entities": [[2, 4, "TaskName", "sentence classification"]]}
{"text": "For POS Tagging and NER , we fine - tune 20 epochs .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": ", the hyper - parameters are set as follows : 768 hidden units , 12 heads , GELU activation , a dropout rate of 0.1 , 512 max input length , 12 layers in encoder , 12 layers in decoder .", "entities": [[17, 18, "MethodName", "GELU"]]}
{"text": "In the pre - training stage , we pre - train the model from scratch and follow ProphetNet ( Yan et al , 2020 ) to randomly mask a continuous span ( with a fixed length 9 ) in every 64 tokens .", "entities": [[17, 18, "MethodName", "ProphetNet"], [19, 22, "DatasetName", "Yan et al"]]}
{"text": "( Devlin et al , 2019 ) , XLM ( Conneau and Lample , 2019 ) and XLM - R base All models are ( 12 - layer ) based ones .", "entities": [[8, 9, "MethodName", "XLM"], [17, 18, "MethodName", "XLM"]]}
{"text": "Note that , all results are reproduced by this paper , except the XLM \u2020 results on XNLI are from Conneau and Lample ( 2019 ) .", "entities": [[13, 14, "MethodName", "XLM"], [17, 18, "DatasetName", "XNLI"]]}
{"text": "QG M - BERT - - 0.1 - 7.8 0.1 0.1 - 0.2 - - 0.1 - - - - - - - 1.4 XLM - Rbase - - 0.1 - 6.0 0.0 0.0 - 0.1 - - 0.0 - - - - - - - 1 . We find ( 1 ) Unicoder LC performs slightly better than M - BERT and XLM - R base on the 9 understanding tasks , as it is pre - trained based on multilingual and bilingual corpora at the same time and uses TLM ; .", "entities": [[3, 4, "MethodName", "BERT"], [24, 25, "MethodName", "XLM"], [61, 62, "MethodName", "BERT"], [63, 64, "MethodName", "XLM"]]}
{"text": "But it is not a fair comparison , because they use different text denoising tasks ( sentence prediction vs. span prediction ) and different generation mechanisms ( single - token prediction vs. multi - token prediction ) .", "entities": [[13, 14, "TaskName", "denoising"]]}
{"text": "Table 4 chooses English as the pivot language , as all tasks in XGLUE have labeled data in English .", "entities": [[13, 14, "DatasetName", "XGLUE"]]}
{"text": "To answer these questions , we evaluate Unicoder on XNLI and NTG using different pivot languages in fine - tuning and list comparison results in Table 5 and Table 6 , respectively .", "entities": [[9, 10, "DatasetName", "XNLI"]]}
{"text": "i in Table 5 and Table 6 , its best result is often achieved when the model is fine - tuned using l i as the pivot language ; ( 2 ) For XNLI in Table 5 , the best pivot languages are Spanish ( es ) , Greek ( el ) and Turkish ( tr ) , rather than English ( en ) .", "entities": [[33, 34, "DatasetName", "XNLI"]]}
{"text": "We evaluate Unicoder on XNLI and NTG using this fine - tuning method and list evaluation results in Table 7 and Table 8 , respectively .", "entities": [[4, 5, "DatasetName", "XNLI"]]}
{"text": "We find multi - language fine - tuning can achieve better results than pivot - language fine - tuning on both XNLI and NTG .", "entities": [[21, 22, "DatasetName", "XNLI"]]}
{"text": "To reduce the experimental cost , we evaluate Unicoder on 5 understanding tasks : XNLI , PAWS - X , NC , QAM and QADSM , using their merged English labeled data in fine - tuning .", "entities": [[14, 15, "DatasetName", "XNLI"], [16, 19, "DatasetName", "PAWS - X"]]}
{"text": "We find PAWS - X and QADSM can benefit from the joint fine - tuning strategy , but XNLI , NC and QAM can not .", "entities": [[2, 5, "DatasetName", "PAWS - X"], [18, 19, "DatasetName", "XNLI"]]}
{"text": "Dataset GLUE includes 9 natural language understanding tasks that are labeled in English only .", "entities": [[1, 2, "DatasetName", "GLUE"], [4, 7, "TaskName", "natural language understanding"]]}
{"text": "Comparing to GLUE , XGLUE not only expands task annotations to multiple languages , but also includes natural language generation tasks .", "entities": [[2, 3, "DatasetName", "GLUE"], [4, 5, "DatasetName", "XGLUE"]]}
{"text": "XNLI ( Conneau et al , 2018 ) , NER ( Sang , 2002 ; Sang and De Meulder , 2003 ) , POS Tagging ( Kim et al , 2017 ) , MLQA ( Lewis et al , 2019b ) and PAWS - X ( Yang et al , 2019a ) are 5 multilingual datasets built for specific tasks .", "entities": [[0, 1, "DatasetName", "XNLI"], [9, 10, "TaskName", "NER"], [33, 34, "DatasetName", "MLQA"], [42, 45, "DatasetName", "PAWS - X"]]}
{"text": "XGLUE not only includes these 5 existing tasks , but also introduces 6 new tasks selected from real - world scenarios ( i.e. , Search , Ads and News ) .", "entities": [[0, 1, "DatasetName", "XGLUE"]]}
{"text": "This makes XGLUE have more practical values .", "entities": [[2, 3, "DatasetName", "XGLUE"]]}
{"text": "XTREME ) is a concurrent work of XGLUE .", "entities": [[0, 1, "DatasetName", "XTREME"], [7, 8, "DatasetName", "XGLUE"]]}
{"text": "Comparing to it , XGLUE includes both understanding and generation tasks , which , to the best of our knowledge , is the first attempt in the cross - lingual dataset construction efforts . )", "entities": [[4, 5, "DatasetName", "XGLUE"]]}
{"text": "is a RoBERTa - version XLM without using translation language model in pre - training .", "entities": [[2, 3, "MethodName", "RoBERTa"], [5, 6, "MethodName", "XLM"]]}
{"text": "It is trained based on a much larger multilingual corpus ( i.e. Com - mon Crawl ) and become the new state - of - the - art on XNLI .", "entities": [[29, 30, "DatasetName", "XNLI"]]}
{"text": "In this paper , we use both the Common Crawl corpus and the bilingual corpus , aiming to build a stronger baseline model on XGLUE .", "entities": [[8, 10, "DatasetName", "Common Crawl"], [24, 25, "DatasetName", "XGLUE"]]}
{"text": "BART ( Lewis et al , 2019a ) and ProphetNet ( Yan et al , 2020 ) are two latest generative pre - trained models .", "entities": [[0, 1, "MethodName", "BART"], [9, 10, "MethodName", "ProphetNet"], [11, 14, "DatasetName", "Yan et al"]]}
{"text": "We borrow ideas from these two works and extend Unicoder to cross - lingual generation tasks , which goes a step further to verify and explore different text generation approaches in the cross - lingual scenario .", "entities": [[27, 29, "TaskName", "text generation"]]}
{"text": "We present XGLUE as a new cross - lingual benchmark and conduct comprehensive evaluations with interesting findings observed .", "entities": [[2, 3, "DatasetName", "XGLUE"]]}
{"text": "A The fine - tune parameters of Unicoder on XGLUE .", "entities": [[9, 10, "DatasetName", "XGLUE"]]}
{"text": "WikiGraphs : A Wikipedia Text - Knowledge Graph Paired Dataset", "entities": [[0, 1, "DatasetName", "WikiGraphs"]]}
{"text": "We present a new dataset of Wikipedia articles each paired with a knowledge graph , to facilitate the research in conditional text generation , graph generation and graph representation learning .", "entities": [[20, 23, "TaskName", "conditional text generation"], [24, 26, "TaskName", "graph generation"], [27, 30, "TaskName", "graph representation learning"]]}
{"text": "Our new dataset WikiGraphs is collected by pairing each Wikipedia article from the established WikiText - 103 benchmark ( Merity et al , 2016 ) with a subgraph from the Freebase knowledge graph ( Bollacker et al , 2008 ) .", "entities": [[3, 4, "DatasetName", "WikiGraphs"]]}
{"text": "We present baseline graph neural network and transformer model results on our dataset for 3 tasks : graph text generation , graph text retrieval and text graph retrieval .", "entities": [[18, 20, "TaskName", "text generation"]]}
{"text": "Paired image - caption datasets enable models to describe visual scenes in natural language ( Lin et al , 2014 ; , paired streams of speech and transcription data makes it possible to train speech recognition systems ( Garofolo et al , 1993 ;", "entities": [[34, 36, "TaskName", "speech recognition"]]}
{"text": "Panayotov et al , 2015 ) or text - to - speech synthesis models ( Oord et al , 2016 ) , and parallel corpus of text in different languages enable learned machine translation models ( Barrault et al , 2020 ) .", "entities": [[7, 13, "TaskName", "text - to - speech synthesis"], [32, 34, "TaskName", "machine translation"]]}
{"text": "There has been many prior efforts trying to build datasets for learning graph text generation models ( Jin et al , 2020 ; Gardent et al , 2017 ; Lebret et al , 2016 ) .", "entities": [[13, 15, "TaskName", "text generation"]]}
{"text": "This represents a significant contrast with the state - ofthe - art text generation models ( Dai et al , 2019 ; Brown et al , 2020 ) , which can already generate very fluent and long text that spans thousands of tokens over multiple paragraphs .", "entities": [[12, 14, "TaskName", "text generation"]]}
{"text": "We attempt to bridge this gap , with the goal of advancing the state - of - the - art graph text generation models , graph representation learning models and also text - conditioned graph generative models .", "entities": [[21, 23, "TaskName", "text generation"], [25, 28, "TaskName", "graph representation learning"]]}
{"text": "To make the text generation results on our dataset directly comparable to the state - of - the - art , we chose the set of Wikipedia articles from the established language modeling benchmark WikiText - 103 ( Merity et al , 2016 ) , which contains a subset of high - quality Wikipedia articles .", "entities": [[3, 5, "TaskName", "text generation"]]}
{"text": "Out of many exciting new tasks that this dataset enables , we present 3 possibilities : graph text generation , graph text retrieval , and text graph retrieval .", "entities": [[17, 19, "TaskName", "text generation"]]}
{"text": "The models we considered were based on the recent Transformer - XL ( Dai et al , 2019 ) model , and we adapted it to condition the text generation on the KG in different ways .", "entities": [[9, 12, "MethodName", "Transformer - XL"], [28, 30, "TaskName", "text generation"]]}
{"text": "Among these , Gardent et al ( 2017 ) crowdsourced human annotators to verbalize RDF triplets taken from DBpedia ( Auer et al , 2007 ) to a few sentences ( WebNLG ) and this caused errors in annotation that were fixed with a few updates through years .", "entities": [[18, 19, "DatasetName", "DBpedia"], [31, 32, "DatasetName", "WebNLG"]]}
{"text": "The Gen - Wiki dataset ( Jin et al , 2020 ) is automatically constructed by querying KGs in DBpedia with the title of articles in Wikipedia followed by filtering and entity annotation .", "entities": [[19, 20, "DatasetName", "DBpedia"]]}
{"text": "We construct our WikiGraphs dataset by extracting a subgraph from Freebase ( Bollacker et al , 2008 ) for each Wikipedia article following a scalable automatic process .", "entities": [[3, 4, "DatasetName", "WikiGraphs"]]}
{"text": "Compared to previous work , our WikiGraphs dataset contains significantly larger graphs and longer text ( Table 1 ) .", "entities": [[6, 7, "DatasetName", "WikiGraphs"]]}
{"text": "Models for graph - text paired data Recent state of art language models are based on the Transformer architecture ( Vaswani et al , 2017 ) that uses the self attention mechanism .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "The Transformer - XL ( Dai et al , 2019 ) model further introduces a segment level recurrence with a novel positional encoding resulting in impressive performance in long sequences by capturing dependencies beyond a fixed length window .", "entities": [[1, 4, "MethodName", "Transformer - XL"]]}
{"text": "The most recent prior work on graph - to - text generation follows an encoder - decoder architecture ( Koncel - Kedziorski et al , 2019 ; Jin et al , 2020 ) , where the graph part is encoded with a GNN model , e.g. Graph Attention Network ( GAT ) ( Veli\u010dkovi\u0107 et al , 2018 ) .", "entities": [[10, 12, "TaskName", "text generation"], [46, 49, "MethodName", "Graph Attention Network"], [50, 51, "MethodName", "GAT"]]}
{"text": "The models we benchmarked for graph - to - text generation were based on the Transformer - XL architecture and conditioned on the graph through a GNN , making full use of the graph structure and capable of generating very long text comparable to the state - of - the - art .", "entities": [[9, 11, "TaskName", "text generation"], [15, 18, "MethodName", "Transformer - XL"]]}
{"text": "Basic statistics about our WikiGraphs dataset are listed in Table 2 .", "entities": [[4, 5, "DatasetName", "WikiGraphs"]]}
{"text": "Therefore the text part of WikiGraphs appears to be sufficient to reproduce and benchmark against the state - of - the - art text generative models .", "entities": [[5, 6, "DatasetName", "WikiGraphs"]]}
{"text": "The skewed distribution of nodes and edges in our dataset reflect the nature of KG 's like Freebase , and presents new challenges to graph representation learning models .", "entities": [[24, 27, "TaskName", "graph representation learning"]]}
{"text": "Specifically , we consider three tasks : text generation conditioned on the graph , graph retrieval given the text , and text retrieval given the graph .", "entities": [[7, 9, "TaskName", "text generation"]]}
{"text": "In order to incorporate graph information into an advanced language model , we adapt the recent Transformer - XL model ( Dai et al , 2019 ) to also attend to the graph features .", "entities": [[16, 19, "MethodName", "Transformer - XL"]]}
{"text": "At a high - level our model embeds the graph into a set of embedding vectors , and then exposes these embeddings to the Transformer - XL model as extra \" token \" embeddings to condition on .", "entities": [[24, 27, "MethodName", "Transformer - XL"]]}
{"text": "O = Masked - Softmax ( A ) V where [ a b ] stands for concatenation on the sequence dimension and thus A R T \u00d7 ( T + T ) and V R ( T + T ) \u00d7d h , where d h is the head dimension .", "entities": [[4, 5, "MethodName", "Softmax"]]}
{"text": "In other words , comparing to the original Transformer - XL , our model also computes the attention scores between the text queries Q t and both the text keys K t and the graph keys K g .", "entities": [[8, 11, "MethodName", "Transformer - XL"]]}
{"text": "The BoW vector is further projected using a linear layer to a latent space .", "entities": [[8, 10, "MethodName", "linear layer"]]}
{"text": "This model can be further improved , e.g. by using word embeddings and text summarization techniques , but we leave these for future work .", "entities": [[10, 12, "TaskName", "word embeddings"], [13, 15, "TaskName", "text summarization"]]}
{"text": "We reimplement the Transformer - XL model in Jax ( Bradbury et al , 2018 ) .", "entities": [[3, 6, "MethodName", "Transformer - XL"]]}
{"text": "In our experiments , we employ the base model in ( Dai et al , 2019 ) , except that we increase the tail shrinkage factor used for the adaptive softmax and input representations from 1 to 4 , which saves 63 % of the parameters without compromising the performance .", "entities": [[29, 31, "MethodName", "adaptive softmax"]]}
{"text": "Our first task is text generation conditioned on the graph .", "entities": [[4, 6, "TaskName", "text generation"]]}
{"text": "We show the reverse - BLEU score with or without prompting the original title at the start of the text generation .", "entities": [[5, 7, "MetricName", "BLEU score"], [19, 21, "TaskName", "text generation"]]}
{"text": "Unlike previous use cases for BLEU score where there are many references for one generated sample , here we have only one ground truth reference but we can generate multiple samples .", "entities": [[5, 7, "MetricName", "BLEU score"]]}
{"text": "We therefore simply swapped the reference with the samples when computing the score , which we term as the reverse - BLEU ( rBLEU ) .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "We have also tried other ways of computing the BLEU score and find that they do n't change how models compare against each other .", "entities": [[9, 11, "MetricName", "BLEU score"]]}
{"text": "We also see that conditioned on the graphs , model perplexity did n't improve , but the relevance of the samples measured by the BLEU scores did improve significantly .", "entities": [[10, 11, "MetricName", "perplexity"], [24, 25, "MetricName", "BLEU"]]}
{"text": "We can observe that there is a big difference between using message passing ( \u2265 1 layers ) or not ( 0 layers ) in terms of rBLEU score , but increasing the number of message passing layers does not change the results significantly .", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "We believe however , that these results can be improved by employing bigger and more powerful graph representation learning models , and potentially use initial node and edge representations better than bag - of - words .", "entities": [[16, 19, "TaskName", "graph representation learning"]]}
{"text": "To measure the results we use standard ranking metrics including recall@K , which computes the fraction of times the correct pair is included in the top K predictions , as well as mean average precision ( mAP ) .", "entities": [[33, 35, "MetricName", "average precision"], [36, 37, "MetricName", "mAP"]]}
{"text": "In this paper , we present WikiGraphs , a new graphtext paired dataset with significantly larger graphs and longer text compared to previous datasets of similar nature .", "entities": [[6, 7, "DatasetName", "WikiGraphs"]]}
{"text": "We show that the text part of this data is a good benchmark for state - of - the - art text generation models , and the paired dataset can help us benchmark models that are capable of generating long and coherent text conditioned on a graph structure .", "entities": [[21, 23, "TaskName", "text generation"]]}
{"text": "Bassist Adam Clayton said , \" At the time it sounded like a foreign language , whereas now we understand how it works \" .", "entities": [[1, 2, "MethodName", "Adam"]]}
{"text": "Named after General Winfield Scott , who achieved renown during the Mexican @ - @ American War , during the middle of the 19th century the fort served as a military base for US Army action in what was the edge of settlement in 1850 .", "entities": [[2, 3, "DatasetName", "General"]]}
{"text": "Rise to popularity after its release prompted U2 to record two songs with bassist Adam Clayton and guitarist Joe Bono , as they felt that it was their breakthrough album .", "entities": [[14, 15, "MethodName", "Adam"]]}
{"text": "Inspired by Romania roots rock and roll , \" You Can ' t Take This Away \" is a song about a man who suffers from severe nightmares .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "The protagonist of the game , Percy , the leader of the Knights of the Old Republic , is a member of the Knights of the Old Republic , and is appointed to lead a military coup to overthrow the Irish Republic and destroy the Home Nations ' military forces .", "entities": [[46, 47, "DatasetName", "Nations"]]}
{"text": "After the defeat at Fort Sumner , General Douglas , the commander of the Union forces , planned and executed a number of attacks on Fort Sumner .", "entities": [[7, 8, "DatasetName", "General"]]}
{"text": "In November 1864 , General Douglas was defeated at the Battle of Lake Logan .", "entities": [[4, 5, "DatasetName", "General"]]}
{"text": "On July 1 , 1861 , General Douglas sent three large armies from the Mexican @ - @ American War , a series of forts west of the Rockies , to attack Fort Vicksburg .", "entities": [[6, 7, "DatasetName", "General"]]}
{"text": "It is named in honor of General William Scott , a U.S. Army general and the first commander of the Army of the Potomac .", "entities": [[6, 7, "DatasetName", "General"]]}
{"text": "In 1808 , William Bolivar Buckner led a large movement to remove the western boundary of Texas , including Fort Scott .", "entities": [[16, 17, "DatasetName", "Texas"]]}
{"text": "Although the West Texas Aftermath quickly became a national concern , the new governor was unable to raise sufficient funds to maintain Fort Scott .", "entities": [[3, 4, "DatasetName", "Texas"]]}
{"text": "Fort Dix was named after General John Scott , a U.S. Army general and the first governor of Texas , who was killed in an assassination attempt on June 20 , 1855 .", "entities": [[5, 6, "DatasetName", "General"], [18, 19, "DatasetName", "Texas"]]}
{"text": "The site was purchased by Native Americans in 1803 and the site was added to the National Register of Historic Places in 1962 .", "entities": [[20, 21, "DatasetName", "Places"]]}
{"text": "Since 1998 , the site has been subject to an extensive series of historic markers and features that are important in preservation of American historic sites in Texas .", "entities": [[27, 28, "DatasetName", "Texas"]]}
{"text": "In 1837 , the Illinois General Assembly passed legislation creating Fort Scott as a federal park , and in the same year the state agreed to purchase the site in honor of the site 's new state of Louisiana .", "entities": [[5, 6, "DatasetName", "General"]]}
{"text": "UMDuluth - CS8761 at SemEval - 2018 Task 9 : Hypernym Discovery using Hearst Patterns , Co - occurrence frequencies and Word Embeddings", "entities": [[10, 12, "TaskName", "Hypernym Discovery"], [21, 23, "TaskName", "Word Embeddings"]]}
{"text": "Hypernym Discovery is the task of identifying potential hypernyms for a given term .", "entities": [[0, 2, "TaskName", "Hypernym Discovery"]]}
{"text": "This paper explores several approaches that rely on co - occurrence frequencies of word pairs , Hearst Patterns based on regular expressions , and word embeddings created from the UMBC corpus .", "entities": [[24, 26, "TaskName", "word embeddings"]]}
{"text": "Hypernym - hyponym pairs exhibit an is - a relationship where a hypernym is a generalization of a hyponym .", "entities": [[6, 9, "DatasetName", "is - a"]]}
{"text": "Note that hypernym discovery is distinct from hypernym detection , where the problem is to detect if a hyponym - hypernym relationship exists between a given pair , such as lemongrass - grass .", "entities": [[2, 4, "TaskName", "hypernym discovery"]]}
{"text": "The first and second module calculates the cooccurrence frequencies between the input term and words in context using the pre - processed UMBC Corpus and the Hearst Pattern set extracted from the UMBC Corpus .", "entities": [[14, 17, "DatasetName", "words in context"]]}
{"text": "The third module uses the IS - A Hearst Pattern set extracted from UMBC Corpus to obtain hypernyms .", "entities": [[5, 8, "DatasetName", "IS - A"]]}
{"text": "We use the 28 GB tokenized version of UMBC corpus which is part - of - speech tagged and divided among 408 files .", "entities": [[12, 15, "DatasetName", "part - of"]]}
{"text": "( c ) Co - occurrence frequencies from IS - A Corpus : All the words which occur at least once in the context of the input term in the IS - A Corpus are listed as candidate hypernyms for this term .", "entities": [[8, 11, "DatasetName", "IS - A"], [30, 33, "DatasetName", "IS - A"]]}
{"text": "Applying word similarity to word embeddings : A fixed distance value called Phi is used to extract words at this distance to the input term in the UMBC Embedding .", "entities": [[1, 3, "TaskName", "word similarity"], [4, 6, "TaskName", "word embeddings"]]}
{"text": "Hence , the part - of - speech tag for all candidate hypernyms is noun .", "entities": [[3, 6, "DatasetName", "part - of"]]}
{"text": "This restricts our search space to words with noun part - of - speech tag and bigram or trigram phrases with a noun head word .", "entities": [[9, 12, "DatasetName", "part - of"]]}
{"text": "It is further filtered by removing punctuation marks and words with part - of - speech tags other than noun , verb , adverb or adjective .", "entities": [[11, 14, "DatasetName", "part - of"]]}
{"text": "This module uses hypernym - hyponym pairs from the IS - A Corpus 2.2.1 which are in the form hyponym : hypernym .", "entities": [[9, 12, "DatasetName", "IS - A"]]}
{"text": "By looking at the training scores of these modules , we merge the co - occurrence frequencies from IS - A corpus that have higher ranks followed by the co - occurrence frequencies from Normalized corpus and Hearst Pattern corpus .", "entities": [[18, 21, "DatasetName", "IS - A"]]}
{"text": "However , the IS - A module seemed to fetch good candidates for both entity and concept data .", "entities": [[3, 6, "DatasetName", "IS - A"]]}
{"text": "This project was carried out as a part of CS 8761 , Natural Language Processing , a graduate level class offered in Fall 2017 at the University of Minnesota , Duluth by Dr. Ted Pedersen .", "entities": [[9, 10, "DatasetName", "CS"]]}
{"text": "Large Scale Substitution - based Word Sense Induction", "entities": [[5, 8, "TaskName", "Word Sense Induction"]]}
{"text": "Furthermore , by training a static word embeddings algorithm on the sense - tagged corpus , we obtain high - quality static senseful embeddings .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed .", "entities": [[8, 9, "DatasetName", "WiC"], [14, 16, "TaskName", "outlier detection"]]}
{"text": "Traditionally , this kind of ambiguity was dealt via word sense disambiguation ( WSD ) , a task that disambiguates word forms in context between symbolic sense - ids from a sense inventory such as WordNet ( Miller , 1992 ) or , more recently , BabelNet ( Navigli and Ponzetto , 2010 ) .", "entities": [[9, 12, "TaskName", "word sense disambiguation"]]}
{"text": "This can be remedied by word sense induction ( WSI ) , a task where the input is a given word - type and a corpus , and the output is a derived sense inventory for that word .", "entities": [[5, 8, "TaskName", "word sense induction"]]}
{"text": "The introduction of large - scale pre - trained LMs and Masked LMs ( MLM ) seemingly made WSI / WSD tasks obsolete : instead of representing tokens with symbols that encode sense information , each token is associated with a contextualized vector embeddings that captures various aspects of its in - context semantics , including the word - sense .", "entities": [[14, 15, "DatasetName", "MLM"]]}
{"text": "However , contextualized embeddings also have some major shortcomings : most notably for our case , they are expensive to store ( e.g. BERT embeddings are 768 or 1024 floating point numbers for each token ) , and are hard to index and query at scale .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "An MLM is then used to derive top - k word substitutes for each word , and these substitutevectors are clustered to derive word senses .", "entities": [[1, 2, "DatasetName", "MLM"]]}
{"text": "This combines the high - accuracy of the MLM - based approach , with the symbolic representation provided by discrete sense annotations .", "entities": [[5, 6, "MetricName", "accuracy"], [8, 9, "DatasetName", "MLM"]]}
{"text": "We show two applications of the discrete annotations , the first one is senseaware information retrieval ( 7 ) , and the second is high - quality senseful static word embeddings we can derive by training a static embeddings model on the large sense annotated corpus ( 8 ) .", "entities": [[14, 16, "TaskName", "information retrieval"], [29, 31, "TaskName", "word embeddings"]]}
{"text": "Deriving word - sense clusters for all of English Wikipedia words that appear as single - token words in BERT - LARGE 's ( Devlin et al , 2019 ) vocabulary , and assigning a sense to each occurrence in the corpus , required 100 hours of cheap P100 GPUs ( 5 hours of wall - clock time on 20 single GPU machines ) followed by roughly 4 hours on a single 96 - cores CPU machines .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "The whole process requires less than 50 GB of disk space , and costs less than 150 $ on Google Cloud platform .", "entities": [[19, 20, "DatasetName", "Google"]]}
{"text": "We show that with the produced annotated corpora it is easy to serve sense - aware information retrieval applications ( 7 ) .", "entities": [[16, 18, "TaskName", "information retrieval"]]}
{"text": "This results in state - of - theart sense - aware embeddings , which we evaluate both on an existing WiC benchmark ( Pilehvar and Camacho - Collados , 2019 ) and on a new challenging benchmark which we create ( 9 ) .", "entities": [[20, 21, "DatasetName", "WiC"]]}
{"text": "Word Sense Induction and Disambiguation Previous challenges like Jurgens and Klapaftis ( 2013 ) focused on word sense induction for small sized datasets .", "entities": [[0, 3, "TaskName", "Word Sense Induction"], [16, 19, "TaskName", "word sense induction"]]}
{"text": "An alternative approach for sense tagging is based on Word Sense Disambiguation ( WSD ) .", "entities": [[9, 12, "TaskName", "Word Sense Disambiguation"]]}
{"text": "Among different supervisied WSD methods , Zhong and Ng ( 2010 ) suggested a SVM based approach and Melamud et al ( 2016 ) ;", "entities": [[14, 15, "MethodName", "SVM"]]}
{"text": "Contextualized BERT vectors contain sense information , and clustering the contextualized vectors results in sense clusters .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "However , as shown by Amrami and Goldberg ( 2019 ) , MLM based wordsubstitutes also contain the relevant semantic information , and are much cheaper to store : each word - i d in BERTLARGE 's vocabulary can be represented by 2 bytes , and storing the top - 5 substitutes for each corpus position requires less than 20 GB of storage space .", "entities": [[12, 13, "DatasetName", "MLM"]]}
{"text": "Given raw text , we annotate each word with its top - k substitutes , create inverted word index , find best clusters for each distinct lemma and associate all corpus words with a matching cluster .", "entities": [[26, 27, "DatasetName", "lemma"]]}
{"text": "In order to perform WSI at scale , we keep the main intuition from Amrami and Goldberg ( 2019 ) , namely to cluster sparse vectors of lemmas of the top - k MLM - derived word substitutions .", "entities": [[33, 34, "DatasetName", "MLM"]]}
{"text": "However , for scalability , we iterate over the corpus sentences and collect the top - k substitutes for all words in the sentence at once based on a single BERT call for that sentence .", "entities": [[30, 31, "MethodName", "BERT"]]}
{"text": "This precludes us from using the dynamic - patterns component of their method , which requires separately running BERT for each word in each sentence .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "4 Annotation : We run BERT - large - cased - wholeword - masking on English Wikipedia , inferring substitutes for all corpus positions .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "The average number of senses per lemma is 3.13 .", "entities": [[6, 7, "DatasetName", "lemma"]]}
{"text": "One major benefit of the community detection algorithms is that they naturally produces a dynamic number of clusters , and provide a list of interpretable discrete representative lemmas for each cluster .", "entities": [[5, 7, "TaskName", "community detection"]]}
{"text": "Following Jurgens ( 2011 ) , we pose identifying sense - specific clusters as a community detection problem , where a community is defined as a group of connected nodes that are more connected to each other than to the rest of the graph .", "entities": [[15, 17, "TaskName", "community detection"]]}
{"text": "For each word w in the vocabulary , we construct a graph G w = ( V w , E w ) where each vertex v V w is a substitute - word predicted by the MLM for w , and an edge ( u , v ) E w connects substitutes that are predicted for the same instance .", "entities": [[36, 37, "DatasetName", "MLM"]]}
{"text": "x i w } | Community detection A community in a subgraph corresponds to a set of tokens that tend to co - occur in top - k substitutes of many instances , and not co - occur with top - k substitutes of other instances .", "entities": [[5, 7, "TaskName", "Community detection"]]}
{"text": "We start by intrinsically evaluating the WSI clustering method on : ( a ) SemEval 2010 and SemEval 2013 ; and ( b ) a new test set we develop for largescale WSI .", "entities": [[17, 19, "DatasetName", "SemEval 2013"]]}
{"text": "SemEval 2010 Task 14 and SemEval 2013 Task 13 ( Jurgens and Klapaftis , 2013 ) .", "entities": [[5, 7, "DatasetName", "SemEval 2013"]]}
{"text": "Our method performs best on SemEval 2010 and comparable to state - of - the - art results on SemEval 2013 .", "entities": [[19, 21, "DatasetName", "SemEval 2013"]]}
{"text": "With the notion that word sense induction systems should be robust to different annotations schemes , we gave two fluent English speakers 100 sentences for each of the 20 ambiguous words from CoarseWSD - 20 .", "entities": [[4, 7, "TaskName", "word sense induction"]]}
{"text": "Given a similar naming convention between systems and annotators , we report F1 scores of systems ' tagging accuracy with respect to the manual annotations .", "entities": [[12, 13, "MetricName", "F1"], [18, 19, "MetricName", "accuracy"]]}
{"text": "A benefit of a WSI approach compared to WSD methods is that it does not rely on a pre - specified sense inventory , and can be applied to any corpus for which a BERT - like model is available .", "entities": [[34, 35, "MethodName", "BERT"]]}
{"text": "As this dataset is larger than the Wikipedia dump , the process required roughly 145 GPU hours and resulting in 14 , 225 sense - annotated lemmas , with an average number of 2.89 senses per lemma .", "entities": [[36, 37, "DatasetName", "lemma"]]}
{"text": "While senses mosaic 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "( the common mosaic virus of plants ) and mosaic 2 ( \" something resembling a mosaic \" , \" mosaic of .. \" ) are represented in Wikipedia , senses mosaic 1 ( the mosaic genetic disorder ) and mosaic 3 ( mosaic is a quality , e.g. , \" mosaic border \" , \" mosaic pattern \" ) are specific to the scientific corpora ( The Wikipedia corpora , on the other hand , includes a sense of mosaic as a decorative art - form , which is not represented in Pubmed ) .", "entities": [[93, 94, "DatasetName", "Pubmed"]]}
{"text": "7 Sense - aware Information Retrieval An immediate application of a high quality sensetagged corpus is sense - aware retrieval .", "entities": [[4, 6, "TaskName", "Information Retrieval"]]}
{"text": "We incorporate the sense information in the SPIKE extractive search system ( Shlain et al , 2020 ) 9 for Wikipedia and Pubmed datasets .", "entities": [[22, 23, "DatasetName", "Pubmed"]]}
{"text": "Learning static word embeddings of senseambiguous words is a long standing research goal ( Reisinger and Mooney , 2010 ; Huang et al , 2012 ) .", "entities": [[2, 4, "TaskName", "word embeddings"]]}
{"text": "These include Outlier Detection ( Camacho - Collados and Navigli , 2016 ; Blair et al , 2016 ) , Term Set Expansion ( Roark and Charniak , 2000 )", "entities": [[2, 4, "TaskName", "Outlier Detection"]]}
{"text": "( Unlike the animal / experimental pig senses ) this helps sense inferring about words that are represented in the MLM as multi - tokens words ( Even though these correspond to less - frequent and often less ambiguous words ( Hern\u00e1ndez - Fern\u00e1ndez et al , 2016 ; Fenk - Oczlon et al , 2010 ; Zipf , 1945 ) ) .", "entities": [[20, 21, "DatasetName", "MLM"]]}
{"text": "9 Sense - aware Embeddings Evaluation", "entities": [[4, 6, "TaskName", "Embeddings Evaluation"]]}
{"text": "Pilehvar and Camacho - Collados ( 2019 ) introduced the WiC dataset for the task of classifying word meaning in context .", "entities": [[10, 11, "DatasetName", "WiC"]]}
{"text": "Each instance in WiC has a target word and two contexts in which it appears .", "entities": [[3, 4, "DatasetName", "WiC"]]}
{"text": "Acc .", "entities": [[0, 1, "MetricName", "Acc"]]}
{"text": "Another setup for evaluating word embeddings is that of outlier detection : given a set of words , identify which one does not belong to the set ( Blair et al , 2016 ) .", "entities": [[4, 6, "TaskName", "word embeddings"], [9, 11, "TaskName", "outlier detection"]]}
{"text": "Outlier detection instances are composed of in - group elements and a set of outliers from a related semantic space .", "entities": [[0, 2, "TaskName", "Outlier detection"]]}
{"text": "Existing outlier detection datasets either did not explicitly target sense - ambiguous words ( 8 - 8 - 8 ( Camacho - Collados and Navigli , 2016 ) , WikiSem500 ( Blair et al , 2016 ) ) or explicitly removed ambiguous words altogether ( 25 - 8 - 8 - sem ( Brink Andersen et al , 2020 ) ) .", "entities": [[1, 3, "TaskName", "outlier detection"], [29, 30, "DatasetName", "WikiSem500"]]}
{"text": "Ambiguity - driven Outlier Detection .", "entities": [[3, 5, "TaskName", "Outlier Detection"]]}
{"text": "We construct a challenge set for outlier detection that specifically targets ambiguous cases .", "entities": [[6, 8, "TaskName", "outlier detection"]]}
{"text": "For example : In - group : zeus , hades , poseidon , aphrodite , ares , athena , artemis Outliers : mercury , odysseus , jesus , sparta , delphi , rome , wrath , atlanta Distractor : nike Here , a model which does not explicitly represent the greek - god sense of nike is likely to place it far away from the in - group instances , causing it to be mistakenly marked as the outlier .", "entities": [[19, 20, "DatasetName", "artemis"], [24, 25, "DatasetName", "odysseus"]]}
{"text": "Word2vec and GloVe accuracy scores are low while having high OPP scores .", "entities": [[2, 3, "MethodName", "GloVe"], [3, 4, "MetricName", "accuracy"]]}
{"text": "These will position the distractor and the outlier furthest away from the group items while not designed to make the hard decision required for high Accuracy .", "entities": [[25, 26, "MetricName", "Accuracy"]]}
{"text": "Our sense - aware embeddings strongly outperform GloVe and word2vec which do not include senses .", "entities": [[7, 8, "MethodName", "GloVe"]]}
{"text": "Our embeddings also outperform the word embeddings proposed in De - Conf ( Pilehvar and Collier , 2016 ) , which are the best performing sense embeddings on WiC which are also publicly available .", "entities": [[5, 7, "TaskName", "word embeddings"], [28, 29, "DatasetName", "WiC"]]}
{"text": "We demonstrate the utility of such large - scale sense annotation , both in the context of a scientific search application , and for deriving high - quality senseaware static word embeddings .", "entities": [[30, 32, "TaskName", "word embeddings"]]}
{"text": "As a secondary contribution , we also develop a new variant of the Outlier Detection evaluation task , which explicitly targets ambiguous words .", "entities": [[13, 15, "TaskName", "Outlier Detection"]]}
{"text": "This project has received funding from the European Research Council ( ERC ) under the European Union 's Horizon 2020 research and innovation programme , grant agreement No . 802774 ( iEX - TRACT ) .", "entities": [[33, 34, "DatasetName", "TRACT"]]}
{"text": "For each word we detail F1 scores of the most frequent sense ( MFS ) , Babelfy , and our proposed system .", "entities": [[5, 6, "MetricName", "F1"]]}
{"text": "When using a single - prototype vector - space models , Camacho - Collados and Navigli ( 2016 ) proposed a procedure for detecting outliers based on semantic similarity using compactness score : c ( w )", "entities": [[27, 29, "TaskName", "semantic similarity"]]}
{"text": "In - Batch Negatives for Knowledge Distillation with Tightly - Coupled Teachers for Dense Retrieval", "entities": [[5, 7, "MethodName", "Knowledge Distillation"]]}
{"text": "We present an efficient training approach to text retrieval with dense representations that applies knowledge distillation using the Col - BERT late - interaction ranking model .", "entities": [[14, 16, "MethodName", "knowledge distillation"], [20, 21, "MethodName", "BERT"]]}
{"text": "The advantage of the bi - encoder teacherstudent setup is that we can efficiently add inbatch negatives during knowledge distillation , enabling richer interactions between teacher and student models .", "entities": [[18, 20, "MethodName", "knowledge distillation"]]}
{"text": "In addition , using Col - BERT as the teacher reduces training cost compared to a full cross - encoder .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Experiments on the MS MARCO passage and document ranking tasks and data from the TREC 2019", "entities": [[3, 5, "DatasetName", "MS MARCO"], [7, 9, "TaskName", "document ranking"], [14, 15, "DatasetName", "TREC"]]}
{"text": "For well over half a century , solutions to the ad hoc retrieval problem - where the system 's task is return a list of top k texts from an arbitrarily large corpus D that maximizes some metric of quality such as average precision or NDCG - has been dominated by sparse vector representations , for example , bag - of - words BM25 .", "entities": [[42, 44, "MetricName", "average precision"]]}
{"text": "Even in modern multi - stage ranking architectures , which take advantage of large pretrained transformers such as BERT ( Devlin et al , 2019 ) , the models are deployed as rerankers over initial candidates retrieved based on sparse vector representations ; this is sometimes called \" first - stage retrieval \" .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "One well - known example of this design is the BERT - based reranker of Nogueira and Cho ( 2019 ) ; see Lin et al ( 2020 ) for a recent survey .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "There is also work that explores knowledge distillation ( KD )", "entities": [[6, 8, "MethodName", "knowledge distillation"]]}
{"text": "In light of existing work on hard negative mining and knowledge distillation , we propose to improve the effectiveness of single - vector bi - encoders with a more efficient KD approach : in - batch KD using a bi - encoder teacher .", "entities": [[10, 12, "MethodName", "knowledge distillation"]]}
{"text": "d \u2212 q0 q 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "d + q0 q 0 q q 2 q d \u2212 q 1 d + q q 2 d \u2212 q2 d + q2 q 0 q 1 q 2 d + q0 d + q 1 d + q2 d \u2212 q0 d", "entities": [[4, 5, "DatasetName", "0"], [25, 26, "DatasetName", "0"]]}
{"text": "In - batch KD Teacher Student Embeddings q 0 q q 2 d \u2212 q0 d \u2212 q 1 d \u2212 q2 d + q0 d + q 1 d + q2 d \u2212 q2 d + q2 d \u2212 q 1 d + q", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "Figure 1 : Illustration of the differences between pairwise knowledge distillation and our proposed in - batch knowledge distillation .", "entities": [[9, 11, "MethodName", "knowledge distillation"], [17, 19, "MethodName", "knowledge distillation"]]}
{"text": "The contribution of this work is a simple technique for efficiently adding in - batch negative samples during knowledge distillation when training a single - vector bi - encoder .", "entities": [[18, 20, "MethodName", "knowledge distillation"]]}
{"text": "We focus on improving the training efficiency and retrieval effectiveness of dense retrieval and begin by formalizing it as a dense representation learning problem .", "entities": [[21, 23, "TaskName", "representation learning"]]}
{"text": "To be more specific , we propose to use knowledge distillation to enrich training signals and stabilize the representation learning procedure of bi - encoder models in the context of the well - known Noise - Contrastive Estimation ( NCE ) framework .", "entities": [[9, 11, "MethodName", "knowledge distillation"], [18, 20, "TaskName", "representation learning"]]}
{"text": "Following the work of Mnih and Kavukcuoglu ( 2013 ) , we formulate a common objective for dense representation learning for passage retrieval .", "entities": [[18, 20, "TaskName", "representation learning"], [21, 23, "TaskName", "passage retrieval"]]}
{"text": "Given a query q and a parameterized scoring function \u03c6 \u03b8 that computes the relevance between a query and a candidate passage p , we define a probability distribution over documents in a corpus D with respect to relevance , as follows : P q \u03b8 ( p , D )", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [45, 46, "HyperparameterName", "\u03b8"]]}
{"text": "= exp ( \u03c6 \u03b8 ( q , p ) )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "p D exp ( \u03c6 \u03b8 ( q , p ) )", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "A typical bi - encoder uses a simple scoring function for \u03c6 \u03b8 , for example , the inner product of two vectors , as shown above .", "entities": [[12, 13, "HyperparameterName", "\u03b8"]]}
{"text": "This is already setting aside the cost of using pretrained transformers such as BERT as the encoder to compute h q and h p .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "Other than designing sophisticated sampling methods for p , training bi - encoder models using knowledge distillation ( KD ) with effective teacher models is another promising approach ( Hofst\u00e4tter et al , 2020 ) .", "entities": [[15, 17, "MethodName", "knowledge distillation"]]}
{"text": "In this case , we aim to make the bi - encoder model mimic the teacher model 's probability distribution as follows : P q \u03b8 ; student ( p , D )", "entities": [[25, 26, "HyperparameterName", "\u03b8"]]}
{"text": "To improve retrieval effectiveness , one can leverage pre - computed scores from pretrained models such as cross - encoders , e.g. , BERT , bi - encoders , e.g. , ColBERT , or ensembled scores from multiple models \u03c6\u03b8 = j \u03c6\u03b8 ; j .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "teacher ( p , D B ) log P q \u03b8 ; teacher ( p , D B ) P q \u03b8 ; student ( p , D B ) .", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [21, 22, "HyperparameterName", "\u03b8"]]}
{"text": "For example , a \" vanilla \" crossencoder design using BERT can be denoted as : \u03c6\u03b8 ; Cat W f ( h q p ) , ( 5 ) where the ranking score is first computed by the hidden representation of the concatenation q p from BERT ( along with the standard special tokens ) and then mapped to a scalar by a pooling operation f and a mapping matrix W .", "entities": [[10, 11, "MethodName", "BERT"], [47, 48, "MethodName", "BERT"]]}
{"text": "Although effective , due to BERT 's quadratic complexity with respect to input sequence length , this design makes exhaustive combinations between a query and possible candidates impractical , since this requires evaluating cross - encoders | B | 2 times to compute Eq .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "i q h j p , ( 6 ) where i and j are the indices of token representations of a query q and a passage p of Col - BERT ( Khattab and Zaharia , 2020 ) .", "entities": [[30, 31, "MethodName", "BERT"]]}
{"text": "The contribution of our work is in - batch knowledge distillation with a tightly - coupled teacher .", "entities": [[9, 11, "MethodName", "knowledge distillation"]]}
{"text": "The student encoder outputs single - vector dense representations ( h q , h p ) by performing average pooling over the token embeddings from the final layer .", "entities": [[18, 20, "MethodName", "average pooling"]]}
{"text": "Given that in - batch negative sampling is an efficient way to add more information into knowledge distillation , we wonder whether our tightly - coupled teacher design works well when applied to more sophisticated sampling methods .", "entities": [[16, 18, "MethodName", "knowledge distillation"]]}
{"text": "In this section , we conduct experiments on the MS MARCO passage and document corpora .", "entities": [[9, 11, "DatasetName", "MS MARCO"]]}
{"text": "For passage ranking , we first train models on BM25 negatives as warm - up and compare different KD methods .", "entities": [[1, 3, "TaskName", "passage ranking"]]}
{"text": "For document ranking , following previous work Zhan et al , 2020 ; Lu et al , 2021 ) , we start with our BM25 warmed - up checkpoint for passage ranking and conduct additional hard negative training .", "entities": [[1, 3, "TaskName", "document ranking"], [30, 32, "TaskName", "passage ranking"]]}
{"text": "We compare different bi - encoder models using BERT - base as the backbone , which uses single 768 - dim vectors to represent each query and passage : 1 . Baseline : a single - vector bi - encoder trained with in - batch negatives , as discussed in Section 2.1 , which is similar to Karpukhin et", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "but with a smaller batch size .", "entities": [[4, 6, "HyperparameterName", "batch size"]]}
{"text": "We also compare against two models , KD - T1 and KD - T2 , which use BERT - base bi - encoders as student models .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "In the former , the student is distilled from a BERT - base cross - encoder , while the latter is distilled from ensembled cross - encoders comprising BERT - base , BERT - large , and ALBERTlarge .", "entities": [[10, 11, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"], [32, 33, "MethodName", "BERT"]]}
{"text": "In addition , we adopt pairwise softmax probabilities from fine - tuned ColBERT to train KD - ColBERT for comparison .", "entities": [[6, 7, "MethodName", "softmax"]]}
{"text": "First , we see that pairwise KD methods show significant improvements over the baseline , indicat - 0 1 2 3 4 5 6 7 8 9 Index Size ( 10 ing that information from BM25 negatives can not be fully exploited without teacher models .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "We suspect that they have comparable capabilities to discriminate most paired passages ( BM25 negative vs. positive samples ) , i.e. , Col - BERT is good enough to guide bi - encoder student models to discriminate them .", "entities": [[24, 25, "MethodName", "BERT"]]}
{"text": "We start with a small synthetic corpus composed of the relevant passages and the top - 1000 BM25 candidates of the 6980 ( 43 ) queries from MARCO Dev ( TREC - DL ' 19 ) .", "entities": [[30, 31, "DatasetName", "TREC"]]}
{"text": "ANCE uses RoBERTa - base as its backbone .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}
{"text": "LTRe also use RoBERTa - base as its backbone .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}
{"text": "3 . SEED - Encoder", "entities": [[2, 3, "DatasetName", "SEED"]]}
{"text": "In contrast , hard negative training using Col - BERT 's in - batch KD further boosts ranking effectiveness , especially when our teacher ( ColBERT ) 2 Re - encoding the entire corpus takes \u223c10 hours on one GPU .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "To validate the effectiveness and generality of our training strategy , we conduct further experiments on document retrieval using the MS MARCO document ranking dataset .", "entities": [[20, 22, "DatasetName", "MS MARCO"], [22, 24, "TaskName", "document ranking"]]}
{"text": "This dataset contains 3.2 M web pages gathered from passages in the MS MARCO passage ranking dataset .", "entities": [[12, 14, "DatasetName", "MS MARCO"], [14, 16, "TaskName", "passage ranking"]]}
{"text": "Following the FirstP setting for document retrieval described in , we feed the first 512 tokens of each document for encoding , and start with the warmed - up checkpoint for our encoder 's parameters trained for passage retrieval ( using BM25 negatives , as described in Section 4.1.1 ) .", "entities": [[37, 39, "TaskName", "passage retrieval"]]}
{"text": "This may be due to the fact that FirstP document retrieval is very different from passage retrieval , making zero - shot transfer ineffective .", "entities": [[15, 17, "TaskName", "passage retrieval"]]}
{"text": "To sum up , in the document ranking task , TCT - ColBERT yields competitive effectiveness with a one - time index refresh and outperforms other computationally expensive methods with one additional index refresh .", "entities": [[6, 8, "TaskName", "document ranking"]]}
{"text": "We illustrate the endto - end tradeoffs in terms of quality , time , and space of different dense - sparse hybrid combinations on the passage retrieval tasks .", "entities": [[25, 27, "TaskName", "passage retrieval"]]}
{"text": "\u03b1 \u03c6 sp ( q , p ) +", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "min p D ds \u03c6 ds ( q , p ) , if p / D ds \u03b1 min p Dsp \u03c6 sp ( q , p ) + \u03c6 ds ( q , p ) , if p / D sp \u03b1 \u03c6 sp ( q , p ) + \u03c6 ds ( q , p ) , otherwise .", "entities": [[17, 18, "HyperparameterName", "\u03b1"], [42, 43, "HyperparameterName", "\u03b1"]]}
{"text": "We conduct dense - sparse hybrid experiments with sparse retrieval ( BM25 ranking ) on the original passages ( denoted BM25 ) and on passages with docTTTTTquery document expansion ( Nogueira and Lin , 2019 ) ( denoted doc2query - T5 ) .", "entities": [[40, 41, "MethodName", "T5"]]}
{"text": "Table 5 shows passage retrieval results in terms of ranking effectiveness , query latency , and storage requirements ( i.e. , index size ) for each model and Table 6 reports the component latencies of our TCT - ColBERT dense - sparse hybrid .", "entities": [[3, 5, "TaskName", "passage retrieval"]]}
{"text": "Moving ( Dai and Callan , 2020 ) .243 .551 55 4 doc2query - T5 ( Nogueira and Lin , 2019 ) .277 .551 64 14 Dense retrieval : single - vector TAS - B ( Hofst\u00e4tter et al , 2021 ) .343 .722 64 13 RocketQA", "entities": [[14, 15, "MethodName", "T5"]]}
{"text": ".359 .719 107 13 Dense retrieval : multi - vector ME - BERT ( Luan et al , 2021 ) .334 .687 - 96 ColBERT ( Khattab and Zaharia , 2020 ) .360 - 458 154 Hybrid dense + sparse CLEAR ( Gao et al , 2020b ) .338 .699 - 17 a ME - HYBRID - E", "entities": [[12, 13, "MethodName", "BERT"], [40, 41, "DatasetName", "CLEAR"]]}
{"text": "( Luan et al , 2021 ) .343 .706 - 100 TAS - B + doc2query - T5 ( Hofst\u00e4tter et al , 2021 ) .", "entities": [[17, 18, "MethodName", "T5"]]}
{"text": "+ doc2query - T5 compares favorably with a recent advanced model , TAS - B + doc2query - T5 ( Hofst\u00e4tter et al , 2021 ) , which introduces topic - aware sampling and dual teachers , incorporating part of our TCT - ColBERT work .", "entities": [[3, 4, "MethodName", "T5"], [18, 19, "MethodName", "T5"]]}
{"text": "We propose a teacher - student knowledge distillation approach using tightly coupled bi - encoders that enables exhaustive use of query - passage combinations in each minibatch .", "entities": [[6, 8, "MethodName", "knowledge distillation"]]}
{"text": "First , E T ( \u03b5 ) = \u03b5 .", "entities": [[5, 6, "HyperparameterName", "\u03b5"], [8, 9, "HyperparameterName", "\u03b5"]]}
{"text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "Unlike recent language representation models ( Peters et al , 2018a ; Radford et al , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "As a result , the pre - trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models for a wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications .", "entities": [[8, 9, "MethodName", "BERT"], [38, 40, "TaskName", "question answering"]]}
{"text": "BERT is conceptually simple and empirically powerful .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "These include sentence - level tasks such as natural language inference ( Bowman et al , 2015 ;", "entities": [[8, 11, "TaskName", "natural language inference"]]}
{"text": "Williams et al , 2018 ) and paraphrasing ( Dolan and Brockett , 2005 ) , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level ( Tjong Kim Sang and De Meulder , 2003 ; Rajpurkar et al , 2016 ) .", "entities": [[38, 41, "TaskName", "named entity recognition"], [42, 44, "TaskName", "question answering"]]}
{"text": "The feature - based approach , such as ELMo ( Peters et al , 2018a ) , uses task - specific architectures that include the pre - trained representations as additional features .", "entities": [[8, 9, "MethodName", "ELMo"]]}
{"text": "The fine - tuning approach , such as the Generative Pre - trained Transformer ( OpenAI GPT ) ( Radford et al , 2018 ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters .", "entities": [[13, 14, "MethodName", "Transformer"], [16, 17, "MethodName", "GPT"]]}
{"text": "Such restrictions are sub - optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions .", "entities": [[29, 31, "TaskName", "question answering"]]}
{"text": "In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "BERT alleviates the previously mentioned unidirectionality constraint by using a \" masked language model \" ( MLM ) pre - training objective , inspired by the Cloze task ( Taylor , 1953 ) .", "entities": [[0, 1, "MethodName", "BERT"], [16, 17, "DatasetName", "MLM"]]}
{"text": "Unlike left - toright language model pre - training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .", "entities": [[11, 12, "DatasetName", "MLM"], [33, 34, "MethodName", "Transformer"]]}
{"text": "Unlike Radford et al ( 2018 ) , which uses unidirectional language models for pre - training , BERT uses masked language models to enable pretrained deep bidirectional representations .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "BERT advances the state of the art for eleven NLP tasks .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "Pre - trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch ( Turian et al , 2010 ) .", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "These approaches have been generalized to coarser granularities , such as sentence embeddings Logeswaran and Lee , 2018 ) or paragraph embeddings ( Le and Mikolov , 2014 ) .", "entities": [[11, 13, "TaskName", "sentence embeddings"]]}
{"text": "To train sentence representations , prior work has used objectives to rank candidate next sentences ( Jernite et al , 2017 ; Logeswaran and Lee , 2018 ) , left - to - right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives ( Hill et al , 2016 ) .", "entities": [[48, 50, "MethodName", "denoising autoencoder"]]}
{"text": "ELMo and its predecessor ( Peters et al , 2017 ( Peters et al , , 2018a generalize traditional word embedding research along a different dimension .", "entities": [[0, 1, "MethodName", "ELMo"]]}
{"text": "When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks ( Peters et al , 2018a ) including question answering ( Rajpurkar et al , 2016 ) , sentiment analysis ( Socher et al , 2013 ) , and named entity recognition ( Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[3, 5, "TaskName", "word embeddings"], [12, 13, "MethodName", "ELMo"], [32, 34, "TaskName", "question answering"], [42, 44, "TaskName", "sentiment analysis"], [53, 56, "TaskName", "named entity recognition"]]}
{"text": "Similar to ELMo , their model is feature - based and not deeply bidirectional .", "entities": [[2, 3, "MethodName", "ELMo"]]}
{"text": "Fedus et al ( 2018 ) shows that the cloze task can be used to improve the robustness of text generation models .", "entities": [[19, 21, "TaskName", "text generation"]]}
{"text": "At least partly due to this advantage , OpenAI GPT ( Radford et al , 2018 ) achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark ( Wang et al , 2018a ) .", "entities": [[9, 10, "MethodName", "GPT"], [33, 34, "DatasetName", "GLUE"]]}
{"text": "Left - to - right language model - BERT BERT E [ CLS ] E 1 E [ SEP ] ...", "entities": [[8, 9, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"]]}
{"text": "There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference ( Conneau et al , 2017 ) and machine translation ( McCann et al , 2017 ) .", "entities": [[17, 20, "TaskName", "natural language inference"], [28, 30, "TaskName", "machine translation"]]}
{"text": "Computer vision research has also demonstrated the importance of transfer learning from large pre - trained models , where an effective recipe is to fine - tune models pre - trained with I m a - geNet ( Deng et al , 2009 ; Yosinski et al , 2014 ) .", "entities": [[9, 11, "TaskName", "transfer learning"], [36, 37, "MethodName", "geNet"], [38, 41, "DatasetName", "Deng et al"]]}
{"text": "We introduce BERT and its detailed implementation in this section .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "For finetuning , the BERT model is first initialized with the pre - trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "A distinctive feature of BERT is its unified architecture across different tasks .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "Model Architecture BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation described in Vaswani et", "entities": [[2, 3, "MethodName", "BERT"], [12, 13, "MethodName", "Transformer"]]}
{"text": "Because the use of Transformers has become common and our implementation is almost identical to the original , we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al ( 2017 ) as well as excellent guides such as \" The Annotated Transformer . \"", "entities": [[49, 50, "MethodName", "Transformer"]]}
{"text": "BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes .", "entities": [[0, 1, "MethodName", "BERT"], [1, 2, "MethodName", "BASE"], [12, 13, "MethodName", "GPT"]]}
{"text": "Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left .", "entities": [[5, 6, "MethodName", "BERT"], [6, 7, "MethodName", "Transformer"], [15, 16, "MethodName", "GPT"], [16, 17, "MethodName", "Transformer"]]}
{"text": "4 Input / Output Representations To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "A \" sequence \" refers to the input token sequence to BERT , which may be a single sentence or two sentences packed together .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "We use WordPiece embeddings ( Wu et al , 2016 ) with a 30 , 000 token vocabulary .", "entities": [[2, 3, "MethodName", "WordPiece"]]}
{"text": "Unlike Peters et al ( 2018a ) and Radford et al ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre - train BERT .", "entities": [[37, 38, "MethodName", "BERT"]]}
{"text": "Instead , we pre - train BERT using two unsupervised tasks , described in this section .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "former is often referred to as a \" Transformer encoder \" while the left - context - only version is referred to as a \" Transformer decoder \" since it can be used for text generation .", "entities": [[8, 9, "MethodName", "Transformer"], [25, 27, "MethodName", "Transformer decoder"], [34, 36, "TaskName", "text generation"]]}
{"text": "We refer to this procedure as a \" masked LM \" ( MLM ) , although it is often referred to as a Cloze task in the literature ( Taylor , 1953 ) .", "entities": [[12, 13, "DatasetName", "MLM"]]}
{"text": "In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM .", "entities": [[18, 19, "MethodName", "softmax"]]}
{"text": "In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random .", "entities": [[12, 13, "MethodName", "WordPiece"]]}
{"text": "In contrast to denoising auto - encoders ( Vincent et al , 2008 ) , we only predict the masked words rather than reconstructing the entire input .", "entities": [[3, 4, "TaskName", "denoising"]]}
{"text": "Then , T i will be used to predict the original token with cross entropy loss .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling .", "entities": [[6, 8, "TaskName", "Question Answering"], [12, 15, "TaskName", "Natural Language Inference"]]}
{"text": "However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .", "entities": [[7, 9, "TaskName", "sentence embeddings"], [18, 19, "MethodName", "BERT"]]}
{"text": "It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark ( Chelba et al , 2013 ) in order to extract long contiguous sequences .", "entities": [[21, 24, "DatasetName", "Billion Word Benchmark"]]}
{"text": "Fine - tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .", "entities": [[11, 12, "MethodName", "Transformer"], [13, 14, "MethodName", "BERT"]]}
{"text": "BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "For each task , we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "At the input , sentence A and sentence B from pre - training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text - pair in text classification or sequence tagging .", "entities": [[42, 44, "TaskName", "question answering"], [55, 57, "TaskName", "text classification"]]}
{"text": "At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis .", "entities": [[22, 24, "TaskName", "question answering"], [44, 46, "TaskName", "sentiment analysis"]]}
{"text": "In this section , we present BERT fine - tuning results on 11 NLP tasks .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al , 2018a ) is a collection of diverse natural language understanding tasks .", "entities": [[1, 2, "DatasetName", "General"], [6, 7, "DatasetName", "GLUE"], [21, 24, "TaskName", "natural language understanding"]]}
{"text": "Detailed descriptions of GLUE datasets are included in Appendix B.1 .", "entities": [[3, 4, "DatasetName", "GLUE"]]}
{"text": "To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .", "entities": [[5, 6, "DatasetName", "GLUE"]]}
{"text": "We compute a standard classification loss with C and W , i.e. , log ( softmax ( CW T ) ) .", "entities": [[5, 6, "MetricName", "loss"], [15, 16, "MethodName", "softmax"]]}
{"text": "Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking .", "entities": [[2, 3, "MethodName", "BERT"], [3, 4, "MethodName", "BASE"], [6, 7, "MethodName", "GPT"]]}
{"text": "On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing .", "entities": [[3, 4, "DatasetName", "GLUE"], [7, 8, "MethodName", "BERT"], [18, 19, "MethodName", "GPT"]]}
{"text": "We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .", "entities": [[3, 4, "MethodName", "BERT"], [7, 8, "MethodName", "BERT"], [8, 9, "MethodName", "BASE"]]}
{"text": "The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100k crowdsourced question / answer pairs ( Rajpurkar et al , 2016 ) .", "entities": [[0, 5, "DatasetName", "The Stanford Question Answering Dataset"], [6, 7, "DatasetName", "SQuAD"]]}
{"text": "The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE . 10 https://gluebenchmark.com/leaderboard Wikipedia containing the answer , the task is to predict the answer text span in the passage .", "entities": [[1, 2, "DatasetName", "GLUE"], [18, 19, "DatasetName", "GLUE"]]}
{"text": "As shown in Figure 1 , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .", "entities": [[8, 10, "TaskName", "question answering"]]}
{"text": "The probability of word i being the start of the answer span is computed as a dot product between T i and S followed by a softmax over all of the words in the paragraph : P i =", "entities": [[26, 27, "MethodName", "softmax"]]}
{"text": "The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems .", "entities": [[5, 6, "DatasetName", "SQuAD"]]}
{"text": "We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al , 2017 ) befor fine - tuning on SQuAD .", "entities": [[4, 6, "TaskName", "data augmentation"], [15, 16, "DatasetName", "TriviaQA"], [28, 29, "DatasetName", "SQuAD"]]}
{"text": "In fact , our single BERT model outperforms the top ensemble system in terms of F1 score .", "entities": [[5, 6, "MethodName", "BERT"], [15, 17, "MetricName", "F1 score"]]}
{"text": "The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic .", "entities": [[1, 2, "DatasetName", "SQuAD"], [6, 7, "DatasetName", "SQuAD"]]}
{"text": "We use a simple approach to extend the SQuAD v1.1 BERT model for this task .", "entities": [[8, 9, "DatasetName", "SQuAD"], [10, 11, "MethodName", "BERT"]]}
{"text": "The TriviaQA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers .", "entities": [[1, 2, "DatasetName", "TriviaQA"], [9, 10, "DatasetName", "TriviaQA"]]}
{"text": "We predict a non - null answer when\u015d i , j > s null + \u03c4 , where the threshold \u03c4 is selected on the dev set to maximize F1 .", "entities": [[29, 30, "MetricName", "F1"]]}
{"text": "We did not use TriviaQA data for this model .", "entities": [[4, 5, "DatasetName", "TriviaQA"]]}
{"text": "The results compared to prior leaderboard entries and top published work ( Sun et al , 2018 ; Wang et al , 2018b ) are shown in Table 3 , excluding systems that use BERT as one of their components .", "entities": [[34, 35, "MethodName", "BERT"]]}
{"text": "The Situations With Adversarial Generations ( SWAG ) dataset contains 113k sentence - pair completion examples that evaluate grounded commonsense inference ( Zellers et al , 2018 ) .", "entities": [[6, 7, "DatasetName", "SWAG"]]}
{"text": "When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) .", "entities": [[6, 7, "DatasetName", "SWAG"]]}
{"text": "The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .", "entities": [[32, 33, "MethodName", "softmax"]]}
{"text": "BERT LARGE outperforms the authors ' baseline ESIM+ELMo system by +27.1 % and OpenAI GPT by 8.3 % .", "entities": [[0, 1, "MethodName", "BERT"], [14, 15, "MethodName", "GPT"]]}
{"text": "In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data , fine - tuning scheme , and hyperparameters as BERT BASE : No NSP : A bidirectional model which is trained using the \" masked LM \" ( MLM ) but without the \" next sentence prediction \" ( NSP ) task .", "entities": [[9, 10, "MethodName", "BERT"], [30, 31, "MethodName", "BERT"], [31, 32, "MethodName", "BASE"], [49, 50, "DatasetName", "MLM"]]}
{"text": "A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM .", "entities": [[26, 27, "DatasetName", "MLM"]]}
{"text": "This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme .", "entities": [[6, 7, "MethodName", "GPT"]]}
{"text": "In Table 5 , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQuAD 1.1 .", "entities": [[13, 14, "DatasetName", "QNLI"], [15, 16, "DatasetName", "MNLI"], [18, 19, "DatasetName", "SQuAD"]]}
{"text": "The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD .", "entities": [[7, 8, "DatasetName", "MLM"], [17, 18, "DatasetName", "MRPC"], [19, 20, "DatasetName", "SQuAD"]]}
{"text": "For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context .", "entities": [[1, 2, "DatasetName", "SQuAD"]]}
{"text": "In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top .", "entities": [[19, 20, "MethodName", "BiLSTM"]]}
{"text": "This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models .", "entities": [[6, 7, "DatasetName", "SQuAD"]]}
{"text": "The BiLSTM hurts performance on the GLUE tasks .", "entities": [[1, 2, "MethodName", "BiLSTM"], [6, 7, "DatasetName", "GLUE"]]}
{"text": "We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does .", "entities": [[28, 29, "MethodName", "ELMo"]]}
{"text": "In this section , we explore the effect of model size on fine - tuning task accuracy .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training procedure as described previously .", "entities": [[5, 6, "MethodName", "BERT"], [10, 13, "HyperparameterName", "number of layers"]]}
{"text": "Results on selected GLUE tasks are shown in Table 6 .", "entities": [[3, 4, "DatasetName", "GLUE"]]}
{"text": "For example , the largest Transformer explored in Vaswani et", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "al ( 2017 ) is ( L=6 , H=1024 , A=16 ) with 100 M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H=512 , A=2 ) with 235 M parameters ( Al - Rfou et al , 2018 ) .", "entities": [[23, 24, "MethodName", "Transformer"]]}
{"text": "By contrast , BERT BASE contains 110 M parameters and BERT LARGE contains 340 M parameters .", "entities": [[3, 4, "MethodName", "BERT"], [4, 5, "MethodName", "BASE"], [10, 11, "MethodName", "BERT"]]}
{"text": "All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre - trained model , and all parameters are jointly fine - tuned on a downstream task .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "In this section , we compare the two approaches by applying BERT to the CoNLL - 2003 Named Entity Recognition ( NER ) task ( Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[11, 12, "MethodName", "BERT"], [17, 20, "TaskName", "Named Entity Recognition"], [21, 22, "TaskName", "NER"]]}
{"text": "In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data .", "entities": [[4, 5, "MethodName", "BERT"], [12, 13, "MethodName", "WordPiece"]]}
{"text": "Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output .", "entities": [[16, 17, "MethodName", "CRF"]]}
{"text": "We use the representation of the first sub - token as the input to the token - level classifier over the NER label set .", "entities": [[21, 22, "TaskName", "NER"]]}
{"text": "To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT .", "entities": [[31, 32, "MethodName", "BERT"]]}
{"text": "These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer .", "entities": [[17, 18, "MethodName", "BiLSTM"]]}
{"text": "BERT LARGE performs competitively with state - of - the - art methods .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "This demonstrates that BERT is effective for both finetuning and feature - based approaches .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre - training is an integral part of many language understanding systems .", "entities": [[5, 7, "TaskName", "transfer learning"], [15, 19, "TaskName", "unsupervised pre - training"]]}
{"text": "The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces .", "entities": [[6, 7, "MethodName", "WordPiece"]]}
{"text": "We use a gelu activation ( Hendrycks and Gimpel , 2016 ) rather than the standard relu , following OpenAI GPT .", "entities": [[3, 4, "MethodName", "gelu"], [16, 17, "MethodName", "relu"], [20, 21, "MethodName", "GPT"]]}
{"text": "The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) .", "entities": [[2, 3, "MethodName", "BERT"], [3, 4, "MethodName", "BASE"]]}
{"text": "13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "For fine - tuning , most model hyperparameters are the same as in pre - training , with the exception of the batch size , learning rate , and number of training epochs .", "entities": [[22, 24, "HyperparameterName", "batch size"], [25, 27, "HyperparameterName", "learning rate"]]}
{"text": "A.4 Comparison of BERT , ELMo , and OpenAI GPT", "entities": [[3, 4, "MethodName", "BERT"], [5, 6, "MethodName", "ELMo"], [9, 10, "MethodName", "GPT"]]}
{"text": "Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT .", "entities": [[8, 10, "TaskName", "representation learning"], [12, 13, "MethodName", "ELMo"], [15, 16, "MethodName", "GPT"], [17, 18, "MethodName", "BERT"]]}
{"text": "Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach .", "entities": [[9, 10, "MethodName", "BERT"], [12, 13, "MethodName", "GPT"], [18, 19, "MethodName", "ELMo"]]}
{"text": "The most comparable existing pre - training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus .", "entities": [[9, 10, "MethodName", "BERT"], [12, 13, "MethodName", "GPT"], [22, 23, "MethodName", "Transformer"]]}
{"text": "In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared .", "entities": [[9, 10, "MethodName", "BERT"], [19, 20, "MethodName", "GPT"]]}
{"text": "The core argument of this work is that the bi - directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained : GPT is trained on the BooksCorpus ( 800 M words ) ; BERT is trained on the BooksCorpus ( 800 M words ) and Wikipedia ( 2 , 500 M words ) .", "entities": [[42, 43, "MethodName", "BERT"], [44, 45, "MethodName", "GPT"], [48, 49, "MethodName", "GPT"], [60, 61, "MethodName", "BERT"]]}
{"text": "The illustration of fine - tuning BERT on different tasks can be seen in Figure 4 .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch .", "entities": [[9, 10, "MethodName", "BERT"], [19, 22, "HyperparameterName", "number of parameters"]]}
{"text": "Among the tasks , The GLUE benchmark includes the following datasets , the descriptions of which were originally summarized in Wang et al ( 2018a ) : MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task ( Williams et al , 2018 ) .", "entities": [[5, 6, "DatasetName", "GLUE"], [27, 28, "DatasetName", "MNLI"], [31, 34, "TaskName", "Natural Language Inference"]]}
{"text": "QQP Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent .", "entities": [[0, 1, "DatasetName", "QQP"], [1, 4, "DatasetName", "Quora Question Pairs"]]}
{"text": "QNLI", "entities": [[0, 1, "DatasetName", "QNLI"]]}
{"text": "Question Natural Language Inference is a version of the Stanford Question", "entities": [[1, 4, "TaskName", "Natural Language Inference"]]}
{"text": "with human annotations of their sentiment ( Socher et al , 2013 ) . CoLA", "entities": [[14, 15, "DatasetName", "CoLA"]]}
{"text": "The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically \" acceptable \" or not ( Warstadt et al , 2018 ) .", "entities": [[3, 5, "TaskName", "Linguistic Acceptability"], [10, 12, "TaskName", "sentence classification"]]}
{"text": "The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources ( Cer et al , 2017 ) .", "entities": [[1, 4, "TaskName", "Semantic Textual Similarity"]]}
{"text": "MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent ( Dolan and Brockett , 2005 ) .", "entities": [[0, 1, "DatasetName", "MRPC"]]}
{"text": "RTE Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ( Bentivogli et al , 2009 ) .", "entities": [[0, 1, "DatasetName", "RTE"], [11, 12, "DatasetName", "MNLI"]]}
{"text": "14 WNLI Winograd NLI is a small natural language inference dataset ( Levesque et al , 2011 ) .", "entities": [[1, 2, "DatasetName", "WNLI"], [7, 10, "TaskName", "natural language inference"]]}
{"text": "We therefore exclude this set to be fair to OpenAI GPT .", "entities": [[10, 11, "MethodName", "GPT"]]}
{"text": "For our GLUE submission , we always predicted the majority class .", "entities": [[2, 3, "DatasetName", "GLUE"]]}
{"text": "In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre - training with the masked language model ( MLM ) objective .", "entities": [[7, 8, "MethodName", "BERT"], [27, 28, "DatasetName", "MLM"]]}
{"text": "We report the Dev results for both MNLI and NER .", "entities": [[7, 8, "DatasetName", "MNLI"], [9, 10, "TaskName", "NER"]]}
{"text": "For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations .", "entities": [[1, 2, "TaskName", "NER"]]}
{"text": "In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .", "entities": [[19, 20, "DatasetName", "MLM"]]}
{"text": "The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre - training ( BERT uses 80 % , 10 % , 10 % ) .", "entities": [[18, 19, "DatasetName", "MLM"], [23, 24, "MethodName", "BERT"]]}
{"text": "For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .", "entities": [[18, 19, "TaskName", "NER"]]}
{"text": "Language Understanding \" We organize the appendix into three sections : Additional implementation details for BERT are presented in Appendix A ; Additional details for our experiments are presented in Appendix B ; and Additional ablation studies are presented in Appendix C. We present additional ablation studies for BERT including : - 10 % of the time :", "entities": [[15, 16, "MethodName", "BERT"], [48, 49, "MethodName", "BERT"]]}
{"text": "The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token .", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "This is not an insurmountable task , building upon the well established field of named entity recognition .", "entities": [[14, 17, "TaskName", "named entity recognition"]]}
{"text": "We do not perform coreference resolution , working only with direct entity mentions .", "entities": [[4, 6, "TaskName", "coreference resolution"]]}
{"text": "One can see the determination of the main character as a multi - class classification problem .", "entities": [[11, 15, "TaskName", "multi - class classification"]]}
{"text": "Unlike typical multi - class classification problems the set of possible classes varies per section being classified .", "entities": [[2, 6, "TaskName", "multi - class classification"]]}
{"text": "Such averages of word embeddings have been shown to be a useful feature in many tasks ( White et al , 2015 ;", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "Our approach of using a binary classifier to rate each possible class , may seem similar to the one - vs - rest approach for multi - class classification .", "entities": [[25, 29, "TaskName", "multi - class classification"]]}
{"text": "With the classical feature set we use logistic regression , with the features being preprocessed with 0 - 1 scaling .", "entities": [[7, 9, "MethodName", "logistic regression"], [16, 17, "DatasetName", "0"]]}
{"text": "With the word embedding feature set we used a radial bias support vector machine , with standardisation during preprocessing , as has been commonly used with word embeddings on other tasks .", "entities": [[11, 14, "MethodName", "support vector machine"], [26, 28, "TaskName", "word embeddings"]]}
{"text": "The first four books of George R. R. Martin 's \" A Song of Ice and Fire \" series ( hereafter referred to as ASOIAF ) ; The two books of Leigh Bardugo 's \" Six of Crows \" duology ( hereafter referred to as SOC ) ; and the first 9 volumes of Robert Jordan 's \" Wheel of Time \" series ( hereafter referred to as WOT ) .", "entities": [[45, 46, "DatasetName", "SOC"]]}
{"text": "ASOIAF and SOC provide ground truth for the main character in the chapter names .", "entities": [[2, 3, "DatasetName", "SOC"]]}
{"text": "The results of the machine learning method on the ASOIAF and SOC are very strong .", "entities": [[11, 12, "DatasetName", "SOC"]]}
{"text": "Almost all the machine learning models resulted in similarly high accuracy .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "The exception to this is word embedding features based model trained on SOC , which for both ASOIAF and WOT test sets performed much worse .", "entities": [[12, 13, "DatasetName", "SOC"]]}
{"text": "SOC has only 91 chapters to generate its training cases from , and the word embedding feature set has 600 dimensions .", "entities": [[0, 1, "DatasetName", "SOC"]]}
{"text": "However , as this achieves such high accuracy for the other texts , further features would not improve accuracy significantly , without additional more difficult training data ( and may cause over - fitting ) .", "entities": [[7, 8, "MetricName", "accuracy"], [18, 19, "MetricName", "accuracy"]]}
{"text": "Both the classical features and the word embeddings work well .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Though , it seems that the classical feature are more robust ; both with smaller training sets ( like SOC ) , and with more difficult test sets ( like WOT ) .", "entities": [[19, 20, "DatasetName", "SOC"]]}
{"text": "While none of the classifiers are perfect , they achieve high enough accuracy to be useful .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "Evaluation of Scientific Elements for Text Similarity in Biomedical Publications", "entities": [[5, 7, "TaskName", "Text Similarity"]]}
{"text": "We also searched for available tools using these schemes and applied four tools for our particular task of ranking biomedical abstracts based on text similarity .", "entities": [[23, 25, "TaskName", "text similarity"]]}
{"text": "Finding an alternative method to animal experiment requires two tasks : ( a ) performing a text similarity task with respect to some aspects of the publication , and ( b ) precisely understanding the proposed method with respect to the 3R principles .", "entities": [[16, 18, "TaskName", "text similarity"]]}
{"text": "We performed a comparison of the output ( rhetorical elements ) from the tools in the scope of a text similarity task on a manually annotated dataset .", "entities": [[19, 21, "TaskName", "text similarity"]]}
{"text": "In this work , we limited our evaluation for text similarity but did not address whether the proposed methods comply with the 3R principles .", "entities": [[9, 11, "TaskName", "text similarity"]]}
{"text": "In summary , the contributions of this work are the following : ( a ) a short survey on existing schemes and corpora for rhetorical elements in scientific publications ; ( b ) the identification of the schemes for which available tools are readily available for use ; and ( c ) the evaluation of the available tools on a biomedical use case for text similarity .", "entities": [[64, 66, "TaskName", "text similarity"]]}
{"text": "The next section presents a survey on the available schemes , followed by the methodology that we propose to compare the tools in the scope of text similarity .", "entities": [[26, 28, "TaskName", "text similarity"]]}
{"text": "PubMed RCT ( Dernoncourt and Lee , 2017 ) .", "entities": [[0, 2, "DatasetName", "PubMed RCT"]]}
{"text": "It is a collection that includes two corpora of 20 , 000 and 200 , 000 medical abstracts annotated ( Green , 2018 ) bio CL Hybrid Green [ Levels 1 - 3 ] 1 . Causation , 1.1 One Group , 1.1.1 Agreement Argu - ments , 1.1.2 Eliminate Candidates , 1.1.3 Explanation - Based , 1.2 Two Group , 1.2.1 Difference , 1.2.2 Analogy ( Causal ) , 1.2.3 Explanation - Based , 2 . Other , 2.1 Classification , 2.2 Confirma - tion one Table 1 : Summary of the selected schemes and corresponding categories , size of the annotated corpora , and topic of the latter .", "entities": [[80, 81, "TaskName", "Classification"]]}
{"text": "Regarding the topics , \" CL \" stands for computational linguistics , \" bio \" for biomedicine , \" chem \" for chemistry , \" CG \" for Computer Graphics , \" phy \" for Physics , \" eng \" for Engineering , \" LS \" for Life Sciences , and \" CS \" for Computer Science . with five categories .", "entities": [[52, 53, "DatasetName", "CS"]]}
{"text": "However , none of the above data seems to be available but we found one schema with annotated corpus : ScienceIE", "entities": [[20, 21, "DatasetName", "ScienceIE"]]}
{"text": "Prasad et al ( 2011 ) defined eight discourse relations in the Biomedical Discourse Relation Bank ( Bio - DRB ) and annotated 24 articles from the GENIA corpus , which was later used in a couple of works ( Ramesh and Yu , 2010 ; Polepalli Ramesh et al , 2012 ) .", "entities": [[17, 18, "DatasetName", "Bio"], [27, 28, "DatasetName", "GENIA"]]}
{"text": "It is a schema in the form of an ontology of 18 relations for the scientific literature , besides three more general relations .", "entities": [[9, 10, "MethodName", "ontology"]]}
{"text": "al ( 2016 ) created an ontology of entities and relations and annotated 400 abstracts about computational linguistic .", "entities": [[6, 7, "MethodName", "ontology"]]}
{"text": "Section 2 ) for the task of text similarity in the scope of our use case of mining alternative methods for animal experiments .", "entities": [[7, 9, "TaskName", "text similarity"]]}
{"text": "We evaluated the selected schemes and tools for the task of text similarity .", "entities": [[11, 13, "TaskName", "text similarity"]]}
{"text": "We experienced many problems with the Ten - sorFlow library while trying the tool 14 developed by ( Eger et al , 2017 ) for the ScienceIE schema .", "entities": [[26, 27, "DatasetName", "ScienceIE"]]}
{"text": "It addresses the PubMed RCT schema , thus provides predictions for five zoning labels , namely , \" Background \" , \" Objective \" , \" Method \" , \" Results \" and \" Conclusions \" .", "entities": [[3, 5, "DatasetName", "PubMed RCT"]]}
{"text": "We utilized the pre - trained models for Conditional Random Fields ( CRF ) as provided by the tool .", "entities": [[12, 13, "MethodName", "CRF"]]}
{"text": "Given that there is no publication , it is not clear what methods are behind the available models , but probably CRF .", "entities": [[21, 22, "MethodName", "CRF"]]}
{"text": "It provides predictions for five schemes but we considered only the \" Discourse Role Classification ( DRC ) \" whose labels are \" Background \" , \" Challenge \" , \" Approach \" , \" Outcome \" and \" Future Work \" .", "entities": [[14, 15, "TaskName", "Classification"]]}
{"text": "The tool utilizes machine learning algorithms , such as Support Vector Machines ( SVM ) and Decision Trees .", "entities": [[13, 14, "MethodName", "SVM"]]}
{"text": "The entity recognition approach is based on various features and uses the CRF algorithm .", "entities": [[12, 13, "MethodName", "CRF"]]}
{"text": "We evaluated the tools for the task of text similarity .", "entities": [[8, 10, "TaskName", "text similarity"]]}
{"text": "We performed text similarity using the TextFlow tool ( Mrabet et al , 2017 ) and utilized these similarity scores to rank the candidate documents .", "entities": [[2, 4, "TaskName", "text similarity"]]}
{"text": "We compared the tools based on the metrics of P@10 , R@10 and F@10 that assess the performance of the various tools for the ranking task .", "entities": [[11, 12, "MetricName", "R@10"]]}
{"text": "The maximum scores represent the maximum value of P@10 , R@10 and F@10 that could have been obtained by any of the approaches .", "entities": [[10, 11, "MetricName", "R@10"]]}
{"text": "We could not find any difference in the text similarity scores ( as computed by TextFlow ) when considering different order of the same labels in the concatenation of the text .", "entities": [[8, 10, "TaskName", "text similarity"]]}
{"text": "With respect to the methods behind the tools , ArguminSci , which is based on LSTM , performed slightly better than the ones based on CRF ( Achakulvisut et al Prasad and Kan ) and superior than the machine learning algorithms in MAZEA .", "entities": [[15, 16, "MethodName", "LSTM"], [25, 26, "MethodName", "CRF"]]}
{"text": "However , we did not evaluate the predictions made by the tools , but only their impact in a specific text similarity task .", "entities": [[20, 22, "TaskName", "text similarity"]]}
{"text": "We utilized the predictions from these tools for assessing the text similarity between documents and further ranking them in the scope of mining alternative methods to animal testing .", "entities": [[10, 12, "TaskName", "text similarity"]]}
{"text": "For Task 1 our final submission consisted of an ensemble of two different multilingual models , that differ in the way they process the input source ( original sentence ) and hypothesis ( machine translation ) .", "entities": [[33, 35, "TaskName", "machine translation"]]}
{"text": "To this end , we train a cross - lingual transformer ( XLM - RoBERTa ( Conneau et al , 2020 ) )", "entities": [[12, 13, "MethodName", "XLM"], [14, 15, "MethodName", "RoBERTa"]]}
{"text": "For these experiments we build on the OpenKiwi architecture ( Kepler et al , 2019 ) , using a pre - trained xlm - roberta - large encoder as a feature predictor .", "entities": [[22, 23, "MethodName", "xlm"]]}
{"text": "Then , source and hypothesis features are generated using average pooling over the hypothesis embeddings and forwarded to the estimator module which corresponds to a feed - forward layer .", "entities": [[9, 11, "MethodName", "average pooling"]]}
{"text": "We use the mBART ( Liu et al , 2020 ) encoder - decoder architecture to encode the source and force - decode the hypothesis .", "entities": [[3, 4, "MethodName", "mBART"]]}
{"text": "We list the extracted features below : TP sentence average of word translation probability - of MT output generated in different stochastic passes .", "entities": [[11, 13, "TaskName", "word translation"]]}
{"text": "The QE data is relatively limited , making it harder to train multilingual models with a large number of parameters without over - fitting .", "entities": [[17, 20, "HyperparameterName", "number of parameters"]]}
{"text": "Thus , as explained in 3.1 we aimed to investigate whether we could obtain models that generalise better and are more robust to noise and out - of - distribution data by training the XLM - RoBERTa model first on a larger - yet noisier and out - of - domain dataset .", "entities": [[34, 35, "MethodName", "XLM"], [36, 37, "MethodName", "RoBERTa"]]}
{"text": "We also show that using the trained XLM - RoBERTa encoder from the M1 M model can prove beneficial for the predictions on post - edited data of Task 2 ( see Table 3 ) .", "entities": [[7, 8, "MethodName", "XLM"], [9, 10, "MethodName", "RoBERTa"]]}
{"text": "Both variations use a pre - trained XLM - RoBERTa ( large ) encoder to extract features as described for Task 1 , but differ in the training of the encoder .", "entities": [[7, 8, "MethodName", "XLM"], [9, 10, "MethodName", "RoBERTa"]]}
{"text": "In the second variation we swap the original pre - trained model with the XLM - RoBERTa model that has been trained on the Metrics data as described in 3.1.2 .", "entities": [[14, 15, "MethodName", "XLM"], [16, 17, "MethodName", "RoBERTa"]]}
{"text": "Thus , the difference in target score range and distribution could affect the magnitude of predicted scores and the distance to the ground truth values , which is reflected in the MAE and RMSE metrics .", "entities": [[31, 32, "MetricName", "MAE"], [33, 34, "MetricName", "RMSE"]]}
{"text": "Similarly to Task 1 , the primary evaluation metric for the sentence level sub - task of Task 2 is the Pearson r coefficient , 2 : Results for Task 1 with the M2 predictorestimator ( mBART ) and different uncertainty handling additions .", "entities": [[36, 37, "MethodName", "mBART"]]}
{"text": "\" KL \" signifies the incorporation of KL loss , \" G\"the incorporation of glass - box features and MCD the addition of MC dropout .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "In Table 6 is an excerpt of the training configuration used for training the M2 models using the mBART encoder - decoder :", "entities": [[18, 19, "MethodName", "mBART"]]}
{"text": "This work was supported by the P2020 programs MAIA ( contract 045909 ) and Unbabel4EU ( contract 042671 ) , by the European Research Council ( ERC StG Deep - SPIN 758969 ) , and by the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia through contract UIDB/50008/2020 .", "entities": [[30, 31, "MethodName", "SPIN"]]}
{"text": "Structure - Aware Abstractive Conversation Summarization via Discourse and Action Graphs", "entities": [[5, 6, "TaskName", "Summarization"]]}
{"text": "Abstractive conversation summarization has received much attention recently .", "entities": [[2, 3, "TaskName", "summarization"]]}
{"text": "To this end , we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization , by first incorporating discourse relations between utterances and action triples ( \" WHO - DOING - WHAT \" ) in utterances through structured graphs to better encode conversations , and then designing a multi - granularity decoder to generate summaries by combining all levels of information .", "entities": [[20, 21, "TaskName", "summarization"]]}
{"text": "We have publicly released our code at https://github.com/ GT - SALT / Structure - Aware - BART .", "entities": [[16, 17, "MethodName", "BART"]]}
{"text": "As a result , how to organize massive everyday interactions into natural , concise , and informative text , i.e. , abstractive conversation summarization , starts to gain importance .", "entities": [[23, 24, "TaskName", "summarization"]]}
{"text": "Significant progress has been made on abstractive summarization for structured document via pointer generator ( See et al , 2017 ) , reinforcement methods ( Paulus et al , 2018 ; and pre - trained models ( Liu and Lapata , 2019 ; Lewis et al , 2020 ; .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "The annotated summary is Simon was on the phone before , so he did n't here Helen calling .", "entities": [[16, 17, "DatasetName", "Helen"]]}
{"text": "Simon will fetch Helen some tissues .", "entities": [[3, 4, "DatasetName", "Helen"]]}
{"text": "In order to summarize the unstructured and complex conversations , a growing body of research has been conducted , such as transferring document summarization methods to conversation settings ( Shang et al , 2018 ; Gliwa et al , 2019 ) , adopting hierarchical models , or incorporating conversation structures like topic segmentation ( Liu et al , 2019b ; Chen and Yang , 2020 ) , dialogue acts ( Goo and Chen , 2018 ) , and conversation stages ( Chen and Yang , 2020 ) .", "entities": [[22, 24, "TaskName", "document summarization"]]}
{"text": "To this end , we present a structure - aware sequence - to - sequence model , in which we equip abstractive conversation summarization models with rich conversation structures through two types of graphs : discourse relation graph and action graph .", "entities": [[23, 24, "TaskName", "summarization"]]}
{"text": "Explicitly modeling these utterances relations in conversations can aid models in recognizing key content for succinct and informative summarization .", "entities": [[18, 19, "TaskName", "summarization"]]}
{"text": "For instance , in Figure 1 ( b ) , the action graph provides explicit information between Simon , fetch , and tissues for the utterance it is Simon who will fetch the tissues , making models less likely to generate summaries with wrong references ( e.g. , Helen will fetch the tissues ) .", "entities": [[48, 49, "DatasetName", "Helen"]]}
{"text": "To sum up , our contributions are : ( 1 ) We pro - pose to utilize discourse relation graphs and action graphs to better encode conversations for conversation summarization .", "entities": [[29, 30, "TaskName", "summarization"]]}
{"text": "( 3 ) We demonstrate the effectiveness of our proposed methods through experiments on a largescale conversation summarization dataset , SAM - Sum ( Gliwa et al , 2019 ) .", "entities": [[17, 18, "TaskName", "summarization"]]}
{"text": "( 4 ) We further show that our structure - aware models can generalize well in new domains such as debate summarization .", "entities": [[21, 22, "TaskName", "summarization"]]}
{"text": "Document Summarization Compared to extractive document summarization ( Gupta and Lehal , 2010 ; Narayan et al , 2018 ; Liu and Lapata , 2019 ) , abstractive document summarization is generally considered more challenging and has received more attention .", "entities": [[0, 2, "TaskName", "Document Summarization"], [4, 7, "TaskName", "extractive document summarization"], [28, 30, "TaskName", "document summarization"]]}
{"text": "Various methods have been designed to tackle abstractive document summarization like sequence - to - sequence models ( Rush et al , 2015 ) , pointer generators ( See et al , 2017 ) , reinforcement learning methods ( Paulus et al , 2018 ; and pre - trained models ( Lewis et al , 2020 ; .", "entities": [[8, 10, "TaskName", "document summarization"]]}
{"text": "To generate faithful abstractive document summaries ( Maynez et al , 2020 ) , graphbased models were introduced recently such as extracting entity types ( Fernandes et al , 2018 ; , leveraging knowledge graphs Zhu et al , 2020a ) or designing extra fact correction modules .", "entities": [[33, 35, "TaskName", "knowledge graphs"]]}
{"text": "Inspired by these graph - based methods , we also construct action graphs for generating more factual conversation summaries .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Conversation Summarization Extractive dialogue summarization ( Murray et al , 2005 ) has been studied extensively via statistical machine learning methods such as skip - chain CRFs ( Galley , 2006 ) , SVM with LDA models ( Wang and Cardie , 2013 ) , and multi - sentence compression algorithms ( Shang et al , 2018 ) .", "entities": [[1, 2, "TaskName", "Summarization"], [4, 5, "TaskName", "summarization"], [33, 34, "MethodName", "SVM"], [35, 36, "MethodName", "LDA"], [48, 50, "DatasetName", "sentence compression"]]}
{"text": "Abstractive conversation summarization overcomes these issues by designing hierarchical models , incorporating commonsense knowledge ( Feng et al , 2020 ) , or leveraging conversational structures like dialogue acts ( Goo and Chen , 2018 ) , key point sequences ( Liu et al , 2019a ) , topic segments ( Liu et al , 2019b ; and stage developments ( Chen and Yang , 2020 ) .", "entities": [[2, 3, "TaskName", "summarization"]]}
{"text": "Moreover , less attention has been paid to identify the actions of different speakers and how they interact with or refer to each other , leading to unfaithful summarization with incorrect references or wrong reasoning ( Gliwa et al , 2019 ) .", "entities": [[28, 29, "TaskName", "summarization"]]}
{"text": "To fill these gaps , we propose to explicitly model actions within utterances , and relations between utterances in conversations in a structured way , by using discourse relation graphs and action graphs and further combining these through relational graph encoders and multigranularity decoders for abstractive conversation summarization .", "entities": [[47, 48, "TaskName", "summarization"]]}
{"text": "Formally , for a given conversation C = { u 0 , ... , u m } with m utterances , we construct discourse relation graph G D = ( V D , E D ) , where V D is the set of nodes representing Elementary Discourse Units ( EDUs ) , and E D is the adjacent matrix that describes the relations between EDUs , and action graph G", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "Qin et al , 2017 ) , which has been shown effective for dialogue understanding like identifying the decisions in multi - party dialogues ( Bui et al , 2009 ) and detecting salient content in email conversations ( McKeown et al , 2007 ) .", "entities": [[13, 15, "TaskName", "dialogue understanding"]]}
{"text": "As a result , explicitly incorporating the discourse relations will help neural summarization models better encode the unstructured conversations and concentrate on the most salient utterances to generate more informative and less redundant summaries .", "entities": [[12, 13, "TaskName", "summarization"]]}
{"text": "We then utilize this pre - trained parser to predict the discourse relations within conversations in our SAMSum corpus ( Gliwa et al , 2019 ) .", "entities": [[17, 19, "DatasetName", "SAMSum corpus"]]}
{"text": "To this end , we extract \" WHO - DOING - WHAT \" triples from utterances and construct action graphs for conversation summarization Huang et al , 2020b , a ) .", "entities": [[22, 23, "TaskName", "summarization"]]}
{"text": "Then we extract \" WHO - DOING - WHAT \" ( subjectpredicate - object ) triples from transformed conversations using the open information extraction ( Ope - nIE ) systems 1 ( Angeli et al , 2015 ) .", "entities": [[21, 24, "TaskName", "open information extraction"]]}
{"text": "We initialize our utterance encoder F U ( . ) with a pre - trained encoder , i.e. , BART - base ( Lewis et al , 2020 ) , and encode tokens {", "entities": [[19, 20, "MethodName", "BART"]]}
{"text": "x i , 0 , ... , x i , l } in an utterance u i into its hidden representation : { h U i , 0 , ... ,", "entities": [[3, 4, "DatasetName", "0"], [27, 28, "DatasetName", "0"]]}
{"text": "h U i , l } = F U ( { x i , 0 , ... , x i , l } )", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "x i , 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "x i , 0 from the utterance encoder , i.e. ,", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "h U i , 0 , to initialize the i - th", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "Structured Graph Attention Network Based on Graph Attention Network ( Veli\u010dkovi\u0107 et al , 2018 ) , we utilize these relations between nodes to encode each node W , W e and a are trainable parameters .", "entities": [[1, 4, "MethodName", "Graph Attention Network"], [6, 9, "MethodName", "Graph Attention Network"]]}
{"text": "\u03c3 is the activation function ,", "entities": [[3, 5, "HyperparameterName", "activation function"]]}
{"text": "v D i in G D or v A i in G A through : \u03b1 ij = exp \u03c3 a T", "entities": [[15, 16, "HyperparameterName", "\u03b1"]]}
{"text": "i = \u03c3 ( j N i \u03b1 ij", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "and F A ( . , . ) , we then obtain the hidden representations of these nodes as : { h D 0 , ... , h D m } = F D", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "( { v D 0 , ... , v D m } , E D ) ( 2 ) { h A 0", "entities": [[4, 5, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": ", ... , h A n } = F A ( { x A 0 , ... , x A n } , E A ) ( 3 )", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "= Softmax ( W p\u0177 ) ( 5 )", "entities": [[1, 2, "MethodName", "Softmax"]]}
{"text": "To better incorporate the information in constructed graphs , different from the traditional pretrained BART model ( Lewis et al , 2020 ) , we improve the BART transformer decoder with two extra cross attentions ( Discourse Attention and Action Attention ) added to each decoder layer , which attends to the encoded node representations in discourse relation graphs and action graphs .", "entities": [[14, 15, "MethodName", "BART"], [27, 28, "MethodName", "BART"], [28, 30, "MethodName", "transformer decoder"]]}
{"text": "In each decoder layer , after performing the original cross attentions over every token in utterances { h U i , 0 : l } and getting the utterance - attended representation x U , multi - granularity decoder then conducts cross attentions over nodes { h D 0 : m } and { h A 0 : n } that are encoded from graph encoders in parallel , to obtain the discourse - attended representation x D and action - attended representation", "entities": [[21, 22, "DatasetName", "0"], [48, 49, "DatasetName", "0"], [56, 57, "DatasetName", "0"]]}
{"text": "To alleviate the negative impact of randomly initialized graph encoders and cross attentions over graphs on pre - trained BART decoders at early stages and accelerate the learning of newlyintroduced modules during training , we apply ReZero ( Bachlechner et al , 2020 ) to the residual connection after attending to graphs in each decoder layer :", "entities": [[19, 20, "MethodName", "BART"], [36, 37, "MethodName", "ReZero"], [46, 48, "MethodName", "residual connection"]]}
{"text": "We trained and evaluated our models on a conversation summarization dataset SAMSum ( Gliwa et al , 2019 ) covering messenger - like conversations about daily topics , such as arranging meetings and discussing events .", "entities": [[9, 10, "TaskName", "summarization"], [11, 12, "DatasetName", "SAMSum"]]}
{"text": "( Misra et al , 2015 ) , a debate summarization corpus .", "entities": [[10, 11, "TaskName", "summarization"]]}
{"text": "Transformer ( Vaswani et al , 2017 ) :", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "We trained transformer seq2seq models following the OpenNMT ( Klein et al , 2017 ) .", "entities": [[3, 4, "MethodName", "seq2seq"]]}
{"text": "D - HGN ( Feng et al , 2020 ) incorporated commonsense knowledge from ConceptNet ( Liu and Singh , 2004 ) for dialogue summarization .", "entities": [[14, 15, "DatasetName", "ConceptNet"], [24, 25, "TaskName", "summarization"]]}
{"text": "( Dror et al , 2018 ) and found that S - BART w. Discourse & Action significantly outperformed the base BART ( p < 0.05 ) .", "entities": [[12, 13, "MethodName", "BART"], [21, 22, "MethodName", "BART"]]}
{"text": "BART ( Lewis et al , 2020 ) : We utilized BART 2 , and separated utterances by a special token .", "entities": [[0, 1, "MethodName", "BART"], [11, 12, "MethodName", "BART"]]}
{"text": "Multi - View Seq2Seq ( Chen and Yang , 2020 ) utilized topic and stage views on top of BART for summarizing conversations .", "entities": [[3, 4, "MethodName", "Seq2Seq"], [19, 20, "MethodName", "BART"]]}
{"text": "Here we implemented it based on BART - base models .", "entities": [[6, 7, "MethodName", "BART"]]}
{"text": "We used the BART - base model to initialize our sequence - to - sequence model for training in all experiments .", "entities": [[3, 4, "MethodName", "BART"]]}
{"text": "For graph cross attentions added to BART decoder layers , we set the number of attention heads as 2 .", "entities": [[6, 7, "MethodName", "BART"]]}
{"text": "Automatic Evaluation We evaluated all the models with the widely used automatic metric , All model variants of S - BART received significantly higher ratings than BART ( student t - test , p < 0.05 ) .", "entities": [[20, 21, "MethodName", "BART"], [26, 27, "MethodName", "BART"]]}
{"text": "We found that , compared to simple sequence - to - sequence models ( Pointer Generator and Transformer ) , incorporating extra information such as commonsense knowledge from ConceptNet ( D - HGN ) increased the ROUGE metrics .", "entities": [[17, 18, "MethodName", "Transformer"], [28, 29, "DatasetName", "ConceptNet"]]}
{"text": "When equipped with pre - trained models and simple conversation structures such as topics and conversation stages , Multi - View Seq2Seq boosted ROUGE scores .", "entities": [[21, 22, "MethodName", "Seq2Seq"]]}
{"text": "Incorporating discourse relation graphs or action graphs helped the performances of summarization , suggesting the effectiveness of explicitly modeling relations between utterances and the associations between speakers and actions within utterances .", "entities": [[11, 12, "TaskName", "summarization"]]}
{"text": "This indicates that , our structure - aware models with discourse and action graphs could help abstractive conversation summarization , and these two graphs complemented each other in generating better summaries .", "entities": [[18, 19, "TaskName", "summarization"]]}
{"text": "Specifically , we asked annotators from Amazon Mechanical Turk to score a set of randomly sampled 100 generated summaries from ground - truth , BART and our structured models , using a Likert scale from 1 ( worst ) to 5 ( best ) in terms of factualness ( e.g. , associates actions with the right actors ) , succinctness ( e.g. , does not contain redundant information ) , and informativeness ( e.g. , covers the most important content )", "entities": [[24, 25, "MethodName", "BART"]]}
{"text": "As shown in Table 4 , S - BART that utilized structured information from discourse relation graphs and action graphs generated significantly better summaries with respect to factualness , succinctness , and informativeness .", "entities": [[8, 9, "MethodName", "BART"]]}
{"text": "This might because that the incorporation of structured information such as discourse relations helped S - BART to recognize the salient parts in conversations , and thus improve the succinctness and informativeness over BART .", "entities": [[16, 17, "MethodName", "BART"], [33, 34, "MethodName", "BART"]]}
{"text": "Modeling the connections between speakers and actions greatly helped generate more factual summaries than the baselines , e.g. , with an increase of 0.27 from BART to S - BART w. Action .", "entities": [[25, 26, "MethodName", "BART"], [29, 30, "MethodName", "BART"]]}
{"text": "To investigate the generalizability of our structureaware models , we then tested the S - BART model trained on SAMSum corpus directly on the debate summarization domain ( ADSC Corpus ( Misra et al , 2015 ) ) in a zero - shot setting .", "entities": [[15, 16, "MethodName", "BART"], [19, 21, "DatasetName", "SAMSum corpus"], [25, 26, "TaskName", "summarization"]]}
{"text": "As shown in Table 3 , our single graph models S - BART w. Discourse and S - BART w. Action boosted ROUGE scores compared to BART , suggesting that utilizing structures can also increase the generalizability of conversation summarization methods .", "entities": [[12, 13, "MethodName", "BART"], [18, 19, "MethodName", "BART"], [26, 27, "MethodName", "BART"], [39, 40, "TaskName", "summarization"]]}
{"text": "This part conducted ablation studies to show the effectiveness of structured graphs in our S - BART .", "entities": [[16, 17, "MethodName", "BART"]]}
{"text": "The Quality of Discourse Relation Graphs We showed how the quality of discourse relation graphs affected the performances of conversation summarization in Table 5 .", "entities": [[20, 21, "TaskName", "summarization"]]}
{"text": "Specifically , we compared the ROUGE scores of S - BART using our constructed discourse relation graphs ( S - BART w. Discourse Graph ) and S - BART using randomly generated discourse relation graphs S - BART w. Random Graph where both connections between nodes and relation types were randomized .", "entities": [[10, 11, "MethodName", "BART"], [20, 21, "MethodName", "BART"], [28, 29, "MethodName", "BART"], [37, 38, "MethodName", "BART"]]}
{"text": "We found that S - BART with our discourse graphs outperformed 6 .", "entities": [[5, 6, "MethodName", "BART"]]}
{"text": "We found that the parallel strategy showed better performances and the sequential ones did not introduce gains compared to S - BART with single graphs .", "entities": [[21, 22, "MethodName", "BART"]]}
{"text": "This demonstrates that discourse relation graphs and action graphs were both important and provided different signals for abstractive conversation summarization .", "entities": [[19, 20, "TaskName", "summarization"]]}
{"text": "( ii ) Compared to discourse graphs , action graphs received higher \u03b1 weights after training in both initializing settings , suggesting that the information from structured action graphs might be harder for the end - to - end BART models to capture .", "entities": [[12, 13, "HyperparameterName", "\u03b1"], [39, 40, "MethodName", "BART"]]}
{"text": "( iii ) Utilizing both graphs spontaneously led to higher ReZero weights , further validating the effectiveness of combining discourse relation graphs and action graphs and their complementary properties .", "entities": [[10, 11, "MethodName", "ReZero"]]}
{"text": "To inspect when our summarization models could help the conversations summarization , we visualized the average number of discourse edges and the average number of action triples in three sets of conversations in examples where both S - BART and BART showed low ROUGE scores ( ROUGE - 1 < 20.0 , ROUGE -", "entities": [[4, 5, "TaskName", "summarization"], [10, 11, "TaskName", "summarization"], [38, 39, "MethodName", "BART"], [40, 41, "MethodName", "BART"]]}
{"text": "When the structures in conversations were simpler ( fewer discourse edges and fewer action triples than the average ) , BART showed similar performance as S - BART .", "entities": [[20, 21, "MethodName", "BART"], [27, 28, "MethodName", "BART"]]}
{"text": "As the structures of conversations become more complex with more discourse relations and more action mentions , S - BART outperformed BART as it explicitly incorporated these structured graphs .", "entities": [[19, 20, "MethodName", "BART"], [21, 22, "MethodName", "BART"]]}
{"text": "However , both BART and S - BART struggled when there were much more interactions beyond certain thresholds , calling for better mechanisms to model structures in conversations for generating better summaries .", "entities": [[3, 4, "MethodName", "BART"], [7, 8, "MethodName", "BART"]]}
{"text": "In this work , we introduced a structure - aware sequence - to - sequence model for abstractive conversation summarization by incorporating discourse relations between utterances , and the connections between speakers and actions within utterances .", "entities": [[19, 20, "TaskName", "summarization"]]}
{"text": "Experiments and ablation studies on SAMSum corpus showed the effectiveness of these structured graphs in aiding the task of conversation summarization via both quantitative and qualitative eval - uation metrics .", "entities": [[5, 7, "DatasetName", "SAMSum corpus"], [20, 21, "TaskName", "summarization"]]}
{"text": "In the future , we plan to extend our current conversation summarization models for various application domains such as emails , debates , and podcasts , and in conversations that might involve longer utterances and more participants in an unsynchronized way .", "entities": [[11, 12, "TaskName", "summarization"]]}
{"text": "( Asher et al , 2016 ) with default settings 5 to get the link prediction and relation classification models to label discourse relations in SAMSum and ADSC corpus .", "entities": [[14, 16, "TaskName", "link prediction"], [17, 19, "TaskName", "relation classification"], [25, 26, "DatasetName", "SAMSum"]]}
{"text": "S - BART with 1 as the initialized ReZero weight outperformed", "entities": [[2, 3, "MethodName", "BART"], [8, 9, "MethodName", "ReZero"]]}
{"text": "This work is supported in part by grants from Google , Amazon and Salesforce .", "entities": [[9, 10, "DatasetName", "Google"]]}
{"text": "SeqMix : Augmenting Active Sequence Labeling via Sequence Mixup", "entities": [[8, 9, "MethodName", "Mixup"]]}
{"text": "Active learning is an important technique for low - resource sequence labeling tasks .", "entities": [[0, 2, "TaskName", "Active learning"]]}
{"text": "We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling .", "entities": [[6, 8, "TaskName", "data augmentation"]]}
{"text": "In SeqMix , we address this challenge by performing mixup for both sequences and token - level labels of the queried samples .", "entities": [[9, 10, "MethodName", "mixup"]]}
{"text": "Furthermore , we design a discriminator during sequence mixup , which judges whether the generated sequences are plausible or not .", "entities": [[8, 9, "MethodName", "mixup"]]}
{"text": "Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27 % - 3.75 % in terms of F 1 scores .", "entities": [[3, 6, "TaskName", "Named Entity Recognition"], [7, 9, "TaskName", "Event Detection"]]}
{"text": "Many NLP tasks can be formulated as sequence labeling problems , such as part - of - speech ( POS ) tagging ( Zheng et al , 2013 ) , named entity recognition ( NER ) ( Lample et al , 2016 ) , and event extraction ( Yang et al , 2019 ) .", "entities": [[13, 16, "DatasetName", "part - of"], [30, 33, "TaskName", "named entity recognition"], [34, 35, "TaskName", "NER"], [45, 47, "TaskName", "event extraction"]]}
{"text": "Active learning is an important technique for sequence labeling in low - resource settings .", "entities": [[0, 2, "TaskName", "Active learning"]]}
{"text": "We study the problem of enhancing active sequence labeling via data augmentation .", "entities": [[10, 12, "TaskName", "data augmentation"]]}
{"text": "However , data augmentation for active sequence labeling is challenging , because we need to generate sentences and token - level labels jointly .", "entities": [[2, 4, "TaskName", "data augmentation"]]}
{"text": "It is also infeasible to apply heuristic data augmentation methods such as context - based words substitution ( Kobayashi , 2018 ) , synonym replacement , random insertion , swap , and deletion ( Wei and Zou , 2019 ) ,", "entities": [[7, 9, "TaskName", "data augmentation"]]}
{"text": "We propose SeqMix , a data augmentation method for generating sub - sequences along with their labels based on mixup ( Zhang et al , 2018 ) .", "entities": [[5, 7, "TaskName", "data augmentation"], [19, 20, "MethodName", "mixup"]]}
{"text": "The discriminator is designed to compute the perplexity scores for all the generated candidate sequences and select the low - perplexity sequences as plausible ones .", "entities": [[7, 8, "MetricName", "perplexity"], [20, 21, "MetricName", "perplexity"]]}
{"text": "We show that SeqMix consistently outperforms standard active sequence labeling baselines under different data usage percentiles with experiments on Named Entity Recognition and Event Detection tasks .", "entities": [[19, 22, "TaskName", "Named Entity Recognition"], [23, 25, "TaskName", "Event Detection"]]}
{"text": "The advantage of SeqMix is especially prominent in low - resource scenarios , achieving 12.06 % , 8.86 % , 16.49 % F 1 improvements to the original active learning approach on the above three datasets .", "entities": [[28, 30, "TaskName", "active learning"]]}
{"text": "Our results also verify the proposed mixup strategies and the discriminator are vital to the performance of SeqMix .", "entities": [[6, 7, "MethodName", "mixup"]]}
{"text": "For example , in the named entity recognition task , we can adopt the BIO ( Beginning , Inside , Outside ) tagging scheme ( M\u00e0rquez et al , 2005 ) to assign labels for each token : the first token of an entity mention with type X is labeled as B - X , the tokens inside that mention are labeled as I - X and the non - entity tokens are labeled as O. Consider a large unlabeled corpus U , traditional active learning starts from a small annotated seed set L , and utilizes a query function \u03c8", "entities": [[5, 8, "TaskName", "named entity recognition"], [84, 86, "TaskName", "active learning"]]}
{"text": "( , K , \u03b3 ( ) ) in each iteration , with the hope of maximally improving model performance with a fixed labeled budget .", "entities": [[4, 5, "HyperparameterName", "\u03b3"]]}
{"text": "With the input sequence x of length T , we denote the model output as f ( | x ; \u03b8 ) .", "entities": [[20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "Our method is generic to any query policies \u03b3 ( ) .", "entities": [[8, 9, "HyperparameterName", "\u03b3"]]}
{"text": "= softmax ( f ( y t | x ; \u03b8 ) ) .", "entities": [[1, 2, "MethodName", "softmax"], [10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "Given a committee consist of C models , the vote entropy for input x is : \u03b3 TE", "entities": [[16, 17, "HyperparameterName", "\u03b3"]]}
{"text": "\u03b3 VE ( x )", "entities": [[0, 1, "HyperparameterName", "\u03b3"]]}
{"text": "To ensure the semantic quality of the generated sequences , we use a discriminator After that , the iterative active learning procedure begins .", "entities": [[19, 21, "TaskName", "active learning"]]}
{"text": "( U , K , \u03b3 ( ) ) .", "entities": [[5, 6, "HyperparameterName", "\u03b3"]]}
{"text": "SeqMix ( X , Y , \u03b1 , \u03b6 ( ) , d ( ) ) and expand the training set as L = L \u222a L * .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "Then we train the model \u03b8 on the newly augmented set L.", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "The iterative active learning procedure terminates when a fixed number of iterations are reached .", "entities": [[2, 4, "TaskName", "active learning"], [9, 12, "HyperparameterName", "number of iterations"]]}
{"text": "Mixup ( Zhang et al , 2018 ) is a data augmentation method that implements linear interpolation in the input space .", "entities": [[0, 1, "MethodName", "Mixup"], [10, 12, "TaskName", "data augmentation"]]}
{"text": "x i , x j along // active learning iterations with augmentation for round in active learning rounds do X = \u03c8", "entities": [[7, 9, "TaskName", "active learning"], [15, 17, "TaskName", "active learning"]]}
{"text": "( U , K , \u03b3 ( ) )", "entities": [[5, 6, "HyperparameterName", "\u03b3"]]}
{"text": "* = SeqMix ( X , Y , \u03b1 , \u03b6 ( ) , d ( ) )", "entities": [[8, 9, "HyperparameterName", "\u03b1"]]}
{"text": "L = L \u222a X , Y \u222a L * \u03b8 = train ( \u03b8 , L ) end Output : The sequence model trained with active data augmentation : \u03b8 with the labels", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [14, 15, "HyperparameterName", "\u03b8"], [27, 29, "TaskName", "data augmentation"], [30, 31, "HyperparameterName", "\u03b8"]]}
{"text": "Through linear combinations on the input level of paired examples and their labels , Mixup regularizes the model to present linear behavior among the training data .", "entities": [[14, 15, "MethodName", "Mixup"]]}
{"text": "Mixup is not directly applicable to generate interpolated samples for text data , because the input space is discrete .", "entities": [[0, 1, "MethodName", "Mixup"]]}
{"text": "Along with the above sequence mixup procedure , we also introduce a pairing strategy that selects sequences for mixup .", "entities": [[5, 6, "MethodName", "mixup"], [18, 19, "MethodName", "mixup"]]}
{"text": "For example , in the NER and event detection tasks , the \" O \" label is dominant in the corpus , which do not refer to any entities or events of interest .", "entities": [[5, 6, "TaskName", "NER"], [7, 9, "TaskName", "event detection"]]}
{"text": "We thus define the labels of interest as valid labels , e.g. , the non - \" O \" labels in NER and event detection , and design a sequence pairing function to select more informative parent sequences for mixup .", "entities": [[21, 22, "TaskName", "NER"], [23, 25, "TaskName", "event detection"], [39, 40, "MethodName", "mixup"]]}
{"text": "We set a threshold \u03b7 0 for \u03b6 ( ) , and the sequence will be considered as an eligible candidate for mixup only when \u03b7 \u2265 \u03b7 0 .", "entities": [[5, 6, "DatasetName", "0"], [22, 23, "MethodName", "mixup"], [28, 29, "DatasetName", "0"]]}
{"text": "Based on the above token - level mixup procedure and the sequence pairing function , we propose three different strategies for generating interpolated labeled sequences .", "entities": [[7, 8, "MethodName", "mixup"]]}
{"text": "These strategies are shown in Figure 1 and described below : Whole - sequence mixup As the name suggests , whole - sequence mixup ( Figure 1 ( a ) ) performs sequence mixing at the whole - sequence level .", "entities": [[14, 15, "MethodName", "mixup"], [23, 24, "MethodName", "mixup"]]}
{"text": "Besides , the paring function \u03b6 ( ) requires that both the two sequences satisfy \u03b7 \u2265 \u03b7 0 .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "Then we perform mixup at all token positions , by employing Equation 7to generate mixed tokens and Equation 8 to generate mixed labels ( note that the mixed labels are soft labels ) .", "entities": [[3, 4, "MethodName", "mixup"]]}
{"text": "Sub - sequence mixup One drawback of the whole - sequence mixup is that it indiscriminately mixes over all tokens , which may include incompatible subsequences and generate implausible sequences .", "entities": [[3, 4, "MethodName", "mixup"], [11, 12, "MethodName", "mixup"]]}
{"text": "To tackle this , we consider sub - sequence mixup ( Figure 1 x", "entities": [[9, 10, "MethodName", "mixup"]]}
{"text": "i , x j , y j ) then \u03bb \u223c Beta ( \u03b1 , \u03b1 ) //", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [15, 16, "HyperparameterName", "\u03b1"]]}
{"text": "mixup the target sub - sequences for t = 1 , , T do Calculate e t by Eq .", "entities": [[0, 1, "MethodName", "mixup"]]}
{"text": "y k = y k \u2212 y ksub", "entities": [[1, 3, "HyperparameterName", "k ="]]}
{"text": "If x isub X isub , x jsub X jsub , such that their \u03b7 \u2265 \u03b7 0 , we have \u03b6 ( x i , y", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "In this way , sub - sequence mixup is expected to keep the syntax structure of the original sequence , while providing data diversity .", "entities": [[7, 8, "MethodName", "mixup"]]}
{"text": "Label - constrained sub - sequence mixup can be considered as a special case of sub - sequence mixup , where the constraints inherit sub - sequence mixup , and further require that the sub - sequence labels are consistent .", "entities": [[6, 7, "MethodName", "mixup"], [18, 19, "MethodName", "mixup"], [27, 28, "MethodName", "mixup"]]}
{"text": "Mixup in the Embedding Space Mixup in the Label Space ( c ) Label - constrained sub - sequence mixup Figure 1 : Illustration of the three variants of SeqMix .", "entities": [[0, 1, "MethodName", "Mixup"], [5, 6, "MethodName", "Mixup"], [19, 20, "MethodName", "mixup"]]}
{"text": "We use s = 5 , \u03b7 0 = 3 5 for whole - sequence mixup and s = 3 , \u03b7 0 = 2 3 for sub - sequence mixup and label - constrained sub - sequence mixup .", "entities": [[7, 8, "DatasetName", "0"], [15, 16, "MethodName", "mixup"], [22, 23, "DatasetName", "0"], [30, 31, "MethodName", "mixup"], [38, 39, "MethodName", "mixup"]]}
{"text": "For the mixup in the embedding space , we take the embedding in E which is closest to the raw mixed embedding as the generated embedding .", "entities": [[2, 3, "MethodName", "mixup"]]}
{"text": "For the mixup in the label space , the mixed label can be used as the pseudo label .", "entities": [[2, 3, "MethodName", "mixup"]]}
{"text": "version is called label - constrained sub - sequence mixup .", "entities": [[9, 10, "MethodName", "mixup"]]}
{"text": "Comparing the three variants , label - constrained sub - sequence mixup gives the most restrictions to pairing parent samples , sub - sequence mixup sets the sub - sequence - level pattern , while wholesequence mixup just requires \u03b7 \u2265 \u03b7 0 for the sequences with the same length .", "entities": [[11, 12, "MethodName", "mixup"], [24, 25, "MethodName", "mixup"], [36, 37, "MethodName", "mixup"], [42, 43, "DatasetName", "0"]]}
{"text": "During sequence mixup , the mixing coefficient \u03bb determines the strength of interpolation .", "entities": [[2, 3, "MethodName", "mixup"]]}
{"text": "When \u03bb approximates 0 or 1 , the generated sequence will be similar to one of the parent sequences , while the \u03bb around 0.5 produces relatively diverse generation .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "To maintain the quality of mixed sequences , we set a discriminator to score the perplexity of the sequences .", "entities": [[15, 16, "MetricName", "perplexity"]]}
{"text": "Hence , a blind low perplexity setting is undesirable .", "entities": [[5, 6, "MetricName", "perplexity"]]}
{"text": "The overall sequence mixup and selection procedure is illustrated in Algorithm 2 .", "entities": [[3, 4, "MethodName", "mixup"]]}
{"text": "We conduct experiments on three sequence labeling datasets for the named entity recognition ( NER ) and event detection tasks .", "entities": [[10, 13, "TaskName", "named entity recognition"], [14, 15, "TaskName", "NER"], [17, 19, "TaskName", "event detection"]]}
{"text": "( 1 ) CoNLL - 03 ( Tjong Kim Sang and De Meulder , 2003 ) is a corpus for NER task .", "entities": [[20, 21, "TaskName", "NER"]]}
{"text": "2 ( 2 ) ACE05 is a corpus for event detection .", "entities": [[9, 11, "TaskName", "event detection"]]}
{"text": "( 3 ) Webpage ( Ratinov and Roth , 2009 ) is a NER corpus with 20 webpages related to computer science conference and academic websites .", "entities": [[13, 14, "TaskName", "NER"]]}
{"text": "The sequence model is initialed on a small seed set , then it performs five iterates of active learning .", "entities": [[17, 19, "TaskName", "active learning"]]}
{"text": "For the query policy , we use random sampling and the three active learning policies mentioned in Section 2.2 .", "entities": [[12, 14, "TaskName", "active learning"]]}
{"text": "We use BERT - base - cased for the NER task as the underlying model , and BERT - basemultilingual - cased for the event trigger detection task .", "entities": [[2, 3, "MethodName", "BERT"], [9, 10, "TaskName", "NER"], [17, 18, "MethodName", "BERT"]]}
{"text": "We use the sub - sequence window length s = { 5 , 5 , 4 } , the valid label density \u03b7 0 = { 0.6 , 0.2 , 0.5 } for CoNLL - 03 , ACE05 and Webpage , respectively .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "The augment rate is set as 0.2 , and the discriminator score range is set as ( 0 , 500 ) .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "The main results are presented in Figure 2 , where we use NTE sampling as the default active learning policy .", "entities": [[17, 19, "TaskName", "active learning"]]}
{"text": "The best SeqMix method ( sub - sequence mixup with NTE sampling ) outperforms the strongest active learning baselines by 2.95 % on CoNLL - 03 , 2.27 % on ACE05 and 3.75 % on WebPage in terms of F 1 score on average .", "entities": [[8, 9, "MethodName", "mixup"], [16, 18, "TaskName", "active learning"]]}
{"text": "Among all the three SeqMix variants , subsequence mixup gives the overall best performance ( label - constrained sub - sequence mixup achieves very close performance with sub - sequence mixup on ACE05 dataset ) , but whole - sequence mixup does not yield a consistent improvement to the original active learning method .", "entities": [[8, 9, "MethodName", "mixup"], [21, 22, "MethodName", "mixup"], [30, 31, "MethodName", "mixup"], [40, 41, "MethodName", "mixup"], [50, 52, "TaskName", "active learning"]]}
{"text": "This is because the whole - sequence mixup may generate semantically poor new sequences .", "entities": [[7, 8, "MethodName", "mixup"]]}
{"text": "To justify that SeqMix can provide improvement to the active learning framework with various query policies , we employ different query policies with SeqMix augmentation under the same experiment setting as Figure 2 ( a ) .", "entities": [[9, 11, "TaskName", "active learning"]]}
{"text": "We use subsequence mixup with NTE sampling as the backbone and change the perplexity score range of the discriminator .", "entities": [[3, 4, "MethodName", "mixup"], [13, 14, "MetricName", "perplexity"]]}
{"text": "The result in Table 1 demonstrates the discriminator provides a stable improvement for the last four data usage percentiles , and the discriminator with score range ( 0 , 500 ) can boost the model by 1.07 % F 1 score , averaged by all the data usage percentiles .", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "( U , K , \u03b3 ( ) )", "entities": [[5, 6, "HyperparameterName", "\u03b3"]]}
{"text": "| in { 0 .2 , 0.4 , 0.6 , 0.8 , 1.0 } and keep the number of initial data usage same to investigate the effect of augment rate for data augmentation .", "entities": [[3, 4, "DatasetName", "0"], [31, 33, "TaskName", "data augmentation"]]}
{"text": "However , the performance variance based on the augment rate is not prominent compared to the improvement provided by SeqMix to the active learning framework .", "entities": [[22, 24, "TaskName", "active learning"]]}
{"text": "Valid tag density \u03b7 0 .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "We search the valid tag density \u03b7 0 as Section 3.2 defined by varying the sub - sequence window length s and the required number of valid tag n within the window .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "When s is too large , sub - sequence mixup tends to behave like wholesequence mixup , where the too long sub - sequence generation can hardly maintain the rationality of syntax and semantics as before .", "entities": [[9, 10, "MethodName", "mixup"], [15, 16, "MethodName", "mixup"]]}
{"text": "The high \u03b7 0 with long window length may result in an insufficient amount of eligible parent sequences .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "Mixing parameter \u03b1 .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "The parameter \u03b1 decides the distribution \u03bb \u223c Beta ( \u03b1 , \u03b1 ) , and the coefficient \u03bb directly involved the mixing of tokens and labels .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b1"], [12, 13, "HyperparameterName", "\u03b1"]]}
{"text": "Figure 5 presents a generation example via subsequence mixup .", "entities": [[8, 9, "MethodName", "mixup"]]}
{"text": "For the convenience of presentation , we set the length of sub - sequence s = 3 and the valid label density threshold \u03b7 0 = 2 3 .", "entities": [[24, 25, "DatasetName", "0"]]}
{"text": "which is sampled from Beta ( \u03b1 , \u03b1 ) .", "entities": [[6, 7, "HyperparameterName", "\u03b1"], [8, 9, "HyperparameterName", "\u03b1"]]}
{"text": "Ren et al , 2020 ; Zhang et al , 2019 ; Yu et al , 2020 ) and active learning ( Shen et al , 2017 ; Hazra et al , 2019 ; Liu et al , 2018 ; Fang et al , 2017 ; Gao et al , 2019 ) .", "entities": [[19, 21, "TaskName", "active learning"]]}
{"text": "In this study , we mainly focus on active learning approaches which select samples based on the query policy design .", "entities": [[8, 10, "TaskName", "active learning"]]}
{"text": "More recently , Shen et al ( 2017 ) ; Hazra et al ( 2019 ) ; Liu et al ( 2018 ) ; Fang et al ( 2017 ) further improve the aforementioned active learning approaches to improve the sampling diversity as well as the generalization ability of models on low - resource scenarios .", "entities": [[34, 36, "TaskName", "active learning"]]}
{"text": "These works mainly claim the sample efficiency provided by the active learning approach but do not study data augmentation for active sequence labeling . Interpolation - based Regularizations Mixup implements interpolation in the input space to regularize models ( Zhang et al , 2018 ) .", "entities": [[10, 12, "TaskName", "active learning"], [16, 18, "DatasetName", "study data"], [28, 29, "MethodName", "Mixup"]]}
{"text": "Recently , the Mixup variants ( Verma et al , 2019 ; Summers and Dinneen , 2019 ; Guo et al , 2019b ) turn to perform interpolation in the hidden space to capture higher - level information .", "entities": [[3, 4, "MethodName", "Mixup"]]}
{"text": "Guo et al ( 2019a ) ; Chen et al ( 2020a ) apply hidden - space Mixup for text classification .", "entities": [[17, 18, "MethodName", "Mixup"], [19, 21, "TaskName", "text classification"]]}
{"text": "These works , however , have not explored how to perform mixup for sequences with token - level labels , nor do they consider the quality of the mixed - up samples .", "entities": [[11, 12, "MethodName", "mixup"]]}
{"text": "Text Augmentation Our work is also related to text data augmentation .", "entities": [[9, 11, "TaskName", "data augmentation"]]}
{"text": "Although these methods can augment the training set and improve the performance of text classification model , they fail to generate sequences and labels simultaneously , thus can not be adapted to our problem where tokenlevel labels are required during training .", "entities": [[13, 15, "TaskName", "text classification"]]}
{"text": "Instead , in our study , we propose a new framework SeqMix for data augmentation to facilitate sequence labeling task .", "entities": [[13, 15, "TaskName", "data augmentation"]]}
{"text": "Moreover , it can be naturally combined with existing active learning approaches and further promote the performance .", "entities": [[9, 11, "TaskName", "active learning"]]}
{"text": "We propose a simple data augmentation method SeqMix to enhance active sequence labeling .", "entities": [[4, 6, "TaskName", "data augmentation"]]}
{"text": "By performing sequence mixup in the latent space , Se - qMix improves data diversity during active learning , while being able to generate plausible augmented sequences .", "entities": [[3, 4, "MethodName", "mixup"], [16, 18, "TaskName", "active learning"]]}
{"text": "This method is generic to different active learning policies and various sequence labeling tasks .", "entities": [[6, 8, "TaskName", "active learning"]]}
{"text": "Our experiments demonstrate that SeqMix can improve active learning baselines consistently for NER and event detection tasks ; and its benefits are especially prominent in low - data regimes .", "entities": [[7, 9, "TaskName", "active learning"], [12, 13, "TaskName", "NER"], [14, 16, "TaskName", "event detection"]]}
{"text": "For future research , it is interesting to enhance SeqMix with language models during the mixup process , and harness external knowledge for further improving diversity and plausibility .", "entities": [[15, 16, "MethodName", "mixup"]]}
{"text": "CoNLL - 03 : https://github.com/ synalp / NER / tree / master / corpus/ CoNLL - 2003 .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "For the active learning paradigm , we split the training set as Table 3 .", "entities": [[2, 4, "TaskName", "active learning"]]}
{"text": "The active learners are initialized on the seed set , then they implement 5 active learning rounds .", "entities": [[14, 16, "TaskName", "active learning"]]}
{"text": "For the baselines , we take random sampling and 3 active learning approaches - LC sampling , NTE sampling , and QBC sampling as Section 2.2 .", "entities": [[10, 12, "TaskName", "active learning"]]}
{"text": "We implement bert - base - cased as the underlying model for the NER task and bert - base - multilingualcased as the underlying model for the event detection task .", "entities": [[13, 14, "TaskName", "NER"], [27, 29, "TaskName", "event detection"]]}
{"text": "We use the model from Huggingface Transformer codebase 3 , and the repository 4 to finetune our model for sequence labeling task .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "3 https://github.com/huggingface/ transformers 4 https://github.com/kamalkraj/BERT - NER", "entities": [[6, 7, "TaskName", "NER"]]}
{"text": "In Section 3.2 , we construct a table of tokens W and their corresponding contextual embedding E. For our underlying BERT model , we use the vocabulary provided by the tokenizer to build up W , and the embedding initialized on the training set as E. We also need to construct a special token collection to exclude some generation in the process of sequence mixing .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "For example , BERT places token [ CLS ] and [ SEP ] at the starting position and the ending position for sentence , and pad the inputs with [ PAD ] .", "entities": [[3, 4, "MethodName", "BERT"], [30, 31, "DatasetName", "PAD"]]}
{"text": "( 2 ) The sub - sequence window length s and the valid label density threshold \u03b7 0 vary from the datasets .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "For CoNLL - 03 , s = 5 , \u03b7 0 = 0.6 ; for ACE05 , s = 5 , \u03b7 0", "entities": [[10, 11, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "= 0.2 ; for Web - Page , s = 4 , \u03b7 0 = 0.5 .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "( 4 ) The discriminator score range is set as ( 0 , 500 ) for all the datasets .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "A named entity is correct only if it is an exact match of the corresponding entity in the data file .", "entities": [[10, 12, "MetricName", "exact match"]]}
{"text": "Sub - sequence mixup is trained with ( 1+\u03b1 ) times data , where the \u03b1 denotes the augment rate .", "entities": [[3, 4, "MethodName", "mixup"], [15, 16, "HyperparameterName", "\u03b1"]]}
{"text": "For the 5 - round active learning with SeqMix augmentation , our program runs about 500 seconds for WebPage dataset , 1700 seconds for the CoNLL slicing dataset , and 3.5 hours for ACE 2005 .", "entities": [[5, 7, "TaskName", "active learning"], [33, 35, "DatasetName", "ACE 2005"]]}
{"text": "For the discriminator score range , we first examine the perplexity score distribution of the CoNLL training set .", "entities": [[10, 11, "MetricName", "perplexity"]]}
{"text": "Then determine an approximate score range ( 0 , 2000 ) first .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "The result shows different \u03b1 did not influence the augmentation performance much .", "entities": [[4, 5, "HyperparameterName", "\u03b1"]]}
{"text": "Prior work in event detection ( Ritter et al , 2012 ) has demonstrated that simple linguistic indicators ( phrases or n - grams ) can be useful in the accurate discovery of events in social media .", "entities": [[3, 5, "TaskName", "event detection"]]}
{"text": "Previous works on complaint identification have applied text mining with LDA and sentiment analysis on user - generated content Duan et al , 2013 ) .", "entities": [[10, 11, "MethodName", "LDA"], [12, 14, "TaskName", "sentiment analysis"]]}
{"text": "The first group of features are based on simple semantic properties such as n - grams , word embeddings , and part of speech tags .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "Likewise , we also experiment with two sentiment lexicons : MPQA ( Wilson et al , 2005 ) , NRC ( Mohammad et al , 2013 ) for assigning sentiment scores to tweets .", "entities": [[10, 11, "DatasetName", "MPQA"]]}
{"text": "We trained a logistic regression model for complaint detection using each one of the features described in section 4.1 .", "entities": [[3, 5, "MethodName", "logistic regression"]]}
{"text": "Our proposed model , C - BiGRU , combines a Convolutional Neural Network ( CNN ) with a bidirectional Recurrent Neural Network ( RNN ) .", "entities": [[6, 7, "MethodName", "BiGRU"]]}
{"text": "The ever - increasing amount of user - generated data introduces new challenges in terms of automatic content moderation , especially regarding hate speech and offensive language detection .", "entities": [[22, 27, "DatasetName", "hate speech and offensive language"]]}
{"text": "Several recent workshops and conferences such as TRAC ( Kumar et al , 2018 ) , ALW2 ( Fi\u0161er et al , 2018 ) , and GermEval ( Wiegand et al , 2018 ) show the growing importance of this subject .", "entities": [[9, 10, "DatasetName", "Kumar"]]}
{"text": "The SemEval 2019 shared task 6 ( Zampieri et al , 2019b ) further addresses this topic by introducing the Offensive Language Identification Dataset ( OLID ) , which consists of tweets , labeled with a three - level annotation model ( Zampieri et al , 2019a ) .", "entities": [[21, 23, "TaskName", "Language Identification"], [25, 26, "DatasetName", "OLID"]]}
{"text": "In section 4 we report the results of our experiments on the OLID dataset and the additionally used GermEval dataset .", "entities": [[12, 13, "DatasetName", "OLID"]]}
{"text": "Several methods and models have been presented in literature over the last decade to address the predicament of identifying hate speech , offensive language , and online aggressiveness .", "entities": [[19, 21, "DatasetName", "hate speech"]]}
{"text": "Their proposed algorithm uses unigram , bigram , and trigram tokens as features , weighted by the respective TF - IDF , as well as Part - of - Speech ( POS ) tagging and different metrics to determine the readability and sentiment of a tweet .", "entities": [[25, 28, "DatasetName", "Part - of"]]}
{"text": "Logisticregression and linear SVM result in the best performance for a wide range of assessed classifiers .", "entities": [[3, 4, "MethodName", "SVM"]]}
{"text": "Various approaches utilized deep learning models for text categorization . proposed a character - level convolutional network for text classification on large - scale datasets .", "entities": [[7, 9, "TaskName", "text categorization"], [18, 20, "TaskName", "text classification"]]}
{"text": "Gamb\u00e4ck and Sikdar ( 2017 ) further experimented with convolutional networks in the context of online hate speech classification .", "entities": [[16, 18, "DatasetName", "hate speech"]]}
{"text": "Zhang et al ( 2018 ) suggest an architecture similar to our network , where a convolutional filter extracts features from pretrained word embeddings .", "entities": [[22, 24, "TaskName", "word embeddings"]]}
{"text": "After max pooling , the feature maps are processed using a unidirectional GRU .", "entities": [[1, 3, "MethodName", "max pooling"], [12, 13, "MethodName", "GRU"]]}
{"text": "Their model is compared to a bag - of - n - gram model on various multi - class hate speech datasets and shows promising results .", "entities": [[19, 21, "DatasetName", "hate speech"]]}
{"text": "The OLID dataset contains 13 , 240 tweets , with 4 , 400 offensive and 8 , 840 non - offensive tweets ( 66.77 % offensive , 33.23 % non - offensive ) .", "entities": [[1, 2, "DatasetName", "OLID"]]}
{"text": "To compensate for the imbalanced class distributions and weigh each class equally , we choose the macro averaged F1 - score of both classes as our main evaluation metric .", "entities": [[18, 21, "MetricName", "F1 - score"]]}
{"text": "We compare the performance of several classifiers , namely multinomial Naive Bayes ( NB ) , SVM , Decision Tree ( DT ) , and Logistic Regression ( LogR ) and conduct a grid search to optimize our hyper - parameters .", "entities": [[16, 17, "MethodName", "SVM"], [25, 27, "MethodName", "Logistic Regression"]]}
{"text": "We further choose ReLu as activation function .", "entities": [[3, 4, "MethodName", "ReLu"], [5, 7, "HyperparameterName", "activation function"]]}
{"text": "Gated Recurrent Units ( GRU ) as initially proposed by are used in RNNs to capture long - term dependencies of input sequences .", "entities": [[4, 5, "MethodName", "GRU"]]}
{"text": "Similar to Long Short - Term Memory ( LSTM ) units ( Hochreiter and Schmidhuber , 1997 )", "entities": [[2, 7, "MethodName", "Long Short - Term Memory"], [8, 9, "MethodName", "LSTM"]]}
{"text": "GRU are able to overcome the vanishing gradient problem by using a gating mechanism .", "entities": [[0, 1, "MethodName", "GRU"]]}
{"text": "GRU have shown to achieve comparable results to LSTM in sequence modeling tasks and are able to outperform the latter on smaller data sets ( Chung et al , 2014 ) .", "entities": [[0, 1, "MethodName", "GRU"], [8, 9, "MethodName", "LSTM"]]}
{"text": "The recurrent layer in our model consists of a bidirectional GRU , where the concatenated feature maps , which resulted from the convolutional layer , are used as input for the GRU layer .", "entities": [[9, 11, "MethodName", "bidirectional GRU"], [31, 32, "MethodName", "GRU"]]}
{"text": "Simultaneously , the reversed copy of the input sequence is used for the second GRU layer .", "entities": [[14, 15, "MethodName", "GRU"]]}
{"text": "Both GRU layers return a hidden state for each processed feature map .", "entities": [[1, 2, "MethodName", "GRU"]]}
{"text": "Afterwards , a global max pooling layer reduces the output space to ( 1 \u00d7 128 ) nodes .", "entities": [[4, 6, "MethodName", "max pooling"]]}
{"text": "The output neuron utilizes the sigmoid activation function .", "entities": [[5, 7, "MethodName", "sigmoid activation"]]}
{"text": "Furthermore , we adopt early stopping and use 10 % of the training data as validation split .", "entities": [[4, 6, "MethodName", "early stopping"]]}
{"text": "The evaluation of the baseline model for the OLID gold test set is not possible at the time of writing , since the gold test data have not yet been released .", "entities": [[8, 9, "DatasetName", "OLID"]]}
{"text": "By using a bidirectional GRU instead of a unidirectional LSTM , we are able to capture past and future information about the input sequence and exploit the better performance of GRU networks on smaller datasets .", "entities": [[3, 5, "MethodName", "bidirectional GRU"], [9, 10, "MethodName", "LSTM"], [30, 31, "MethodName", "GRU"]]}
{"text": "The discrepancy between the results of our cross - validation and achieved score on the OLID test set might be explained by the small amount of test tweets , which may lead to imprecise results for the submitted runs .", "entities": [[15, 16, "DatasetName", "OLID"]]}
{"text": "Ross et al ( 2017 ) show that it can be difficult to measure the agreement of annotators about hate speech in the light of the European refugee crisis .", "entities": [[19, 21, "DatasetName", "hate speech"]]}
{"text": "Inclusion of figurative language detection has proved to enhance many NLP tasks , such as argument mining and so - called hidden hate speech ( Mitrovi\u0107 et al , 2017 ) , which is also one of our future directions .", "entities": [[15, 17, "TaskName", "argument mining"], [22, 24, "DatasetName", "hate speech"]]}
{"text": "In this work , we seek to better understand the effects of COVID - 19 on mental health by examining discussions within mental health support communities on Reddit .", "entities": [[27, 28, "DatasetName", "Reddit"]]}
{"text": "In this work , we use Reddit , a popular social media platform , to study how COVID - 19 has impacted the behavior of groups of users who express mental health concerns .", "entities": [[6, 7, "DatasetName", "Reddit"]]}
{"text": "Reddit is a particularly well - suited platform for studying mental health due to its semi - anonymous nature , which encourages user honesty and reduces inhibitions associated with self - disclosure ( De Choudhury and De , 2014 ) .", "entities": [[0, 1, "DatasetName", "Reddit"]]}
{"text": "Additionally , Reddit contains subreddits that act as mental health support forums ( e.g. , r / Anxiety , r / depression , r / SuicideWatch ) , which enable a more targeted analysis of users experiencing different mental health conditions .", "entities": [[2, 3, "DatasetName", "Reddit"]]}
{"text": "A number of existing works have focused on characterizing patterns of discourse within these mental health communities on Reddit .", "entities": [[18, 19, "DatasetName", "Reddit"]]}
{"text": "Other studies of Reddit mental health communities have aimed to quantify and forecast changes in user behavior .", "entities": [[3, 4, "DatasetName", "Reddit"]]}
{"text": "Kumar et al ( 2015 ) examined how posting activity in r / SuicideWatch changes following a celebrity suicide .", "entities": [[0, 1, "DatasetName", "Kumar"]]}
{"text": "Jacobson et al ( 2020 ) explored the short - term impact of stay - at - home orders in the United States by analyzing changes in the rates of mental health - related Google search queries immediately after orders were issued .", "entities": [[34, 35, "DatasetName", "Google"]]}
{"text": "Li et al ( 2020 ) measured psycholinguistic attributes of posts on Weibo , a Chinese social media platform , before and after the Chinese National Health Commission declared COVID - 19 to be an epidemic .", "entities": [[12, 13, "DatasetName", "Weibo"]]}
{"text": "Wolohan ( 2020 ) used a Long Short - Term Memory model to classify depression among Reddit users in April 2020 , finding a higher than normal depression rate .", "entities": [[6, 11, "MethodName", "Long Short - Term Memory"], [16, 17, "DatasetName", "Reddit"]]}
{"text": "We collect Reddit posts from three mental health subreddits using the Pushshift API 1", "entities": [[2, 3, "DatasetName", "Reddit"]]}
{"text": "day ) , making it feasible to treat daily values as a time series .", "entities": [[12, 14, "TaskName", "time series"]]}
{"text": "May 31 , 2020 ) , roughly delineating when COVID - 19 began to have a serious impact on those in the United States , where the majority of Reddit users are concentrated .", "entities": [[29, 30, "DatasetName", "Reddit"]]}
{"text": "This choice of dates was informed by our analysis of the rates at which COVID - 19 related words were discussed in each subreddit ( see Section 5.1 ) , which we found hovered around 0 - 5 % before rising sharply near the beginning of March .", "entities": [[35, 36, "DatasetName", "0"]]}
{"text": "We first create time series for a number of metrics that could be affected by the pandemic , encompassing activity levels and text content ( Section 4.1 ) .", "entities": [[3, 5, "TaskName", "time series"]]}
{"text": "We then use a time series intervention analysis technique to determine whether there are significant changes in our metrics during the pandemic ( Section 4.2 ) .", "entities": [[4, 6, "TaskName", "time series"]]}
{"text": "To study changes in content that occur during the pandemic , we use the LIWC lexicon ( Pennebaker et al , 2015 ) and Latent Dirichlet Allocation ( LDA ) topic modeling ( Blei et al , 2003 ) .", "entities": [[28, 29, "MethodName", "LDA"]]}
{"text": "We select a single model to use in our analysis by examining their coherence scores , a measure of the semantic similarity of high probability words within each topic ( Mimno et al , 2011 ) .", "entities": [[20, 22, "TaskName", "semantic similarity"]]}
{"text": "We treat the task of identifying changes in subreddit activity patterns as a time series intervention analysis problem .", "entities": [[13, 15, "TaskName", "time series"]]}
{"text": "Our basic approach involves : ( 1 ) fitting a time series model to the pre - COVID observations for each of the metrics described above and then ( 2 ) examining how the values forecasted by the model compare to the observed values during the post - COVID time period .", "entities": [[10, 12, "TaskName", "time series"]]}
{"text": "It is worth noting that the one study we found examining the impact of an event on activity within mental health subreddits employs a different approach : they use a t - test to compare the observations from \" before \" vs \" after \" the event ( Kumar et al , 2015 ) .", "entities": [[48, 49, "DatasetName", "Kumar"]]}
{"text": "We smooth each time series and remove day - ofweek related fluctuations by computing a sevenday rolling mean over the time series .", "entities": [[3, 5, "TaskName", "time series"], [20, 22, "TaskName", "time series"]]}
{"text": "This model was initially created by Facebook to forecast time series on their platform , such as the number of events created per day or the number of active users ; we find that our time series , also compiled from social media , have many similar properties .", "entities": [[9, 11, "TaskName", "time series"], [35, 37, "TaskName", "time series"]]}
{"text": "The third term , h ( t ) , represents holidays ; we find that adding the default list of US holidays provided by Prophet reduces error for most our our time series in the pre - COVID period , likely because the Reddit population is centered in the United States .", "entities": [[31, 33, "TaskName", "time series"], [43, 44, "DatasetName", "Reddit"]]}
{"text": "Finally , t represents the error , in this case fluctuations in the time series that are not captured by the model .", "entities": [[13, 15, "TaskName", "time series"]]}
{"text": "Our alternate hypothesis is that there was a change in the trend of the time series ( which may be attributable to .", "entities": [[14, 16, "TaskName", "time series"]]}
{"text": "Discussion When choosing the date to consider as the beginning of the post - COVID period in our time series analysis , we considered March 1st , 2020 as a sensible date , as it aligns with the time at which the United States ( where the majority of Reddit users reside ) began to take COVID - 19 seriously .", "entities": [[18, 21, "TaskName", "time series analysis"], [49, 50, "DatasetName", "Reddit"]]}
{"text": "To give us a better idea of how common language dimensions have changed , while the LDA - derived topics allow us to explore areas of discussion that are typically of concern in these subreddits .", "entities": [[16, 17, "MethodName", "LDA"]]}
{"text": "It is also possible that compared to the general population , Reddit users are more likely to have jobs that can be done remotely during the pandemic , as they are more likely to have college degrees than the general population .", "entities": [[11, 12, "DatasetName", "Reddit"]]}
{"text": "In this study , we examined how COVID - 19 has influenced the online behavior of individuals who discuss mental health concerns by analyzing activity within the r / Anxiety , r / depression , and r / SuicideWatch communities on Reddit .", "entities": [[41, 42, "DatasetName", "Reddit"]]}
{"text": "Figure 4 shows the topics identified by the LDA model .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "4 : Topics identified by the LDA topic model .", "entities": [[6, 7, "MethodName", "LDA"]]}
{"text": "= p \u2212 p 0 p 0 ( 1 \u2212 p 0 ) ( 1 + r ) /n", "entities": [[4, 5, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "( 1 \u2212 r ) ( 2 ) wherep is the proportion of observations outside of the prediction interval in the post - COVID period , p 0 = 0.05 , n is the number of observations in the post - COVID period , and r is the lag - 1 correlation coefficient of the pre - COVID data .", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "Our corrected \u03b1 = 0.05/294", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "Any opinions , findings , conclusions , or recommendations in this material are those of the authors and do not necessarily reflect the views of the Precision Health initiative , the NSF , or the John Templeton Foundation .", "entities": [[26, 27, "MetricName", "Precision"]]}
{"text": "Comments from online news articles are annotated in the SEN - SEI corpus , which contains human - authored summaries of 1.8k comments posted on Guardian articles ( Barker et al , 2016 ) .", "entities": [[9, 10, "DatasetName", "SEN"]]}
{"text": "Some of the most recent and relevant discourse corpora from online sources related to this work include the following : Concepts related to persuasiveness have been studied , including annotations for \" convincing - ness \" in debate forums ( Habernal and Gurevych , 2016 ) , influencers in discussions from blogs and Wikipedia ( Biran et al , 2012 ) , and user relations as a proxy of persuasion in reddit ( Tan et al , 2016 ; Wei et al , 2016 ) .", "entities": [[71, 72, "DatasetName", "reddit"]]}
{"text": "Other qualities of user - generated text that are not covered in this work but have been investigated before include metaphor ( Jang et al , 2014 ) and tolerance ( Mukherjee et al , 2013 ) in online discussion threads , \" dogmatism \" of reddit users ( Fast and Horvitz , 2016 ) , and argumentation units in discussions related to technology .", "entities": [[46, 47, "DatasetName", "reddit"]]}
{"text": "Sympathetic : A warm , friendly comment that expresses positive emotion or sympathy .", "entities": [[10, 11, "DatasetName", "emotion"]]}
{"text": "2 Yahoo filters comments containing hate speech ( Nobata et al , 2016 ) and abusive language using a combination of manual review and automatic algorithms , and these comments are not included in our corpus .", "entities": [[5, 7, "DatasetName", "hate speech"], [15, 17, "TaskName", "abusive language"]]}
{"text": "They then predict each dimension with lexical and LDA topic features .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "A related line of work builds lexico - semantic resources for sentiment analysis with a focus on how the participants of an event are affected by it .", "entities": [[11, 13, "TaskName", "sentiment analysis"]]}
{"text": "Deng et al ( 2013 ) annotate how participants of an event are affected , and Deng & Wiebe ( 2014 ) show that this assists inference about the author 's sentiment towards entities or events .", "entities": [[0, 3, "DatasetName", "Deng et al"]]}
{"text": "Balahur et al ( 2012 ) use the narratives produced by the ISEAR questionnaire ( Scherer et al , 1986 ) for first - person examples of particular emotions ( \" I felt angry when X and then Y happened \" ) and extract sequences of subject - verbobject triples , which they then annotate for seven basic emotions .", "entities": [[12, 13, "DatasetName", "ISEAR"]]}
{"text": "For example , obligations to do things one does not feel like doing ( Row 4 ) , or a job that does not engage personal decision making or involvement ( lack of autonomy ) can make one feel unhappy .", "entities": [[26, 28, "TaskName", "decision making"]]}
{"text": "Our top - down method is based on mapping general event types from FrameNet to the theoretical categories enumerated in Table 1 .", "entities": [[13, 14, "DatasetName", "FrameNet"]]}
{"text": "We show that FrameNet features do provide an interesting level of generalization but much of the compositional semantics of events is still missing from this characterization ( Section 4 ) .", "entities": [[3, 4, "DatasetName", "FrameNet"]]}
{"text": "To develop features related to these frame categories , we apply SEMAFOR ( Das et al , 2013 ) to label the ECHO posts with their corresponding frames using FrameNet 1.5 ( Baker et al , 2015 ; Baker , 2014 ) .", "entities": [[29, 30, "DatasetName", "FrameNet"]]}
{"text": "We train an SVM with each feature subset , and evaluate the models on our test set , with results in Table 7 .", "entities": [[3, 4, "MethodName", "SVM"]]}
{"text": "All features achieve good F1 for the positive class , but not the negative class .", "entities": [[4, 5, "MetricName", "F1"]]}
{"text": "The SVM trained with just eudaimonic features produces the highest F1 score for the negative class , highlighting the role of eudaimonic related events in negative well - being .", "entities": [[1, 2, "MethodName", "SVM"], [10, 12, "MetricName", "F1 score"]]}
{"text": "We also apply Autoslog - TS , a weakly supervised linguistic - pattern learner as a way of learning some compositional patterns .", "entities": [[5, 6, "MethodName", "TS"]]}
{"text": "In order to enable selection of particular patterns , AutoSlog - TS computes statistics on the strength of association of each pattern with each class , i.e. P ( POSITIVE | p ) and P ( NEGATIVE | p ) , along with the pattern 's overall frequency .", "entities": [[11, 12, "MethodName", "TS"]]}
{"text": "We define two tuning parameters for each class : \u03b8 f , the frequency with which a pattern occurs , \u03b8 p , the probability with which a pattern is associated with the given class .", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "Here we select \u03b8 f and \u03b8 p to optimize F1 on our test set .", "entities": [[3, 4, "HyperparameterName", "\u03b8"], [6, 7, "HyperparameterName", "\u03b8"], [10, 11, "MetricName", "F1"]]}
{"text": "Several lexicosyntactic patterns fit within our wellbeing categories but are not captured by frames , while as expected there are overlaps between FrameNet and Autoslog as well .", "entities": [[22, 23, "DatasetName", "FrameNet"]]}
{"text": "One large class includes straightforward lexical patterns : FINISHED , FIN - ISH , and FINALLY which we associate with feelings of comptence .", "entities": [[10, 11, "DatasetName", "FIN"]]}
{"text": "The frames also show many specific types of food ( cake ) , and we use a comprehensive list from DBpedia ( Lehmann et al , 2014 ) to collapse all these to the general type FOOD , allowing us to develop patterns such as MADE FOOD .", "entities": [[20, 21, "DatasetName", "DBpedia"]]}
{"text": "Across the two versions of the lemma , the positive patterns provide several expressions for savouring ( WENT / GO ON / FOR [ a walk , a hike , a ride ] , WENT / GO SHOPPING / SWIMMING , WENT / GO TO [ the mall , a movie ] ) .", "entities": [[6, 7, "DatasetName", "lemma"]]}
{"text": "We have shown that FrameNet provides useful generalizations , while the linguistic pattern learner AutoSlog illustrates the details and challenges of the compositional nature of user 's descriptions of their daily experiences .", "entities": [[4, 5, "DatasetName", "FrameNet"]]}
{"text": "Moreover , we have demonstrated that , independently , each of these methods can produce performance similar to that of conventional lexical methods with a feature space that is smaller , and , in the case of FrameNet features , psychologically grounded .", "entities": [[37, 38, "DatasetName", "FrameNet"]]}
{"text": "Our Autoslog exploration moreover reveals a way of exploring the space of patterns that our FrameNet mapping has missed .", "entities": [[15, 16, "DatasetName", "FrameNet"]]}
{"text": "Tired of Topic Models ?", "entities": [[2, 4, "TaskName", "Topic Models"]]}
{"text": "Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too !", "entities": [[3, 5, "TaskName", "Word Embeddings"]]}
{"text": "Topic models are a useful analysis tool to uncover the underlying themes within document collections .", "entities": [[0, 2, "TaskName", "Topic models"]]}
{"text": "The dominant approach is to use probabilistic topic models that posit a generative story , but in this paper we propose an alternative way to obtain topics : clustering pretrained word embeddings while incorporating document information for weighted clustering and reranking top words .", "entities": [[7, 9, "TaskName", "topic models"], [30, 32, "TaskName", "word embeddings"]]}
{"text": "We provide benchmarks for the combination of different word embeddings and clustering algorithms , and analyse their performance under dimensionality reduction with PCA .", "entities": [[8, 10, "TaskName", "word embeddings"], [19, 21, "TaskName", "dimensionality reduction"], [22, 23, "MethodName", "PCA"]]}
{"text": "The best performing combination for our approach performs as well as classical topic models , but with lower runtime and computational complexity .", "entities": [[12, 14, "TaskName", "topic models"]]}
{"text": "Topic models are the standard approach for exploratory document analysis ( Boyd - Graber et al , 2017 ) , which aims to uncover main themes and underlying narratives within a corpus .", "entities": [[0, 2, "TaskName", "Topic models"]]}
{"text": "This work explores an alternative to topic modeling by casting ' key themes ' or ' topics ' as clusters of word types under the modern distributed representation learning paradigm : unsupervised pre - trained word embeddings provide a representation for each word type as a vector , allowing us to cluster them based on their distance in high - dimensional space .", "entities": [[27, 29, "TaskName", "representation learning"], [35, 37, "TaskName", "word embeddings"]]}
{"text": "The goal of this work is not to strictly outperform , but rather to benchmark standard clustering of modern embedding methods against the classical approach of Latent Dirichlet Allocation ( LDA ; Blei et al , 2003 ) .", "entities": [[30, 31, "MethodName", "LDA"]]}
{"text": "Aside from reporting the best performing combination of word embeddings and clustering algorithm , we are also interested in whether there are consistent patterns : embeddings which perform consistently well across clustering algorithms might be good representations for unsupervised document analysis , clustering algorithms that perform consistently well are more likely to generalize to future word embedding methods .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "To make our approach reliably work as well as LDA , we incorporate corpus frequency statistics directly into the clustering algorithm , and quantify the effects of two key methods , 1 ) weighting terms during clustering and 2 ) reranking terms for obtaining the top J representative words .", "entities": [[9, 10, "MethodName", "LDA"]]}
{"text": "Our contributions are as follows : We systematically apply centroid - based clustering algorithms on top of a variety of pretrained word embeddings and embedding methods for document analysis .", "entities": [[21, 23, "TaskName", "word embeddings"]]}
{"text": "Through weighted clustering and reranking of top words we obtain sensible topics ; the best performing combination is comparable with LDA , but with smaller time complexity and empirical runtime .", "entities": [[20, 21, "MethodName", "LDA"]]}
{"text": "We show that further speedups are possible by reducing the embedding dimensions by up to 80 % using PCA .", "entities": [[18, 19, "MethodName", "PCA"]]}
{"text": "Analyzing documents by clustering word embeddings is a natural idea - clustering has been used for readability assessment ( Cha et al , 2017 ) , argument mining ( Reimers et al , 2019 ) , document classification and document clustering ( Sano et al , 2017 ) , inter alia .", "entities": [[4, 6, "TaskName", "word embeddings"], [26, 28, "TaskName", "argument mining"], [36, 38, "TaskName", "document classification"]]}
{"text": "So far , however , clustering word embeddings has not seen much success for the purposes of topic modeling .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "While many modern efforts have attempted to incorporate word embeddings into the probabilistic LDA framework ( Liu et al , 2015 ; Nguyen et al , 2015 ; Das et al , 2015 ; Batmanghelich et al , 2016 ; Xun et al , 2017 ; Dieng et al , 2019 ) , relatively little work has examined the feasibility of clustering embeddings directly .", "entities": [[8, 10, "TaskName", "word embeddings"], [13, 14, "MethodName", "LDA"]]}
{"text": "Sridhar ( 2015 ) targets short texts where LDA performs poorly in particular , fitting GMMs to learned word2vec representations .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "We focus on centroid based k - means ( KM ) , Spherical k - means ( SK ) , and k - medoids ( KD ) for hard clustering , and von Mises - Fisher Models ( VMFM ) and Gaussian Mixture Models ( GMM ) for soft clustering ; as pre - trained embeddings we consider word2vec ( Mikolov et al , 2013 ) , GloVe ( Pennington et al , 2014 ) , FastText ( Bojanowski et al , 2017 , Spherical ( Meng et al , 2019 ) , ELMo ( Peters et al , 2018 ) , andBERT ( Devlin et al , 2018 ) .", "entities": [[67, 68, "MethodName", "GloVe"], [76, 77, "MethodName", "FastText"], [93, 94, "MethodName", "ELMo"]]}
{"text": "In traditional topic modeling ( LDA ) , the top J words are those with highest probability under each topic - word distribution .", "entities": [[5, 6, "MethodName", "LDA"]]}
{"text": "Note that it is possible to extend this approach to obtain the top topics given a document : compute similarity scores between learned topic cluster centers and all word embeddings from that particular document , and normalize them using softmax to obtain a ( non - calibrated ) probability distribution .", "entities": [[28, 30, "TaskName", "word embeddings"], [39, 40, "MethodName", "softmax"]]}
{"text": "The intuition of weighted clustering is based on the formulation of classical LDA which models the probability of the word type t belonging to a topic i as N t , i + \u03b2t t N t i + \u03b2 t , where N t , i refers to the number of times word type t has been assigned to topic i , and \u03b2 is a parameter of the Dirichlet prior on the pertopic word distribution .", "entities": [[12, 13, "MethodName", "LDA"], [39, 40, "HyperparameterName", "\u03b2"], [64, 65, "HyperparameterName", "\u03b2"]]}
{"text": "In contrast , LDA via collapsed Gibbs sampling has a complexity of O ( tkN ) , where N is the number of all tokens , so when N n , clustering methods can potentially achieve better performance - complexity tradeoffs .", "entities": [[3, 4, "MethodName", "LDA"]]}
{"text": "Note that running ELMo and BERT over documents also requires iterating over all tokens , but only once , and not for every topic and iteration .", "entities": [[3, 4, "MethodName", "ELMo"], [5, 6, "MethodName", "BERT"]]}
{"text": "For readily available pretrained word embeddings such as word2vec , FastText , GloVe and Spherical , the embeddings can be considered as ' given ' as the practioner does not need to generate these embeddings from scratch .", "entities": [[4, 6, "TaskName", "word embeddings"], [10, 11, "MethodName", "FastText"], [12, 13, "MethodName", "GloVe"]]}
{"text": "However for contextual embeddings such as ELMo and BERT , there is additional computational cost in obtaining these embeddings before clustering , which requires passing through RNN and transformer layers respectively .", "entities": [[6, 7, "MethodName", "ELMo"], [8, 9, "MethodName", "BERT"]]}
{"text": "We use standard pretrained ELMo and BERT models in our experiments and therefore do not consider the runtime of training these models from scratch .", "entities": [[4, 5, "MethodName", "ELMo"], [6, 7, "MethodName", "BERT"]]}
{"text": "The NPMI scores presented in Table 1 are averaged across cluster centers initialized using 5 random seeds .", "entities": [[16, 17, "DatasetName", "seeds"]]}
{"text": "For contextualized word embeddings ( BERT and ELMo ) , sentences served as the context window to obtain the token representations .", "entities": [[2, 4, "TaskName", "word embeddings"], [5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "ELMo"]]}
{"text": "Subword representations were averaged for BERT , which performs better than just using the first subword .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "Running LDA with MALLET ( McCallum , 2002 ) takes a minute , but performs no better than KM w r , which takes little more than 10 seconds on CPU using sklearn ( Pedregosa et al , 2011 ) , and 3 - 4 seconds using a simple implementation using JAX ( Bradbury et al , 2018 ) on GPU .", "entities": [[1, 2, "MethodName", "LDA"]]}
{"text": "This suggests that reweighting using term frequency is effective for clustering without the need for ad - hoc restriction of infrequent terms - without it , all combinations perform poorly compared to LDA .", "entities": [[32, 33, "MethodName", "LDA"]]}
{"text": "The top topic words before and after reranking for BERT - GMM w have an average Jaccard similarity score of 0.910 , indicating that the cluster centers learned by weighted GMMs are already centered at word types of high frequency in the training corpus .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "Spherical embeddings and BERT perform consistently well across both datasets .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "For 20NG , KM w r Spherical and LDA both achieve 0.26 NPMI .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "For Reuters , GMM w r BERT achieves the top NPMI score of 0.15 compared to 0.12 of LDA .", "entities": [[6, 7, "MethodName", "BERT"], [18, 19, "MethodName", "LDA"]]}
{"text": "Word2vec and ELMo ( using only the last layer 8 ) perform poorly compared to the other embeddings .", "entities": [[2, 3, "MethodName", "ELMo"]]}
{"text": "Fast - Text and GloVe can achieve similar performance to BERT on 20NG but are slightly inferior on Reuters .", "entities": [[4, 5, "MethodName", "GloVe"], [10, 11, "MethodName", "BERT"]]}
{"text": "We find that our approach yields a greater diversity within topics as compared to LDA while achieving comparable coherence scores ( App . D ) .", "entities": [[14, 15, "MethodName", "LDA"]]}
{"text": "We apply PCA to the word embeddings before clustering to investigate the amount of redundancy in the dimensions of large embeddings , which impact clustering complexity ( 4 ) .", "entities": [[2, 3, "MethodName", "PCA"], [5, 7, "TaskName", "word embeddings"]]}
{"text": "We observe that KM w r can consistently reduce the number of dimensions across different embedding types without loss of performance .", "entities": [[18, 19, "MetricName", "loss"]]}
{"text": "We outlined a methodology for clustering word embeddings for unsupervised document analysis , and presented a systematic comparison of various influential embedding methods and clustering algorithms .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Our experiments suggest that pretrained word embeddings ( both contextualized and non - contextualized ) , combined with tf - weighted k - means and tf - based reranking , provide a viable alternative to traditional topic modeling at lower complexity and runtime .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "We thank Aaron Mueller , Pamela Shapiro , Li Ke , Adam Poliak , Kevin Duh and the anonymous reviewers for their feedback .", "entities": [[11, 12, "MethodName", "Adam"]]}
{"text": "We present the different topics generated using LDA ( Table 7 ) and topics generated using BERT KM w r for the Reuters dataset (", "entities": [[7, 8, "MethodName", "LDA"], [16, 17, "MethodName", "BERT"]]}
{"text": "Despite the widespread success of selfsupervised learning via masked language models ( MLM ) , accurately capturing fine - grained semantic relationships in the biomedical domain remains a challenge .", "entities": [[12, 13, "DatasetName", "MLM"]]}
{"text": "This is of paramount importance for entity - level tasks such as entity linking where the ability to model entity relations ( especially synonymy ) is pivotal .", "entities": [[12, 14, "TaskName", "entity linking"]]}
{"text": "We design a scalable metric learning framework that can leverage UMLS , a massive collection of biomedical ontologies with 4M+ concepts .", "entities": [[4, 6, "TaskName", "metric learning"], [10, 11, "DatasetName", "UMLS"]]}
{"text": "In contrast with previous pipelinebased hybrid systems , SAPBERT offers an elegant one - model - for - all solution to the problem of medical entity linking ( MEL ) , achieving a new state - of - the - art ( SOTA ) on six MEL benchmarking datasets .", "entities": [[25, 27, "TaskName", "entity linking"]]}
{"text": "Biomedical entity 2 representation is the foundation for a plethora of text mining systems in the medical domain , facilitating applications such as literature search ( Lee et al , 2016 ) , clinical decision making ( Roberts et al , 2015 ) and relational knowledge discovery ( e.g. chemical - disease , drug - drug and protein - protein relations , Wang et al 2018 ) .", "entities": [[16, 18, "DatasetName", "medical domain"], [34, 36, "TaskName", "decision making"]]}
{"text": "poses a major challenge to representation learning .", "entities": [[5, 7, "TaskName", "representation learning"]]}
{"text": "In parallel , self - supervised learning has shown tremendous success in NLP via leveraging the masked language modelling ( MLM ) objective to learn semantics from distributional representations Liu et al , 2019 ) .", "entities": [[3, 7, "TaskName", "self - supervised learning"], [17, 19, "TaskName", "language modelling"], [20, 21, "DatasetName", "MLM"]]}
{"text": "To address the aforementioned issue , we propose to pretrain a Transformer - based language model on the biomedical knowledge graph of UMLS ( Bodenreider , 2004 ) , the largest interlingua of biomedical ontologies .", "entities": [[11, 12, "MethodName", "Transformer"], [22, 23, "DatasetName", "UMLS"]]}
{"text": "UMLS contains a comprehensive collection of biomedical synonyms in various forms ( UMLS 2020AA has 4M+ concepts and 10M+ synonyms which stem from over 150 controlled vocabularies including MeSH , SNOMED CT , RxNorm , Gene Ontology and OMIM ) .", "entities": [[0, 1, "DatasetName", "UMLS"], [12, 13, "DatasetName", "UMLS"], [36, 37, "MethodName", "Ontology"]]}
{"text": "To cope with the immense size of UMLS , we sample hard training pairs from the knowledge base and use a scalable metric learning loss .", "entities": [[7, 8, "DatasetName", "UMLS"], [22, 24, "TaskName", "metric learning"], [24, 25, "MetricName", "loss"]]}
{"text": "We name our model as Self - aligning pretrained BERT ( SAPBERT ) .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "The effectiveness of the pretraining in SAP - BERT is especially highlighted in the scientific language domain where SAPBERT outperforms previous SOTA even without fine - tuning on any MEL datasets .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "We design a metric learning framework that learns to self - align synonymous biomedical entities .", "entities": [[3, 5, "TaskName", "metric learning"]]}
{"text": "The framework can be used as both pretraining on UMLS , and fine - tuning on task - specific datasets .", "entities": [[9, 10, "DatasetName", "UMLS"]]}
{"text": "We use an existing BERT model as our starting point .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "For the self - alignment pretraining step , X \u00d7 Y is the set of all ( name , CUI 5 ) pairs in UMLS , e.g. ( Remdesivir , C4726677 ) ; while for the finetuning step , it is formed as an entity mention and its corresponding mapping from the ontology , e.g. ( scratchy throat , 102618009 ) .", "entities": [[24, 25, "DatasetName", "UMLS"], [52, 53, "MethodName", "ontology"]]}
{"text": "Given any pair of tuples ( x i , y i ) , ( x j , y j ) X \u00d7 Y , the goal of the self - alignment is to learn a function f ( ; \u03b8 ) : X R d parameterised by \u03b8 .", "entities": [[39, 40, "HyperparameterName", "\u03b8"], [47, 48, "HyperparameterName", "\u03b8"]]}
{"text": "We model f by a BERT model with its output [ CLS ] token regarded as the representation of the input .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "During the learning , a sampling procedure selects the informative pairs of training samples and uses them in the pairwise metric learning loss function ( introduced shortly ) .", "entities": [[20, 22, "TaskName", "metric learning"], [22, 23, "MetricName", "loss"]]}
{"text": "2 . For biomedical entities , this step can be particularly useful as most examples can be easily classified while a small set of very hard ones cause the most challenge to representation learning .", "entities": [[32, 34, "TaskName", "representation learning"]]}
{"text": "A similar but not identical triplet mining condition was used by Schroff et al ( 2015 ) for face recognition to select hard negative samples .", "entities": [[18, 20, "TaskName", "face recognition"]]}
{"text": "We compute the pairwise cosine similarity of all the BERT - produced name representations and obtain a similarity matrix S R | X b | \u00d7 | X b | where each entry S ij corresponds to the cosine similarity between the i - th and j - th names in the mini - batch b.", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "e \u03b1 ( S in \u2212 )", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "The MS loss leverages similarities among and between positive and negative pairs to re - weight the importance of the samples .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "Data Preparation Details for UMLS Pretraining .", "entities": [[4, 5, "DatasetName", "UMLS"]]}
{"text": "We download the full release of UMLS 2020AA version .", "entities": [[6, 7, "DatasetName", "UMLS"]]}
{"text": "NCBI BC5CDR - d BC5CDR -", "entities": [[1, 2, "DatasetName", "BC5CDR"], [4, 5, "DatasetName", "BC5CDR"]]}
{"text": "c MedMentions AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 the improvement comparing to the base model ( the deeper the more ) .", "entities": [[1, 2, "DatasetName", "MedMentions"], [3, 4, "DatasetName", "COMETA"]]}
{"text": "On COMETA , the results inside the parentheses added the supervised SOTA 's dictionary back - off technique ( Basaldella et al , 2020 ) . \"", "entities": [[1, 2, "DatasetName", "COMETA"]]}
{"text": "We experiment on 6 different English MEL datasets : 4 in the scientific domain ( NCBI , Dogan et al 2014 ; BC5CDR - c and BC5CDR - d , Li", "entities": [[22, 23, "DatasetName", "BC5CDR"], [26, 27, "DatasetName", "BC5CDR"]]}
{"text": "et al 2016 ; MedMentions , Mohan and Li 2018 ) and 2 in the social media domain ( COMETA , Basaldella et al 2020 andAskAPatient , Limsopatham andCollier 2016 ) .", "entities": [[4, 5, "DatasetName", "MedMentions"], [19, 20, "DatasetName", "COMETA"]]}
{"text": "We report Acc @1 and Acc @5 ( denoted as @1 and @5 ) for evaluating performance .", "entities": [[2, 3, "MetricName", "Acc"], [5, 6, "MetricName", "Acc"]]}
{"text": "In all experiments , SAPBERT denotes further pretraining with our self - alignment method on UMLS .", "entities": [[15, 16, "DatasetName", "UMLS"]]}
{"text": "Except for numbers reported in previous papers , all results are the average of five runs with different random seeds .", "entities": [[19, 20, "DatasetName", "seeds"]]}
{"text": "Similar to pretraining , a positive pair list is generated through traversing the combinations of mention and all ground truth synonyms where mentions are from the training set and ground truth synonyms are from the reference ontology .", "entities": [[36, 37, "MethodName", "ontology"]]}
{"text": "On scientific language datasets , we train for 3 epochs while on AskAPatient and COMETA we train for 15 and 10 epochs respectively .", "entities": [[14, 15, "DatasetName", "COMETA"]]}
{"text": "* BERT + SAPBERT ( Tab . 1 , top ) .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "We illustrate the impact of SAPBERT pretraining over 7 existing BERT - based models ( * BERT = { BIOBERT , PUBMEDBERT , ... } ) .", "entities": [[10, 11, "MethodName", "BERT"], [16, 17, "MethodName", "BERT"]]}
{"text": "For the social media domain , the SOTA are Basaldella et al ( 2020 ) and GEN - RANK ( Xu et al , 2020 ) on COMETA and AskAPatient respectively .", "entities": [[27, 28, "DatasetName", "COMETA"]]}
{"text": "All these SOTA methods combine BERT with heuristic modules such as tf - idf , string matching and information retrieval system ( i.e. Apache Lucene ) in a multi - stage manner .", "entities": [[5, 6, "MethodName", "BERT"], [18, 20, "TaskName", "information retrieval"]]}
{"text": "However , after fine - tuning on the social media datasets ( using the MS loss introduced earlier ) , SAPBERT outperforms SOTA significantly , indicating that knowledge acquired during the selfaligning pretraining can be adapted to a shifted domain without much effort .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "While maintaining the same pretraining scheme with the same SAPBERT online mining + MS loss , instead of training from the full model of PUBMEDBERT , we insert new ADAPTER layers between Transformer layers of the fixed PUBMEDBERT , and only train the weights of these ADAPTER layers .", "entities": [[14, 15, "MetricName", "loss"], [32, 33, "MethodName", "Transformer"]]}
{"text": "SAPBERT can be easily inserted into existing BERT - based MEL systems by initialising the systems with SAPBERT pretrained weights .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "Strikingly , without any fine - tuning on task - specific labelled data , SAPBERT already outperforms the previous supervised SOTA ( sophisticated hybrid entity linking systems ) on multiple datasets in the scientific language domain .", "entities": [[24, 26, "TaskName", "entity linking"]]}
{"text": "Our work opens new avenues to explore for general domain self - alignment ( e.g. by leveraging knowledge graphs such as DBpedia ) .", "entities": [[17, 19, "TaskName", "knowledge graphs"], [21, 22, "DatasetName", "DBpedia"]]}
{"text": "We plan to incorporate other types of relations ( i.e. , hypernymy and hyponymy ) and extend our model to sentence - level representation learning .", "entities": [[23, 25, "TaskName", "representation learning"]]}
{"text": "NCBI disease ( Dogan et al , 2014 ) is a corpus containing 793 fully annotated PubMed abstracts and 6 , 881 mentions .", "entities": [[0, 2, "DatasetName", "NCBI disease"]]}
{"text": "The mentions are mapped into the MEDIC dictionary ( Davis et al , 2012 ) .", "entities": [[6, 7, "DatasetName", "MEDIC"]]}
{"text": "We denote this dataset as \" NCBI \" in our experiments . BC5CDR", "entities": [[12, 13, "DatasetName", "BC5CDR"]]}
{"text": "The disease mentions are mapped into the MEDIC dictionary like the NCBI disease corpus .", "entities": [[7, 8, "DatasetName", "MEDIC"], [11, 14, "DatasetName", "NCBI disease corpus"]]}
{"text": "We denote the disease and chemical mention sets as \" BC5CDRd \" and \" BC5CDR - c \" respectively .", "entities": [[14, 15, "DatasetName", "BC5CDR"]]}
{"text": "For NCBI and BC5CDR we use the same data and evaluation protocol by Sung et al ( 2020 ) .", "entities": [[3, 4, "DatasetName", "BC5CDR"]]}
{"text": "11 MedMentions ( Mohan and Li , 2018 ) is a verylarge - scale entity linking dataset containing over 4 , 000 abstracts and over 350 , 000 mentions linked to UMLS 2017AA .", "entities": [[1, 2, "DatasetName", "MedMentions"], [14, 16, "TaskName", "entity linking"], [31, 32, "DatasetName", "UMLS"]]}
{"text": "According to Mohan and Li ( 2018 ) , training TAGGERONE , a very popular MEL system , on a subset of MedMentions require > 900 GB of RAM .", "entities": [[22, 23, "DatasetName", "MedMentions"], [28, 29, "MethodName", "RAM"]]}
{"text": "Its massive number of mentions and more importantly the used reference ontology ( UMLS 2017AA has 3M+ concepts ) make the application of most MEL systems infeasible .", "entities": [[11, 12, "MethodName", "ontology"], [13, 14, "DatasetName", "UMLS"]]}
{"text": "However , through our metric learning formulation , SAPBERT can be applied on MedMentions with minimal effort .", "entities": [[4, 6, "TaskName", "metric learning"], [13, 14, "DatasetName", "MedMentions"]]}
{"text": "12 COMETA ( Basaldella et al , 2020 ) is a recently released large - scale MEL dataset that specifically focuses on MEL in the social media domain , containing around 20k medical mentions extracted from health - related discussions on reddit.com .", "entities": [[1, 2, "DatasetName", "COMETA"]]}
{"text": "We list all the versions of BERT models used in this study , linking to the specific versions in Tab . 5 .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "All BERT models refer to the BERT Base architecture in this paper .", "entities": [[1, 2, "MethodName", "BERT"], [6, 7, "MethodName", "BERT"]]}
{"text": "COMETA ( s.g . ) and ( z.g . ) are the stratified ( general ) and zeroshot ( general ) split respectively .", "entities": [[0, 1, "DatasetName", "COMETA"]]}
{"text": "model NCBI BC5CDR - d BC5CDR -", "entities": [[2, 3, "DatasetName", "BC5CDR"], [5, 6, "DatasetName", "BC5CDR"]]}
{"text": "c MedMentions AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 SIEVE - BASED ( D'Souza and Ng , 2015 ) 84.7 - 84.1 - 90.7 - - - WORDCNN ( Limsopatham and Collier , 2016 ) - - - - - - - - 81.4 - - - WORDGRU+TF - IDF ( Tutubalina et al , 2018 ) - - - - - - - - 85.7 - - - TAGGERONE 87.7 - 88.9 - 94.1 - OOM OOM - - - - NORMCO ( Wright et al , 2019 ) 87.8 - 88.0 - - - - - - - - - BNE ( Phan et al , 2019 ) 87.7 - 90.6 - 95.8 - - - - - - - BERTRANK", "entities": [[1, 2, "DatasetName", "MedMentions"], [3, 4, "DatasetName", "COMETA"]]}
{"text": "We use COMETA ( zeroshot general ) as a benchmark for selecting learning objectives .", "entities": [[2, 3, "DatasetName", "COMETA"]]}
{"text": "Note that this split of COMETA is different from the stratified - general split used in Tab .", "entities": [[5, 6, "DatasetName", "COMETA"]]}
{"text": "InfoNCE has been very popular in selfsupervised learning and contrastive learning ( Oord et al , 2018 ; He et al , 2020 ) .", "entities": [[0, 1, "MethodName", "InfoNCE"], [9, 11, "MethodName", "contrastive learning"]]}
{"text": "FL is supported by Grace & Thomas C.H. Chan Cambridge Scholarship .", "entities": [[9, 10, "DatasetName", "Cambridge"]]}
{"text": "Practical summarization systems are expected to produce summaries of varying lengths , per user needs .", "entities": [[1, 2, "TaskName", "summarization"]]}
{"text": "While a couple of early summarization benchmarks tested systems across multiple summary lengths , this practice was mostly abandoned due to the assumed cost of producing reference summaries of multiple lengths .", "entities": [[5, 6, "TaskName", "summarization"]]}
{"text": "For that , we have analyzed a couple of datasets as a case study , using several variants of the ROUGE metric that are standard in summarization evaluation .", "entities": [[26, 27, "TaskName", "summarization"]]}
{"text": "This result paves the way to practically evaluating varying - length summaries with simple , possibly existing , summarization benchmarks .", "entities": [[18, 19, "TaskName", "summarization"]]}
{"text": "Automated summarization systems typically produce a text that mimics a manual summary .", "entities": [[1, 2, "TaskName", "summarization"]]}
{"text": "Consequently , output length has been a common tunable parameter in pre - neural summarization systems and has been incorporated recently in few neural models as well ( Kikuchi et al , 2016 ;", "entities": [[14, 15, "TaskName", "summarization"]]}
{"text": "It was originally assumed that summarization systems should be assessed across multiple summary lengths .", "entities": [[5, 6, "TaskName", "summarization"]]}
{"text": "However , due to the high cost incurred , subsequent DUC and TAC ( NIST , 2018 ) benchmarks ( 2003 ) ( 2004 ) ( 2005 ) ( 2006 ) ( 2007 ) ( 2008 ) ( 2009 ) ( 2010 ) ( 2011 ) ( 2012 ) ( 2013 ) ( 2014 ) , as well as the more recently popular datasets CNN / Daily Mail ( Nallapati et al , 2016 ) and Gigaword ( Graff et al , 2003 ) , included references and evaluation for just one summary length per input text .", "entities": [[64, 68, "DatasetName", "CNN / Daily Mail"]]}
{"text": "In this paper , we propose that the summarization community should consider resuming evaluating summarization systems over multiple length outputs , as it would allow better assessment of length - related performance within and across systems ( illustrated in Section 3 ) .", "entities": [[8, 9, "TaskName", "summarization"], [14, 15, "TaskName", "summarization"]]}
{"text": "Our promising results suggest repeating the assessment methodology presented here in future work , to test our question over more recent and broader summarization datasets and human evaluation schemes .", "entities": [[23, 24, "TaskName", "summarization"]]}
{"text": "Here , we first examine the relevance of our proposal to reinstitute summarization evaluation over multiple summary lengths .", "entities": [[12, 13, "TaskName", "summarization"]]}
{"text": "We turn to the DUC 2001 and 2002 multi - document summarization datasets , which , to the best of our knowledge , are the only available datasets that provide the necessary requirements for this analysis ( see Table 1 ) .", "entities": [[8, 12, "TaskName", "multi - document summarization"]]}
{"text": "Specifically , we use several variants of the ROUGE metric ( Lin , 2004 ) , which is almost exclusively utilized as an automatic evaluation metric class for summarization .", "entities": [[28, 29, "TaskName", "summarization"]]}
{"text": "As expected when measuring ROUGE Recall against a fixed reference length , longer system summaries typically cover more of the reference summaries content than shorter ones , yielding higher scores .", "entities": [[5, 6, "MetricName", "Recall"]]}
{"text": "We proposed the potential value of evaluating summarization systems at different summary lengths .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "Being able to perform in - depth chat with humans in a closed domain is a precondition before an open - domain chatbot can ever be claimed .", "entities": [[22, 23, "TaskName", "chatbot"]]}
{"text": "Being able to converse like humans in a closed domain is a precondition before an intelligent opendomain chatbot , which further requires transiting among various domains , can be designed", "entities": [[3, 4, "DatasetName", "converse"], [17, 18, "TaskName", "chatbot"]]}
{"text": "Selecting proper domain knowledge to support response generation at all the different situations is challenging ( Milward and Beveridge , 2003 ; Shen et al , 2019 ) .", "entities": [[6, 8, "TaskName", "response generation"]]}
{"text": "In this work , we direct our focus to the movie domain and present a large - scale , crowdsourced Chinese dataset with fine - grained annotations in hope of boosting the study towards a human - like closed - domain chatbot .", "entities": [[41, 42, "TaskName", "chatbot"]]}
{"text": "As for the named entity recognition , we labeled 5 kinds of entities : movie names , director , actor , type and role ( first 5 aspects ) .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}
{"text": "To speed up the annotation process , we first define a set of handcrafted regular expressions , which covers most frequent patterns at each class , to train a DA and NER classifier .", "entities": [[31, 32, "TaskName", "NER"]]}
{"text": "Knowledge Linkage We extract fact knowledge from the structured table in Douban Movie 6 , a popular Chinese platform for movies .", "entities": [[11, 12, "DatasetName", "Douban"]]}
{"text": "Apart from the objective knowledge , we also crawl movie comments from Douban Movie to support the generation of responses expressing subjective feelings .", "entities": [[12, 13, "DatasetName", "Douban"]]}
{"text": "For utterances labeled as inform / request feeling , we compare them with Douban comments from the same movie and compute the similarity score based on weighted average of edit distance , Jaccard distance , tf - idf , sentence vector cosine similarity , common words and entities .", "entities": [[13, 14, "DatasetName", "Douban"]]}
{"text": "Inspired by this , our dialogue generation model is implemented as a Transformer - based language model like GPT2 ( Radford et al , 2019 ; .", "entities": [[0, 1, "DatasetName", "Inspired"], [5, 7, "TaskName", "dialogue generation"], [12, 13, "MethodName", "Transformer"]]}
{"text": "Recommending other aspects require assembling recommendation systems of different domains , which is beyond the scope of this paper .", "entities": [[5, 7, "TaskName", "recommendation systems"]]}
{"text": "Compared with the traditional way of multi - label classification , casting it as sequence prediction is better at addressing the coexistence of multiple DAs and capturing the sequential dependencies among the hierarchy ( Raffel et al , 2019 ; Vedula et al , 2020 ) .", "entities": [[6, 10, "TaskName", "multi - label classification"]]}
{"text": "By this means , before predicting a DA , the model can condition on both the dialogue context and its previous DAs to improve the accuracy .", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "We replace the MLP with our language model encoder to get the embedding for knowledge .", "entities": [[3, 4, "DatasetName", "MLP"]]}
{"text": "We compare our models with the Chinese RoBERTa ( Liu et Table 7 measures the performance of retrieving fact knowledge , movie comments and recommen - dation respectively .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}
{"text": "It will converse actively at some risk of containing fact errors .", "entities": [[2, 3, "DatasetName", "converse"]]}
{"text": "We use its chat service through Weibo .", "entities": [[6, 7, "DatasetName", "Weibo"]]}
{"text": "A good chatbot should balance well these skills ( Adiwardana et al , 2020 ) .", "entities": [[2, 3, "TaskName", "chatbot"]]}
{"text": "SEA can reflect how it behaves as a general chatbot while FIA can better test its capability at incorporating domain knowledge .", "entities": [[9, 10, "TaskName", "chatbot"]]}
{"text": "This suggests the most crucial bottleneck lies in the interaction with movie - specific knowledge and seamlessly incorporating it into the response generation .", "entities": [[21, 23, "TaskName", "response generation"]]}
{"text": "We present MovieChats : a movie - domain chatbot built upon a large - scale , high - quality conversational corpus with fine - grained annotations .", "entities": [[8, 9, "TaskName", "chatbot"]]}
{"text": "We thank anonymous reviewers and the dialogue system team at Wechat AI for their valuable comments .", "entities": [[10, 11, "DatasetName", "Wechat"]]}
{"text": "Xiaoyu Shen was funded by IMPRS - CS fellowship .", "entities": [[7, 8, "DatasetName", "CS"]]}
{"text": "Since successful ML applications thrive on feature engineering ( Domingos , 2012 ) , we conduct feature selection to evaluate and determine the best feature sets for the models .", "entities": [[6, 8, "TaskName", "feature engineering"], [16, 18, "MethodName", "feature selection"]]}
{"text": "Simple ML methods with feature selection return satisfactory results , and the performance further improves by the ensemble classifier .", "entities": [[4, 6, "MethodName", "feature selection"]]}
{"text": "Among the popular ML methods in suicide literature is logistic regression ( LR ) ( Walsh et al , 2017 ; De Choudhury et", "entities": [[9, 11, "MethodName", "logistic regression"]]}
{"text": "We also experiment with K - Nearest Neighbors with different distance ( uniform , weighted ) and neighborhood ( k { 3 , 5 , 8 } ) settings , but we eliminate it for low within - dataset results .", "entities": [[4, 8, "MethodName", "K - Nearest Neighbors"]]}
{"text": "Additionally , we evaluate support vector machines ( SVM ) for their popularity in suicide research ( Zhu et al , 2020 ; Pestian et al , 2020 ; O'Dea et al , 2015 ) .", "entities": [[8, 9, "MethodName", "SVM"]]}
{"text": "SVM with rbf kernel proves to be successful but requires costly parameter tuning , while linear SVM ( lSVM ) shows success on withindataset evaluations with less cost .", "entities": [[0, 1, "MethodName", "SVM"], [16, 17, "MethodName", "SVM"]]}
{"text": "To convert them to probabilities , we apply probability calibration with logistic regression ( CalibratedClassifierCV ) .", "entities": [[11, 13, "MethodName", "logistic regression"]]}
{"text": "Feature selection : Following the ML method selections , we evaluate the effect of feature selection on ML performance .", "entities": [[0, 2, "MethodName", "Feature selection"], [14, 16, "MethodName", "feature selection"]]}
{"text": "To compute feature importance scores , we also use the LR .", "entities": [[2, 4, "TaskName", "feature importance"]]}
{"text": "The feature selection results of the selected ML methods for two subtasks are in Figure 2 .", "entities": [[1, 3, "MethodName", "feature selection"]]}
{"text": "Long Short Term Memory ( LSTM )", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "( Gers et al , 1999 ) , and Gated Recurrent Unit ( GRU ) known for overcoming vanishing and exploding gradient problems faced by vanilla RNNs during training ( Cho et al , 2014 ) .", "entities": [[9, 12, "MethodName", "Gated Recurrent Unit"], [13, 14, "MethodName", "GRU"]]}
{"text": "After assessing various configurations of both architectures , we settle on a multi - layer bi - directional GRU with the following characteristics : embedding dimen - sion=256 , number of layers=2 , batch size=32 .", "entities": [[18, 19, "MethodName", "GRU"]]}
{"text": "We call this model GRU - Bert .", "entities": [[4, 5, "MethodName", "GRU"]]}
{"text": "We include a drop - out to regularise learning and a fully connected layer with a Sigmoid activation to produce the classification for each tweet .", "entities": [[16, 18, "MethodName", "Sigmoid activation"]]}
{"text": "Meanwhile , GRU - Bert and MNB return the lowest false positive rates ( FPR ) for this subtask , which might be a critical rate to consider in real - life applications in social media domains .", "entities": [[2, 3, "MethodName", "GRU"]]}
{"text": "Based on the LOO results , we select three different methods we were allowed to submit for the evaluation of the test set : LR , wEns , and GRU - Bert .", "entities": [[29, 30, "MethodName", "GRU"]]}
{"text": "We choose LR and wEns for their high performance on LOO experiments , while we select GRU - Bert for measuring how a DL method would generalize over the test sets .", "entities": [[16, 17, "MethodName", "GRU"]]}
{"text": "The baseline classifier provided by the organizers is also a logistic regression .", "entities": [[10, 12, "MethodName", "logistic regression"]]}
{"text": "In subtask 1 , the test set results show that feature selection can considerably enhance the performance of ML models compared to the baseline .", "entities": [[10, 12, "MethodName", "feature selection"]]}
{"text": "Meanwhile , though the baseline of CLPsych2021 is the same as our LR , our additional MV and feature selection together enable LR to substantially outperform the baseline .", "entities": [[18, 20, "MethodName", "feature selection"]]}
{"text": "Future work on DL will include deeper , more complex , and noise immune methods that could integrate Convolutional neural networks ( CNN ) , deeper LSTM or GRU layers , and experiments with various word embedding models .", "entities": [[26, 27, "MethodName", "LSTM"], [28, 29, "MethodName", "GRU"]]}
{"text": "Inspired by real - life applications , we focus on assessing suicide risk on the tweet level .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Preface : General Chair", "entities": [[2, 3, "DatasetName", "General"]]}
{"text": "Our full program committee of BUG hard - working individuals who reviewed the conference 's 1 , 318 submissions ( including secondary reviewers ) .", "entities": [[5, 6, "DatasetName", "BUG"]]}
{"text": "Adversarial Multi - Criteria Learning for Chinese Word Segmentation .", "entities": [[6, 9, "TaskName", "Chinese Word Segmentation"]]}
{"text": "The computational linguistics and natural language processing community is experiencing an episode of deep fascination with representation learning .", "entities": [[16, 18, "TaskName", "representation learning"]]}
{"text": "Like many other presenters at this conference , I will describe new ways to use representation learning in models of natural language .", "entities": [[15, 17, "TaskName", "representation learning"]]}
{"text": "I will focus on three examples , text simplification , source code generation , and movie summarization .", "entities": [[7, 9, "TaskName", "text simplification"], [11, 13, "TaskName", "code generation"], [16, 17, "TaskName", "summarization"]]}
{"text": "She was the first recipient of the Karen Sparck Jones award of the British Computer Society , recognizing key contributions to NLP and information retrieval .", "entities": [[23, 25, "TaskName", "information retrieval"]]}
{"text": "Pretrained language models have served as the backbone for many state - of - the - art NLP results .", "entities": [[0, 3, "TaskName", "Pretrained language models"]]}
{"text": "We explore alternatives to full - scale task - specific pretraining of language models through the use of adapter modules , a parameter - efficient approach to transfer learning .", "entities": [[27, 29, "TaskName", "transfer learning"]]}
{"text": "We further explore direct use of adapters without pretraining and find that the direct finetuning performs mostly on par with pretrained adapter models , contradicting previously proposed benefits of continual pretraining in full pretraining fine - tuning strategies .", "entities": [[29, 31, "TaskName", "continual pretraining"]]}
{"text": "Pretrained Language Models ( PLM ) are predominant in tackling current Natural Language Processing ( NLP ) tasks .", "entities": [[0, 3, "TaskName", "Pretrained Language Models"]]}
{"text": "Most PLMs based on the Transformer architecture ( Vaswani et al , 2017 ) are first trained on massive text corpora with the selfsupervised objective to learn word representations ( Devlin et al , 2019 ; Liu et al , 2019 ) , and then are fine - tuned for a specific target task .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "Inspired by the benefits of pretraining , there have been studies demonstrate the effects of continued pretraining on the domain of a target task or the target task dataset ( Mitra et al , 2020 ; Han and Eisenstein , 2019 ; Gururangan et al , 2020 ) .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Gururangan et al , 2020 adapt PLMs on the target task by further pretraining RoBERTa ( Liu et al , 2019 ) on the target text corpus before it is fine - tuned for the corresponding task and showed that this task adaptation consistently improves the performance for text classification tasks .", "entities": [[14, 15, "MethodName", "RoBERTa"], [48, 50, "TaskName", "text classification"]]}
{"text": "Finetuning with adapters mostly matches the performance of those with the full fine - tuning strategy on many NLP tasks including GLUE benchmark ( Wang et al , 2018 ) and reduces the size of the model from 100s of MB to the order of MB ( Pfeiffer et al , 2020b ) .", "entities": [[21, 22, "DatasetName", "GLUE"]]}
{"text": "We only train the adapter modules in the second phase of pretraining as well as the fine - tuning stage to achieve both parameter efficiency and the benefits of continual pretraining and compare those with the adapter - based model without pretraining .", "entities": [[29, 31, "TaskName", "continual pretraining"]]}
{"text": "Surprisingly , we find that directly fine - tuning adapters performs mostly on par with the pre - trained adapter model and outperforms the full TAPT , contradicting the previously proposed benefits of continual pretraining in the full pretraining fine - tuning scheme .", "entities": [[33, 35, "TaskName", "continual pretraining"]]}
{"text": "Pre - trained language model We use RoBERTa ( Liu et al , 2019 ) , a Transformer - based language model that is pre - trained on a massive text corpus , following Gururangan et al , 2020 .", "entities": [[7, 8, "MethodName", "RoBERTa"], [17, 18, "MethodName", "Transformer"]]}
{"text": "RoBERTa is an extension of BERT ( Devlin et al , 2019 ) with optimized hyperparameters and a modification of the pretraining objective , which excludes next sentence prediction and only uses the randomly masked tokens in the input sentence .", "entities": [[0, 1, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}
{"text": "To evaluate the performance of RoBERTa on a certain task , a classification layer is appended on top of the language model after the pretraining and all the parameters in RoBERTa are trained in a supervised way using the label of the dataset .", "entities": [[5, 6, "MethodName", "RoBERTa"], [30, 31, "MethodName", "RoBERTa"]]}
{"text": "In this paper , training word representations using RoBERTa on a masked language modeling task will be referred to as pretraining .", "entities": [[8, 9, "MethodName", "RoBERTa"], [11, 14, "TaskName", "masked language modeling"]]}
{"text": "Although RoBERTa achieves strong performance by simply fine - tuning the PLMs on a target task , there can be a distributional mismatch between the pretraining and target corpora .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}
{"text": "In TAPT , the second phase of pretraining is per - Figure 1 : The adapter achitecture in the Transformer layer ( Pfeiffer et al , 2020a ) formed with RoBERTa using the unlabeled text corpus of the target task , and then it is fine - tuned on the target task .", "entities": [[19, 20, "MethodName", "Transformer"], [30, 31, "MethodName", "RoBERTa"]]}
{"text": "Adapter Adapter modules have been employed as a feature extractor in computer vision ( Rebuffi et al , 2017 ) and have been recently adopted in the NLP literature as an alternative approach to fully fine - tuning PLMs .", "entities": [[0, 1, "MethodName", "Adapter"], [1, 2, "MethodName", "Adapter"]]}
{"text": "Pfeiffer et al , 2020c use two types of adapter : language - specific adapters and taskspecific adapters for cross - lingual transfer .", "entities": [[19, 23, "TaskName", "cross - lingual transfer"]]}
{"text": "However , the language adapters involve invertible adapters after the embedding layer to capture token - level language representation when those are trained via masked language modeling in the pretraining stage , whereas the task adapters are simply embedded in each transformer layer and trained in the fine - tuning stage to learn the task representation .", "entities": [[24, 27, "TaskName", "masked language modeling"]]}
{"text": "However , we perform fine - tuning pre - trained parameters of the language adapter modules for evaluation to align with ( Maas et al , 2011 ) ) and low - resource ( CHEMPROT ( Kringelum et al , 2016 ) , ACL - ARC ( Jurgens et al , 2018 ) , SCIERC ( Luan et", "entities": [[34, 35, "DatasetName", "CHEMPROT"], [43, 46, "DatasetName", "ACL - ARC"], [54, 55, "DatasetName", "SCIERC"]]}
{"text": "For pretraining adapters , we added the adapter module in each transformer layer of RoBERTa using adaptertransformer ( Pfeiffer et al , 2020b ) 1 and continued pretraining all the weights in adapter layers on target text corpus while keeping the original parameters in RoBERTa fixed .", "entities": [[14, 15, "MethodName", "RoBERTa"], [44, 45, "MethodName", "RoBERTa"]]}
{"text": "After finishing the second phase of pretraining , we performed fine - tuning of RoBERTa by training the weights in the adapters and the final classification layers while keeping all of the parameters in RoBERTa frozen .", "entities": [[14, 15, "MethodName", "RoBERTa"], [34, 35, "MethodName", "RoBERTa"]]}
{"text": "We covered news and review texts that are similar to the pretraining corpus of RoBERTa as well as scientific domains in which text corpora can have largely different distributions from those of RoBERTa .", "entities": [[14, 15, "MethodName", "RoBERTa"], [32, 33, "MethodName", "RoBERTa"]]}
{"text": "For TAPT , we train the entire parameters of the RoBERTa via masked language modeling ( MLM ) on the target dataset , whereas for the adapter - based model , we embed the language adapters in each transformer layer and add invertible adapters after the embedding layers to perform MLM while freezing the original parameters of RoBERTa , following Pfeiffer et al , 2020c .", "entities": [[10, 11, "MethodName", "RoBERTa"], [12, 15, "TaskName", "masked language modeling"], [16, 17, "DatasetName", "MLM"], [50, 51, "DatasetName", "MLM"], [57, 58, "MethodName", "RoBERTa"]]}
{"text": "We perform fine - tuning parameters that are pretrained via MLM for both TAPT and the adapter model .", "entities": [[10, 11, "DatasetName", "MLM"]]}
{"text": "First , we reproduce the performance of RoBERTa and TAPT in Gururangan et al , 2020 as presented in Appendix C. Then we proceed to the adapter - based approach .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}
{"text": "To investigate the benefits of task - adaptive pretraining with adapters , we compare the performance of the pre - trained adapter model with the model without pretraining , i.e. , directly fine - tuning adapters in RoBERTa on the target task .", "entities": [[37, 38, "MethodName", "RoBERTa"]]}
{"text": "Since the weights of the adapters are randomly initialized , we empirically found that a larger learning rate worked well compared to the full fine - tuning experiments .", "entities": [[16, 18, "HyperparameterName", "learning rate"]]}
{"text": "Surprisingly , for the average F 1 score , the adapter - based model without task - adaptive pretraining performs best , followed by the other adapter with the pretraining model , TAPT , and the baseline RoBERTa .", "entities": [[37, 38, "MethodName", "RoBERTa"]]}
{"text": "F 1 score is averaged over 5 random seeds for low - resource tasks ( CHEMPROT , ACL - ARC , SCIERC , HYPER ) due to the high variance .", "entities": [[8, 9, "DatasetName", "seeds"], [15, 16, "DatasetName", "CHEMPROT"], [17, 20, "DatasetName", "ACL - ARC"], [21, 22, "DatasetName", "SCIERC"]]}
{"text": "For high - resource tasks ( RCT , AGNEWS , HELPFULNESS , IMDB ) , we report the F 1 score from a single random seed for each task .", "entities": [[12, 13, "DatasetName", "IMDB"]]}
{"text": "In contrast , TAPT outperforms baseline RoBERTa from 2e - 5 , where both TAPT and baseline RoBERTa perform best .", "entities": [[6, 7, "MethodName", "RoBERTa"], [17, 18, "MethodName", "RoBERTa"]]}
{"text": "Inspired by the results of the adapter models , we perform the same experiments for the full model ( baseline RoBERTa and TAPT ) on our implementation by sweeping the learning rates and the number of epochs .", "entities": [[0, 1, "DatasetName", "Inspired"], [20, 21, "MethodName", "RoBERTa"], [34, 37, "HyperparameterName", "number of epochs"]]}
{"text": "We hypothesize that proper hyperparameter settings such as a larger learning rate or increasing the number of training steps in the fine - tuning stage can improve the performance of baseline RoBERTa , making pretraining on the unlabeled target task less effective .", "entities": [[10, 12, "HyperparameterName", "learning rate"], [31, 32, "MethodName", "RoBERTa"]]}
{"text": "The average F 1 score of baseline RoBERTa greatly increases and surprisingly , it surpasses the performance of TAPT in some tasks .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}
{"text": "The results ensure that although pretraining PLMs on the target task results in better performance , one can achieve comparable performance by simply using a larger learning rate or increasing training steps in the fine - tuning stage while skipping the pretraining step that is computationally demanding compared to the fine - tuning .", "entities": [[26, 28, "HyperparameterName", "learning rate"]]}
{"text": "Character - level convolutional networks for text classification .", "entities": [[6, 8, "TaskName", "text classification"]]}
{"text": "Each score is averaged over 5 random seeds .", "entities": [[7, 8, "DatasetName", "seeds"]]}
{"text": "F 1 score is averaged over 5 random seeds for low - resource tasks ( CHEMPROT , ACL - ARC , SCIERC , HYPER ) due to the high variance .", "entities": [[8, 9, "DatasetName", "seeds"], [15, 16, "DatasetName", "CHEMPROT"], [17, 20, "DatasetName", "ACL - ARC"], [21, 22, "DatasetName", "SCIERC"]]}
{"text": "For high - resource tasks ( RCT , AGNEWS , HELPFULNESS , IMDB ) , we report the F 1 score from a single random seed for each task .", "entities": [[12, 13, "DatasetName", "IMDB"]]}
{"text": "Comparing CRF and LSTM performance on the task of morphosyntactic tagging of non - standard varieties of South Slavic languages", "entities": [[1, 2, "MethodName", "CRF"], [3, 4, "MethodName", "LSTM"]]}
{"text": "While one system relies on the traditional method for sequence labeling ( conditional random fields ) , the other relies on its neural alternative ( bidirectional long short - term memory ) .", "entities": [[26, 31, "MethodName", "long short - term memory"]]}
{"text": "While the first system , JANES , relies on the traditional method for sequence labeling , namely conditional random fields ( CRF ) , the second system , JSI , relies on the currently hugely popular neural networks , more precisely bidirectional long short - term memories ( BiLSTM ) .", "entities": [[21, 22, "MethodName", "CRF"], [48, 49, "MethodName", "BiLSTM"]]}
{"text": "The JSI system ( the name comes from the name of our current employer , the Jo\u017eef Stefan Institute ) is an adaptation of the BiLSTM tagger written in pytorch 2 , with some added modifications .", "entities": [[25, 26, "MethodName", "BiLSTM"]]}
{"text": "The architecture of the submitted system is the following : a character - level subnetwork , consisting of a character embedding layer of 16 dimensions and a BiLSTM layer with 25 units the main network concatenating the character - level representation of a word from the subnetwork described above ( 25 * 2 , i.e. , 50 dimensions ) , and the word embedding layer ( 100 dimensions ) feeding this concatenated 150 - dimensional character - and word - level representation into a BiL - STM layer with 100 units the per - token BiLSTM output being fed to a fully - connected layer with 256 units and a final softmax layer for prediction While developing this architecture , we investigated the impact of various setups on the Slovene dataset .", "entities": [[27, 28, "MethodName", "BiLSTM"], [95, 96, "MethodName", "BiLSTM"], [111, 112, "MethodName", "softmax"]]}
{"text": "The results of experimenting with ( 1 ) different pretrained word embeddings , ( 2 ) the impact of adding different character - level representations , ( 3 ) fine - tuning the model on in - domain data and ( 4 ) pretraining the character - level encoder on a inflectional lexicon , are shown in Table 2 .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "The first group of results considers different ways of pretraining word embeddings .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "The word embeddings were always pretrained on the web data available for each language .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "We considered only two tools for pretraining word embeddings : word2vec ( Mikolov et al , 2013 ) and fasttext ( Bojanowski et al , 2017 ) , and two architectures , CBOW and Skipgram .", "entities": [[7, 9, "TaskName", "word embeddings"], [19, 20, "MethodName", "fasttext"]]}
{"text": "Comparing word2vec and fasttext ( word2vec skipgram vs. fasttext skipgram ) , fasttext shows a slightly better performance , but the difference gets more obvious ( almost half a point in token accuracy ) once fasttext is used to generate representations for the words not present in the pretrained word embeddings ( fasttext skipgram generated ) .", "entities": [[3, 4, "MethodName", "fasttext"], [8, 9, "MethodName", "fasttext"], [12, 13, "MethodName", "fasttext"], [32, 33, "MetricName", "accuracy"], [35, 36, "MethodName", "fasttext"], [49, 51, "TaskName", "word embeddings"], [52, 53, "MethodName", "fasttext"]]}
{"text": "skipgram 0.8550 \u00b1 0.0041 fasttext skipgram 0.8578 \u00b1 0.0041 fasttext skipgram generated 0.8596 \u00b1 0.0031 added character - level encoding 0.8780 \u00b1 0.0030 added bidirectional encoding 0.8790 \u00b1 0.0032 additionally tuned on in - domain data 0.8836 \u00b1 0.0026 pretrained character - level encoder on web data 0.8855 \u00b1 0.0015 Table 2 : Initial experiments on the JSI system , performed on the Slovene dataset .", "entities": [[4, 5, "MethodName", "fasttext"], [9, 10, "MethodName", "fasttext"]]}
{"text": "The second group of experiments considers the impact of adding character - level representations of each token to the word representation via a dedicated character - level BiLSTM .", "entities": [[27, 28, "MethodName", "BiLSTM"]]}
{"text": "Running three epochs on the concatenation of all datasets , and then additional two epochs only on the in - domain Twitter data , consistently improved the results for around half an accuracy point .", "entities": [[32, 33, "MetricName", "accuracy"]]}
{"text": "In this shared task the web data were automatically tagged with a CRF tagger relying on a lexicon Ljube\u0161i\u0107 and Erjavec , 2016 ) , therefore we transformed the automatically - tagged web data into a lexicon by ( 1 ) picking only token - tag pairs occurring at least 100 times in the web data and ( 2 ) selecting only the most frequent token - tag pair per token .", "entities": [[12, 13, "MethodName", "CRF"]]}
{"text": "The results on pretraining the character - level encoder show that the improvement lies below half an accuracy point , but this improvement showed to be consistent across all the three languages .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "We compare to this system as it is very straightforward to add information from an inflectional lexicon as additional features to a CRF - based system .", "entities": [[22, 23, "MethodName", "CRF"]]}
{"text": "Reported metric is token - level accuracy .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Namely , we removed the fully connected layer between the main BiLSTM and the softmax layer , which is actually the most frequent setup for sequence labeling .", "entities": [[11, 12, "MethodName", "BiLSTM"], [14, 15, "MethodName", "softmax"]]}
{"text": "When comparing the most frequent errors between the two systems , the JANES CRF - based system seems to have more problems with the traditional discrimination between different context - dependent cases of nouns , which points to the direction that BiLSTMs are better at modeling long - range dependencies as discriminating between the nominative and the accusative case often requires a very wide context .", "entities": [[13, 14, "MethodName", "CRF"]]}
{"text": "On the other hand , what the BiLSTM system seems to be worse at is discriminating between different cases for prepositions , which heavily depends on the following adjective or noun .", "entities": [[7, 8, "MethodName", "BiLSTM"]]}
{"text": "While confusing an accusative preposition ( Sa ) for a locative one ( Sl ) the BiLSTM system did 23 times , this happened to the CRF system 17 times .", "entities": [[16, 17, "MethodName", "BiLSTM"], [26, 27, "MethodName", "CRF"]]}
{"text": "In the opposite direction , the BiLSTM system did 19 mistakes while the CRF system did one mistake less , namely 18 of them .", "entities": [[6, 7, "MethodName", "BiLSTM"], [13, 14, "MethodName", "CRF"]]}
{"text": "We have shown that CRFs with well defined features come very close to the performance of the stronger BiLSTM models , the difference between those two being bigger as the data are more nonstandard .", "entities": [[18, 19, "MethodName", "BiLSTM"]]}
{"text": "For the CRF system , we have shown that using contextual , suffixal and distributional features gives very good results .", "entities": [[2, 3, "MethodName", "CRF"]]}
{"text": "The latter make an inflectional lexicon mostly obsolete , with just minor improvements in accuracy if features from large inflectional lexicons are added .", "entities": [[14, 15, "MetricName", "accuracy"]]}
{"text": "For the BiLSTM system , we have shown that encoding a character - level representation of a word is the single most useful intervention , with minor improvements obtained through proper word embedding pretraining , fine - tuning on in - domain data and pretraining the character - level encoder on pairs of words and MSD tags from a large automatically tagged web corpus .", "entities": [[2, 3, "MethodName", "BiLSTM"], [55, 56, "DatasetName", "MSD"]]}
{"text": "An Empirical Investigation of Word Alignment Supervision for Zero - Shot Multilingual Neural Machine Translation", "entities": [[4, 6, "TaskName", "Word Alignment"], [13, 15, "TaskName", "Machine Translation"]]}
{"text": "Zero - shot translations is a fascinating feature of Multilingual Neural Machine Translation ( MNMT ) systems .", "entities": [[11, 13, "TaskName", "Machine Translation"]]}
{"text": "In this paper , we investigate the benefits of an explicit alignment to language labels in Transformer - based MNMT models in the zero - shot context , by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label .", "entities": [[16, 17, "MethodName", "Transformer"], [36, 38, "TaskName", "word alignment"]]}
{"text": "Multilingual Neural Machine Translation ( MNMT ) focuses on translation between multiple language pairs through a single optimized neural model , and has been explored from different angles witnessing a rapid progress in recent years ( Arivazhagan et al , 2019b ; Dabre et al , 2020 ; Lin et al , 2021 ) .", "entities": [[2, 4, "TaskName", "Machine Translation"]]}
{"text": "We study three different settings : ( a ) standard word alignment between corresponding words , ( b ) alignments between all target words and the language label in the input string , and ( c ) the union between the former two .", "entities": [[10, 12, "TaskName", "word alignment"]]}
{"text": "To produce word alignments between parallel sentences , i.e. , Figure 1 ( a ) , we use the awesome - align tool ( Dou and Neubig , 2021 ) , a recent work that leverages multilingual BERT ( Devlin et al , 2019 ) to extract the links .", "entities": [[37, 38, "MethodName", "BERT"]]}
{"text": "To train Many - to - Many MNMT models , we use a 6 - layer Transformer architecture ( Vaswani et al , 2017 ) , prepending a language label in the input to indicate the target language ( Johnson et al , 2017 ) .", "entities": [[16, 17, "MethodName", "Transformer"]]}
{"text": "As there are no zero - shot test sets provided by the competition , we use the test portion from the Tatoeba - challenge ( Tiedemann , 2020 ) , 4 in all possible language pair combinations included in the challenge .", "entities": [[21, 22, "DatasetName", "Tatoeba"]]}
{"text": "Following related work ( Aharoni et al , 2019 ; , we apply joint Byte - Pair Encoding ( BPE ) segmentation ( Sennrich et al , 2016 ; Kudo and Richardson , 2018 ) , with a shared vocabulary size of 32 K symbols for TED Talks and 64 K for WMT - 2018 and OPUS - 100 .", "entities": [[19, 20, "MethodName", "BPE"]]}
{"text": "As evaluation measure , we use tokenized BLEU", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "We use fasttext as a language identification tool ( Joulin et al , 2017 ) , counting how many times the translation language matches the reference target language .", "entities": [[2, 3, "MethodName", "fasttext"], [5, 7, "TaskName", "language identification"]]}
{"text": "The Transformer models follow the base setting of Vaswani et", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "al ( 2017 ) , with three different random seeds in each run .", "entities": [[9, 10, "DatasetName", "seeds"]]}
{"text": "Scores for each language pair are available in the supplementary material .", "entities": [[9, 11, "DatasetName", "supplementary material"]]}
{"text": "Regarding translation quality on the language pairs seen during training ( EN X and X EN columns ) , average BLEU scores from all models end up in the same ballpark .", "entities": [[20, 21, "MetricName", "BLEU"]]}
{"text": "In this work we present an empirical comparative evaluation of integrating different alignment methods in Transformer - based models for highly multilingual English - centric MT setups .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "We use training and testing data as provided by the WMT 2018 news translation task organizers .", "entities": [[10, 13, "DatasetName", "WMT 2018 news"]]}
{"text": "The benchmark contains a total of 14 language pairs : { Chinese , Czech , Estonian , Finnish , German , Russian , Turkish } \u2194English .", "entities": [[0, 2, "DatasetName", "The benchmark"]]}
{"text": "We use the OpenNMT - py framework ( Klein et al , 2017 ) , and the Transformer base model setting ( Vaswani et al , 2017 ) .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "Specifically , we use 6 layers for the encoder and the decoder , 512 as model dimension , and 2048 as hidden dimension .", "entities": [[19, 20, "DatasetName", "2048"]]}
{"text": "We train the models with three random seeds each , for 200 K training steps for the TED Talks and WMT - 2018 benchmarks , while for 500 K training steps for the OPUS - 100 .", "entities": [[7, 8, "DatasetName", "seeds"]]}
{"text": "We experiment with pretraining a BERT - based question - answering model on different QA datasets from MRQA , as well as conversational QA datasets like CoQA and QuAC .", "entities": [[5, 6, "MethodName", "BERT"], [17, 18, "DatasetName", "MRQA"], [26, 27, "DatasetName", "CoQA"], [28, 29, "DatasetName", "QuAC"]]}
{"text": "Our results show that models pretrained on CoQA and QuAC perform better than their counterparts that are pretrained on MRQA datasets .", "entities": [[7, 8, "DatasetName", "CoQA"], [9, 10, "DatasetName", "QuAC"], [19, 20, "DatasetName", "MRQA"]]}
{"text": "Question Answering ( QA ) involves constructing an answer for a given question in either an extractive or an abstractive manner .", "entities": [[0, 2, "TaskName", "Question Answering"]]}
{"text": "Recently , QA based solutions have also been proposed to evaluate factuality ( Wang et al , 2020 ) and faithfulness ( Durmus et al , 2020 ) of abstractive summarization systems .", "entities": [[30, 31, "TaskName", "summarization"]]}
{"text": "In addition to popular QA benchmarks like SQuAD ( Rajpurkar et al , 2016 ) , and MRQA - 2019 ( Fisch et al , 2019 ) , we have seen QA challenges that require reasoning over human dialogue .", "entities": [[7, 8, "DatasetName", "SQuAD"], [17, 18, "DatasetName", "MRQA"]]}
{"text": "Some notable examples being QuAC ( Choi et al , 2018 ) and CoQA ( Reddy et al , 2019 ) .", "entities": [[4, 5, "DatasetName", "QuAC"], [13, 14, "DatasetName", "CoQA"]]}
{"text": "Feng et al ( 2020 ) introduced a new dataset of goal - oriented dialogues ( Doc2Dial ) that are grounded in the associated documents .", "entities": [[16, 17, "DatasetName", "Doc2Dial"]]}
{"text": "Each sample in the dataset consists of an information - seeking conversation between a user and an agent where agent 's responses are grounded in FAQ - like webpages .", "entities": [[17, 18, "DatasetName", "agent"], [19, 20, "DatasetName", "agent"]]}
{"text": "DialDoc shared task derives its training data from the Doc2Dial dataset and proposes two subtasks which require the participants to ( 1 ) identify the grounding knowledge in form of document span for the next agent turn ; and ( 2 ) generate the next agent response in natural language .", "entities": [[9, 10, "DatasetName", "Doc2Dial"], [35, 36, "DatasetName", "agent"], [45, 46, "DatasetName", "agent"]]}
{"text": "We pretrain our model on different QA datasets like SQuAD , different subsets of MRQA - 2019 training set , and conversational QA datasets like CoQA and QuAC .", "entities": [[9, 10, "DatasetName", "SQuAD"], [14, 15, "DatasetName", "MRQA"], [25, 26, "DatasetName", "CoQA"], [27, 28, "DatasetName", "QuAC"]]}
{"text": "Our experiments suggest that conversational QA datasets are more useful than MRQA - 2019 data or its subsets .", "entities": [[11, 12, "DatasetName", "MRQA"]]}
{"text": "Dataset used in the DialDoc shared - task is derived from Doc2Dial dataset ( Feng et al , 2020 ) , a new dataset with goal - oriented document - grounded dialogue .", "entities": [[11, 12, "DatasetName", "Doc2Dial"]]}
{"text": "It includes a set of documents and conversations between a user and an agent grounded in the associated document .", "entities": [[13, 14, "DatasetName", "agent"]]}
{"text": "Pre - processing Using the pre - processing scripts provided by the task organizers , we converted the Doc2Dial dataset into SQuAD v2.0 format with questions containing the latest user utterance as well as all previous turns in the conversation .", "entities": [[18, 19, "DatasetName", "Doc2Dial"], [21, 22, "DatasetName", "SQuAD"]]}
{"text": "To extract the beginning and the ending positions of the answer span within the document , the encoded embeddings are sent to a linear layer to output two logits that correspond to the probability of the position being the start and end position of the answer span .", "entities": [[23, 25, "MethodName", "linear layer"]]}
{"text": "The training loss is computed using the Cross - Entropy loss function .", "entities": [[2, 3, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}
{"text": "Motivated by this , we experimented with further pretraining the QA model on different out - of - domain QA datasets to gauge its benefits on Doc2Dial ( Table 1 ) .", "entities": [[26, 27, "DatasetName", "Doc2Dial"]]}
{"text": "Firstly , we briefly describe the different datasets used for the continual pretraining of our transformer - based QA models .", "entities": [[11, 13, "TaskName", "continual pretraining"]]}
{"text": "MRQA - 19 Shared task (", "entities": [[0, 1, "DatasetName", "MRQA"]]}
{"text": "Fisch et al , 2019 ) CoQA ( Reddy et al , 2019 ) .", "entities": [[6, 7, "DatasetName", "CoQA"]]}
{"text": "To compute these scores , we use the metrics for SQuAD from huggingface .", "entities": [[10, 11, "DatasetName", "SQuAD"]]}
{"text": "Datasets like SQuAD , NewsQA , and NaturalQuestions are more useful than SearchQA , and Trivi - aQA .", "entities": [[2, 3, "DatasetName", "SQuAD"], [4, 5, "DatasetName", "NewsQA"], [12, 13, "DatasetName", "SearchQA"]]}
{"text": "However , pretraining on complete MRQA - 2019 training set does not outperform the individual datasets suggesting that merely introducing more pretraining data might not result in improved performance .", "entities": [[5, 6, "DatasetName", "MRQA"]]}
{"text": "Furthermore , conversational QA datasets like CoQA and QuAC , which are more similar in their setup to DialDoc , perform substantially better than any of the other MRQA - 2019 training datasets .", "entities": [[6, 7, "DatasetName", "CoQA"], [8, 9, "DatasetName", "QuAC"], [28, 29, "DatasetName", "MRQA"]]}
{"text": "Models pretrained on QuAC or CoQA outperform those pretrained on SQuAD .", "entities": [[3, 4, "DatasetName", "QuAC"], [5, 6, "DatasetName", "CoQA"], [10, 11, "DatasetName", "SQuAD"]]}
{"text": "However , combining CoQA and QuAC during pretraining does not seem to help with the performance on validation or testdev split .", "entities": [[3, 4, "DatasetName", "CoQA"], [5, 6, "DatasetName", "QuAC"]]}
{"text": "Analyzing Different Transformer Variants Table 3 also contains the results for experiments where albert - xl is used to encode the questioncontext pair .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "Rank Score : We chose the answer span which was the highest average rank score .", "entities": [[1, 2, "MetricName", "Score"]]}
{"text": "Probability Score : We chose the answer span which was the highest average probability score .", "entities": [[1, 2, "MetricName", "Score"]]}
{"text": "We investigate the disparate impact of pretraining on different MRQA - 19 datasets on the Doc2Dial shared task .", "entities": [[9, 10, "DatasetName", "MRQA"], [15, 16, "DatasetName", "Doc2Dial"]]}
{"text": "We observe that the SQuAD , NewsQA , and NaturalQuestions ( NQ ) has compartaively longer answers than the other datasets .", "entities": [[4, 5, "DatasetName", "SQuAD"], [6, 7, "DatasetName", "NewsQA"], [11, 12, "DatasetName", "NQ"]]}
{"text": "However , we do not observe a noticeable difference in terms of question length , context length or relative position of the answer in the context , with respect to the other datasets .", "entities": [[15, 17, "HyperparameterName", "context length"]]}
{"text": "We use the classifier to gauge the distribution of answer types on MRQA datasets and Doc2Dial .", "entities": [[12, 13, "DatasetName", "MRQA"], [15, 16, "DatasetName", "Doc2Dial"]]}
{"text": "We observe from Figure 2 that a majority of questions in Doc2Dial require a descriptive answer .", "entities": [[11, 12, "DatasetName", "Doc2Dial"]]}
{"text": "These DESC type questions are more prevelant in SQuAD , NewsQA , and NQ , which might explain their efficacy .", "entities": [[8, 9, "DatasetName", "SQuAD"], [10, 11, "DatasetName", "NewsQA"], [13, 14, "DatasetName", "NQ"]]}
{"text": "To ascertain the benefit of intelligent sampling , we pretrain on a much smaller subset of the SQuAD , NewsQA , and NaturalQuestions dataset , which we obtain via intelligent sampling .", "entities": [[17, 18, "DatasetName", "SQuAD"], [19, 20, "DatasetName", "NewsQA"]]}
{"text": "Our submission to the DialDoc subtask 1 performs continual pretraining of a transformer - based encoder on out - of - domain QA datasets .", "entities": [[8, 10, "TaskName", "continual pretraining"]]}
{"text": "Experiments with different QA datasets suggest that conversational QA datasets like CoQA and QuAC are highly beneficial as their setup is substantially similar to Doc2Dial , the downstream dataset of interest .", "entities": [[11, 12, "DatasetName", "CoQA"], [13, 14, "DatasetName", "QuAC"], [24, 25, "DatasetName", "Doc2Dial"]]}
{"text": "In this work , we tackle the task of question answering ( QA ) for English language text .", "entities": [[9, 11, "TaskName", "question answering"]]}
{"text": "P - Stance : A Large Dataset for Stance Detection in Political Domain", "entities": [[8, 10, "TaskName", "Stance Detection"]]}
{"text": "Stance detection determines whether the author of a text is in favor of , against or neutral to a specific target and provides valuable insights into important events such as presidential election .", "entities": [[0, 2, "TaskName", "Stance detection"]]}
{"text": "However , progress on stance detection has been hampered by the absence of large annotated datasets .", "entities": [[4, 6, "TaskName", "stance detection"]]}
{"text": "In this paper , we present P - STANCE , a large stance detection dataset in the political domain , which contains 21 , 574 labeled tweets .", "entities": [[12, 14, "TaskName", "stance detection"]]}
{"text": "Moreover , our P - STANCE dataset can facilitate research in the fields of cross - domain stance detection such as cross - target stance detection where a classifier is adapted from a different but related target .", "entities": [[17, 19, "TaskName", "stance detection"], [24, 26, "TaskName", "stance detection"]]}
{"text": "The goal of the stance detection task is to determine whether the author of a piece of text is in favor of , against , or neutral toward a specific target ( Mohammad et al , 2016b ; K\u00fc\u00e7\u00fck and Can , 2020 ; ALDayel and Magdy , 2021 ) .", "entities": [[4, 6, "TaskName", "stance detection"]]}
{"text": "Political figures , who usually receive considerable attention and involve themselves in a large number of political events , are great targets to study stance detection .", "entities": [[24, 26, "TaskName", "stance detection"]]}
{"text": "Even though stance detection has received a lot of attention , the annotated data are usually limited , which poses strong challenges to supervised models .", "entities": [[2, 4, "TaskName", "stance detection"]]}
{"text": "In an effort to minimize these drawbacks , we present P - STANCE , a dataset for stance detection whose primary goal is to bridge these gaps by making it possible to run large - scale evaluations that require a deeper semantic understanding .", "entities": [[17, 19, "TaskName", "stance detection"]]}
{"text": "The main motivation of building this dataset is to provide a new benchmark for in - target stance detection where a classifier is trained and validated on the same target .", "entities": [[17, 19, "TaskName", "stance detection"]]}
{"text": "However , we show additional interest in constructing a large corpus to facilitate research on cross - target stance detection where a classifier is adapted from different but related target .", "entities": [[18, 20, "TaskName", "stance detection"]]}
{"text": "More interestingly , P - Stance enables a new task in stance detection , which is cross - topic stance detection where a classifier is adapted from the same target but with different topics in the past .", "entities": [[11, 13, "TaskName", "stance detection"], [19, 21, "TaskName", "stance detection"]]}
{"text": "Our contributions include the following : 1 ) We present P - STANCE , a large dataset for stance detection composed of 21 , 574 tweets sampled from over 2.8 million tweets collected from Twitter .", "entities": [[18, 20, "TaskName", "stance detection"]]}
{"text": "The most common stance detection task on social media is target - specific stance detection ( ALDayel and Magdy , 2021 ) which aims to identify the stance toward a set of figures or topics ( Hasan and Ng , 2014 ;", "entities": [[3, 5, "TaskName", "stance detection"], [13, 15, "TaskName", "stance detection"]]}
{"text": "Besides target - specific stance detection , multi - target stance detection Darwish et", "entities": [[4, 6, "TaskName", "stance detection"], [10, 12, "TaskName", "stance detection"]]}
{"text": "al , 2017 ; Li and Caragea , 2021a ) , and claimbased stance detection ( Qazvinian et al , 2011 ; Derczynski et al , 2015 ; Ferreira and Vlachos , 2016 ; Bar - Haim et al , 2017 ; Rao and Pomerleau , 2017 ; Derczynski et al , 2017 ; Gorrell et al , 2019 ) are other popular trends of stance detection .", "entities": [[13, 15, "TaskName", "stance detection"], [22, 23, "DatasetName", "Derczynski"], [48, 49, "DatasetName", "Derczynski"], [65, 67, "TaskName", "stance detection"]]}
{"text": "Multitarget stance detection aims to jointly identify the stance toward two or more targets in the same text .", "entities": [[1, 3, "TaskName", "stance detection"]]}
{"text": "Unlike the target - specific stance detection and multi - target stance detection where the target is usually a prominent figure or topic , in claimbased stance detection the target is a claim , which could be an article headline or a rumor 's post .", "entities": [[5, 7, "TaskName", "stance detection"], [11, 13, "TaskName", "stance detection"], [26, 28, "TaskName", "stance detection"]]}
{"text": "Interestingly , despite substantial progress on stance detection , large - scale annotated datasets are limited .", "entities": [[6, 8, "TaskName", "stance detection"]]}
{"text": "We compare our P - STANCE dataset with some existing stance detection datasets in Table 2 .", "entities": [[10, 12, "TaskName", "stance detection"]]}
{"text": "We can observe that the sizes of existing stance detection datasets are smaller than ours except for the WT - WT dataset ( Conforti et al , 2020b ) in the financial domain .", "entities": [[8, 10, "TaskName", "stance detection"], [18, 21, "DatasetName", "WT - WT"]]}
{"text": "However , the average tweet length of WT - WT is much shorter when compared with our P - STANCE .", "entities": [[7, 10, "DatasetName", "WT - WT"]]}
{"text": "Moreover , more explicit mentions of targets and lexical cues of stance appear in the sentences of WT - WT dataset .", "entities": [[17, 20, "DatasetName", "WT - WT"]]}
{"text": "In our work , we focus on the political domain and our P - STANCE , which contains much longer sentences and less surfacelevel lexical cues , can serve as a new challenging benchmark for stance detection tasks .", "entities": [[35, 37, "TaskName", "stance detection"]]}
{"text": "Different from classifying the stance detection tasks by target type ( i.e. , one specific target , multiple targets , or a claim ) , we can also categorize the stance detection as in - target and cross - target stance detection by the training setting .", "entities": [[4, 6, "TaskName", "stance detection"], [30, 32, "TaskName", "stance detection"], [40, 42, "TaskName", "stance detection"]]}
{"text": "Most previous works focused on the in - target stance detection where a classifier is trained and validated on the same target ( Mohammad et al , 2016b ; Zarrella and Marsh , 2016 ;", "entities": [[9, 11, "TaskName", "stance detection"]]}
{"text": "However , sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets , which motivates the studies of cross - target stance detection ( Augenstein et al , 2016 ;", "entities": [[14, 16, "TaskName", "stance detection"], [35, 37, "TaskName", "stance detection"]]}
{"text": "In this paper , we show that our P - STANCE dataset can be also used to evaluate the model performance of cross - target stance detection and provides opportunities for exploring more crosstarget tasks by interacting with previous SemEval - 2016 ( Mohammad et al , 2016a ) and Multi - Target stance datasets .", "entities": [[25, 27, "TaskName", "stance detection"]]}
{"text": "In addition , P - STANCE enables the exploration of largescale deep learning models including pre - trained language models , e.g. , BERT ( Devlin et al , 2019 ) and BERTweet ( Nguyen et al , 2020 ) .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "We fine - tune the BERT and BERTweet models on our dataset and compare them with other strong baselines .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "In this section , we detail the creation and the particularities of P - STANCE , our large political stance detection dataset composed of 21 , 574 tweets collected during the 2020 U.S. presidential election .", "entities": [[19, 21, "TaskName", "stance detection"]]}
{"text": "We used a set of query hashtags as seeds to collect target - related tweets , which can be categorized as favor hashtags , against hashtags and neutral hashtags ( Mohammad et al , 2016a ) .", "entities": [[8, 9, "DatasetName", "seeds"]]}
{"text": "3 ) We kept only the tweets in English because our goal in this work is to build an English stance detection dataset .", "entities": [[20, 22, "TaskName", "stance detection"]]}
{"text": "We leave multilingual stance detection as future work .", "entities": [[3, 5, "TaskName", "stance detection"]]}
{"text": "We will provide this large - scale repository of tweets ( which we call P - STANCE - EXT ) alongside P - STANCE , in hope that it will spur further research in the field of semisupervised learning for stance detection .", "entities": [[40, 42, "TaskName", "stance detection"]]}
{"text": "A model can detect the stance from these hashtags without extracting effective representations for the meanings of sentences , which makes stance detection easier .", "entities": [[21, 23, "TaskName", "stance detection"]]}
{"text": "2 ) The average length of tweets in previous datasets is short , and there are more explicit mentions of targets and rich sentiment and emotion words that can easily reveal the stance toward the target .", "entities": [[25, 26, "DatasetName", "emotion"]]}
{"text": "These characteristics contribute to making P - STANCE a challenging dataset for stance detection .", "entities": [[12, 14, "TaskName", "stance detection"]]}
{"text": "In this section , we first introduce two benchmark datasets of stance detection in 4.1 .", "entities": [[11, 13, "TaskName", "stance detection"]]}
{"text": "The union of these datasets and our P - STANCE dataset provides opportunities for studying the cross - target stance detection ( 5.2 ) and cross - topic stance detection ( 5.3 ) .", "entities": [[19, 21, "TaskName", "stance detection"], [28, 30, "TaskName", "stance detection"]]}
{"text": "In the next section , we show how to perform various stance detection tasks with the union of these datasets and our P - STANCE dataset .", "entities": [[11, 13, "TaskName", "stance detection"]]}
{"text": "First , the F1 - score of label \" Favor \" and \" Against \" is calculated as follows : F f avor = 2P f avor R f avor P f avor +", "entities": [[3, 6, "MetricName", "F1 - score"]]}
{"text": "BiLSTM ( Schuster and Paliwal , 1997 ) :", "entities": [[0, 1, "MethodName", "BiLSTM"]]}
{"text": "CNN ( Kim , 2014 ) : Similar to BiLSTM , the vanilla CNN only takes tweets as inputs and does not consider the target information .", "entities": [[9, 10, "MethodName", "BiLSTM"]]}
{"text": "TAN is an attention - based LSTM model that extracts target specific features .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "A BiLSTM that uses conditional encoding for stance detection .", "entities": [[1, 2, "MethodName", "BiLSTM"], [7, 9, "TaskName", "stance detection"]]}
{"text": "The target information is first encoded by a BiLSTM , whose hidden representations are then used to initialize another BiLSTM with tweets as inputs .", "entities": [[8, 9, "MethodName", "BiLSTM"], [19, 20, "MethodName", "BiLSTM"]]}
{"text": "BiCE is also a strong baseline for crosstarget stance detection .", "entities": [[8, 10, "TaskName", "stance detection"]]}
{"text": "al , 2018 ) : CrossNet is another model for cross - target stance detection .", "entities": [[13, 15, "TaskName", "stance detection"]]}
{"text": "GCAE is a strong baseline for aspect - based sentiment analysis and we apply it to our stance detection task .", "entities": [[6, 11, "TaskName", "aspect - based sentiment analysis"], [17, 19, "TaskName", "stance detection"]]}
{"text": "BERT ( Devlin et al , 2019 ) :", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "We fine - tune the BERT - base on the stance detection task .", "entities": [[5, 6, "MethodName", "BERT"], [10, 12, "TaskName", "stance detection"]]}
{"text": "( Nguyen et al , 2020 ) : BERTweet is another pre - trained language model following the training procedure of RoBERTa ( Liu et al , 2019 ) .", "entities": [[21, 22, "MethodName", "RoBERTa"]]}
{"text": "Similar to BERT , we fine - tune the pretrained BERTweet to predict the stance by appending a linear classification layer to the hidden representation of the [ CLS ] token .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "In this section , we present the set of experiments performed on various stance detection tasks on our dataset and show the results obtained by using the aforementioned baselines .", "entities": [[13, 15, "TaskName", "stance detection"]]}
{"text": "In - target stance detection is a stance detection task where a classifier is trained and validated on the same target .", "entities": [[3, 5, "TaskName", "stance detection"], [7, 9, "TaskName", "stance detection"]]}
{"text": "Moreover , we can observe that both BERTweet and BERT perform well and have the minimum performance drops compared with the other baselines , which demonstrates that self - attention mechanism can better capture target - specific representations .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "Despite substantial progress on the stance detection , sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets , which motivates the studies of crosstarget stance detection .", "entities": [[5, 7, "TaskName", "stance detection"], [20, 22, "TaskName", "stance detection"], [39, 41, "TaskName", "stance detection"]]}
{"text": "The model of cross - target stance detection is first trained and validated on a source target , and then tested on a destination target .", "entities": [[6, 8, "TaskName", "stance detection"]]}
{"text": "In this subsection , we show that our P - STANCE dataset can be also used to evaluate the model performance of cross - target stance detection and provides opportunities for exploring more cross - target tasks by interacting with previous SemEval - 2016 and Multi - Target stance datasets .", "entities": [[25, 27, "TaskName", "stance detection"]]}
{"text": "Experimental results of cross - target stance detection are shown in Table 10 .", "entities": [[6, 8, "TaskName", "stance detection"]]}
{"text": "BERTweet still achieves the best performance on almost all target configurations , making it a highly competitive model for crosstarget stance detection task .", "entities": [[20, 22, "TaskName", "stance detection"]]}
{"text": "Therefore , motivated by a desire to improve the models ' generalization ability to transfer knowledge from historical data , we come up with a new stance detection task , named cross - topic stance detection .", "entities": [[26, 28, "TaskName", "stance detection"], [34, 36, "TaskName", "stance detection"]]}
{"text": "Specifically , in this task , the model of cross - topic stance detection is first trained on the data of a target ( e.g. , Donald Trump ) in 2016 , and then validated and tested on the data of the same target in 2020 .", "entities": [[12, 14, "TaskName", "stance detection"]]}
{"text": "Since target \" Joe Biden \" is absent from the previous stance detection datasets , we use targets \" Donald Trump \" and \" Bernie Sanders \" for evaluation .", "entities": [[11, 13, "TaskName", "stance detection"]]}
{"text": "We can observe that BERTweet still performs best on this task and the overall model performance of cross - topic stance detection is better than that of cross - target stance detection due to the use of the same target in evaluation stage .", "entities": [[20, 22, "TaskName", "stance detection"], [30, 32, "TaskName", "stance detection"]]}
{"text": "During elections , there is a considerable amount of data generated by users expressing their opinions about candidates , out of which only a small amount can be annotated and used for supervised stance detection .", "entities": [[33, 35, "TaskName", "stance detection"]]}
{"text": "We run experiments with different training sets , and report the F1 - scores obtained on the entire testing set .", "entities": [[11, 12, "MetricName", "F1"]]}
{"text": "In this paper , we introduced P - STANCE , an English stance detection dataset in the political domain , which is larger and more challenging compared with previous datasets for stance detection .", "entities": [[12, 14, "TaskName", "stance detection"], [31, 33, "TaskName", "stance detection"]]}
{"text": "Composed of 21 , 574 tweets that were collected during the 2020 USA election , P - STANCE can serve as a new benchmark for stance detection and enable future research in other stance detection tasks , e.g. , cross - target stance detection and cross - topic stance detection .", "entities": [[25, 27, "TaskName", "stance detection"], [33, 35, "TaskName", "stance detection"], [42, 44, "TaskName", "stance detection"], [48, 50, "TaskName", "stance detection"]]}
{"text": "Experimental results show that the BERTweet model significantly outperforms other strong baselines not only on intarget stance detection , but also on cross - target and cross - topic stance detection .", "entities": [[16, 18, "TaskName", "stance detection"], [29, 31, "TaskName", "stance detection"]]}
{"text": "Future work includes constructing another large dataset for a more challenging task , i.e. , multi - target stance detection , and studying the multilingual stance detection with the union of P - STANCE and other multilingual datasets .", "entities": [[18, 20, "TaskName", "stance detection"], [25, 27, "TaskName", "stance detection"]]}
{"text": "An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction", "entities": [[8, 11, "TaskName", "Grammatical Error Correction"]]}
{"text": "The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models .", "entities": [[9, 12, "TaskName", "grammatical error correction"]]}
{"text": "To date , many studies have tackled grammatical error correction ( GEC ) as a machine translation ( MT ) task , in which ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language .", "entities": [[7, 10, "TaskName", "grammatical error correction"], [15, 17, "TaskName", "machine translation"]]}
{"text": "Here , | D | denotes the number of sentence pairs in the dataset D. Let \u0398 represent all trainable parameters of the model .", "entities": [[16, 17, "HyperparameterName", "\u0398"]]}
{"text": "Our objective is to find the optimal parameter set \u0398 that minimizes the following objective function L ( D , \u0398 ) for the given training data D : L ( D , \u0398 )", "entities": [[9, 10, "HyperparameterName", "\u0398"], [20, 21, "HyperparameterName", "\u0398"], [33, 34, "HyperparameterName", "\u0398"]]}
{"text": "The other is to use D p for pretraining , namely , minimizing L ( D p , \u0398 ) to acquire \u0398 , and then fine - tuning the model by minimizing L ( D g , \u0398 ) ; hereinafter , we refer to this setting as PRETRAIN .", "entities": [[18, 19, "HyperparameterName", "\u0398"], [22, 23, "HyperparameterName", "\u0398"], [38, 39, "HyperparameterName", "\u0398"]]}
{"text": "Evaluation We report results on BEA - valid , the official test set of the BEA - 2019 shared task ( BEA - test ) , the CoNLL - 2014 test set ( CoNLL - 2014 ) ( Ng et al , 2014 ) , and the JFLEG test set ( JFLEG ) ( Napoles et al , 2017 ) .", "entities": [[47, 48, "DatasetName", "JFLEG"], [51, 52, "DatasetName", "JFLEG"]]}
{"text": "All reported results ( except ensemble ) are the average of five distinct trials using five different random seeds .", "entities": [[18, 19, "DatasetName", "seeds"]]}
{"text": "We use the GLEU metric ( Napoles et al , 2015 ( Napoles et al , , 2016 for JFLEG .", "entities": [[19, 20, "DatasetName", "JFLEG"]]}
{"text": "Model We adopt the Transformer EncDec model ( Vaswani et al , 2017 ) using the fairseq toolkit ( Ott et al , 2019 ) and use the \" Transformer ( big ) \" settings of Vaswani et al ( 2017 ) .", "entities": [[4, 5, "MethodName", "Transformer"], [29, 30, "MethodName", "Transformer"]]}
{"text": "Optimization For the JOINT setting , we opti - mize the model with Adam ( Kingma and Ba , 2015 ) .", "entities": [[13, 14, "MethodName", "Adam"]]}
{"text": "For the PRETRAIN setting , we pretrain the model with Adam and then fine - tune it on BEA - train using Adafactor ( Shazeer and Stern , 2018 ) 10 .", "entities": [[10, 11, "MethodName", "Adam"], [22, 23, "MethodName", "Adafactor"]]}
{"text": "Synthetic Spelling Error ( SSE )", "entities": [[2, 3, "MetricName", "Error"], [4, 5, "MethodName", "SSE"]]}
{"text": "Sentence - level Error Detection ( SED ) SED classifies whether a given sentence contains a grammatical error .", "entities": [[3, 4, "MetricName", "Error"]]}
{"text": "We use the re - implementation of the BERT - based SED model ( Asano et al , 2019 ) .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "Table 5 presents the results of applying SSE , 13 http://www.statmt.org/wmt19/ R2L , and SED .", "entities": [[7, 8, "MethodName", "SSE"]]}
{"text": "However , unfortunately , incorporating SED decreased the performance on CoNLL - 2014 and JFLEG .", "entities": [[14, 15, "DatasetName", "JFLEG"]]}
{"text": "Adobe AMPS 's Submission for Very Low Resource Supervised Translation Task at WMT20", "entities": [[9, 10, "TaskName", "Translation"]]}
{"text": "Our primary submission is a subword - level Transformer - based neural machine translation model trained on original training bitext .", "entities": [[8, 9, "MethodName", "Transformer"], [12, 14, "TaskName", "machine translation"]]}
{"text": "This paper describes our submissions to the shared task on Very Low Resource Supervised Machine Translation at WMT 2020 .", "entities": [[14, 16, "TaskName", "Machine Translation"], [17, 19, "DatasetName", "WMT 2020"]]}
{"text": "We submit supervised neural machine translation ( NMT ) systems for both translation directions , Upper Sorbian German and German Upper Sorbian .", "entities": [[4, 6, "TaskName", "machine translation"]]}
{"text": "A big advantage of such systems over phrase - based statistical machine translation ( PBSMT ) ( Koehn et al , 2003 ) models is that they can be trained end - to - end .", "entities": [[11, 13, "TaskName", "machine translation"]]}
{"text": "All of our systems follow the Transformer architecture ( Vaswani et al , 2017 ) .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "In the following sections , we begin by briefly describing the Transformer architecture and backtranslation .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "The Transformer model is the dominant architecture within current NMT models due to its superior performance on several language pairs .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "We adopt the Transformer base architecture available under the fairseq 2 ( Ott et al , 2019 ) library for all our models .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "While not quite the same , one could think of this approach as having some similarities with transfer learning ( Zoph et al , 2016 ) as well as domain adaptation ( Luong and Manning , 2015 ; Freitag and Al - Onaizan , 2016 ) for machine translation .", "entities": [[17, 19, "TaskName", "transfer learning"], [29, 31, "TaskName", "domain adaptation"], [47, 49, "TaskName", "machine translation"]]}
{"text": "In addition , we also made use of monolingual data from each language for 2 https://github.com/pytorch/fairseq two purposes - learning Byte Pair Encodings ( BPE ) ( Sennrich et al , 2016b ) and backtranslation .", "entities": [[24, 25, "MethodName", "BPE"]]}
{"text": "We learned joint BPE 4 with 32 K merge operations over this corpus and applied them to the parallel training data to get vocabularies for each language .", "entities": [[3, 4, "MethodName", "BPE"]]}
{"text": "Our primary system is a Transformer base model , trained on the parallel training corpus for both translation directions till 60 epochs .", "entities": [[5, 6, "MethodName", "Transformer"]]}
{"text": "Label smoothing was set to 0.1 and dropout to 0.3 .", "entities": [[0, 2, "MethodName", "Label smoothing"]]}
{"text": "Fine - tuning was carried out by loading pretrained checkpoints and adding extra training flags in reset - optimizer and reset - lr - scheduler .", "entities": [[18, 19, "HyperparameterName", "optimizer"]]}
{"text": "In addition to evaluating on blind test sets , we also report BLEU scores on the development test set in the same table .", "entities": [[12, 13, "MetricName", "BLEU"]]}
{"text": "Looking at the small improvements achieved by using only the mixed corpus for training , increasing its size by combining upsampled auth data with more synth data might lead to even further jumps in the BLEU scores .", "entities": [[35, 36, "MetricName", "BLEU"]]}
{"text": "In this paper , we described our Transformer model for supervised machine translation for Upper Sorbian - German language pair .", "entities": [[7, 8, "MethodName", "Transformer"], [11, 13, "TaskName", "machine translation"]]}
{"text": "We take note of relatively high BLEU scores achieved by our primary systems ( and those of other participants ) on this low - resource language pair , which could relate to the high quality of the training corpus .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "Prior work ( Ding et al , 2019 ) has shown that the number of BPE merge operations has a significant effect on the performance of NMT systems .", "entities": [[15, 16, "MethodName", "BPE"]]}
{"text": "NNEMBs at SemEval - 2017 Task 4 : Neural Twitter Sentiment Classification : a Simple Ensemble Method with Different Embeddings", "entities": [[11, 12, "TaskName", "Classification"]]}
{"text": "Recently , neural twitter sentiment classification has become one of state - of - thearts , which requires less feature engineering work compared with traditional methods .", "entities": [[19, 21, "TaskName", "feature engineering"]]}
{"text": "We make an assumption that different word embeddings cover different words and encode different semantic knowledge , thus using them together can improve the generalizations and performances of neural models .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "The performances of these methods often depend on the quality of feature engineering work , and building a state - ofthe - art system is difficult for novices .", "entities": [[11, 13, "TaskName", "feature engineering"]]}
{"text": "These methods first learn word embeddings from large - scale twitter corpus , then tune neural networks by the tweets which have distant labels , and finally fine - tune the proposed models by the annotated datasets .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "Learning word embeddings using in - domain data is an effective way to boost model performances ( Mikolov et al , 2013 ;", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "In this paper , we use the different word embedding sets to boost the performances of our neural networks , which only include released different word embeddings sets and the word embedding set derived from the released Yelp large - scale datasets by Skip - gram ( Mikolov et al , 2013 ) .", "entities": [[25, 27, "TaskName", "word embeddings"]]}
{"text": "We have many choices of neural networks ( e.g. , LSTM , RNN and GRU ) for our method , here we consider RCNN ( Lei et al , 2016 ) in our method .", "entities": [[10, 11, "MethodName", "LSTM"], [14, 15, "MethodName", "GRU"]]}
{"text": "RCNN has non - consecutive convolution and adaptive gated decay , which aims to capture longerrange , non - consecutive patterns in a weighted manner .", "entities": [[5, 6, "MethodName", "convolution"]]}
{"text": "Given a sequence of words which are denoted as { x i } l i=1 , the corresponding word embeddings { x i } l i=1 are derived using the embedding matrix", "entities": [[18, 20, "TaskName", "word embeddings"]]}
{"text": "E. Then , RCNN obtains their corresponding hidden vectors { h i } l i=1 using the convolution operation and gating mechanism .", "entities": [[17, 18, "MethodName", "convolution"]]}
{"text": "After obtaining hidden vectors , RCNN uses a pooling operation to get fixed - sized vector presentation , which is fed into softmax layer to finish the prediction .", "entities": [[22, 23, "MethodName", "softmax"]]}
{"text": "The ngram convolution operation and gating decay are described as follows :", "entities": [[2, 3, "MethodName", "convolution"]]}
{"text": "t\u22121 + ( 1 \u2212 \u03bbt ) ( c ( n\u22121 ) t\u22121 + Wnxt ) , ht = tanh ( c ( n ) t + b ) , where W \u03bb , U \u03bb , b \u03bb , b and W * are learnable parameters , \u03c3 is sigmoid function which rescales the value into ( 0 , 1 ) , is dot product , \u03bb t is gating value determining how much information of x t and previous patterns is added into the hidden vector , c ( i ) t refer to the vector for accumulated previous patterns which are ended with x t include i consecutive tokens .", "entities": [[58, 59, "DatasetName", "0"]]}
{"text": "When \u03bb t = 0 , the convolution becomes a standard n - gram convolution .", "entities": [[4, 5, "DatasetName", "0"], [7, 8, "MethodName", "convolution"], [14, 15, "MethodName", "convolution"]]}
{"text": "We also can build a deep RCNN by adding several convolution layer on top of hidden vectors derived from the bottom convolution layer .", "entities": [[10, 11, "MethodName", "convolution"], [21, 22, "MethodName", "convolution"]]}
{"text": "Here we consider the RCNN with d convolution layers , which outputs { h d i } l i=1 .", "entities": [[7, 8, "MethodName", "convolution"]]}
{"text": "Finally , text representation is fed into a softmax layer .", "entities": [[8, 9, "MethodName", "softmax"]]}
{"text": "The softmax layer outputs the probability distribution over | Y | categories for the distributed representation , which is defined as : p ( r ) = softmax ( W class k r ) .", "entities": [[1, 2, "MethodName", "softmax"], [27, 28, "MethodName", "softmax"]]}
{"text": "Our method outperforms other systems in accuracy , but performs worse in R Average , especially in R Negative .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "To improve the accuracy of predicateargument structure ( PAS ) analysis , large - scale training data and knowledge for PAS analysis are indispensable .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "We focus on a specific domain , specifically Japanese blogs on driving , and construct two wide - coverage datasets as a form of QA using crowdsourcing : a PAS - QA dataset and a reading comprehension QA ( RC - QA ) dataset .", "entities": [[35, 37, "TaskName", "reading comprehension"]]}
{"text": "Such analysis is called semantic role labeling ( SRL ) or predicate - argument structure ( PAS ) analysis .", "entities": [[4, 7, "TaskName", "semantic role labeling"]]}
{"text": "We also construct a QA dataset for reading comprehension ( RC - QA ) in the same domain and jointly use the two datasets in the MC model to improve PAS analysis .", "entities": [[7, 9, "TaskName", "reading comprehension"]]}
{"text": "FitzGerald et al ( 2018 ) and constructed QA - SRL Bank 2.0 and QAMRs using crowdsourcing , respectively .", "entities": [[8, 13, "DatasetName", "QA - SRL Bank 2.0"]]}
{"text": "For example , Rajpurkar et al ( 2016 ) constructed SQuAD 1.1 , which contains 100 K crowdsourced questions and answer spans in a Wikipedia article .", "entities": [[10, 11, "DatasetName", "SQuAD"]]}
{"text": "al ( 2018 ) updated SQuAD 1.1 to 2.0 by adding unanswerable questions .", "entities": [[5, 6, "DatasetName", "SQuAD"]]}
{"text": "As a previous study of transfer learning of MC models to other tasks , Pan et al ( 2018 ) pre - trained an MC model using an RC - QA dataset and transfered the pre - trained knowledge to sequence - tosequence models .", "entities": [[5, 7, "TaskName", "transfer learning"]]}
{"text": "They used SQuAD 1.1 as the RC - QA dataset and experimented on translation and summarization .", "entities": [[2, 3, "DatasetName", "SQuAD"], [15, 16, "TaskName", "summarization"]]}
{"text": "We construct a driving - domain RC - QA dataset in the same way as SQuAD 1.1 .", "entities": [[15, 16, "DatasetName", "SQuAD"]]}
{"text": "We adopt BERT ( Devlin et al , 2019 ) as an MC model .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "We trained a Japanese pre - trained BERT model using Japanese Wikipedia , which consists of approximately 18 million sentences .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "The input sentences were segmented into words by JUMAN++ , and words were broken into subwords by applying BPE ( Sennrich et al , 2016 ) .", "entities": [[18, 19, "MethodName", "BPE"]]}
{"text": "The parameters of BERT are the same as English BERT BASE .", "entities": [[3, 4, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"], [10, 11, "MethodName", "BASE"]]}
{"text": "As an evaluation measure , EM ( Exact Match ) is used for all the MC models .", "entities": [[5, 6, "MetricName", "EM"], [7, 9, "MetricName", "Exact Match"]]}
{"text": "EM is defined as ( the number of questions in which the system answer matches the gold answer in the dataset ) /", "entities": [[0, 1, "MetricName", "EM"]]}
{"text": "In particular , the stepwise training method based on BERT was the most effective , which outperformed the previous state - of - the - art NN - PAS model .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "The Driving Experience extracting CRF tool ( Iwai et al , 2018 ) judges that three or more sentences out of four sentences are driving experience .", "entities": [[4, 5, "MethodName", "CRF"]]}
{"text": "These large - scale pre - trained models , although successful , fatefully suffer from slow inference speed due to enormous computation cost mainly from cross - modal attention in Transformer architecture .", "entities": [[30, 31, "MethodName", "Transformer"]]}
{"text": "We propose a simple yet highly effective approach , LightningDOT that accelerates the inference time of ITR by thousands of times , without sacrificing accuracy .", "entities": [[24, 25, "MetricName", "accuracy"]]}
{"text": "In fact , Light - ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k , COCO and Multi30 K , outperforming existing pre - trained models that consume 1000\u00d7 magnitude of computational hours .", "entities": [[18, 19, "DatasetName", "Flickr30k"], [20, 21, "DatasetName", "COCO"]]}
{"text": "( c ) Pre - trained V+L models with deep Transformer .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "With the advent of Transformer ( Vaswani et al , 2017 ) and BERT ( Devlin et al , 2019 ) , crossmodal retrieval tasks are more recently dominated by vision - and - language ( V+L ) pre - trained models , such as ViLBERT , UNITER , OSCAR ( Li et al , 2020b ) , and VILLA .", "entities": [[4, 5, "MethodName", "Transformer"], [13, 14, "MethodName", "BERT"], [45, 46, "MethodName", "ViLBERT"], [47, 48, "MethodName", "UNITER"], [49, 50, "MethodName", "OSCAR"]]}
{"text": "These models benefit from the self - attention mechanism in Transformer architecture , learning joint image+text embeddings through pre - training objectives such as masked language modeling ( MLM ) and masked region modeling ( MRM )", "entities": [[10, 11, "MethodName", "Transformer"], [24, 27, "TaskName", "masked language modeling"], [28, 29, "DatasetName", "MLM"]]}
{"text": "For example , UNITER builds upon 12/24 Transformer layers , and trains over 10 million image+text pairs .", "entities": [[3, 4, "MethodName", "UNITER"], [7, 8, "MethodName", "Transformer"]]}
{"text": "The inference time of such large models with 110 million parameters is 48 seconds on average for text query from COCO dataset ( Chen et al , 2015 ) , not scalable in real - life applications serving millions of queries per second .", "entities": [[20, 21, "DatasetName", "COCO"]]}
{"text": "To make real - time ITR possible with low latency , we ask a bold question : can we go back to the beginning , reverting to simple dot product for efficient cross - modal retrieval ?", "entities": [[32, 36, "TaskName", "cross - modal retrieval"]]}
{"text": "To make this retro experiment feasible , we rely on Transformer to pre - train high - quality image and text encoders , but use efficient dot product for multimodal fusion instead of computationally heavy self - attention .", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "For model training , we propose three learning objectives to jointly train two Transformer blocks :", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "Specifically , Visual - embedding fused MLM ( namely VMLM ) and Semantic - embedding fused MRM ( namely SMRM ) ensure cross - modal information is harnessed even without cross - modality self - attention .", "entities": [[6, 7, "DatasetName", "MLM"]]}
{"text": "A cross - modal retrieval objective ( namely CMR ) encourages the model to learn multimodal fusion through pre - training .", "entities": [[1, 5, "TaskName", "cross - modal retrieval"]]}
{"text": "Experiments on popular ITR benchmarks show that LightningDOT is 600/1900 times faster than existing pre - trained models on Flickr30k / COCO , while achieving new state - of - the - art results .", "entities": [[19, 20, "DatasetName", "Flickr30k"], [21, 22, "DatasetName", "COCO"]]}
{"text": "Inspired by the success of Transformer - based ( Vaswani et al , 2017 ) language model pre - training ( Devlin et al , 2019 ; Yang et al , 2019 ; Raffel et al , 2020 ;", "entities": [[0, 1, "DatasetName", "Inspired"], [5, 6, "MethodName", "Transformer"]]}
{"text": "et al , 2020bLi et al , , 2019a has become the prevailing paradigm in learning multimodal representations , with strong results on tasks such as image - text retrieval ( Kiros et al , 2014 ) , visual question answering ( Antol et al , 2015 ) and referring expression comprehension ( Yu et al , 2016 ) .", "entities": [[38, 41, "DatasetName", "visual question answering"], [49, 52, "TaskName", "referring expression comprehension"]]}
{"text": "Multi - task learning and adversarial training are also explored .", "entities": [[0, 4, "TaskName", "Multi - task learning"]]}
{"text": "To the best of our knowledge , our work is the first known effort on pre - training visualsemantic embedding that enables low - latency realtime cross - modal retrieval .", "entities": [[26, 30, "TaskName", "cross - modal retrieval"]]}
{"text": "Ours is concurrent work with CLIP ( Radford et al , 2021 ) .", "entities": [[5, 6, "MethodName", "CLIP"]]}
{"text": "Image - Text Retrieval Early cross - modal embedding works ( Kiros et al , 2014 ; Faghri et al , 2017 ) focus on using a twostream model to learn a unified visual - semantic embedding , with progressive improvement on two popular benchmarks : Flickr30 K ( Plummer et al , 2015 ) and COCO ( Chen et al , 2015 ) .", "entities": [[56, 57, "DatasetName", "COCO"]]}
{"text": "By exploiting large - scale image - text datasets , pretrained V+L models further push the performance on Flickr30 K and COCO .", "entities": [[21, 22, "DatasetName", "COCO"]]}
{"text": "Given a dataset of paired image and text { ( i , t ) } , we first extract region features v = { v 0 , v 1 , . . .", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "w = { w 0 , w 1 , ... , w T } ( w j R dw , T is the number of tokens ) following Devlin et al ( 2019 ) .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "Similarly , the text encoding process can be written as f \u03b8 L ( w ) =", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "z = { z 0 , . . .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "We regard the output [ CLS ] embedding h 0 as global image representation , and z 0 as global text representation .", "entities": [[9, 10, "DatasetName", "0"], [17, 18, "DatasetName", "0"]]}
{"text": "L MLM ( t )", "entities": [[1, 2, "DatasetName", "MLM"]]}
{"text": "= \u2212 log P \u03b8 L ( w m | w \\m )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "To leverage this cross - modal relation , we propose visualembedding fused MLM ( VMLM ) , in which the paired image i is considered as additional input when training the model to reconstruct masked tokens in sentence t.", "entities": [[12, 13, "DatasetName", "MLM"]]}
{"text": "The loss function of VMLM can be formulated as : L VMLM ( t , i )", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "= \u2212 log P \u03b8 ( w m | w \\m , i )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "Similar to MLM , Masked Region Modeling ( MRM ) is supervised by : L MRM ( i )", "entities": [[2, 3, "DatasetName", "MLM"]]}
{"text": "= D \u03b8mrm ( v m , f \u03b8 V ( v \\m ) )", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "Among the variants of MRM , we consider Masked Region Feature Regression ( MRFR ) with L2 distance and Masked Region Classification with KL - Divergence ( MRC - kl ) , due to their proven success in learning V+L representations .", "entities": [[21, 22, "TaskName", "Classification"]]}
{"text": "| | g \u03b8mrc ( y k ) ) , where \u03b8 mrc is the parameters of a trainable MLP that maps feature vector", "entities": [[11, 12, "HyperparameterName", "\u03b8"], [19, 20, "DatasetName", "MLP"]]}
{"text": "x k to the object class distribution c ( x k ) predicted by Faster R - CNN .", "entities": [[14, 18, "MethodName", "Faster R - CNN"]]}
{"text": "To incorporate language information encoded in the paired text , we extend MRM to Semanticembedding fused MRM ( SMRM ) , where the global text representation z 0 is exploited when reconstructing masked regions .", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "= D \u03b8mrm ( v m , f \u03b8 V ( v \\m ) , t )", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "= 1 M M k=1 D \u03b8mrm ( v m k , h m k + z 0 ) .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "Moreover , the extra parameters \u03b8 mlm and \u03b8 mrm is not needed at downstream inference so will not slow down the retrieval .", "entities": [[5, 6, "HyperparameterName", "\u03b8"], [6, 7, "DatasetName", "mlm"], [8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "Cross - modal Retrieval Objective ( CMR )", "entities": [[0, 4, "TaskName", "Cross - modal Retrieval"]]}
{"text": "Beyond image or text focused reconstructive objectives , we also propose a new pre - training task , Cross - modal Retrieval ( CMR ) , to leverage the paired information between image and text .", "entities": [[18, 22, "TaskName", "Cross - modal Retrieval"]]}
{"text": "In order to capture both image - retrieval and textretrieval supervision signals in a single forwardbackward pass , we propose a bi - directional variant of contrastive loss .", "entities": [[27, 28, "MetricName", "loss"]]}
{"text": "During finetuning stage , we directly adopt CMR loss to supervise the training process .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "For simplicity , we take text - to - image retrieval as an example to introduce the real - time inference pipeline ( Figure 2 Offline Feature Extraction Image retrieval task requires the model to rank every image i in an image database I based on its similarity to a text query t.", "entities": [[5, 11, "TaskName", "text - to - image retrieval"], [28, 30, "TaskName", "Image retrieval"]]}
{"text": "Note that the entire image - to - index process , including Faster - RCNN feature extraction and Transformer encoding , can all be conducted offline .", "entities": [[18, 19, "MethodName", "Transformer"]]}
{"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}
{"text": "Re - ranking To further improve retrieval accuracy , we propose a two - stage approach by adopting an optional re - ranking model .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "In the first stage , we use LightningDOT to retrieve top - M images ( or texts ) , where M is an integer much smaller R@1 R@5", "entities": [[26, 27, "MetricName", "R@1"], [27, 28, "MetricName", "R@5"]]}
{"text": "R@10 R@1 R@5", "entities": [[0, 1, "MetricName", "R@10"], [1, 2, "MetricName", "R@1"], [2, 3, "MetricName", "R@5"]]}
{"text": "R@10", "entities": [[0, 1, "MetricName", "R@10"]]}
{"text": "AR R@1", "entities": [[1, 2, "MetricName", "R@1"]]}
{"text": "R@5", "entities": [[0, 1, "MetricName", "R@5"]]}
{"text": "R@10 R@1 R@5", "entities": [[0, 1, "MetricName", "R@10"], [1, 2, "MetricName", "R@1"], [2, 3, "MetricName", "R@5"]]}
{"text": "R@10 AR", "entities": [[0, 1, "MetricName", "R@10"]]}
{"text": "For pre - training , we use pre - processed data provided by , including 4.2 million 8 The computation time of LightningDOT is negligible compared to that of UNITER .", "entities": [[29, 30, "MethodName", "UNITER"]]}
{"text": "Therefore , the empirical speed is proportional to the number of pairs UNITER has to rank : constant M for LightningDOT + UNITER vs. the whole database ( index ) size for UNITER only .", "entities": [[12, 13, "MethodName", "UNITER"], [22, 23, "MethodName", "UNITER"], [32, 33, "MethodName", "UNITER"]]}
{"text": "images with 9.5 million associated captions from COCO ( Chen et al , 2015 ) , VG ( Krishna et al , 2017 ) , Conceptual Captions ( Sharma et al , 2018 ) , and SBU captions ( Ordonez et al , 2011 ) .", "entities": [[7, 8, "DatasetName", "COCO"], [25, 27, "DatasetName", "Conceptual Captions"]]}
{"text": "For evaluation , we use Flickr30k ( Plummer et al , 2015 ) and COCO ( Lin et al , 2014 ) datasets , which include 31K/123 K images , respectively , each associated with 5 human - written captions .", "entities": [[5, 6, "DatasetName", "Flickr30k"], [14, 15, "DatasetName", "COCO"]]}
{"text": "Following ( Faghri et al , 2017 ) , we split COCO into 114K/5K/5 K and Flickr30 K into 29K/1k/1k images for train , validation and test .", "entities": [[11, 12, "DatasetName", "COCO"]]}
{"text": "Specifically , our model improves over CAAN ( SOTA method with cross - attention ) by 3.3 % ( 73.5 vs. 70.2 ) on COCO and 9.5 % ( 89.3 vs. 79.8 ) on Flickr30 K in terms of AR .", "entities": [[24, 25, "DatasetName", "COCO"]]}
{"text": "When compared with methods without cross - attention ( VSE++ ( Faghri et al , 2017 ) and SCO ( Huang et al , 2018 ) ) , LightningDOT achieves nearly Model COCO Full ( 123 K Images )", "entities": [[32, 33, "DatasetName", "COCO"]]}
{"text": "Text Retrieval Image Retrieval Text Retrieval Image Retrieval R@5", "entities": [[2, 4, "TaskName", "Image Retrieval"], [6, 8, "TaskName", "Image Retrieval"], [8, 9, "MetricName", "R@5"]]}
{"text": "R@10 R@20 R@5 R@10", "entities": [[0, 1, "MetricName", "R@10"], [2, 3, "MetricName", "R@5"], [3, 4, "MetricName", "R@10"]]}
{"text": "AR R@5", "entities": [[1, 2, "MetricName", "R@5"]]}
{"text": "R@10 R@20 R@5 R@10", "entities": [[0, 1, "MetricName", "R@10"], [2, 3, "MetricName", "R@5"], [3, 4, "MetricName", "R@10"]]}
{"text": "LightningDOT with / without UNITER - base re - ranker is significantly faster .", "entities": [[4, 5, "MethodName", "UNITER"]]}
{"text": "Although LightningDOT achieves slightly lower AR than UNITER ( pretraining method with cross - attention ) , with 3.5/1.1 points drop on Flickr30K / COCO , it is 600/1900 \u00d7 faster than UNITER during inference time .", "entities": [[7, 8, "MethodName", "UNITER"], [25, 26, "DatasetName", "COCO"], [33, 34, "MethodName", "UNITER"]]}
{"text": "We further apply second - stage re - ranking , and use UNITER to score top - M retrieved image - text pairs from LightningDOT to obtain the final top - K ranked lists .", "entities": [[12, 13, "MethodName", "UNITER"]]}
{"text": "With re - ranking , LightningDOT achieves an instant performance lift , surpassing UNITER on both benchmarks , while still 46 - 95 times faster than UNITER .", "entities": [[13, 14, "MethodName", "UNITER"], [26, 27, "MethodName", "UNITER"]]}
{"text": "With an even stronger re - ranker OSCAR , LightningDOT achieves similar results to the state - of - the - art performance on COCO .", "entities": [[7, 8, "MethodName", "OSCAR"], [24, 25, "DatasetName", "COCO"]]}
{"text": "To demonstrate the efficiency of LightningDOT , we use UNITER - base as baseline to compare inference speed .", "entities": [[9, 10, "MethodName", "UNITER"]]}
{"text": "We also compare with a more lightweight cross - attention method SCAN ( Lee et al , 2018 ) , which uses GRU ( Chung et al , 2014 ) instead of a 12 - layer Transformer .", "entities": [[11, 12, "DatasetName", "SCAN"], [22, 23, "MethodName", "GRU"], [36, 37, "MethodName", "Transformer"]]}
{"text": "As shown in Table 3 , SCAN is \u223c1.9\u00d7 faster than UNITER - base across both benchmarks , as the computational cost of GRU is much cheaper than that of Transformer ( performance drop is significant though ) .", "entities": [[6, 7, "DatasetName", "SCAN"], [11, 12, "MethodName", "UNITER"], [23, 24, "MethodName", "GRU"], [30, 31, "MethodName", "Transformer"]]}
{"text": "However , the speedup from SCAN is limited , as it computes cross - attention between each query and all images .", "entities": [[5, 6, "DatasetName", "SCAN"]]}
{"text": "On the other hand , LightningDOT is 639\u00d7 faster than UNITER on Flickr30K. When tested with 5 times more i m - ages in COCO , the speedup from LightningDOT is 1927\u00d7. Even with re - ranking , LightningDOT is still much more efficient than UNITER - base ( 46\u00d7 faster on Flickr30 K and 95\u00d7 faster on COCO ) .", "entities": [[10, 11, "MethodName", "UNITER"], [24, 25, "DatasetName", "COCO"], [45, 46, "MethodName", "UNITER"], [58, 59, "DatasetName", "COCO"]]}
{"text": "To mimic a real - life scenario for image retrieval , where the candidate pool contains hundreds of thousands of images , we combine all images from training , validation and test set to form a larger candidate pool .", "entities": [[8, 10, "TaskName", "image retrieval"]]}
{"text": "We refer this setting on both benchmarks as Flickr30k - full ( 31k ) and COCO - full ( 123k ) .", "entities": [[8, 9, "DatasetName", "Flickr30k"], [15, 16, "DatasetName", "COCO"]]}
{"text": "Our algorithm is 6 , 591\u00d7 faster on Flickr30k - full and 23 , 869\u00d7 faster on COCO - full , which clearly shows the advantage of LightningDOT and its potential in real - world applications .", "entities": [[8, 9, "DatasetName", "Flickr30k"], [17, 18, "DatasetName", "COCO"]]}
{"text": "With re - ranking , LightningDOT is still more than 1 , 000\u00d7 and 2 , 000\u00d7 faster on Flickr30kfull and COCO - full , respectively .", "entities": [[21, 22, "DatasetName", "COCO"]]}
{"text": "In general , for other re - rankers such as OSCAR , our algorithm can approximately speed up inference by N images /M times , where N images is the number of candidate images , and M is number of re - ranked images from top - M retrieved results by LightningDOT .", "entities": [[10, 11, "MethodName", "OSCAR"]]}
{"text": "Our method achieves reasonably good performance , with AR of 44.4 on COCO and 70.2 on Flickr30K. Re - ranking further lifts AR to 56.4 and 76.2 .", "entities": [[12, 13, "DatasetName", "COCO"]]}
{"text": "Results from UNITER or SCAN are not included as the computation of pairwise scores is extremely expensive , given the excessive amount of retrieval candidates .", "entities": [[2, 3, "MethodName", "UNITER"], [4, 5, "DatasetName", "SCAN"]]}
{"text": "R@5", "entities": [[0, 1, "MetricName", "R@5"]]}
{"text": "R@10 R@1 R@5", "entities": [[0, 1, "MetricName", "R@10"], [1, 2, "MetricName", "R@1"], [2, 3, "MetricName", "R@5"]]}
{"text": "R@10 AR", "entities": [[0, 1, "MetricName", "R@10"]]}
{"text": "Image Retrieval LightningDOT R@1 R@5", "entities": [[0, 2, "TaskName", "Image Retrieval"], [3, 4, "MetricName", "R@1"], [4, 5, "MetricName", "R@5"]]}
{"text": "R@10 R@1 R@5", "entities": [[0, 1, "MetricName", "R@10"], [1, 2, "MetricName", "R@1"], [2, 3, "MetricName", "R@5"]]}
{"text": "R@10 AR image retrieval and text retrieval .", "entities": [[0, 1, "MetricName", "R@10"], [2, 4, "TaskName", "image retrieval"]]}
{"text": "In addition , We compare all models with the same setting : cache as much as possible for fastest speed , where our model outperforms others in both speed and space on image retrieval .", "entities": [[32, 34, "TaskName", "image retrieval"]]}
{"text": "The proposed algorithm maps each image to a 768 - dimensional vector , which only consumes about 300Mb storage space for the whole COCO dataset .", "entities": [[23, 24, "DatasetName", "COCO"]]}
{"text": "For crossattention models such as SCAN , UNITER or OS - CAR , they also need to cache image features , which typically requires to save a 36 x 2048 dimensional vector per image , and it consumes about 28 GB storage space for COCO dataset .", "entities": [[5, 6, "DatasetName", "SCAN"], [7, 8, "MethodName", "UNITER"], [11, 12, "DatasetName", "CAR"], [29, 30, "DatasetName", "2048"], [44, 45, "DatasetName", "COCO"]]}
{"text": "We conduct ablation studies on Flickr30 K ( Table 4 ) and compare LightningDOT ( L4 ) against 3 ablated instances : ( i ) \" R - CNN only \" ( L1 ) :", "entities": [[26, 29, "MethodName", "R - CNN"]]}
{"text": "image representations are extracted from Faster R - CNN directly , with no image encoder applied ; ( ii ) \" + Image Encoder \" ( L2 ) : regional features are encoded with a 12 - layer Transformer as the image encoder ; ( iii ) \" + PT \u2020 \" ( L3 ) :", "entities": [[5, 9, "MethodName", "Faster R - CNN"], [38, 39, "MethodName", "Transformer"]]}
{"text": "our model is pre - trained with MLM+MRM+CMR , then finetuned on Flickr30K. Note that the difference between MLM vs. VMLM and MRM vs. SMRM is whether the predictions of masked tokens ( regions ) rely on infused embeddings from the other modality .", "entities": [[18, 19, "DatasetName", "MLM"]]}
{"text": "Multi30 K and COCO datasets .", "entities": [[3, 4, "DatasetName", "COCO"]]}
{"text": "S - LIWE ( Wehrmann et al , 2019 ) , MULE , SMALR ( Burns et al , 2020 ) , pre - trained method M 3 P ( Huang et al , 2020a ) Results show that \" R - CNN only \" is not sufficient in learning good image representations for ITR task , while image encoder with Transformer architecture can effectively learn contextualized image representations , hence achieving better performance .", "entities": [[40, 43, "MethodName", "R - CNN"], [61, 62, "MethodName", "Transformer"]]}
{"text": "Specially , we evaluate Lightning - DOT under the translate - test setting , which is to translate the test captions in other languages to English by leveraging Machine Translation ( MT ) tool .", "entities": [[28, 30, "TaskName", "Machine Translation"]]}
{"text": "We consider two benchmarks : Multi30 K ( Elliott et al , 2016 ( Elliott et al , , 2017Barrault et al , 2018 ) with captions in German , French and Czech ; and COCO Japanese ( Yoshikawa et al , 2017 ) and Chinese ( Li et al , 2019b ) .", "entities": [[35, 36, "DatasetName", "COCO"]]}
{"text": "Average Recall ( AR ) is used as the evaluation metric .", "entities": [[1, 2, "MetricName", "Recall"]]}
{"text": "( Burns et al , 2020 ) , which all exploit captions in different languages to learn multilingual or language - agnostic word embeddings .", "entities": [[22, 24, "TaskName", "word embeddings"]]}
{"text": "For fair comparison , we report performance of UNITER under the same translate - test setting , which is finetuned with English captions only and tested on translated captions .", "entities": [[8, 9, "MethodName", "UNITER"]]}
{"text": "We show an example of image retrieval results here at figure 4 for query as \" Sky view of a blue and yellow biplane flying near each other \" .", "entities": [[5, 7, "TaskName", "image retrieval"]]}
{"text": "Light - ningDOT outperforms previous state of the art , while significantly speeding up inference time by 600 - 2000\u00d7 on Flickr30 K and COCO image - text retrieval benchmarks .", "entities": [[24, 25, "DatasetName", "COCO"]]}
{"text": "When evaluating on ITR under the multilingual setting , we consider two benchmarks : Multi30 K ( Elliott et al , 2016 ( Elliott et al , , 2017Barrault et al , 2018 ) and COCO Japanese ( Yoshikawa et al , 2017 ) and Chinese ( Li et al , 2019b ) .", "entities": [[35, 36, "DatasetName", "COCO"]]}
{"text": "We adopt the same train / val / test split as in Flickr30K. COCO Japanese ( Yoshikawa et al , 2017 ) collected 820 K Japanese captions for 165 K COCO images ( Lin et al , 2014 ) .", "entities": [[13, 14, "DatasetName", "COCO"], [30, 31, "DatasetName", "COCO"]]}
{"text": "We use the same train / dev / test splits for COCO Japanese as in Karpathy and Fei - Fei ( 2015 ) , and present results on the 1 K test set .", "entities": [[11, 12, "DatasetName", "COCO"]]}
{"text": "Similarly , Li et al ( 2019b ) collected 1 - 2 Chinese captions per image for 20 K COCO images to build COCO Chinese .", "entities": [[19, 20, "DatasetName", "COCO"], [23, 24, "DatasetName", "COCO"]]}
{"text": "We present the detailed inference time of UNITERbase , SCAN the proposed LightningDOT and LightningDOT with UNITER - base re - ranker in Table 7 , measured by seconds / query .", "entities": [[9, 10, "DatasetName", "SCAN"], [16, 17, "MethodName", "UNITER"]]}
{"text": "UNITER clearly is the slowest , as the 12 - layer Transformer model inference needs to be run between each query and all images .", "entities": [[0, 1, "MethodName", "UNITER"], [11, 12, "MethodName", "Transformer"]]}
{"text": "Comparing between Flickr30k - test and COCO - test , its inference time scales up linearly with the number of images .", "entities": [[2, 3, "DatasetName", "Flickr30k"], [6, 7, "DatasetName", "COCO"]]}
{"text": "With the lightweight GRU ( Chung et al , 2014 ) , SCAN is \u223c1.9\u00d7 faster than UNITER .", "entities": [[3, 4, "MethodName", "GRU"], [12, 13, "DatasetName", "SCAN"], [17, 18, "MethodName", "UNITER"]]}
{"text": "Across all settings , LightningDOT is significantly faster than both cross - attention methods ( UNITER - base and SCAN ) .", "entities": [[15, 16, "MethodName", "UNITER"], [19, 20, "DatasetName", "SCAN"]]}
{"text": "When adding UNITER - base as the re - ranker , our method slows down by \u223c10 , but still achieves decent speedup .", "entities": [[2, 3, "MethodName", "UNITER"]]}
{"text": "We show several qualitative results of image retrieval ( top - 10 ) .", "entities": [[6, 8, "TaskName", "image retrieval"]]}
{"text": "All results are retrieved from COCO -", "entities": [[5, 6, "DatasetName", "COCO"]]}
{"text": "For example , \" romantic \" only appears twice in the whole COCO dataset annotations , yet the top retrieved images are all topic - related ( Figure 5 ) .", "entities": [[12, 13, "DatasetName", "COCO"]]}
{"text": "We also present image retrieval results where the text query is sampled from COCO dataset .", "entities": [[3, 5, "TaskName", "image retrieval"], [13, 14, "DatasetName", "COCO"]]}
{"text": "The model dimensions are set to ( L=12 , H=768 , A=12 ) for both image encoder and language encoder , where L is the number of stacked Transformer blocks ; H stands for hidden activation dimension , and A is the number of attention heads .", "entities": [[28, 29, "MethodName", "Transformer"]]}
{"text": "The total number of parameters in LightningDOT is 220M. Pre - training and finetuning learn the parameters of both encoders .", "entities": [[2, 5, "HyperparameterName", "number of parameters"]]}
{"text": "We set the L2 weight decay to be 0.01 .", "entities": [[4, 6, "MethodName", "weight decay"]]}
{"text": "During pre - training , we follow UNITER to randomly sample 1 task per minibatch update .", "entities": [[7, 8, "MethodName", "UNITER"]]}
{"text": "Our models are trained for 15 epochs on Flickr30k , and 20 epochs on COCO .", "entities": [[8, 9, "DatasetName", "Flickr30k"], [14, 15, "DatasetName", "COCO"]]}
{"text": "Over an annotated corpus we apply an Expectation - Maximisation ( EM ) algorithm , having included within the features linguistic information related to its placement .", "entities": [[11, 12, "MetricName", "EM"]]}
{"text": "Section 2 summarises the related work concerning text classification efforts and genre studies related to communication objectives .", "entities": [[7, 9, "TaskName", "text classification"]]}
{"text": "Regarding reviews , most of the work developed refers to sentiment analysis or polarity classifica - tion ( Cambria et al , 2013 ) .", "entities": [[10, 12, "TaskName", "sentiment analysis"]]}
{"text": "In the ADESSE verb senses Mental , material , relational , verbal , existential and modulation FREELING features PoS tagging : noun , adjective , pronoun , verb ( tense , aspect , ... ) , etc . realm of reviews , opinion and sentiment annotation , we can take advantage for example of MARL Ontology Specification 1 , a data schema that has been used in the EuroSentiment Project ( Buitelaar et al , 2013 ) or directly related to reviews from a Sentiment Analysis perspective ( Santosh and Vardhan , 2015 ) .", "entities": [[55, 56, "MethodName", "Ontology"], [84, 86, "TaskName", "Sentiment Analysis"]]}
{"text": "DBPedia has been already proved useful for Wikipedia articles researchers .", "entities": [[0, 1, "DatasetName", "DBPedia"]]}
{"text": "Drammar ( Lombardo and Damiano , 2012 ) and OntoMedia ( Jewell et al , 2005 ) are ontology - based models for annotating features of media and cultural narratives .", "entities": [[18, 19, "MethodName", "ontology"]]}
{"text": "Such relation embeddings are appealing because they can , in principle , encode relational knowledge in a more finegrained way than is possible with knowledge graphs .", "entities": [[24, 26, "TaskName", "knowledge graphs"]]}
{"text": "We find that the resulting relation embeddings are highly competitive on analogy ( unsupervised ) and relation classification ( supervised ) benchmarks , even without any task - specific fine - tuning .", "entities": [[16, 18, "TaskName", "relation classification"]]}
{"text": "One of the most widely studied aspects of word embeddings is the fact that word vector differences capture lexical relations ( Mikolov et al , 2013a ) .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "While not being directly connected to downstream performance on NLP tasks , this ability of word embeddings is nonetheless important .", "entities": [[15, 17, "TaskName", "word embeddings"]]}
{"text": "1 Source code to reproduce our experimental results and the model checkpoints are available in the following repository : https://github.com/asahi417/relbert ing ( Fernandez et al , 2018 ) , completion and retrieval of Web tables ( Zhang et al , 2019 ) , ontology completion ( Bouraoui and Schockaert , 2019 ) and information retrieval in the medical domain ( Arguello Casteleiro et al , 2020 ) .", "entities": [[43, 44, "MethodName", "ontology"], [53, 55, "TaskName", "information retrieval"], [57, 59, "DatasetName", "medical domain"]]}
{"text": "More generally , relational similarity ( or analogy ) plays a central role in computational creativity ( Goel , 2019 ) , legal reasoning ( Ashley , 1988 ; Walton , 2010 ) , ontology alignment ( Raad and Evermann , 2015 ) and instance - based learning ( Miclet et al , 2008 ) .", "entities": [[34, 35, "MethodName", "ontology"]]}
{"text": "Brown et al , 2020 ) , we may wonder whether such models are able to capture lexical relations in a more faithful or fine - grained way than traditional word embeddings .", "entities": [[30, 32, "TaskName", "word embeddings"]]}
{"text": "Despite the relatively small size of this dataset , we show that the resulting fine - tuned LM allows us to produce high - quality relation embeddings , as confirmed in our extensive evaluation in analogy and relation classification tasks .", "entities": [[37, 39, "TaskName", "relation classification"]]}
{"text": "For instance , Petroni et al ( 2019 ) use BERT for link prediction .", "entities": [[10, 11, "MethodName", "BERT"], [12, 14, "TaskName", "link prediction"]]}
{"text": "they create the input \" Dante was born in < mask > \" and then look at the predictions of BERT for the masked token to retrieve the correct answer .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "It is notable that BERT is thus used for extracting relational knowledge without any fine - tuning .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "al ( 2020 ) argue that the feed - forward layers of transformer - based LMs act as neural memories , which would suggest that e.g. \" the place where Dante is born \" is stored as a property of Florence .", "entities": [[40, 41, "MethodName", "Florence"]]}
{"text": "Another notable work focusing on link prediction is ( Bosselut et al , 2019 ) , where GPT is fine - tuned to complete triples from commonsense knowledge graphs , in particular ConceptNet ( Speer et al , 2017 ) and ATOMIC .", "entities": [[5, 7, "TaskName", "link prediction"], [17, 18, "MethodName", "GPT"], [27, 29, "TaskName", "knowledge graphs"], [32, 33, "DatasetName", "ConceptNet"], [41, 42, "DatasetName", "ATOMIC"]]}
{"text": "While their model was able to generate new knowledge graph triples , it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre - trained GPT model , or whether this rather comes from the ability to generalise from the training triples .", "entities": [[34, 35, "MethodName", "GPT"]]}
{"text": "For the ConceptNet dataset , for instance , Jastrz\u0119bski et al ( 2018 ) found that most test triples are in fact minor variations of training triples .", "entities": [[2, 3, "DatasetName", "ConceptNet"]]}
{"text": "Along the same lines , Yao et al ( 2011 ) proposed a generative probabilistic model , inspired by LDA ( Blei et al , 2003 ) , in which relations are viewed as latent variables ( similar to topics in LDA ) .", "entities": [[19, 20, "MethodName", "LDA"], [41, 42, "MethodName", "LDA"]]}
{"text": "Turney ( 2005 ) proposed a method called Latent Relational Analysis ( LRA ) , which uses matrix factorization to learn relation embeddings based on co - occurrences of word pairs and dependency paths .", "entities": [[12, 13, "DatasetName", "LRA"]]}
{"text": "In recent years , a number of strategies based on distributional models have been explored that rely on similar intuitions but go beyond simple vector operations of word embeddings .", "entities": [[27, 29, "TaskName", "word embeddings"]]}
{"text": "2 For instance , Jameel et al ( 2018 ) introduced a variant of the GloVe word embedding model , in which relation vectors are jointly learned with word vectors .", "entities": [[15, 16, "MethodName", "GloVe"]]}
{"text": "In SeVeN ( Espinosa - Anke and Schockaert , 2018 ) and RELATIVE ( Camacho - Collados et al , 2019 ) , relation vectors are computed by averaging the embeddings of context words , while pair2vec uses an LSTM to summarise the contexts in which two given words occur , and Washio and Kato ( 2018 ) learn embeddings of dependency paths to encode word pairs .", "entities": [[39, 40, "MethodName", "LSTM"]]}
{"text": "Another line of work is based on the idea that relation embeddings should facilitate link prediction , i.e. given the first word and a relation vector , we should be able to predict the second word ( Marcheggiani and Titov , 2016 ; Simon et al , 2019 ) .", "entities": [[14, 16, "TaskName", "link prediction"]]}
{"text": "However , it should be noted that they focus on learning relation vectors from individual sentences , as a pre - training task for applications such as few - shot relation extraction .", "entities": [[30, 32, "TaskName", "relation extraction"]]}
{"text": "which has proven effective in factual knowledge probing ( Petroni et al , 2019 ) and text classification ( Schick and Sch\u00fctze , 2021 ; Tam et al , 2021 ; Le Scao and Rush , 2021 ) , among many others .", "entities": [[16, 18, "TaskName", "text classification"]]}
{"text": "The method then iteratively finds the best token to replace each mask , based on the gradient of the task - specific loss function .", "entities": [[22, 23, "MetricName", "loss"]]}
{"text": "To fine - tune the LM , we need training data and a loss function .", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "The loss function is based on the following intuition : the embeddings of word pairs that belong to the same relation type should be closer together than the embeddings of pairs that belong to different relations .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "Triplet Loss We draw a triplet from the relation dataset by selecting an anchor pair a = ( h a , t a ) , a positive example", "entities": [[0, 2, "MethodName", "Triplet Loss"]]}
{"text": "Classification Loss Following SBERT ( Reimers and Gurevych , 2019 ) , we use a classifier to predict whether two word pairs belong to the same relation .", "entities": [[0, 1, "TaskName", "Classification"]]}
{"text": "The classifier is jointly trained with the LM using the negative log likelihood loss function :", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "Note how the effective batch size thus increases quadratically , while the number of vectors that needs to be encoded by the LM remains unchanged .", "entities": [[4, 6, "HyperparameterName", "batch size"]]}
{"text": "Similar in - batch negative sampling has been shown to be effective in information retrieval ( Karpukhin et al , 2020 ;", "entities": [[13, 15, "TaskName", "information retrieval"]]}
{"text": "First we optimize the prompt over the training set with the triplet loss L t while the parameters of the LM are frozen .", "entities": [[11, 13, "MethodName", "triplet loss"]]}
{"text": "Subsequently , we fine - tune the LM with the resulting prompt , using the sum of the triplet loss L t and the classification loss L c over the same training set .", "entities": [[18, 20, "MethodName", "triplet loss"], [25, 26, "MetricName", "loss"]]}
{"text": "We do not use the classification loss during the prompt optimisation , as that would involve training the classifier while optimizing the prompt .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "We select the best hyper - parameters of the prompting methods based on the final loss over the validation set .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "We use RoBERTa ( Liu et al , 2019 ) as our main LM , where the initial weights were taken from the roberta - large model checkpoint shared by the Huggingface transformers model hub ( Wolf et al , 2020 ) .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}
{"text": "In each iteration , one of the input tokens is re - sampled and the loss is re - computed across the entire training set .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "4 For P - tuning , we train the weights that define the trigger embeddings ( i.e. the weights of the input vectors and the parameters of the LSTM ) for 2 epochs .", "entities": [[28, 29, "MethodName", "LSTM"]]}
{"text": "We evaluate RelBERT on two relation - centric tasks : solving analogy questions ( unsupervised ) and lexical relation classification ( supervised ) .", "entities": [[18, 20, "TaskName", "relation classification"]]}
{"text": "We use the five analogy datasets that were considered by Ushio et al ( 2021 ) : the SAT analogies dataset ( Turney et al , 2003 ) , the U2 and U4 analogy datasets , which were collected from an educational website 5 , and datasets that were derived 6 from BATS ( Gladkova et al , 2016 ) and the Google analogy dataset ( Mikolov et al , 2013b ) .", "entities": [[62, 63, "DatasetName", "Google"]]}
{"text": "In particular , they contain 37/337 ( SAT ) , 24/228 ( U2 ) , 48/432 ( U4 ) , 50/500 ( Google ) , and 199/1799 ( BATS ) questions for validation / testing .", "entities": [[22, 23, "DatasetName", "Google"]]}
{"text": "We will refer to this full version of the SAT dataset as SAT \u2020. Lexical Relation Classification We consider the task of predicting which relation a given word pair belongs to .", "entities": [[15, 17, "TaskName", "Relation Classification"]]}
{"text": "To solve this task , we train a multi - layer perceptron ( MLP ) which takes the ( frozen ) RelBERT embedding of the word pair as input .", "entities": [[13, 14, "DatasetName", "MLP"]]}
{"text": "We consider the following widelyused multi - class relation classification benchmarks : K&H+N", "entities": [[8, 10, "TaskName", "relation classification"]]}
{"text": "( Santus et al , 2016b ) , EVALution ( Santus et al , 2015 ) , and CogALex - V Subtask 2 ( Santus et al , 2016a ) .", "entities": [[8, 9, "DatasetName", "EVALution"]]}
{"text": "Table 1 shows the size of the training , validation and test sets for each of the relation classification dataset .", "entities": [[17, 19, "TaskName", "relation classification"]]}
{"text": "The hyperparameters of the MLP classifier are tuned on the validation set of each dataset .", "entities": [[4, 5, "DatasetName", "MLP"]]}
{"text": "As baselines , we consider two standard word embedding models : GloVe ( Pennington et al , 2014 ) and FastText ( Bojanowski et al , 2017 ) , where word pairs are represented by the vector difference of their word embeddings ( diff ) .", "entities": [[11, 12, "MethodName", "GloVe"], [20, 21, "MethodName", "FastText"], [40, 42, "TaskName", "word embeddings"]]}
{"text": "tion of the two word embeddings ( cat ) and their element - wise multiplication 8 ( dot ) .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "In particular , a four - word tuple ( a , b , c , d ) is encoded using a custom prompt and perplexity based scoring strategies are used to determine whether the word pair ( a , b ) has the same relation as the word pair ( c , d ) .", "entities": [[24, 25, "MetricName", "perplexity"]]}
{"text": "Finally , for the SAT \u2020 dataset , we compare with the published results from GPT - 3 ( Brown et al , 2020 ) , LRA ( Turney , 2005 ) and SuperSim ( Turney , 2013 ) ; for relation classification we report the published results of the LexNet ( Shwartz et al , 2016 ) and SphereRE relation classification models , taking the results from the latter publication .", "entities": [[15, 16, "MethodName", "GPT"], [26, 27, "DatasetName", "LRA"], [41, 43, "TaskName", "relation classification"], [60, 62, "TaskName", "relation classification"]]}
{"text": "In this section , we present our main experimental results , testing the relation embeddings learned by RelBERT on analogy questions ( Section 5.1 ) and relation classification ( Section 5.2 ) .", "entities": [[26, 28, "TaskName", "relation classification"]]}
{"text": "The RelBERT models substantially outperform the baselines on all datasets , except for the Google analogy dataset .", "entities": [[14, 15, "DatasetName", "Google"]]}
{"text": "We observe a particularly large improvement over the word embedding and SotA models on the EVALution dataset .", "entities": [[15, 16, "DatasetName", "EVALution"]]}
{"text": "The Google analogy dataset has been shown to be biased toward word similarity and therefore to be well suited to word embeddings ( Linzen , 2016 ; Rogers et al , 2017", "entities": [[1, 2, "DatasetName", "Google"], [11, 13, "TaskName", "word similarity"], [20, 22, "TaskName", "word embeddings"]]}
{"text": "This raises an important question : Does RelBERT provide us with a way to extract relational knowledge from the parameters of the As a further analysis , Table 5 shows a breakdown of the Google and BATS analogy results , showing the average performance on each of the top - level categories from these datasets .", "entities": [[34, 35, "DatasetName", "Google"]]}
{"text": "10 While RelBERT is outperformed by FastText on the morphological relations , it should be noted that the differences are small , while such relations are of a very different nature than those from the SemEval dataset .", "entities": [[6, 7, "MethodName", "FastText"]]}
{"text": "Nearest Neighbors barista : coffee baker : bread , brewer : beer , bartender : cocktail , winemaker : wine , bartender : drink , baker : cake bag : plastic bottle : plastic , bag : leather , container : plastic , box : plastic , jug : glass , bottle : glass duck : duckling chicken : chick , pig : piglet , cat : kitten , ox : calf , butterfly : larvae , bear : cub cooked : raw raw : cooked , regulated : unregulated , sober : drunk , loaded : unloaded , armed : unarmed , published : unpublished chihuahua : dog dachshund : dog , poodle : dog , terrier : dog , chinchilla : rodent , macaque : monkey , dalmatian : dog dog : dogs cat : cats , horse : horses , pig : pigs , rat : rats , wolf : wolves , monkey : monkeys spy : espionage pirate : piracy , robber : robbery , lobbyist : lobbying , scout : scouting , terrorist : terrorism , witch : witchcraft", "entities": [[121, 122, "MethodName", "chinchilla"]]}
{"text": "Figure 3 compares the performance of RelBERT with that of the vanilla pre - trained RoBERTa model ( i.e. when only the prompt is optimized ) .", "entities": [[15, 16, "MethodName", "RoBERTa"]]}
{"text": "In Figure 3 , we also compare the performance of our main RelBERT model , which is based on RoBERTa , with versions that were instead initialized with BERT ( Devlin et al , 2019 ) and ALBERT ( Lan et al , 2019 ) .", "entities": [[19, 20, "MethodName", "RoBERTa"], [28, 29, "MethodName", "BERT"], [37, 38, "MethodName", "ALBERT"]]}
{"text": "11 RoBERTa clearly outperforms the other two LMs , which is in accordance with findings from the literature suggesting that RoBERTa captures more semantic knowledge", "entities": [[1, 2, "MethodName", "RoBERTa"], [20, 21, "MethodName", "RoBERTa"]]}
{"text": "The neighbors are those word pairs whose Rel - BERT embedding has the highest cosine similarity within the full pair vocabulary .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "In our experimental results , we found the resulting relation embeddings to be of high quality , outperforming state - of - the - art methods on several analogy and relation classification benchmarks .", "entities": [[30, 32, "TaskName", "relation classification"]]}
{"text": "Among the models tested , we obtained the best results with RoBERTa , when using manually defined templates for encoding word pairs .", "entities": [[11, 12, "MethodName", "RoBERTa"]]}
{"text": "In all cases , the accuracy drop for the models without fine - tuning is substantial .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "We use RoBERTa in our main experiments and here we train RelBERT with ALBERT and BERT instead , and evaluate them on both of the analogy and relation classification tasks .", "entities": [[2, 3, "MethodName", "RoBERTa"], [13, 14, "MethodName", "ALBERT"], [15, 16, "MethodName", "BERT"], [27, 29, "TaskName", "relation classification"]]}
{"text": "In both tasks , we can confirm that RoBERTa achieves the best performance within the LMs , by a relatively large margin in most cases .", "entities": [[8, 9, "MethodName", "RoBERTa"]]}
{"text": "Table 9 shows additional results of word embeddings on analogy test together with RelBERT results .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Table 11 shows the best hyperparameters in the validation set of the MLPs for relation classification .", "entities": [[14, 16, "TaskName", "relation classification"]]}
{"text": "All the trigger tokens are initialized by mask tokens and updated based on the gradient of a loss function L t .", "entities": [[17, 18, "MetricName", "loss"]]}
{"text": "Concretely , let us denote the loss value with template T as L t ( T ) .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "w. Then we evaluate each token based on the loss function as z j = argmin w W j L t ( rep ( T , j , w ) )", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "We ignore any candidates that do not improve current loss value to further enhance the prompt quality .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings", "entities": [[1, 4, "TaskName", "Unsupervised Text Classification"], [7, 9, "TaskName", "Word Embeddings"]]}
{"text": "Text classification aims at mapping documents into a set of predefined categories .", "entities": [[0, 2, "TaskName", "Text classification"]]}
{"text": "Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "It thus provides a practical alternative to situations where lowcost text categorization is needed , as we illustrate with our application to operational risk incidents classification .", "entities": [[10, 12, "TaskName", "text categorization"]]}
{"text": "Document classification is a standard task in machine learning ( Joachims , 1999 ; Sebastiani , 2002 ) .", "entities": [[0, 2, "TaskName", "Document classification"]]}
{"text": "Its applications span a variety of \" use cases and contexts , e.g. , email filtering , news article clustering , clinical document classification , expertquestion matching \" .", "entities": [[22, 24, "TaskName", "document classification"]]}
{"text": "The standard process for text categorization relies on supervised and semisupervised approaches .", "entities": [[4, 6, "TaskName", "text categorization"]]}
{"text": "As hundreds of thousands of incidents had to be processed , text classification seemed a promising approach to assist in that mapping task .", "entities": [[11, 13, "TaskName", "text classification"]]}
{"text": "For that purpose , we tested it on standard text classification corpora .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "The novelty of our method hinges on the diversity of enrichment techniques of the categories label , including expert input that assists the semantic expansion and the use of word embeddings , both generic and domain specific .", "entities": [[29, 31, "TaskName", "word embeddings"]]}
{"text": "In this review of relevant work , we focus predominantly on techniques that have been proposed to overcome the requirement of having a large number of annotated data for standard text classification techniques .", "entities": [[30, 32, "TaskName", "text classification"]]}
{"text": "In a similar way , Miller et al ( 2016 ) represent each target category as a TF - IDF ( termfrequency / inverse document frequency ) vector obtained from Wikipedia and then use this category representation as an informed prior to Latent Dirichlet Allocation ( LDA ) , an unsupervised algorithm that finds the topics that best satisfy the data given the priors .", "entities": [[46, 47, "MethodName", "LDA"]]}
{"text": "Finally , our method makes use of word embeddings as a mean to enrich category label via semantic expansion .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "As far as we know , word embeddings have been used to improve text classification performance through their application as a document representation technique .", "entities": [[6, 8, "TaskName", "word embeddings"], [13, 15, "TaskName", "text classification"]]}
{"text": "Our approach for unsupervised text classification is based on the choice to model the task as a text similarity problem between two sets of words :", "entities": [[3, 6, "TaskName", "unsupervised text classification"], [17, 19, "TaskName", "text similarity"]]}
{"text": "As the taxonomy is designed to be universal , such tokens are not relevant to the text classification task and are thus removed .", "entities": [[16, 18, "TaskName", "text classification"]]}
{"text": "The text similarity metric will be details in section 3.4 .", "entities": [[1, 3, "TaskName", "text similarity"]]}
{"text": "Finally , we make use of word embeddings ( Bengio et al , 2003 ; Mikolov et", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "We adapt the Function - aware Component ( FAC ) originally used in supervised document classification ( Liu et al , 2018 ) .", "entities": [[14, 16, "TaskName", "document classification"]]}
{"text": "In order to evaluate our approach , we conduct experiments on five standard text classification corpora , described listed in Table 1 .", "entities": [[13, 15, "TaskName", "text classification"]]}
{"text": "As we use an unsupervised approach for text classification , we make use of the whole corpus of each dataset by aggregating training and test sets .", "entities": [[7, 9, "TaskName", "text classification"]]}
{"text": "We used the version created by Zhang et al ( 2015 ) who selected 4 largest classes from AG news corpus on the web with each instance containing class index , title and description fields .", "entities": [[18, 20, "DatasetName", "AG news"]]}
{"text": "( 5 ) The Google - Snippets 6 dataset contains the web search results related to 8 different domains such as business , computers and engineering .", "entities": [[4, 5, "DatasetName", "Google"]]}
{"text": "In the standard text classification datasets used in our experiments , category labels contain less than 5 words so representative documents were not relevant in the enrichment process .", "entities": [[3, 5, "TaskName", "text classification"]]}
{"text": "Overall , in addition to the full pipeline , which we refer to as all keywords , we also investigated whether semantic expansion solely through word embeddings could improve performance .", "entities": [[25, 27, "TaskName", "word embeddings"]]}
{"text": "On the unsupervised side , ( 1 ) we calculated a text similarity score between each docu - ment and the set of expert provided keywords ( 2 ) we enriched this list of initial keywords with their synonyms from WordNet .", "entities": [[11, 13, "TaskName", "text similarity"]]}
{"text": "In average , we manually added 9 words per label for 20NewsGroup , 17 words for AGs Corpus and Google - Snippets , 11 words for Yahoo - Answers and 14 words for 5AbstractsGroup .", "entities": [[19, 20, "DatasetName", "Google"]]}
{"text": "We chose 300 for the size of all word embeddings , it has been reported to perform well in classification tasks ( Mikolov et al , 2013a ) .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "The multi - class classification performance was evaluated in terms of precision ( Prec . ) , recall ( Rec . ) and F1 - score ( F1 ) .", "entities": [[1, 5, "TaskName", "multi - class classification"], [23, 26, "MetricName", "F1 - score"], [27, 28, "MetricName", "F1"]]}
{"text": "We notice also that combining all enrichments ( All keywords ) provides a modest increase in performance over embeddings only as shown by the results for Yahoo - Answers , 5AbstractsGroup and Google - Snippets .", "entities": [[32, 33, "DatasetName", "Google"]]}
{"text": "The ORX news service provides publicly reported operational risk loss data to its institutional members .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "For instance , \" In Japan , a building destruction resulting from a massive earthquake has caused power outage making AMD - based servers unbootable \" , could be classified as Natural Disaster , Dysfunctional ICT data processing or handling or Destruction / loss of physical assets among others .", "entities": [[32, 33, "DatasetName", "Disaster"], [43, 44, "MetricName", "loss"]]}
{"text": "In this paper , we present a method for unsupervised text classification based on computing the similarity between the documents to be classified and a rich description of the categories label .", "entities": [[9, 12, "TaskName", "unsupervised text classification"]]}
{"text": "The category label enrichment starts with humanexpert provided keywords but is then expanded through the use of word embeddings .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "We have not explored whether recent advances in word embeddings from instance ELMO ( Peters et al , 2018 ) and BERT ( Devlin et al , 2018 ) could add further benefits .", "entities": [[8, 10, "TaskName", "word embeddings"], [12, 13, "MethodName", "ELMO"], [21, 22, "MethodName", "BERT"]]}
{"text": "Collocation and its special forms , such as idioms , can be useful in many important tasks , e.g. , summarization ( Barrera and Verma , 2012 ) , question - answering ( Barrera et al , 2011 ) , language translation , topic segmentation , authorial style , and so on .", "entities": [[20, 21, "TaskName", "summarization"]]}
{"text": "To tackle this void , we introduce a feature - rich system called ICE ( short for Idiom and Collocation Extractor ) , which has two versions : one is flexible and pipelined seamlessly for research purposes as a component of a larger system such as a question answering system , and the second as a web - based tool for educational purposes .", "entities": [[47, 49, "TaskName", "question answering"]]}
{"text": "We present a case study of three simple and efficient Transformer - based architectures for predicting sentiment and emotion in multimodal data .", "entities": [[10, 11, "MethodName", "Transformer"], [18, 19, "DatasetName", "emotion"]]}
{"text": "Our experiments show that our small models are effective and outperform the publicly released versions of much larger , state - of - the - art multimodal sentiment analysis systems .", "entities": [[26, 29, "TaskName", "multimodal sentiment analysis"]]}
{"text": "These three modalities together make it easier for humans to convey emotion and sentiment .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "Thus , a machine learning model for sentiment analysis needs to learn the features and interactions of all three modalities .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}
{"text": "For example , a frown in the video can alter the emotion expressed in the text transcript , or audio intensity can help determine if a speaker is getting agitated .", "entities": [[11, 12, "DatasetName", "emotion"]]}
{"text": "The Transformer network [ Vaswani et al , 2017 ] , with its self - attention modules , has achieved strong performance in multimodal learning ; attention provides a natural way to model the relationship between pairs of modalities .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "In this work we investigate three small , lightweight , Transformer - based architectures for multimodal sentiment analysis and emotion recog - nition .", "entities": [[10, 11, "MethodName", "Transformer"], [15, 18, "TaskName", "multimodal sentiment analysis"], [19, 20, "DatasetName", "emotion"]]}
{"text": "Our first model is an implementation of the Late Fusion model commonly used as a baseline system , which assigns individual Transformer blocks to each of the three modalities for feature extraction and then combines these unimodal features to learn cross - modal interactions .", "entities": [[21, 22, "MethodName", "Transformer"]]}
{"text": "This model merges the features extracted using a late fusion pipeline , as well as those from an early fusion pipeline , where the three modalities are concatenated and passed through a single Transformer block for feature extraction ; .", "entities": [[33, 34, "MethodName", "Transformer"]]}
{"text": "We present experiments using these three models on three multimodal datasets : IEMOCAP [ Busso et al , 2008 ] , an emotion recognition dataset , and", "entities": [[12, 13, "DatasetName", "IEMOCAP"], [22, 24, "TaskName", "emotion recognition"]]}
{"text": "CMU - MOSI [ Zadeh et al , 2016 ] and CMU - MOSEI [ Zadeh et al , 2018b ] , two multimodal sentiment analysis datasets .", "entities": [[2, 3, "DatasetName", "MOSI"], [11, 14, "DatasetName", "CMU - MOSEI"], [23, 26, "TaskName", "multimodal sentiment analysis"]]}
{"text": "Our main contributions are as follows : We present three lightweight architectures for multimodal sentiment analysis that achieve comparable results to much larger , state - ofthe - art models .", "entities": [[13, 16, "TaskName", "multimodal sentiment analysis"]]}
{"text": "We do not give an exhaustive list of prior work in multimodal sentiment analysis , but focus on recent neural approaches that achieved state - of - the - art performance at their times of publication .", "entities": [[11, 14, "TaskName", "multimodal sentiment analysis"]]}
{"text": "The Memory Fusion Network ( MFN ) of Zadeh et al [ 2018a ] uses a separate LSTM to encode each of the three modalities and then uses attention to model cross - modal interactions for different combinations of modalities .", "entities": [[17, 18, "MethodName", "LSTM"]]}
{"text": "The Recurrent Attended Variation Embedding Network ( RAVEN ) of Wang et al", "entities": [[7, 8, "DatasetName", "RAVEN"]]}
{"text": "[ 2018c ] is an LSTM - based architecture that stores representations of each of the three modalities , which are then combined using a multi - attention block .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "Finally , the Multimodal Cyclic Translation Network ( MCTN ) of Pham et al [ 2019 ] produces multimodal features by translating one modality into another , learning a joint encoding in that direction , and then back - translating to learn a joint encoding in the other direction .", "entities": [[5, 6, "TaskName", "Translation"]]}
{"text": "The Transformer network [ Vaswani et al , 2017 ] has been used widely in neural machine translation [ Tubay and Costa - juss\u00e0 , 2018 ,", "entities": [[1, 2, "MethodName", "Transformer"], [16, 18, "TaskName", "machine translation"]]}
{"text": "Edunov et al , 2018 , Xia et al , 2019 , Devlin et al , 2019 and has proven effective for sentiment analysis and emotion recognition .", "entities": [[22, 24, "TaskName", "sentiment analysis"], [25, 27, "TaskName", "emotion recognition"]]}
{"text": "The Multimodal Transformer ( MuLT ) of Tsai et al [ 2019 ] modifies the Transformer block to compute cross - modal attention for two modalities at a time .", "entities": [[2, 3, "MethodName", "Transformer"], [15, 16, "MethodName", "Transformer"]]}
{"text": "The model works well in the unaligned case , and in the aligned case , it gives state of the art performance the Happy emotion in IEMO - CAP .", "entities": [[24, 25, "DatasetName", "emotion"], [28, 29, "DatasetName", "CAP"]]}
{"text": "The Factorized Multimodal Transformer ( FMT ) of introduces Factorized Multimodal Self - Attention ( FSM ) modules , which compute self - attention over unimodal , bimodal , and trimodal inputs in parallel .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "FMT gives state of the art performance in the word - aligned case on CMU - MOSI and on the Sad , Angry , and Neutral emotions in IEMOCAP .", "entities": [[16, 17, "DatasetName", "MOSI"], [28, 29, "DatasetName", "IEMOCAP"]]}
{"text": "Our three lightweight architectures are comprised of Transformer blocks [", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "The attention block of a Transformer uses multi - head attention , where each head computes scaled dot product attention : attn ( Q , K , V )", "entities": [[5, 6, "MethodName", "Transformer"], [7, 11, "MethodName", "multi - head attention"]]}
{"text": "= softmax QK T", "entities": [[1, 2, "MethodName", "softmax"]]}
{"text": "In addition , Vaswani et al note that positional encodings must be added to Transformer input because there is no sequential information present in the Transformer itself : P E ( pos , 2i )", "entities": [[14, 15, "MethodName", "Transformer"], [25, 26, "MethodName", "Transformer"]]}
{"text": "The outputs of these unimodal Transformers are then merged together using a simple summation , rather than the merge layer used in previous work [ Tsai et al , 2019 ] , and passed to a residual network of linear layers", "entities": [[36, 38, "MethodName", "residual network"]]}
{"text": "MuLT also uses three Transformers , one for each modality , to merge the two pairs sharing that modality as key / value ; our pairwise features are simply concatenated and passed to the output residual network .", "entities": [[35, 37, "MethodName", "residual network"]]}
{"text": "Figure 3 shows our Hybrid Fusion architecture , which uses both an early fusion approach that concatenates the inputs and passes them to a single Transformer to learn trimodal features , as well as a late fusion approach that passes each modality through a separate Transformer to learn unimodal features .", "entities": [[25, 26, "MethodName", "Transformer"], [45, 46, "MethodName", "Transformer"]]}
{"text": "We tune hyperparameter values for our model using the validation sets provided by our evaluation datasets ; we achieve the best validation performance using 8 attention blocks per Transformer , each with 5 attention heads , and a hidden size was set to 40 .", "entities": [[28, 29, "MethodName", "Transformer"]]}
{"text": "IEMOCAP [ Busso et al , 2008 ] consists of video recordings of 151 conversation sessions ( dialogues ) , totaling around 6k verbal interactions .", "entities": [[0, 1, "DatasetName", "IEMOCAP"]]}
{"text": "CMU - MOSI [ Zadeh et al , 2016 ] is a sentiment analysis dataset of 2199 short monologues labeled in the range [ \u22123 , 3 ] , with \u22123 being strongly negative and +3 being strongly positive .", "entities": [[2, 3, "DatasetName", "MOSI"], [12, 14, "TaskName", "sentiment analysis"]]}
{"text": "Following previous work , we report seven - class and binary accuracy , F1 score , mean absolute error , and correlation with human judgments .", "entities": [[11, 12, "MetricName", "accuracy"], [13, 15, "MetricName", "F1 score"]]}
{"text": "CMU - MOSEI [ Zadeh et al , 2018b ] is a sentiment and emotion analysis dataset of 23 K movie reviews from YouTube .", "entities": [[0, 3, "DatasetName", "CMU - MOSEI"], [14, 15, "DatasetName", "emotion"]]}
{"text": "As with CMU - MOSI , it is labeled in the range of [ \u22123 , 3 ] , and its evaluation metrics are the same as in CMU - MOSI .", "entities": [[4, 5, "DatasetName", "MOSI"], [30, 31, "DatasetName", "MOSI"]]}
{"text": "For word - level textual features we use the pretrained , 300 - dimensional , Common Crawl GloVe embeddings [ Pennington et al , 2014 ] .", "entities": [[15, 17, "DatasetName", "Common Crawl"], [17, 19, "MethodName", "GloVe embeddings"]]}
{"text": "We compare our results with the state - of - the - art Multimodal Transformer ( MuLT ) 1", "entities": [[14, 15, "MethodName", "Transformer"]]}
{"text": "[ Tsai et al , 2019 ] and Factorized Multimodal Transformer ( FMT ) , as well as Memory Fusion Network ( MFN )", "entities": [[10, 11, "MethodName", "Transformer"]]}
{"text": "[ Zadeh et al , 2018a ] , Recurrent Attended Variation Embedding Network ( RAVEN )", "entities": [[14, 15, "DatasetName", "RAVEN"]]}
{"text": "[ Zadeh et al , 2018c ] , and Multimodal Cyclic Translation Network ( MCTN )", "entities": [[11, 12, "TaskName", "Translation"]]}
{"text": "We perform fairly well on IEMOCAP , which has around 2717 training samples ; we achieve scores around 1 - 2 % below the best - performing model , FMT .", "entities": [[5, 6, "DatasetName", "IEMOCAP"]]}
{"text": "Late Fusion models give state of the art results on seven - way and binary accuracy , respectively .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "The CMU - MOSEI dataset is much larger than IEMOCAP and CMU - MOSI , with close to 16265 training samples .", "entities": [[1, 4, "DatasetName", "CMU - MOSEI"], [9, 10, "DatasetName", "IEMOCAP"], [13, 14, "DatasetName", "MOSI"]]}
{"text": "Neither MARN [ Zadeh et al , 2018c ] nor FMT reports results on CMU - MOSEI , so they are omitted from Table 3 .", "entities": [[14, 17, "DatasetName", "CMU - MOSEI"]]}
{"text": "The smaller number of parameters in our model reduces the risk of overfitting on smaller datasets , while still achieving good performance on larger datasets .", "entities": [[2, 5, "HyperparameterName", "number of parameters"]]}
{"text": "On the smallest dataset , CMU - MOSI , training MuLT * took just over seven minutes , while FMT * took 2.5 hours .", "entities": [[7, 8, "DatasetName", "MOSI"]]}
{"text": "Our models train in under three minutes and outperform both MuLT * and FMT * , and this difference in training speed holds for CMU - MOSI and CMU - MOSEI as well .", "entities": [[26, 27, "DatasetName", "MOSI"], [28, 31, "DatasetName", "CMU - MOSEI"]]}
{"text": "We also conduct experiments on a substantially reduced IEMOCAP training subset of 1284 samples , matching the size of CMU - MOSI , which we create by randomly sampling from the full IEMO - CAP training set .", "entities": [[8, 9, "DatasetName", "IEMOCAP"], [21, 22, "DatasetName", "MOSI"], [34, 35, "DatasetName", "CAP"]]}
{"text": "Table 5 shows the results of our models , as well as MuLT * and FMT * , retrained on this smaller IEMOCAP training set , and evaluated on the full IEMOCAP test set .", "entities": [[22, 23, "DatasetName", "IEMOCAP"], [31, 32, "DatasetName", "IEMOCAP"]]}
{"text": "We perform ablation experiments on our models using the IEMOCAP dataset ; ablation results for CMU - MOSI and CMU - MOSEI are omitted due to space constraints , but exhibit similar trends .", "entities": [[9, 10, "DatasetName", "IEMOCAP"], [17, 18, "DatasetName", "MOSI"], [19, 22, "DatasetName", "CMU - MOSEI"]]}
{"text": "We have presented three lightweight architectures for multimodal sentiment analysis and emotion recognition .", "entities": [[7, 10, "TaskName", "multimodal sentiment analysis"], [11, 13, "TaskName", "emotion recognition"]]}
{"text": "Our proposed models are much smaller in size compared to existing state - of - the - art models ; they are able to attain new state - of - the - art scores on the CMU - MOSI and CMU - MOSEI datasets on two metrics , while remaining competitive on the others .", "entities": [[38, 39, "DatasetName", "MOSI"], [40, 43, "DatasetName", "CMU - MOSEI"]]}
{"text": "We hope that our simple architectures for sentiment and emotion detection , currently the fastest and best - performing publicly available system , as well as the insights revealed in our experimental results , can be useful for further research in the field .", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "Deep Multi - Task Learning for Aspect Term Extraction with Memory Interaction *", "entities": [[1, 5, "TaskName", "Multi - Task Learning"], [7, 9, "TaskName", "Term Extraction"]]}
{"text": "We propose a novel LSTM - based deep multi - task learning framework for aspect term extraction from user review sentences .", "entities": [[4, 5, "MethodName", "LSTM"], [8, 12, "TaskName", "multi - task learning"], [15, 17, "TaskName", "term extraction"]]}
{"text": "Sentimental sentence constraint is also added for more accurate prediction via another LSTM .", "entities": [[12, 13, "MethodName", "LSTM"]]}
{"text": "The aspect - based sentiment analysis ( ABSA ) task is to identify opinions expressed towards specific entities such as laptop or attributes of entities such as price ( Liu , 2012a ) .", "entities": [[1, 6, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "This task involves three subtasks : Aspect Term Extraction ( ATE ) , Aspect Polarity Detection and Aspect Category Detection .", "entities": [[7, 9, "TaskName", "Term Extraction"], [17, 20, "TaskName", "Aspect Category Detection"]]}
{"text": "However CMLA merely employs standard GRU without extended memories .", "entities": [[5, 6, "MethodName", "GRU"]]}
{"text": "We propose MIN ( Memory Interaction Network ) , a novel LSTM - based deep multi - task learning framework for the ATE task .", "entities": [[11, 12, "MethodName", "LSTM"], [15, 19, "TaskName", "multi - task learning"]]}
{"text": "The aspect - opinion relationship is established based on neural memory interactions between aspect extraction and opinion extraction where the global indicator score of opinion terms and local positional relevance between aspects and opinions are considered .", "entities": [[13, 15, "TaskName", "aspect extraction"]]}
{"text": "To ensure that aspects are from sentimental sentences , MIN employs a third LSTM for sentimental sentence classification facilitating more accurate aspect term extraction .", "entities": [[13, 14, "MethodName", "LSTM"], [16, 18, "TaskName", "sentence classification"], [22, 24, "TaskName", "term extraction"]]}
{"text": "In our multi - task learning framework , three tasks are involved : ( 1 ) aspect term extraction ( ATE ) , ( 2 ) opinion word extraction and ( 3 ) sentimental sentence classification .", "entities": [[2, 6, "TaskName", "multi - task learning"], [17, 19, "TaskName", "term extraction"], [34, 36, "TaskName", "sentence classification"]]}
{"text": "We design a taskspecific LSTM , namely , A - LSTM , O - LSTM and S - LSTM , for tackling each of the above tasks respectively .", "entities": [[4, 5, "MethodName", "LSTM"], [10, 11, "MethodName", "LSTM"], [14, 15, "MethodName", "LSTM"], [18, 19, "MethodName", "LSTM"]]}
{"text": "The first component of our proposed framework consists of A - LSTM and O - LSTM where we equip LSTMs with extended operational memories and some operations are defined over the memories for task - level memory interactions .", "entities": [[11, 12, "MethodName", "LSTM"], [15, 16, "MethodName", "LSTM"]]}
{"text": "This is achieved by employing a vanilla LSTM , namely , S - LSTM .", "entities": [[7, 8, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "The first component of our framework MIN is composed of A - LSTM and O - LSTM .", "entities": [[12, 13, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"]]}
{"text": "A - LSTM involves a large aspect memory H", "entities": [[2, 3, "MethodName", "LSTM"]]}
{"text": "As for O - LSTM , similarly , an opinion memory H", "entities": [[4, 5, "MethodName", "LSTM"]]}
{"text": "We use the aspect term annotations in the training data for training A - LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "Then such opinion words are used as training data for O - LSTM .", "entities": [[12, 13, "MethodName", "LSTM"]]}
{"text": "In the memory - enhanced A - LSTM and O - LSTM , we manually design three kinds of operations : ( 1 ) READ to select n m pieces of aspect ( opinion ) hidden states from the past memories and build H A t ( H O t ) ; ( 2 ) DIGEST to distill an aspect ( opinion ) - specific summary m", "entities": [[7, 8, "MethodName", "LSTM"], [11, 12, "MethodName", "LSTM"]]}
{"text": "A t ( m O t ) from H A t ( H O t ) where influences of opinion terms and relative positions of inputs are considered ; ( 3 ) INTERACT to perform interaction between A - LSTM and O - LSTM using the task specific summaries ( i.e. , m A t and m O t ) .", "entities": [[39, 40, "MethodName", "LSTM"], [43, 44, "MethodName", "LSTM"]]}
{"text": "Consider the work flow of A - LSTM for aspect term extraction .", "entities": [[7, 8, "MethodName", "LSTM"], [10, 12, "TaskName", "term extraction"]]}
{"text": "Since opinion words and aspect terms should co - occur , the goal of A - LSTM participating in memory interactions is to acquire opinion summaries from O - LSTM ( i.e. , m O t ) for better aspect prediction .", "entities": [[16, 17, "MethodName", "LSTM"], [29, 30, "MethodName", "LSTM"]]}
{"text": "First of all , MIN will READ n m pieces of opinion memories which are most related to w t from O - LSTM .", "entities": [[23, 24, "MethodName", "LSTM"]]}
{"text": "[ h O t\u22121 ; ... ; h O t\u2212nm ] where h O t\u2212i is the ( t \u2212 i ) - th hidden state from O - LSTM .", "entities": [[29, 30, "MethodName", "LSTM"]]}
{"text": "Then MIN will DIGEST the collected opinion memories H O t in the A - LSTM .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "In the last operation INTERACT , A - LSTM communicates with O - LSTM by acquiring m", "entities": [[8, 9, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "O t from O - LSTM and incorporating the summary into the memory update .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "A t ( 4 ) where W A * , U A * and b A * are weight parameters of the A - LSTM and \u03c3 is the sigmoid activation function .", "entities": [[24, 25, "MethodName", "LSTM"], [29, 31, "MethodName", "sigmoid activation"]]}
{"text": "A t [ 1 ] is the most immediate hidden memory of A - LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "MIN blends the opinion summary from O - LSTM with the memory from A - LSTM .", "entities": [[8, 9, "MethodName", "LSTM"], [15, 16, "MethodName", "LSTM"]]}
{"text": "Hence , we conduct bi - directional training for A - LSTM .", "entities": [[11, 12, "MethodName", "LSTM"]]}
{"text": "The work flow of memory interaction and the update process of the internal memories in O - LSTM are kept same with those in A - LSTM except the DIGEST operation .", "entities": [[17, 18, "MethodName", "LSTM"], [26, 27, "MethodName", "LSTM"]]}
{"text": "Specifically , we set m A t , the task - specific summary of A - LSTM , as h A t .", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "The second component of MIN is a generic LSTM called S - LSTM for discriminating sentimental sentences and non - sentimental sentences .", "entities": [[8, 9, "MethodName", "LSTM"], [12, 13, "MethodName", "LSTM"]]}
{"text": "Specifically , S - LSTM learns the sentimental representation h S T of the sentence and then feeds it in aspect prediction as a soft constraint : On the whole , our proposed MIN framework has three LSTMs and each of them is differentiable .", "entities": [[4, 5, "MethodName", "LSTM"]]}
{"text": "For A - LSTM and O - LSTM , we use the token - level cross - entropy error between the predicted distribution P ( y T t | x t ) and the gold standard distribution P ( y T , g t | x t ) as the loss function ( T { A , O } ) : Loss ( T )", "entities": [[3, 4, "MethodName", "LSTM"], [7, 8, "MethodName", "LSTM"], [50, 51, "MetricName", "loss"]]}
{"text": "As gold standard annotations for opinion words are not provided , we select words with strong subjectivity from MPQA 1 as potential opinion words .", "entities": [[18, 19, "DatasetName", "MPQA"]]}
{"text": "To evaluate the proposed MIN framework , we perform comparison with the following two groups of methods : ( 1 ) CRF based methods : CRF : Conditional Random Fields with basic feature templates 2 and word embeddings .", "entities": [[21, 22, "MethodName", "CRF"], [25, 26, "MethodName", "CRF"], [36, 38, "TaskName", "word embeddings"]]}
{"text": "Semi - CRF : First - order semi - Markov conditional random fields ( Sarawagi et al , 2004 ) and the feature template in Cuong et al ( 2014 ) is adopted .", "entities": [[2, 3, "MethodName", "CRF"]]}
{"text": "DLIREC ( Toh and Wang , 2014 ) , AUEB ( Xenos et al , 2016 ) : Top - ranked CRF - based systems in ATE subtask in SemEval ABSA challenges ( Pontiki et al , 2014 ( Pontiki et al , , 2016 .", "entities": [[21, 22, "MethodName", "CRF"]]}
{"text": "WDEmb ( Yin et al , 2016 ) : Enhanced CRF with word embeddings , linear context embeddings and dependency path embeddings .", "entities": [[10, 11, "MethodName", "CRF"], [12, 14, "TaskName", "word embeddings"]]}
{"text": "( 2 ) Neural Network based methods LSTM :", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "Vanilla bi - directional LSTM with pre - trained word embeddings 3 .", "entities": [[4, 5, "MethodName", "LSTM"], [9, 11, "TaskName", "word embeddings"]]}
{"text": "Dependency Tree based Recursive Neural Network with CRF extractor 4 .", "entities": [[7, 8, "MethodName", "CRF"]]}
{"text": "For datasets in the restaurant domain , we train word embeddings of dimension 200 with word2vec ( Mikolov et al , 2013 ) on Yelp reviews 5 .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "As we use our own implementation of LSTM , the reported results are different from those in ( Liu et al , 2015 ) 4", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "The dimension of hidden representations are 100 , 20 , 40 for A - LSTM , O - LSTM and S - LSTM respectively .", "entities": [[14, 15, "MethodName", "LSTM"], [18, 19, "MethodName", "LSTM"], [22, 23, "MethodName", "LSTM"]]}
{"text": "The dropout rate for O - LSTM and S - LSTM is 0.4 .", "entities": [[6, 7, "MethodName", "LSTM"], [10, 11, "MethodName", "LSTM"]]}
{"text": "Besides , our MIN outperforms WDEmb , a strong CRF - based system benefiting from several kinds of useful word embeddings , by 2.1 % on D 1 .", "entities": [[9, 10, "MethodName", "CRF"], [19, 21, "TaskName", "word embeddings"]]}
{"text": "With memory interactions and consideration of sentimental sentence , our MIN boosts the performance of vanilla bi - directional LSTM ( +2.0 % and +1.7 % respectively ) .", "entities": [[19, 20, "MethodName", "LSTM"]]}
{"text": "The soft sentimental constraint proves to be useful since MIN is 1.5 % and 1.0 % superior than the framework without S - LSTM on D 1 and D 2 respectively .", "entities": [[23, 24, "MethodName", "LSTM"]]}
{"text": "O - LSTM brings in the largest performance gains on D 2 compared with ablated framework ( i.e. , MIN without O - LSTM ) , verifying our postulation that aspect - opinion \" interaction \" is more effective than only considering aspect terms .", "entities": [[2, 3, "MethodName", "LSTM"], [23, 24, "MethodName", "LSTM"]]}
{"text": "We also observe that the contribution of O - LSTM is less significant than that of bi - directionality on D 1 ( +1.6 % vs +2.0 % ) .", "entities": [[9, 10, "MethodName", "LSTM"]]}
{"text": "We propose Memory Interaction Network ( MIN ) , a multi - task learning framework , to detect aspect terms from the online user reviews .", "entities": [[10, 14, "TaskName", "multi - task learning"]]}
{"text": "A novel LSTM unit with extended memories is developed for memory interactions .", "entities": [[2, 3, "MethodName", "LSTM"]]}
{"text": "Contextual and Non - Contextual Word Embeddings : an in - depth Linguistic Investigation", "entities": [[5, 7, "TaskName", "Word Embeddings"]]}
{"text": "In this paper we present a comparison between the linguistic knowledge encoded in the internal representations of a contextual Language Model ( BERT ) and a contextual - independent one ( Word2vec ) .", "entities": [[22, 23, "MethodName", "BERT"]]}
{"text": "We show that , although BERT is capable of understanding the full context of each word in an input sequence , the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextualindependent model .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "We also find that BERT is able to encode sentence - level properties even within single - word embeddings , obtaining comparable or even superior results than those obtained with sentence representations .", "entities": [[4, 5, "MethodName", "BERT"], [17, 19, "TaskName", "word embeddings"]]}
{"text": "In particular , we define two research questions , aimed at understanding : ( i ) which is the best method for combining BERT and Word2vec word representations into sentence embeddings and how they differently encode properties related to the linguistic structure of a sentence ; ( ii ) whether such sentence - level knowledge is preserved within BERT single - word representations .", "entities": [[23, 24, "MethodName", "BERT"], [29, 31, "TaskName", "sentence embeddings"], [58, 59, "MethodName", "BERT"]]}
{"text": "In this paper : ( i ) we perform an in - depth study aimed at understanding the linguistic knowledge encoded in a contextual ( BERT ) and a contextual - independent ( Word2vec )", "entities": [[25, 26, "MethodName", "BERT"]]}
{"text": "Neural Language Model ; ( ii ) we evaluate the best method for obtaining sentence - level representations from BERT and Word2vec according to a wide spectrum of probing tasks ; ( iii ) we compare the results obtained by BERT and Word2vec according to the different combining methods ; ( iv ) we study whether BERT is able to encode sentence - level properties within its single word representations .", "entities": [[19, 20, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"], [56, 57, "MethodName", "BERT"]]}
{"text": "Li et al , 2016 ; K\u00e1d\u00e1r et al , 2017 ) to more domain specific approaches , such as interpreting attention mechanisms ( Raganato and Tiedemann , 2018 ; Kovaleva et al , 2019 ; Vig and Belinkov , 2019 ) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word / sentence embeddings of a pre - trained model as training features ( Conneau et al , 2018 ; Zhang and Bowman , 2018 ; Hewitt and Liang , 2019 ) .", "entities": [[65, 67, "TaskName", "sentence embeddings"]]}
{"text": "Jawahar et al ( 2019 ) investigated the representations learned at different layers of BERT , showing that lower layer representations are usually better for capturing surface features , while embeddings from higher layers are better for syntactic and semantic properties .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "Using a suite of probing tasks , Tenney et al ( 2019a ) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline : POS tagging , parsing , NER , semantic roles and then coreference .", "entities": [[20, 21, "MethodName", "BERT"], [36, 37, "TaskName", "NER"]]}
{"text": ", instead , quantified differences in the transferability of individual layers between different models , showing that higher layers of RNNs ( ELMo ) are more task - specific ( less general ) , while transformer layers ( BERT ) do not exhibit this increase in task - specificity .", "entities": [[22, 23, "MethodName", "ELMo"], [38, 39, "MethodName", "BERT"]]}
{"text": "In particular , they showed that sentence representations based on averaged Word2vec embeddings are particularly effective and encode a wide amount of information regarding sentence length , while LSTM auto - encoders are very effective at capturing word order and word content .", "entities": [[28, 29, "MethodName", "LSTM"]]}
{"text": "Focusing instead on the geometry of the representation space , Ethayarajh ( 2019 ) first showed that the contextualized word representations of ELMo , BERT and GPT - 2 produce more context specific representations in the upper layers and then proposed a method for creating a new type of static embedding that outperforms GloVe and FastText on many benchmarks , by simply taking the first principal component of contextualized representations in lower layers of BERT .", "entities": [[22, 23, "MethodName", "ELMo"], [24, 25, "MethodName", "BERT"], [26, 27, "MethodName", "GPT"], [53, 54, "MethodName", "GloVe"], [55, 56, "MethodName", "FastText"], [74, 75, "MethodName", "BERT"]]}
{"text": "We studied how layer - wise internal representations of BERT encode a wide spectrum of linguistic properties and how such implicit knowledge differs from that learned by a context - independent model such as Word2vec .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "The first consists in evaluating which is the best method for generating sentence - level embeddings using BERT and Word2vec single - word representations .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "In particular , we defined a simple probing model that takes as input layer - wise BERT and Word2vec combined representations for each sentence of a gold standard Universal Dependencies ( UD ) (", "entities": [[16, 17, "MethodName", "BERT"], [28, 30, "DatasetName", "Universal Dependencies"], [31, 32, "DatasetName", "UD"]]}
{"text": "To do so , we performed our set of probing tasks using the embeddings extracted from both BERT and Word2vec individual tokens .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "In particular , we considered the word representations corresponding to the first , last and two internal tokens for each sentence of the UD dataset .", "entities": [[23, 24, "DatasetName", "UD"]]}
{"text": "In order to perform the probing experiments on gold annotated sentences , we relied on the Universal Dependencies ( UD ) English dataset .", "entities": [[16, 18, "DatasetName", "Universal Dependencies"], [19, 20, "DatasetName", "UD"]]}
{"text": "The dataset includes three UD English treebanks : UD English - ParTUT , a conversion of a multilin - gual parallel treebank consisting of a variety of text genres , including talks , legal texts and Wikipedia articles ( Sanguinetti and Bosco , 2015 ) ; the Universal Dependencies version annotation from the GUM corpus ( Zeldes , 2017 ) ; the English Web Treebank ( EWT ) , a gold standard universal dependencies corpus for English ( Silveira et al , 2014 ) .", "entities": [[4, 5, "DatasetName", "UD"], [8, 9, "DatasetName", "UD"], [47, 49, "DatasetName", "Universal Dependencies"], [53, 54, "DatasetName", "GUM"], [62, 65, "DatasetName", "English Web Treebank"], [72, 74, "DatasetName", "universal dependencies"]]}
{"text": "Specifically , in our work , each probing task correspond to predict the value of a specific linguistic feature automatically extracted from the POS tagged and dependency parsed sentences in the English UD dataset .", "entities": [[32, 33, "DatasetName", "UD"]]}
{"text": "More complex aspects of sentence structure are derived from syntactic annotation and model global and local properties of parsed tree structure , with a focus on subtrees of verbal heads , the order of subjects and objects with respect to the verb , the distribution of UD syntactic relations and features referring to the use of subordination .", "entities": [[46, 47, "DatasetName", "UD"]]}
{"text": "We relied on a pre - trained English version of BERT ( BERT - base uncased , 12 layers ) for the extraction of the contextual word embeddings .", "entities": [[10, 11, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"], [26, 28, "TaskName", "word embeddings"]]}
{"text": "In the same manner as BERT 's contextual representations , we experimented four combining methods : Max - pooling , Min - pooling , Mean and Sum .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "To do so , we firstly extracted from each sentence in the UD dataset the corresponding word embeddings using the output of the internal representations of Word2vec and BERT layers 1 As suggested in Jawahar et al ( 2019 ) ( from input layer - 12 to output layer - 1 ) .", "entities": [[12, 13, "DatasetName", "UD"], [16, 18, "TaskName", "word embeddings"], [28, 29, "MethodName", "BERT"]]}
{"text": "Since the majority of our probing features is correlated to sentence length , we compared probing results with the ones obtained with a baseline computed by measuring the \u03c1 coefficient between the length of the UD sentences and each of the 68 probing features .", "entities": [[35, 36, "DatasetName", "UD"]]}
{"text": "As a general remark , we notice that the scores obtained by Word2vec and BERT 's internal representations outperforms the ones obtained with the correlation baseline , thus showing that both models are capable of implicitly encoding a wide spectrum of linguistic phenomena .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "Interestingly , we can notice that Word2vec sentence representations outperform BERT ones when considering all the probing features in average .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "For what concerns Word2vec representations , we notice that the Sum method prove to be the best one for encoding raw text and syntactic features , while mo - rophosyntactic properties are better represented averaging all the word embeddings ( Mean ) .", "entities": [[37, 39, "TaskName", "word embeddings"]]}
{"text": "In general , best results are obtained with probing tasks related to morphosyntactic and syntactic features , like the distribution of POS ( e.g. upos dist PRON , upos dist VERB ) or the maximum depth of the syntactic tree ( parse depth ) .", "entities": [[34, 36, "HyperparameterName", "maximum depth"]]}
{"text": "If we look instead at the average \u03c1 scores obtained with BERT layerwise representations ( Figure 1 ) , we observe that , differently from Word2vec , best results are the ones related to raw - text features , such as sentence length or Type / Token Ratio .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "The only exceptions mainly concern some of the linguistic features related to syntactic properties , e.g. the average length of dependency links ( avg links len ) or the maximum depth of the syntactic tree ( parse depth ) , for which best scores across layers are obtained with the Sum strategy .", "entities": [[29, 31, "HyperparameterName", "maximum depth"]]}
{"text": "Interestingly , the representations corresponding to the [ CLS ] token , although considered as a summarization of the entire input sequence , achieve results comparable to those obtained with Maxand Minpooling methods .", "entities": [[16, 17, "TaskName", "summarization"]]}
{"text": "In order to investigate more in depth how the linguistic knowledge encoded by BERT across its layers differs from that learned by Word2vec , we report in Table 4 average \u03c1 differences between the two models according to the four combining strategies .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "As a general remark , we can notice that , regardless of the aggregation strategy taken into account , BERT and Word2vec sentence representations achieve quite similar results on average .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "Hence , although BERT is capable of understanding the full context of each word in an input sequence , the amount of linguistic knowledge implicitly encoded in its aggregated sentence representations is still comparable to that which can be achieved with a non - contextual language model .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "In Figure 2 we report instead the differences between BERT and Word2vec scores for all the 68 probing features ( ordered by correlation with sentence length ) .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "As a first remark , we notice that there is a clear distinction in terms of \u03c1 scores between features better predicted by BERT and Word2vec .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "In fact , features most related to syntactic properties ( left heatmap ) are those for which BERT results are generally higher with respect to those obtained with Word2vec .", "entities": [[11, 12, "MethodName", "heatmap"], [17, 18, "MethodName", "BERT"]]}
{"text": "This result demonstrates that BERT , unlike a non - contextual language model as Word2vec , is able to encode information within its representa - tions that involves the entire input sequence , thus making more simple to solve probing tasks that refer to syntatic characteristics .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "Focusing instead on the right heatmap , we observe that Word2vec non - contextual representations are still capable of encoding a wide spectrum of linguistic properties with higher \u03c1 values compared to BERT ones , especially if we consider scores closer to BERT 's output layers ( from - 4 to - 1 ) .", "entities": [[5, 6, "MethodName", "heatmap"], [32, 33, "MethodName", "BERT"], [42, 43, "MethodName", "BERT"]]}
{"text": "Once we have probed the linguistic knowledge encoded by BERT and Word2vec using different strategies for computing sentence embeddings , we investigated how much information about the structure of a sentence is encoded within single - word contextual representations .", "entities": [[9, 10, "MethodName", "BERT"], [17, 19, "TaskName", "sentence embeddings"]]}
{"text": "For doing so , we performed our sentence - level probing tasks using a single BERT word embedding for each sentence in the UD dataset .", "entities": [[15, 16, "MethodName", "BERT"], [23, 24, "DatasetName", "UD"]]}
{"text": "We tested four different words , corresponding to the first , the last and two internal tokens for each sentence in the UD dataset .", "entities": [[22, 23, "DatasetName", "UD"]]}
{"text": "In Table 5 we report average \u03c1 scores obtained by BERT ( BERT - * ) and Word2vec ( Word2vec - * ) according to word - level representations extracted from the four tokens mentioned above .", "entities": [[10, 11, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"]]}
{"text": "As a first remark , we can clearly notice that even with a single - word embedding BERT is able to encode a wide spectrum of sentence - level linguistic properties .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "An interesting observation is that , except for the raw text features , for which the best scores are achieved using [ CLS ] , higher performance are obtained with the embeddings corresponding to BERT - 4 , i.e. the last token of each sentence .", "entities": [[34, 35, "MethodName", "BERT"]]}
{"text": "Comparing the results with those achieved using Word2vec word embeddings , we notice that BERT scores greatly outperform Word2vec for all the probing tasks .", "entities": [[8, 10, "TaskName", "word embeddings"], [14, 15, "MethodName", "BERT"]]}
{"text": "Since the latter results demonstrated that BERT is capable of encoding many sentence - level properties within its single word representations , as a last analysis , we decided to compare these results with the ones obtained using sentence embeddings .", "entities": [[6, 7, "MethodName", "BERT"], [38, 40, "TaskName", "sentence embeddings"]]}
{"text": "In particular , Figure 3 reports probing scores obtained by BERT single word ( tok * ) and Mean sentence representations ( sent ) extracted from the output layer ( - 1 ) and from the layer that achieved best results in average ( - 8 ) .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "As already mentioned , for many of these probing tasks , word embeddings performance is comparable to that obtained with the aggregated sentence representations .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "Interestingly , we can notice that aggregated sentence representations are generally better for predicting properties belonging to the left heatmap , i.e. to the group of features more related to syntactic properties .", "entities": [[19, 20, "MethodName", "heatmap"]]}
{"text": "On the contrary , probing features belonging to the right heatmap , therefore more close to raw text and morphosyntactic properties , are generally better predicted using single word embeddings , especially when considering the inner representations corresponding to the last token in each sentence ( tok 4 ) .", "entities": [[10, 11, "MethodName", "heatmap"], [28, 30, "TaskName", "word embeddings"]]}
{"text": "Focusing instead on differences in performance between the two considered layers , we can notice that regardless of the method used to predict each feature , the representations learned by BERT tend to lose their precision in encoding our set of linguistic properties , most likely because the model is storing task - specific information ( Masked Language Modeling task ) at the expense of its ability to encode general knowledge about the language .", "entities": [[30, 31, "MethodName", "BERT"], [56, 59, "TaskName", "Masked Language Modeling"], [69, 71, "TaskName", "general knowledge"]]}
{"text": "In this paper we studied the linguistic knowledge implicitly encoded in the internal representations of a contextual Language Model ( BERT ) and a contextual - independent one ( Word2vec ) .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "Using a suite of 68 probing tasks and testing different methods for combining word embeddings into sentence representations , we showed that BERT and Word2vec encode a wide set of sentence - level linguistic properties in a similar manner .", "entities": [[13, 15, "TaskName", "word embeddings"], [22, 23, "MethodName", "BERT"]]}
{"text": "Nevertheless , we found that for Word2vec the best method for obtaining sentence representations is the Sum , while BERT is more effective when averaging all the single - word representations ( Mean method ) .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "Moreover , we showed that BERT is able in storing features that are mainly related to raw text and syntactic properties , while Word2vec is good at predicting morphosyntactic characteristics .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "Finally , we showed that BERT is able to encode sentence - level linguistic phenomena even within single - word embeddings , exhibiting comparable or even superior performance than those obtained with aggregated sentence representations .", "entities": [[5, 6, "MethodName", "BERT"], [19, 21, "TaskName", "word embeddings"]]}
{"text": "Abir Rahali ( Rahali et al , 2021 ) proposed a approach for automatic misogyny detection in social media using attention based bidirectional LSTM .", "entities": [[22, 24, "MethodName", "bidirectional LSTM"]]}
{"text": "Mario Anzovino , Elisabetta Fersini ( Anzovino et al , 2018 ) proposed a method for Automatic Identification and Classification of Misogynistic Language on Twitter .", "entities": [[19, 20, "TaskName", "Classification"]]}
{"text": "In this paper they focus on recurrent neural network ( RNN ) approach using a Bidirectional Long Short Term Memory ( Bi - LSTM ) .", "entities": [[23, 24, "MethodName", "LSTM"]]}
{"text": "We have used lemmatization for grouping together the different forms of a word into a single word .", "entities": [[3, 4, "TaskName", "lemmatization"]]}
{"text": "NLTK wordnet ( Loper and Bird , 2002 ) is used for lemmatization .", "entities": [[12, 13, "TaskName", "lemmatization"]]}
{"text": "TfidfVectorizer ( Kumar and Subba , 2020 ) is used for converting the text into numerical features .", "entities": [[2, 3, "DatasetName", "Kumar"]]}
{"text": "Tokenizer by keras library is used for LSTM and Bert .", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "For Logistic regression and SVM we have used TfidfVectorizer from scikit - learn library .", "entities": [[1, 3, "MethodName", "Logistic regression"], [4, 5, "MethodName", "SVM"]]}
{"text": "We have used the scikit - learn library for logistic regression based models and SVM ( support vector machines ) models .", "entities": [[9, 11, "MethodName", "logistic regression"], [14, 15, "MethodName", "SVM"]]}
{"text": "Keras is used for LSTM and BERT .", "entities": [[4, 5, "MethodName", "LSTM"], [6, 7, "MethodName", "BERT"]]}
{"text": "Multivalent Entailment Graphs for Question Answering", "entities": [[4, 6, "TaskName", "Question Answering"]]}
{"text": "We make three contributions : ( 1 ) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies , like DEFEAT ( Biden , Trump ) WIN ( Biden ) ; ( 2 ) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open - domain predicates ; and ( 3 ) we demonstrate the capabilities of these graphs on a novel question answering task .", "entities": [[68, 70, "TaskName", "question answering"]]}
{"text": "Now at Google Research .", "entities": [[2, 3, "DatasetName", "Google"]]}
{"text": "1 Our text might say \" Colonel Mustard killed Mr. Boddy , \" or \" Mr. Boddy was murdered in the kitchen with a candlestick , \" either of which answers the question , but only via natural language inference .", "entities": [[37, 40, "TaskName", "natural language inference"]]}
{"text": "We further pose a true - false question answering task generated automatically from news text .", "entities": [[7, 9, "TaskName", "question answering"]]}
{"text": "We also compare with several baselines , including unsupervised pretrained language models , and show that our directional entailment graphs succeed over non - directional similarity measures in answering questions of fine - grained semantics .", "entities": [[9, 12, "TaskName", "pretrained language models"]]}
{"text": "In previous work predicate arguments are successfully used as these contextual features , but only predicates of the same valency are considered ( e.g. binary predicates entail binary ; unary entail unary ) , and further research computes additional edges in these same - valency graphs such as with link prediction ( Hosseini et al , 2019 ) .", "entities": [[49, 51, "TaskName", "link prediction"]]}
{"text": "Typing is very helpful for entailment graph learning ( Berant et al , 2010 ; Lewis and Steedman , 2013 ; Hosseini et al , 2018 ) .", "entities": [[6, 8, "TaskName", "graph learning"]]}
{"text": "\" We compare our model to several baselines , including strong pretrained language models in an unsupervised setting using similarity .", "entities": [[11, 14, "TaskName", "pretrained language models"]]}
{"text": "BERT ( Devlin et al , 2019 ) generates impressive word representations , even unsupervised ( Petroni et al , 2019 ) , which we compare with on a task of predicate inference .", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "We further test RoBERTa ( Liu et al , 2019 ) to show the impact of robust in - domain pretraining on the same architecture .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}
{"text": "The vertices V are the set of predicates , where each argument has a type from the set of 49 FIGER base types T , e.g. TRAVEL.TO ( : person , : location ) V , and : person , : location T .", "entities": [[20, 21, "DatasetName", "FIGER"]]}
{"text": "( Nguyen et al , 2014 ) , and mapping the IDs to the 49 base FIGER types ( Ling and Weld , 2012 ) .", "entities": [[16, 17, "DatasetName", "FIGER"]]}
{"text": "This task is designed independently of the MGraph as a challenge in information retrieval .", "entities": [[12, 14, "TaskName", "information retrieval"]]}
{"text": "Within each partition we do relation extraction in a process mirroring 4.1 .", "entities": [[5, 7, "TaskName", "relation extraction"]]}
{"text": "For example , a neural model will easily distinguish a random negative like DETONATE ( Google , YouTube ) from a news text discussing Google 's acquisition of YouTube , classifying it as a false event on grounds of dissimilarity alone .", "entities": [[15, 16, "DatasetName", "Google"], [24, 25, "DatasetName", "Google"]]}
{"text": "For example , \" Was YouTube sold to Google ? \" can be answered affirmatively by reading \" Google bought YouTube \" using the graph edge BUY ( x , y ) SELL.TO ( y , x ) .", "entities": [[8, 9, "DatasetName", "Google"], [18, 19, "DatasetName", "Google"]]}
{"text": "BInc scores range from 0 to 1 ; if no entailments are found we assume it is false ( score of 0 ) .", "entities": [[4, 5, "DatasetName", "0"], [21, 22, "DatasetName", "0"]]}
{"text": "BERT and RoBERTa predicate embeddings ( Devlin et al , 2019 ; Liu et al , 2019 ) are used in an unsupervised manner to answer questions based on similarity to the evidence .", "entities": [[0, 1, "MethodName", "BERT"], [2, 3, "MethodName", "RoBERTa"]]}
{"text": "We compute the cosine similarity between the question and each evidence vector , adjusted to a scale of 0 to 1 : sim ( p , q )", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "We test the basic BERT model and RoBERTa model , which has robustly pretrained on 160 GB of text ( 76 GB news ) .", "entities": [[4, 5, "MethodName", "BERT"], [7, 8, "MethodName", "RoBERTa"]]}
{"text": "The models produce a gradation of judgement scores between 0 ( false ) and 1 ( true ) .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "As in earlier work , we slide a classification threshold over the score range to produce a precision - recall curve for each model .", "entities": [[8, 10, "HyperparameterName", "classification threshold"]]}
{"text": "Our model achieves higher precision than BERT and RoBERTa similarity models in the low recall range .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "RoBERTa"]]}
{"text": "We note that RoBERTa bests BERT due to extensive in - domain pretraining .", "entities": [[3, 4, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}
{"text": "At 50 % recall , the MGraph has 76 % precision with RoBERTa at 65 % .", "entities": [[12, 13, "MethodName", "RoBERTa"]]}
{"text": "The UU and BU models also suffer during relation extraction ( parsing ) .", "entities": [[8, 10, "TaskName", "relation extraction"]]}
{"text": "When we fail to parse a second argument for a predicate we assume it only has one and extract a malformed unary , which can interfere with question answering ( e.g. reporting verbs \" explain , \" \" announce , \" etc . which fail to parse with their quote ) .", "entities": [[27, 29, "TaskName", "question answering"]]}
{"text": "Our multivalent entailment graph 's performance has been demonstrated on a question answering task requiring fine - grained semantic understanding .", "entities": [[11, 13, "TaskName", "question answering"]]}
{"text": "We outperform baseline models including a strong similarity measure using unsupervised BERT and RoBERTa , while using far less training data .", "entities": [[11, 12, "MethodName", "BERT"], [13, 14, "MethodName", "RoBERTa"]]}
{"text": "This work was supported in part by ERC H2020 Advanced Fellowship GA 742137 SEMANTAX ,", "entities": [[11, 12, "MethodName", "GA"]]}
{"text": "MLEC - QA : A Chinese Multi - Choice Biomedical Question Answering Dataset", "entities": [[10, 12, "TaskName", "Question Answering"]]}
{"text": "Question Answering ( QA ) has been successfully applied in scenarios of human - computer interaction such as chatbots and search engines .", "entities": [[0, 2, "TaskName", "Question Answering"]]}
{"text": "As a branch of the QA task , Biomedical Question Answering ( BQA ) enables effectively perceiving , accessing , and understanding complex biomedical knowledge by innovative applications , which makes BQA an important QA application in the biomedical domain ( Jin et al , 2021 ) .", "entities": [[9, 11, "TaskName", "Question Answering"]]}
{"text": "We fine - tune five pre - trained language models for machine reading comprehension as the reader .", "entities": [[11, 14, "TaskName", "machine reading comprehension"]]}
{"text": "Open - Domain BQA The Text REtrieval Conference ( TREC ) ( Voorhees and Tice , 2000 ) has triggered the open - domain BQA research .", "entities": [[9, 10, "DatasetName", "TREC"]]}
{"text": "At the time , most traditional BQA systems were employing complex pipelines with question processing , document / passage retrieval , and answer processing modules .", "entities": [[18, 20, "TaskName", "passage retrieval"]]}
{"text": "With the introduction of various BQA datasets that are focused on specific biomedical topics , such as BioASQ ( Tsatsaronis et al , 2015 ) , emrQA ( Pampari et al , 2018 ) and PubMedQA ( Jin et al , 2019 ) , pioneered by Chen et al ( 2017 ) , the modern open - domain BQA systems largely simplified the traditional BQA pipeline to a two - stage retriever - reader framework by combining information retrieval and machine reading comprehension models ( Ben Abacha et al , 2017 , 2019b .", "entities": [[17, 18, "DatasetName", "BioASQ"], [26, 27, "DatasetName", "emrQA"], [35, 36, "DatasetName", "PubMedQA"], [77, 79, "TaskName", "information retrieval"], [80, 83, "TaskName", "machine reading comprehension"]]}
{"text": "Using examination counseling books as information sources may make the retriever - reader more likely to exploit shallow text matching , and complex reasoning is seldom involved .", "entities": [[18, 20, "TaskName", "text matching"]]}
{"text": "Building upon the whole Chinese Wikipedia data , we use a distributed search and analytics engine , Elas - ticSearch , as the document store and document retriever , which supports very fast full - text searches .", "entities": [[17, 18, "DatasetName", "Elas"]]}
{"text": "The document with the highest BM25 score returned by each query is selected as supporting materials for the next stage machine reading comprehension task .", "entities": [[20, 23, "TaskName", "machine reading comprehension"]]}
{"text": "For each question , the j th option is always chosen as the answer to obtain the accuracy distribution of five candidate options .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "We apply an unified framework UER - py ( Zhao et al , 2019 ) to fine - tuning pre - trained language models on the machine reading comprehension task as our reader .", "entities": [[26, 29, "TaskName", "machine reading comprehension"]]}
{"text": "We consider the following five pre - trained language models : Chinese BERT - Base ( denoted as BERT - Base ) and Multilingual Uncased BERT - Base ( denoted as BERT - Base - Multilingual ) ( Devlin et al , 2019 ) , Chinese BERT - Base with whole word masking and pre - trained over larger corpora ( denoted as BERT - wwm - ext ) ( Cui et al , 2019 ) , and the robustly optimized BERTs : Chinese RoBERTa - wwm - ext and Chinese RoBERTa - wwm - ext - large ( Cui et al , 2019 ) .", "entities": [[12, 13, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"], [31, 32, "MethodName", "BERT"], [46, 47, "MethodName", "BERT"], [63, 64, "MethodName", "BERT"], [84, 85, "MethodName", "RoBERTa"], [91, 92, "MethodName", "RoBERTa"]]}
{"text": "Finally , we pass the unnormalized log probabilities of each option through a softmax layer and obtain the option with the highest probability as the predicted answer A \u2032 i .", "entities": [[13, 14, "MethodName", "softmax"]]}
{"text": "As shown in Figure 2 , we implement a two - stage retriever - reader framework : ( 1 ) a retriever first retrieves question relevant documents from Chinese Wikipedia using ElasticSearch , ( 2 ) and then a reader employs machine reading comprehension models to generate answers in given documents retrieved by the retriever .", "entities": [[41, 44, "TaskName", "machine reading comprehension"]]}
{"text": "RoBERTa - wwm - ext - large and BERTwwm - ext perform better than other models on five subsets .", "entities": [[0, 1, "MethodName", "RoBERTa"]]}
{"text": "We compare the best model ( RoBERTa - wwm - ext - large ) performance on both datasets as shown in Table 9 .", "entities": [[6, 7, "MethodName", "RoBERTa"]]}
{"text": "Therefore , the random accuracy on MCMLE is higher than ours .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "It indicates that the poor performance of machine reading comprehension models is coming from the insufficiency of reasoning ability rather than the number of retrieved documents .", "entities": [[7, 10, "TaskName", "machine reading comprehension"]]}
{"text": "In order to benefit researchers on improving the open - domain QA models , and also make advances for Biomedical Question Answering ( BQA ) systems , we present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset to date .", "entities": [[20, 22, "TaskName", "Question Answering"]]}
{"text": "No direct information is available about age and gender distribution .", "entities": [[6, 9, "DatasetName", "age and gender"]]}
{"text": "For five subsets in MLEC - QA , we collect 2006 to 2020 Sprint Paper for the National Medical Licensing Examination - Tianjin Science and Technology Press in PDF format , and then converted them into digital format via Optical Character Recognition ( OCR ) .", "entities": [[39, 42, "TaskName", "Optical Character Recognition"]]}
{"text": "NLP systems for machine translation , summarization , paraphrasing , and other tasks often fail to preserve the compositional semantics of sentences and documents because they model language as bags of words , or at best syntactic trees .", "entities": [[3, 5, "TaskName", "machine translation"], [6, 7, "TaskName", "summarization"]]}
{"text": "Consider how we might use compositional semantic representations in machine translation ( Jones et al , 2012 ) .", "entities": [[9, 11, "TaskName", "machine translation"]]}
{"text": "Let \u0393 be an alphabet , i.e. , a finite set .", "entities": [[1, 2, "HyperparameterName", "\u0393"]]}
{"text": "A hypergraph ( or simply graph ) over a ranked alphabet \u0393 is a tuple G =", "entities": [[11, 12, "HyperparameterName", "\u0393"]]}
{"text": "att G : E G V * G maps each edge to a sequence of nodes ; lab G : E G \u0393 maps each edge to a label such that | att G ( e )", "entities": [[22, 23, "HyperparameterName", "\u0393"]]}
{"text": "The tentacle from e to v i will have label i , so the tentacle labels lie in the set [ k ] where k = rank ( e ) .", "entities": [[24, 26, "HyperparameterName", "k ="]]}
{"text": "Given a graph G , a path in G from a node v to a node v is a sequence ( v 0 , i 1 , e 1 , j 1 , v 1 ) ( v 1 , i 2 , e 2 , j 2 , v 2 ) .", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "e k , j k , v k ) ( 1 ) such that v 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "v , v k = v , and for each r [ k ] , vert ( e r , i r )", "entities": [[3, 5, "HyperparameterName", "k ="]]}
{"text": "Note that the endpoints v 0 and v k of an internal path can be external .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "This mild assumption may be reasonable for applications like AMR parsing , where grammars could be designed so that the external node is always the unique root .", "entities": [[9, 11, "TaskName", "AMR parsing"]]}
{"text": "[ q : Y \u03b1 ]", "entities": [[4, 5, "HyperparameterName", "\u03b1"]]}
{"text": "= Y SCAN [ b ( I ) , X \u01131 . . .", "entities": [[2, 3, "DatasetName", "SCAN"]]}
{"text": "[ b ( J ) , q : Y \u03b1 , \u03c6q ]", "entities": [[9, 10, "HyperparameterName", "\u03b1"]]}
{"text": "Using this , we construct the function \u03c6 0 p : E R ( p ) V * R ( p ) such that for\u0113 E R ( p ) if att ( \u0113 ) = ( v 1 , v 2 ) then \u03c6 0 p ( \u0113 )", "entities": [[8, 9, "DatasetName", "0"], [45, 46, "DatasetName", "0"]]}
{"text": "We write \u03c6 0 p to represent a mapping with no grounded nodes .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "To keep notation uniform , we use dummy nonterminal S * N G that derives S G via the production p 0 .", "entities": [[21, 22, "DatasetName", "0"]]}
{"text": "For graph G , our system includes the axiom : [ ext G , p 0 : S * S G , \u03c6 0 p 0", "entities": [[15, 16, "DatasetName", "0"], [23, 24, "DatasetName", "0"], [25, 26, "DatasetName", "0"]]}
{"text": "[ ext R ( p 0 ) = ext G ] ] .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "As in Earley 's algorithm , we have three inference rules : PREDICT , SCAN and COMPLETE ( Table 2 ) .", "entities": [[14, 15, "DatasetName", "SCAN"]]}
{"text": "SCAN is applied when the edge after the dot is terminal .", "entities": [[0, 1, "DatasetName", "SCAN"]]}
{"text": "Since G consists of one edge , there must be a production q : X G. Apply PREDICT to the axiom and p X : X * X to obtain the item [ \u03c6 p X ( X ) , q : X G , \u03c6 0 q [ ext G = \u03c6 p X ( X ) ]", "entities": [[46, 47, "DatasetName", "0"]]}
{"text": "Apply SCAN to the single terminal edge that makes up G to obtain [ b ( G ) , q : X G , \u03c6 q ] and finally apply COMPLETE to this and the axiom reach the goal [ b ( G ) , p X : X * X , \u03c6 p X ] .", "entities": [[1, 2, "DatasetName", "SCAN"]]}
{"text": "By applying COMPLETE to each such item and applying SCAN to each terminal edge of H we reach the goal [ b ( G ) , p X : X * X , \u03c6 p X ] .", "entities": [[9, 10, "DatasetName", "SCAN"]]}
{"text": "[ { v 3 , v 2 } , Y arg1 arg0 Z , \u03c6 0 s [ ext R ( s ) = ( v 3 , v 2 ) ] ]", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "[ { v3 , v2 } , Y arg1arg0Z , \u03c6 0 s [ ext R ( s ) = ( v3 , v2 ) ]", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "] SCAN : 1 . and e3", "entities": [[1, 2, "DatasetName", "SCAN"]]}
{"text": "] SCAN : 2 . and e2", "entities": [[1, 2, "DatasetName", "SCAN"]]}
{"text": "4 . [ ( v1 ) , Z need , \u03c6 0 u [ ext R ( u ) = ( v1 ) ] ] PREDICT : 3 . and Z need 5 .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "] SCAN : 4 . and e1 = edg need ( v1 , v4 )", "entities": [[1, 2, "DatasetName", "SCAN"]]}
{"text": "Properties 1 and 2 ensure that when we apply SCAN , at least one endpoint of the edge is bound ( fixed ) .", "entities": [[9, 10, "DatasetName", "SCAN"]]}
{"text": "We observe that RGGs could be relaxed to produce graphs with no external nodes by adding a dummy nonterminal S with rank 0 and", "entities": [[22, 23, "DatasetName", "0"]]}
{"text": "A possible alternative would be to consider Restricted DAG Grammars ( RDG ; Bj\u00f6rklund et", "entities": [[7, 8, "DatasetName", "Restricted"]]}
{"text": "This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science , funded by the UK Engineering and Physical Sciences Research Council ( grant EP / L016427/1 ) and the University of Edinburgh ; and in part by a Google faculty research award ( to AL ) .", "entities": [[44, 45, "DatasetName", "Google"]]}
{"text": "Neural Machine Translation with the Transformer and Multi - Source Romance Languages for the Biomedical WMT 2018 task", "entities": [[1, 3, "TaskName", "Machine Translation"], [5, 6, "MethodName", "Transformer"], [15, 17, "DatasetName", "WMT 2018"]]}
{"text": "The Transformer architecture has become the state - of - the - art in Machine Translation .", "entities": [[1, 2, "MethodName", "Transformer"], [14, 16, "TaskName", "Machine Translation"]]}
{"text": "This model , which relies on attention - based mechanisms , has outperformed previous neural machine translation architectures in several tasks .", "entities": [[15, 17, "TaskName", "machine translation"]]}
{"text": "In this system description paper , we report details of training neural machine translation with multi - source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task .", "entities": [[12, 14, "TaskName", "machine translation"], [22, 23, "MethodName", "Transformer"], [32, 34, "DatasetName", "WMT 2018"]]}
{"text": "Neural Machine Translation ( NMT ) ( Bahdanau et al , 2015 ) proved to be competitive with the encoder - decoder architecture based on recurrent neural networks and attention .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}
{"text": "The latter architecture has achieved great success in Machine Translation ( MT ) and it has already been extended to other tasks such as Parsing , Speech Recognition 1 , Speech Translation ( Cros et al , 2018 ) , Chatbots among others .", "entities": [[8, 10, "TaskName", "Machine Translation"], [26, 28, "TaskName", "Speech Recognition"], [31, 32, "TaskName", "Translation"]]}
{"text": "In this paper , we use the Transformer enhanced with the multi - source technique to participate in the Biomedical WMT 2018 task , which can be somehow considered a low - resourced task , given the large quantity of data that it is required for NMT .", "entities": [[7, 8, "MethodName", "Transformer"], [20, 22, "DatasetName", "WMT 2018"]]}
{"text": "The Transformer model is the first NMT model relying entirely on self - attention to compute representations of its input and output without using recurrent neural networks ( RNN ) or convolutional neural networks ( CNN ) .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "The Transformer is an encoder - decoder model that was conceived to solve these problems .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "Without positional encodings , the output of the multi - head attention network would be the same for the sentences \" I love you more than her \" and \" I love her more than you \" .", "entities": [[8, 12, "MethodName", "multi - head attention"]]}
{"text": "The individual attention blocks compute the scaled dot - product attention with different linear projections .", "entities": [[6, 11, "MethodName", "scaled dot - product attention"]]}
{"text": "Finally a position - wise fully connected feed - forward network is used , which consists of two linear transformations with a ReLU activation ( Vinod Nair , 2010 )", "entities": [[22, 23, "MethodName", "ReLU"]]}
{"text": "The third stage is a multi - head attention that not only attends to these past words , but also to the final representations generated by the encoder .", "entities": [[5, 9, "MethodName", "multi - head attention"]]}
{"text": "Finally , a softmax layer allows to map target word scores into target word probabilities .", "entities": [[3, 4, "MethodName", "softmax"]]}
{"text": "In our case , we are using this approach in the Transformer architecture described above and using only inputs from the same language family .", "entities": [[11, 12, "MethodName", "Transformer"]]}
{"text": "The experimental framework is the Biomedical Translation Task ( WMT18 ) 2 .", "entities": [[6, 7, "TaskName", "Translation"]]}
{"text": "Each validation dataset contains 500 sentence pairs .", "entities": [[1, 3, "DatasetName", "validation dataset"]]}
{"text": "Words were segmented by means of Byte - Pair Encoding ( BPE ) ( Sennrich et al , 2015 ) .", "entities": [[11, 12, "MethodName", "BPE"]]}
{"text": "We stopped training when the validation accuracy did not increase in two consecutive epochs .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "The Transformer architecture outperforms WMT17 best system .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "The main conclusions of our experiments are that the multi - source inputs of the same family applied to the Transformer architecture can improve the single input .", "entities": [[20, 21, "MethodName", "Transformer"]]}
{"text": "Neural Relation Extraction for Knowledge Base Enrichment", "entities": [[1, 3, "TaskName", "Relation Extraction"]]}
{"text": "We study relation extraction for knowledge base ( KB ) enrichment .", "entities": [[2, 4, "TaskName", "relation extraction"]]}
{"text": "Previous studies focus on the extraction itself and rely on Named Entity Disambiguation ( NED ) to map triples into the KB space .", "entities": [[11, 13, "TaskName", "Entity Disambiguation"]]}
{"text": "To address this problem , we propose an end - to - end relation extraction model for KB enrichment based on a neural encoder - decoder model .", "entities": [[13, 15, "TaskName", "relation extraction"]]}
{"text": "Our model employs jointly learned word and entity embeddings to support named entity disambiguation .", "entities": [[7, 9, "TaskName", "entity embeddings"], [12, 14, "TaskName", "entity disambiguation"]]}
{"text": "Knowledge bases ( KBs ) , often in the form of knowledge graphs ( KGs ) , have become essential resources in many tasks including Q&A systems , recommender system , and natural language generation .", "entities": [[11, 13, "TaskName", "knowledge graphs"]]}
{"text": "Large KBs such as DBpedia ( Auer et al , 2007 ) , Wikidata ( Vrandecic and Kr\u00f6tzsch , 2014 ) and Yago ( Suchanek et al , 2007 ) contain millions of facts about entities , which are represented in the form of subject - predicate - object triples .", "entities": [[4, 5, "DatasetName", "DBpedia"], [22, 23, "DatasetName", "Yago"]]}
{"text": "Previous studies work on embedding - based model ( Nguyen et al , 2018 ; and entity alignment model ( Chen et al , 2017 ; Trisedya et al , 2019 ) to enrich a knowledge base .", "entities": [[16, 18, "TaskName", "entity alignment"]]}
{"text": "This is the typical situation for KB enrichment ( as opposed to constructing a KB from scratch or performing relation extraction for other purposes , such as Q&A or summarization ) .", "entities": [[19, 21, "TaskName", "relation extraction"], [29, 30, "TaskName", "summarization"]]}
{"text": "Previous studies on relation extraction have employed both unsupervised and supervised approaches .", "entities": [[3, 5, "TaskName", "relation extraction"]]}
{"text": "This paradigm is known as Open Information Extraction ( Open IE )", "entities": [[5, 8, "TaskName", "Open Information Extraction"]]}
{"text": "Most existing methods thus entail the need for Named Entity Disambiguation ( NED ) ( cf .", "entities": [[9, 11, "TaskName", "Entity Disambiguation"]]}
{"text": "Our model harnesses pre - trained word and entity embeddings that are jointly learned with skip gram ( Mikolov et al , 2013 ) andTransE ( Bordes et al , 2013 ) .", "entities": [[8, 10, "TaskName", "entity embeddings"]]}
{"text": "First , the embeddings capture the relationship between words and entities , which is essential for named entity disambiguation .", "entities": [[17, 19, "TaskName", "entity disambiguation"]]}
{"text": "Second , the entity embeddings preserve the relationships between entities , which help to build a highly accurate classifier to filter the invalid extracted triples .", "entities": [[3, 5, "TaskName", "entity embeddings"]]}
{"text": "2 Related Work 2.1 Open Information Extraction Banko et al ( 2007 ) introduced the paradigm of Open Information Extraction ( Open IE ) and proposed a pipeline that consists of three stages : learner , extractor , and assessor .", "entities": [[4, 7, "TaskName", "Open Information Extraction"], [17, 20, "TaskName", "Open Information Extraction"]]}
{"text": "al ( 2018 ) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging .", "entities": [[13, 15, "TaskName", "relation extraction"]]}
{"text": "A bi - LSTM model is trained to predict the label ( entity , predicate , or other ) of each token of the input .", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "Another line of studies use neural learning for semantic role labeling ( He et al , 2018 ) , but the goal here is to recognize the predicate - argument structure of a single input sentence - as opposed to extracting relations from a corpus .", "entities": [[8, 11, "TaskName", "semantic role labeling"]]}
{"text": "The only means to map triples to uniquely identified entities in a KG is by post - processing via entity linking ( NED ) methods ( Shen et al , 2015 ) or by clustering with subsequent mapping ( Gal\u00e1rraga et al , 2014 ) .", "entities": [[19, 21, "TaskName", "entity linking"]]}
{"text": "Inspired by the work of Brin ( 1998 ) , state - of - theart methods employ distant supervision by leveraging seed facts from an existing KG ( Mintz et al , 2009 ; Suchanek et al , 2009 ; Carlson et al , 2010 ) .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "These methods integrate entity linking ( i.e. , NED ) into their models .", "entities": [[3, 5, "TaskName", "entity linking"]]}
{"text": "Our framework consists of three components : data collection module , embedding module , and neural relation extraction module .", "entities": [[16, 18, "TaskName", "relation extraction"]]}
{"text": "The aligned pairs of sentences and triples will later be used as the training data in our neural relation extraction module .", "entities": [[18, 20, "TaskName", "relation extraction"]]}
{"text": "In the embedding module ( detailed in Section 3.3 ) , we propose a joint learning of word and entity embeddings by combining skip - gram ( Mikolov et al , 2013 ) to compute the word embeddings and TransE ( Bordes et al , 2013 ) to compute the entity embeddings .", "entities": [[19, 21, "TaskName", "entity embeddings"], [36, 38, "TaskName", "word embeddings"], [39, 40, "MethodName", "TransE"], [50, 52, "TaskName", "entity embeddings"]]}
{"text": "Moreover , the resulting entity embeddings are used to train a triple classifier that helps filter invalid triples generated by our neural relation extraction model .", "entities": [[4, 6, "TaskName", "entity embeddings"], [22, 24, "TaskName", "relation extraction"]]}
{"text": "In the neural relation extraction module ( detailed in Section 3.4 ) , we propose an n - gram based attention model by expanding the attention mechanism to the n - gram token of a sentence .", "entities": [[3, 5, "TaskName", "relation extraction"]]}
{"text": "The second strategy uses a triple classifier that is trained using the entity embeddings from the joint learning to filter the invalid triples .", "entities": [[12, 14, "TaskName", "entity embeddings"]]}
{"text": "We aim to extract triples from a sentence for KB enrichment by proposing a supervised relation extraction model .", "entities": [[15, 17, "TaskName", "relation extraction"]]}
{"text": "Our relation extraction model is based on the encoder - decoder framework which has been widely used in Neural Machine Translation to translate text from one language to another .", "entities": [[1, 3, "TaskName", "relation extraction"], [19, 21, "TaskName", "Machine Translation"]]}
{"text": "To compute the embeddings of the source and target vocabularies , we propose a joint learning of word and entity embeddings that is effective to capture the similarity between words and entities for named entity disambiguation ( Yamada et al , 2016 ) .", "entities": [[19, 21, "TaskName", "entity embeddings"], [34, 36, "TaskName", "entity disambiguation"]]}
{"text": "We use joint learning by combining skip - gram ( Mikolov et al , 2013 ) to compute the word embeddings and TransE ( Bordes et al , 2013 ) to compute the entity embeddings ( including the relationship embeddings ) , while Yamada et al ( 2016 ) use Wikipedia Link - based Measure ( WLM ) ( Milne and Witten , 2008 ) that does not consider the relationship embeddings .", "entities": [[19, 21, "TaskName", "word embeddings"], [22, 23, "MethodName", "TransE"], [33, 35, "TaskName", "entity embeddings"]]}
{"text": "Here , x is the L1 - Norm of vector x , \u03b3 is a margin hyperparameter , T r is the set of valid relationship triples from a KG G , and T r is the set of corrupted relationship triples ( recall that E is the set of entities in G ) .", "entities": [[12, 13, "HyperparameterName", "\u03b3"]]}
{"text": "We use all triples in Wikidata except those which belong to the testing data to compute the entity embeddings .", "entities": [[17, 19, "TaskName", "entity embeddings"]]}
{"text": "To establish the interaction between the entity and word embeddings , we follow the Anchor Context Model proposed by Yamada et al ( 2016 ) .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "Then , we use the skip - gram method to compute the word embeddings from the generated corpus ( the entity IDs in the modified anchor text are treated as words in the skip - gram model ) .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "Given a sequence of n words [ w 1 , w 2 , ... , w n ] , The model learns the word embeddings , by minimizing the following objective function J W : J W = 1 T n t=1 \u2212c\u2264j\u2264c , j = 0 log P ( w t+j | w t ) ( 5 ) P ( w t+j | w t )", "entities": [[23, 25, "TaskName", "word embeddings"], [46, 47, "DatasetName", "0"]]}
{"text": "The overall objective function of the joint learning of word and entity embeddings is : J = J E + J W ( 7 )", "entities": [[11, 13, "TaskName", "entity embeddings"]]}
{"text": "Our proposed relation extraction model integrates the extraction and canonicalization tasks for KB enrichment in an end - to - end manner .", "entities": [[2, 4, "TaskName", "relation extraction"]]}
{"text": "Because we treat the input and output as a sequence , We use the LSTM networks ( Hochreiter and Schmidhuber , 1997 ) in the encoder and the decoder .", "entities": [[14, 15, "MethodName", "LSTM"]]}
{"text": "The encoder - decoder with attention model ( Bahdanau et al , 2015 ) has been used in machine translation .", "entities": [[18, 20, "TaskName", "machine translation"]]}
{"text": "However , in the relation extraction task , the attention model can not capture the multiword entity names .", "entities": [[4, 6, "TaskName", "relation extraction"]]}
{"text": "This may cause errors in entity alignment , especially when predicting the ID of an entity that is not in the training data .", "entities": [[5, 7, "TaskName", "entity alignment"]]}
{"text": "The attention weights are computed over the n - gram combinations of the word embeddings , and hence the context vector for the decoder is computed as follows .", "entities": [[13, 15, "TaskName", "word embeddings"]]}
{"text": "\u03b1", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "However , the greedy approach ( i.e. , picking the entity with the highest probability of the last softmax layer of the decoder ) may lead the model to extract incorrect entities due to the similarity between entity embeddings ( e.g. , the embeddings of New York City and Chicago may be similar because both are cities in USA ) .", "entities": [[18, 19, "MethodName", "softmax"], [37, 39, "TaskName", "entity embeddings"]]}
{"text": "Our triple classifier is trained with entity embeddings from the joint learning ( see Section 3.3 ) .", "entities": [[6, 8, "TaskName", "entity embeddings"]]}
{"text": "Triple classification is one of the metrics to evaluate the quality of entity embeddings ( Socher et al , 2013 ) .", "entities": [[0, 2, "TaskName", "Triple classification"], [12, 14, "TaskName", "entity embeddings"]]}
{"text": "We train a binary classifier based on the plausibility score ( h + r \u2212 t ) ( the score to compute the entity embeddings ) .", "entities": [[23, 25, "TaskName", "entity embeddings"]]}
{"text": "We use precision , recall , and F1 score as the evaluation metrics .", "entities": [[7, 9, "MetricName", "F1 score"]]}
{"text": "We use 64 dimensions of pre - trained word and entity embeddings ( see Section 3.3 ) .", "entities": [[10, 12, "TaskName", "entity embeddings"]]}
{"text": "We also compare our N - gram Attention model with two encoder - decoder based models including the Single Attention model ( Bahdanau et al , 2015 ) and Transformer model ( Vaswani et al , 2017 ) .", "entities": [[29, 30, "MethodName", "Transformer"]]}
{"text": "In particular , our proposed n - gram attention model achieves the best results in terms of precision , recall , and F1 score .", "entities": [[22, 24, "MetricName", "F1 score"]]}
{"text": "As expected , the combination of the existing models with AIDA achieves higher F1 scores than the combination with NeuralEL as AIDA achieves a higher precision than NeuralEL .", "entities": [[13, 14, "MetricName", "F1"]]}
{"text": "The Transformer model also only yields similar performance to that of the Single Attention model , which is worse than ours .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "We proposed an end - to - end relation extraction model for KB enrichment that integrates the extraction and canonicalization tasks .", "entities": [[8, 10, "TaskName", "relation extraction"]]}
{"text": "Our model thus reduces the error propagation between relation extraction and NED that existing approaches are prone to .", "entities": [[8, 10, "TaskName", "relation extraction"]]}
{"text": "Moreover , we propose a modified beam search and a triple classification that helps the model to generate high - quality triples .", "entities": [[10, 12, "TaskName", "triple classification"]]}
{"text": "These results confirm that our model reduces the error propagation between NED and relation extraction .", "entities": [[13, 15, "TaskName", "relation extraction"]]}
{"text": "CODEX : A Comprehensive Knowledge Graph Completion Benchmark", "entities": [[4, 7, "TaskName", "Knowledge Graph Completion"]]}
{"text": "We present CODEX , a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty .", "entities": [[7, 10, "TaskName", "knowledge graph COmpletion"], [20, 23, "TaskName", "knowledge graph completion"]]}
{"text": "In terms of scope , CODEX comprises three knowledge graphs varying in size and structure , multilingual descriptions of entities and relations , and tens of thousands of hard negative triples that are plausible but verified to be false .", "entities": [[8, 10, "TaskName", "knowledge graphs"]]}
{"text": "Next , we report baseline link prediction and triple classification results on CODEX for five extensively tuned embedding models .", "entities": [[5, 7, "TaskName", "link prediction"], [8, 10, "TaskName", "triple classification"]]}
{"text": "Finally , we differentiate CODEX from the popular FB15 K - 237 knowledge graph completion dataset by showing that CODEX covers more diverse and interpretable content , and is a more difficult link prediction benchmark .", "entities": [[12, 15, "TaskName", "knowledge graph completion"], [32, 34, "TaskName", "link prediction"]]}
{"text": "Knowledge graphs are multi - relational graphs that express facts about the world by connecting entities ( people , places , things , concepts ) via different types of relationships .", "entities": [[0, 2, "TaskName", "Knowledge graphs"]]}
{"text": "The field of automatic knowledge graph completion ( KGC ) , which is motivated by the fact that knowledge graphs are usually incomplete , is an active research direction spanning several subfields of artificial intelligence ( Nickel et al , 2015 ; Ji et al , 2020 ) .", "entities": [[4, 7, "TaskName", "knowledge graph completion"], [18, 20, "TaskName", "knowledge graphs"]]}
{"text": "To address the need for a solid benchmark in KGC , we present CODEX , a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and its sister project Wikipedia .", "entities": [[18, 21, "TaskName", "knowledge graph COmpletion"]]}
{"text": "Our contributions include : Foundations We survey evaluation datasets in encyclopedic knowledge graph completion to motivate a new benchmark ( 2 and Appendix A ) .", "entities": [[11, 14, "TaskName", "knowledge graph completion"]]}
{"text": "Data We introduce CODEX , a benchmark consisting of three knowledge graphs varying in size and structure , entity types , multilingual labels and descriptions , and - unique to CODEX - manually verified hard negative triples ( 3 ) .", "entities": [[10, 12, "TaskName", "knowledge graphs"]]}
{"text": "Benchmarking We conduct large - scale model selection and benchmarking experiments , reporting baseline link prediction and triple classification results on CODEX for five widely used embedding models from different architectural classes ( 5 ) .", "entities": [[6, 8, "TaskName", "model selection"], [14, 16, "TaskName", "link prediction"], [17, 19, "TaskName", "triple classification"]]}
{"text": "Various decentralized versions of FB15 K with , e.g. , entity types ( Xie et al , 2016 ) , sampled negatives ( Socher et al , 2013 ) , and more ( Table 8 ) Centralized repository of three datasets with entity types , multilingual text , and manually annotated hard negatives ( 3 ) Level of difficulty FB15 K has severe train / test leakage from inverse relations ; while removal of inverse relations makes FB15 K - 237 harder than FB15 K , FB15 K - 237 still has a high proportion of easy - to - predict relational patterns ( 6.2 ) Inverse relations removed from all datasets to avoid train / test leakage ( 3.2 ) ; manually annotated hard negatives for the task of triple classification ( 3.4 ) ; few trivial patterns for the task of link prediction ( 6.2 ) CODEX from FB15 K - 237 in terms of both content and difficulty ( 6 ) .", "entities": [[130, 132, "TaskName", "triple classification"], [143, 145, "TaskName", "link prediction"]]}
{"text": "We show that CODEX covers more diverse and interpretable content , and is a more challenging link prediction benchmark .", "entities": [[16, 18, "TaskName", "link prediction"]]}
{"text": "NELL - 995 ( Xiong et al , 2017 ) was taken from the Never Ending Language Learner ( NELL ) sys - tem ( Mitchell et al , 2018 ) , which continuously reads the web to obtain and update its knowledge .", "entities": [[0, 1, "DatasetName", "NELL"], [19, 20, "DatasetName", "NELL"]]}
{"text": "NELL - 995 , a subset of the 995th iteration of NELL , contains 75 , 492 entities , 200 relations , and 154 , 213 triples .", "entities": [[0, 1, "DatasetName", "NELL"], [11, 12, "DatasetName", "NELL"]]}
{"text": "A cursory inspection reveals that many of the triples in NELL - 995 are nonsensical or overly generic , suggesting that NELL - 995 is not a meaningful dataset for KGC evaluation .", "entities": [[10, 11, "DatasetName", "NELL"], [21, 22, "DatasetName", "NELL"]]}
{"text": "1 YAGO3 - 10 ( Dettmers et al , 2018 ) is a subset of YAGO3 ( Mahdisoltani et al , 2014 ) , which covers portions of Wikipedia , Wikidata , and Word - Net .", "entities": [[1, 4, "DatasetName", "YAGO3 - 10"]]}
{"text": "YAGO3 - 10 has 123 , 182 entities , 37 relations , and 1 , 089 , 040 triples mostly limited to facts about people and locations .", "entities": [[0, 3, "DatasetName", "YAGO3 - 10"]]}
{"text": "While YAGO3 - 10 is a highprecision dataset , it was recently shown to be too easy for link prediction because it contains a large proportion of duplicate relations ( Akrami et al , 2020 ;", "entities": [[1, 4, "DatasetName", "YAGO3 - 10"], [18, 20, "TaskName", "link prediction"]]}
{"text": "In addition to large encyclopedic knowledge graphs , it is common to evaluate KGC methods on at least one smaller , domain - specific dataset , typically drawn from the WordNet semantic network ( Miller , 1998 ; Bordes et al , 2013 ) .", "entities": [[5, 7, "TaskName", "knowledge graphs"]]}
{"text": "Other choices include the Unified Medical Language System ( UMLS ) database ( McCray , 2003 ) , the Alyawarra kinship dataset ( Kemp et al , 2006 ) , the Countries dataset ( Bouchard et al , 2015 ) , and variants of a synthetic \" family tree \" ( Hinton , 1986 ) .", "entities": [[9, 10, "DatasetName", "UMLS"]]}
{"text": "Using these seeds , we retrieved an initial set of 380 , 038 entities , 75 relations , and 1 , 156 , 222 triples by querying Wikidata for statements of the form ( head entity of seed type , seed relation type , ? ) .", "entities": [[2, 3, "DatasetName", "seeds"]]}
{"text": "Knowledge graphs are unique in that they only contain positive statements , meaning that triples not observed in a given knowledge graph are not necessarily false , but merely unseen ; this is called the Open World Assumption ( Gal\u00e1rraga et al , 2013 ) .", "entities": [[0, 2, "TaskName", "Knowledge graphs"]]}
{"text": "However , most machine learning tasks on knowledge graphs require negatives in some capacity .", "entities": [[7, 9, "TaskName", "knowledge graphs"]]}
{"text": "For example , in the task of triple classification , the goal is to discriminate between positive ( true ) and negative ( false ) triples .", "entities": [[7, 9, "TaskName", "triple classification"]]}
{"text": "As we show in 5.5 , triple classification over randomly generated negatives is trivially easy for state - of - the - art models because random negatives are generally not meaningful or plausible .", "entities": [[6, 8, "TaskName", "triple classification"]]}
{"text": "The three main types of such patterns in knowledge graphs are symmetry , inversion , and compositionality ( Trouillon et al , 2019 ; .", "entities": [[8, 10, "TaskName", "knowledge graphs"]]}
{"text": "To learn these rules , models must be capable of \" multi - hop \" reasoning on knowledge graphs ( Guu et al , 2015 ) .", "entities": [[17, 19, "TaskName", "knowledge graphs"]]}
{"text": "Next , we benchmark performance on CODEX for the tasks of link prediction and triple classification .", "entities": [[11, 13, "TaskName", "link prediction"], [14, 16, "TaskName", "triple classification"]]}
{"text": "To ensure that models are fairly and accurately compared , we follow Ruffinelli et al ( 2020 ) , who conducted what is ( to the best of our knowledge ) the largest - scale hyperparameter tuning study of knowledge graph embeddings to date .", "entities": [[39, 42, "TaskName", "knowledge graph embeddings"]]}
{"text": "Link prediction The link prediction task is conducted as follows : Given a test triple ( h , r , t ) , we construct queries ( ? , r , t ) and ( h , r , ? ) .", "entities": [[0, 2, "TaskName", "Link prediction"], [3, 5, "TaskName", "link prediction"]]}
{"text": "The goal is of link prediction is to rank true triples ( \u0125 , r , t ) or ( h , r , t ) higher than false and unseen triples .", "entities": [[4, 6, "TaskName", "link prediction"]]}
{"text": "Link prediction performance is evaluated with mean reciprocal rank ( MRR ) and hits@k .", "entities": [[0, 2, "TaskName", "Link prediction"], [10, 11, "MetricName", "MRR"]]}
{"text": "MRR is the average reciprocal of each ground - truth entity 's rank over all ( ? , r , t ) and ( h , r , ? )", "entities": [[0, 1, "MetricName", "MRR"]]}
{"text": "Triple classification Given a triple ( h , r , t ) , the goal of triple classification is to predict a corresponding label y { \u22121 , 1 } .", "entities": [[0, 2, "TaskName", "Triple classification"], [16, 18, "TaskName", "triple classification"]]}
{"text": "Since knowledge graph embedding models output real - valued scores for triples , we convert these scores into labels by selecting a decision threshold per relation on the validation set such that validation accuracy is maximized for the model in question .", "entities": [[1, 4, "TaskName", "knowledge graph embedding"], [33, 34, "MetricName", "accuracy"]]}
{"text": "We measure accuracy and F1 score .", "entities": [[2, 3, "MetricName", "accuracy"], [4, 6, "MetricName", "F1 score"]]}
{"text": "We compare the following embedding methods : RESCAL ( Nickel et al , 2011 ) , TransE", "entities": [[7, 8, "MethodName", "RESCAL"], [16, 17, "MethodName", "TransE"]]}
{"text": "These models represent several classes of architecture , from linear ( RESCAL , TuckER , ComplEx ) to translational ( TransE ) to nonlinear / learned ( ConvE ) .", "entities": [[11, 12, "MethodName", "RESCAL"], [13, 14, "MethodName", "TuckER"], [20, 21, "MethodName", "TransE"]]}
{"text": "As recent studies have observed that training strategies are equally , if not more , important than architecture for link prediction ( Kadlec et al , 2017 ; Lacroix et al , 2018 ; Ruffinelli et al , 2020 ) , we search across a large range of hyperparameters to ensure a truly fair comparison .", "entities": [[19, 21, "TaskName", "link prediction"]]}
{"text": "To this end we use the PyTorch - based LibKGE framework for training and selecting knowledge graph embeddings .", "entities": [[15, 18, "TaskName", "knowledge graph embeddings"]]}
{"text": "In the remainder of this section we outline the most important parameters of our model selection process .", "entities": [[14, 16, "TaskName", "model selection"]]}
{"text": "Loss functions We consider the following loss functions : ( i ) MR or margin ranking , which aims to maximize a margin between positive and negative triples ; ( ii ) BCE or binary cross - entropy , which is computed by applying the logistic sigmoid to triple scores ; and ( iii ) CE or cross - entropy between the softmax over the entire distribution of triple scores and the label distribution over all triples , normalized to sum to one .", "entities": [[6, 7, "MetricName", "loss"], [12, 13, "DatasetName", "MR"], [62, 63, "MethodName", "softmax"]]}
{"text": "Search strategies We select models using the Ax platform , which supports hyperparameter search using both quasi - random sequences of generated configurations and Bayesian optimization ( BO ) with Gaussian processes .", "entities": [[30, 32, "TaskName", "Gaussian processes"]]}
{"text": "Table 5 gives link prediction results .", "entities": [[3, 5, "TaskName", "link prediction"]]}
{"text": "By contrast , TuckER is strongest at modeling compositional relations , so it performs best on CODEX - L , which has a high degree of compositionality .", "entities": [[3, 4, "MethodName", "TuckER"]]}
{"text": "Overall , we find that the choice of loss function in particular significantly impacts model performance .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "As far as negative sampling techniques , we do not find that a single strategy is dominant , suggesting that the choice of loss function is more important .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "Table 6 gives triple classification results .", "entities": [[3, 5, "TaskName", "triple classification"]]}
{"text": "Evidently , triple classification on randomly generated negatives is a nearly - solved task .", "entities": [[2, 4, "TaskName", "triple classification"]]}
{"text": "Hard negatives Classification performance degenerates considerably on our hard negatives , around 8 to 11 percentage points from relative frequency - based sampling and 13 to 19 percentage points from uniformly random sampling .", "entities": [[2, 3, "TaskName", "Classification"]]}
{"text": "In contrast to our link prediction task in which ComplEx and TuckER were by far the strongest models , RESCAL is slightly stronger on the CODEX - S hard negatives , whereas ConvE performs best on the CODEX - M hard negatives .", "entities": [[4, 6, "TaskName", "link prediction"], [11, 12, "MethodName", "TuckER"], [19, 20, "MethodName", "RESCAL"]]}
{"text": "These results indicate that triple classification is indeed a distinct task that requires different architectures and , in many cases , different training strategies ( Appendix F ) .", "entities": [[4, 6, "TaskName", "triple classification"]]}
{"text": "We believe that few recent works use triple classification as an evaluation task because of the lack of true hard negatives in existing benchmarks .", "entities": [[7, 9, "TaskName", "triple classification"]]}
{"text": "We further discuss the impact of such n - nary relations for link prediction in the following section .", "entities": [[12, 14, "TaskName", "link prediction"]]}
{"text": "Next , we compare the datasets in a link prediction task to show that CODEX - M is more difficult .", "entities": [[8, 10, "TaskName", "link prediction"]]}
{"text": "Baseline We devise a \" non - learning \" link prediction baseline .", "entities": [[9, 11, "TaskName", "link prediction"]]}
{"text": "Beyond overall performance , we also compute per - relation improvement of the respective embedding over our baseline in terms of percentage points MRR .", "entities": [[23, 24, "MetricName", "MRR"]]}
{"text": "We conclude that while FB15 K - 237 is a valuable dataset , CODEX is more appropriately difficult for link prediction .", "entities": [[19, 21, "TaskName", "link prediction"]]}
{"text": "We present CODEX , a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and Wikipedia , and show that CODEX is suitable for multiple KGC tasks .", "entities": [[7, 10, "TaskName", "knowledge graph COmpletion"]]}
{"text": "Revival of triple classification We encourage the use of triple classification on CODEX in addition to link prediction because it directly tests discriminative power .", "entities": [[2, 4, "TaskName", "triple classification"], [9, 11, "TaskName", "triple classification"], [16, 18, "TaskName", "link prediction"]]}
{"text": "Fusing text and structure Including text in both the link prediction and triple classification tasks should substantially improve performance .", "entities": [[9, 11, "TaskName", "link prediction"], [12, 14, "TaskName", "triple classification"]]}
{"text": "Furthermore , text can be used for few - shot link prediction , an emerging research direction ( Xiong et al , 2017 ; Shi and Weninger , 2017 ) .", "entities": [[10, 12, "TaskName", "link prediction"]]}
{"text": "Table 8 provides an overview of knowledge graph embedding papers with respect to datasets and evaluation tasks .", "entities": [[6, 9, "TaskName", "knowledge graph embedding"]]}
{"text": "The main evaluation benchmarks are FB15 K ( Bordes et al , 2013 ) , WN18 ( Bordes et al , 2013 ) , FB15 K - 237 ( Toutanova and Chen , 2015 ) , WN18RR ( Dettmers et al , 2018 ) , FB13 ( Socher et al , 2013 ) , WN11 ( Socher et al , 2013 ) , NELL - 995 ( Xiong et al , 2017 ) , YAGO3 - 10 ( Dettmers et al , 2018 , Countries ( Bouchard et al , 2015 ) .", "entities": [[15, 16, "DatasetName", "WN18"], [36, 37, "DatasetName", "WN18RR"], [63, 64, "DatasetName", "NELL"], [74, 77, "DatasetName", "YAGO3 - 10"]]}
{"text": "UMLS ( McCray , 2003 ) , Kinship ( Kemp et al , 2006 ) , Families ( Hinton , 1986 , and other versions of NELL ( Mitchell et al , 2018 ) .", "entities": [[0, 1, "DatasetName", "UMLS"], [7, 8, "DatasetName", "Kinship"], [26, 27, "DatasetName", "NELL"]]}
{"text": "You may also search on Google for the answer , although most claims should be resolvable using Wikipedia and Wikidata alone .", "entities": [[5, 6, "DatasetName", "Google"]]}
{"text": "We briefly overview the five models compared in our link prediction and triple classification tasks .", "entities": [[9, 11, "TaskName", "link prediction"], [12, 14, "TaskName", "triple classification"]]}
{"text": "RESCAL ( Nickel et al , 2011 ) was one of the first knowledge graph embedding models .", "entities": [[0, 1, "MethodName", "RESCAL"], [13, 16, "TaskName", "knowledge graph embedding"]]}
{"text": "RESCAL treats relational learning as tensor decomposition , scoring entity embeddings h , r R de and relation embeddings R R de\u00d7de with the bilinear form", "entities": [[0, 1, "MethodName", "RESCAL"], [9, 11, "TaskName", "entity embeddings"]]}
{"text": "TransE ( Bordes et al , 2013 ) treats relations as translations between entities , i.e. , h + r \u2248 t for h , r , t R de , and scores embeddings with negative Euclidean distance \u2212 h + r \u2212 t .", "entities": [[0, 1, "MethodName", "TransE"]]}
{"text": "TransE is likely the most popular baseline for KGC tasks and the most influential of all KGC embedding papers .", "entities": [[0, 1, "MethodName", "TransE"]]}
{"text": "Formally , its scoring function is given as f ( vec ( f ( [ h ; r ] * \u03c9 ) ) W ) t , where f is a nonlinearity ( originally , ReLU ) , [ h ; r ] denotes a concatenation and twodimensional reshaping of the head and relation embeddings , \u03c9 denotes the filters of the convolutional layer , and vec denotes the flattening of a two - dimensional matrix .", "entities": [[35, 36, "MethodName", "ReLU"]]}
{"text": "TuckER ( Balazevic et al , 2019b ) is a linear model based on the Tucker tensor decomposition , which factorizes a tensor into three lower - rank matrices and a core tensor .", "entities": [[0, 1, "MethodName", "TuckER"]]}
{"text": "The TuckER scoring function for a single triple ( h , r , t ) is given as W \u00d7 1 h \u00d7 2 r \u00d7 3 t , where W is the mode - three core tensor that is shared among all entity and relation embeddings , and \u00d7 n denotes the tensor product along the nth mode of the tensor .", "entities": [[1, 2, "MethodName", "TuckER"]]}
{"text": "TuckER can be seen as a generalized form of other linear KGC embedding models like RESCAL and ComplEx .", "entities": [[0, 1, "MethodName", "TuckER"], [15, 16, "MethodName", "RESCAL"]]}
{"text": "Tables 11 , 12 , and 13 report the best hyperparameter configurations for link prediction on CODEX - S , CODEX - M , and CODEX - L , respectively .", "entities": [[13, 15, "TaskName", "link prediction"]]}
{"text": "Tables 14 and 15 report the best hyperparameter configurations for triple classification on the hard negatives in CODEX - S and CODEX - M , respectively .", "entities": [[10, 12, "TaskName", "triple classification"]]}
{"text": "For embedding initialization , Xv refers to Xavier initialization ( Glorot and Bengio , 2010 ) .", "entities": [[7, 9, "MethodName", "Xavier initialization"]]}
{"text": "Search strategies Recall that we select models using Ax , which supports hyperparameter search using both quasi - random sequences of generated configurations and Bayesian optimization ( BO ) .", "entities": [[2, 3, "MetricName", "Recall"]]}
{"text": "We select the best - performing model by validation MRR over all such combinations .", "entities": [[9, 10, "MetricName", "MRR"]]}
{"text": "In each trial , the model is trained for a maximum of 400 epochs with an early stopping patience of 5 .", "entities": [[16, 18, "MethodName", "early stopping"]]}
{"text": "We reduce the number of epochs to limit resource usage .", "entities": [[3, 6, "HyperparameterName", "number of epochs"]]}
{"text": "Note that we search using MRR as our metric , but the triple classification task measures 0/1 accuracy , not ranking performance .", "entities": [[5, 6, "MetricName", "MRR"], [12, 14, "TaskName", "triple classification"], [17, 18, "MetricName", "accuracy"]]}
{"text": "For triple classification , we choose the model with the highest validation accuracy among the pre - trained models across all negative sampling type / loss function combinations .", "entities": [[1, 3, "TaskName", "triple classification"], [12, 13, "MetricName", "accuracy"], [25, 26, "MetricName", "loss"]]}
{"text": "We start with the gentle introduction to the literature , focusing on pathbased and embedding based methods .", "entities": [[12, 13, "DatasetName", "pathbased"]]}
{"text": "In this tutorial , we focus our discussion on two tightly related problems : automatic construction of knowledge bases from text , and knowledge reasoning for knowledge base completion .", "entities": [[26, 29, "TaskName", "knowledge base completion"]]}
{"text": "We will discuss the following key issues : ( 1 ) data - driven approaches for mining quality phrases from massive , unstructured text corpora ; ( 2 ) entity recognition and typing : preliminaries , challenges , and methodologies ; and ( 3 ) relation extraction : previous efforts , limitations , recent progress , and a joint entity and relation extraction method using distant supervision ; ( 4 ) multi - task and multi - domain learning for lowresource information extraction ; ( 5 ) distill linguistic knowledge into neural models to help low - resource information extraction .", "entities": [[45, 47, "TaskName", "relation extraction"], [58, 63, "TaskName", "joint entity and relation extraction"]]}
{"text": "For the embedding based method , we will briefly describe RESCAL ( Nickel et al , 2011 ) and TransE ( Bordes et al , 2013 ) .", "entities": [[10, 11, "MethodName", "RESCAL"], [19, 20, "MethodName", "TransE"]]}
{"text": "We built the first named entity recognizer on Chinese social media Dredze , 2015 , 2016 ) and closed the gap between NER on English and Chinese social media .", "entities": [[22, 23, "TaskName", "NER"]]}
{"text": "The same technique was applied to build the first relation extractor for cross - sentence , n - ary relation extraction between drug , gene , and mutation ( Peng et al , 2017 ) .", "entities": [[19, 21, "TaskName", "relation extraction"]]}
{"text": "For longer duration of the tutorial , we plan to extend entity and relation extraction parts , and add in more case studies and applications .", "entities": [[13, 15, "TaskName", "relation extraction"]]}
{"text": "Researchers and practitioners in the field of natural language processing , computational linguistic , text mining , information retrieval , semantic web and machine learning .", "entities": [[17, 19, "TaskName", "information retrieval"]]}
