{"text": "Context - Aware Graph Segmentation for Graph - Based Translation", "entities": [[9, 10, "TaskName", "Translation"]]}
{"text": "i is an initial pair , iff it is consistent with the word alignment a ( Och and Ney , 2004 ) .", "entities": [[12, 14, "TaskName", "word alignment"]]}
{"text": "Word alignment is performed by GIZA++ ( Och and Ney , 2003 ) with the heuristic function grow - diag - final - and .", "entities": [[0, 2, "TaskName", "Word alignment"]]}
{"text": "We report BLEU", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "Specifically , the improvements are +2.0/+0.7 BLEU on average on ZH - EN and DE - EN , respectively .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "Recall that , compared with GBMT , GBMT ctx contains three types of rules : basic rules , segmenting rules , and selecting rules .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "Joint Models for Answer Verification in Question Answering Systems", "entities": [[6, 8, "TaskName", "Question Answering"]]}
{"text": "This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection ( AS2 ) modules , which are core components of retrievalbased Question Answering ( QA ) systems .", "entities": [[30, 32, "TaskName", "Question Answering"]]}
{"text": "We tested our models on Wik - iQA , TREC - QA , and a real - world dataset .", "entities": [[9, 10, "DatasetName", "TREC"]]}
{"text": "Automated Question Answering ( QA ) research has received a renewed attention thanks to the diffusion of Virtual Assistants .", "entities": [[1, 3, "TaskName", "Question Answering"]]}
{"text": "Among the different types of methods to implement QA systems , we focus on Answer Sentence Selection ( AS2 ) research , originated from TREC - QA track ( Voorhees and Tice , 1999 ) , as it proposes efficient models that are more suitable for a production setting , e.g. , they are more efficient than those developed in machine reading ( MR ) work .", "entities": [[24, 25, "DatasetName", "TREC"], [63, 64, "DatasetName", "MR"]]}
{"text": "Garg et al ( 2020 ) proposed the TANDA approach based on pre - trained Transformer models , obtaining impressive improvement over the state of the art for AS2 , measured on the two most used datasets , WikiQA ( Yang et al , 2015 ) and TREC - QA ( Wang et al , 2007 ) .", "entities": [[15, 16, "MethodName", "Transformer"], [38, 39, "DatasetName", "WikiQA"], [47, 48, "DatasetName", "TREC"]]}
{"text": "Unfortunately , merging the embeddings from all candidates with standard approaches , e.g. , CNN or LSTM , did not improve over TANDA .", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "A more structured approach to building joint models over sentences can instead be observed in Fact Verification Systems , e.g. , the methods developed in the FEVER challenge ( Thorne et al , 2018a ) .", "entities": [[15, 17, "TaskName", "Fact Verification"], [26, 27, "DatasetName", "FEVER"]]}
{"text": "In this paper , we design joint models for AS2 based on the assumption that , given q and a target answer candidate t , the other answer candidates , ( c 1 , .. c k ) can provide positive , negative , or neutral support to decide the correctness of t. Our first approach exploits Fact Checking research : we adapted a state - of - the - art FEVER system , KGAT ( Liu et al , 2020 ) , for AS2 .", "entities": [[57, 59, "TaskName", "Fact Checking"], [71, 72, "DatasetName", "FEVER"]]}
{"text": "We experimented with our models over three datasets , WikiQA , TREC - QA and WQA , where the latter is an internal dataset built on anonymized customer questions .", "entities": [[9, 10, "DatasetName", "WikiQA"], [11, 12, "DatasetName", "TREC"]]}
{"text": "Finally , it is interesting to mention that MASR improvement is also due to the use of FEVER data for pre - fine - tuning ASC , suggesting that the fact verification inference and the answer support inference are similar .", "entities": [[17, 18, "DatasetName", "FEVER"], [30, 32, "TaskName", "fact verification"]]}
{"text": "Previous work estimates p ( q , c i ) with neural models ( Severyn and Moschitti , 2015 ) , also using attention mechanisms , e.g. , Compare - Aggregate ( Yoon et al , 2019 ) , inter - weighted alignment networks ( Shen et al , 2017 ) , and pre - trained Transformer models , which are the state of the art .", "entities": [[56, 57, "MethodName", "Transformer"]]}
{"text": "Garg et al ( 2020 ) proposed TANDA , which is the current most accurate model on WikiQA and TREC -", "entities": [[17, 18, "DatasetName", "WikiQA"], [19, 20, "DatasetName", "TREC"]]}
{"text": "There has been a large body of work preceding Transformer models , e.g. , ( Laskar et al , 2020 ; Tayyar Madabushi et al , 2018 ; Rao et", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "The loss function for training such networks is constituted by the contribution of all elements of its ranked items .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "These improved early neural networks based on CNN and LSTM for AS2 , but failed to improve the state of the art using pre - trained Transformer models .", "entities": [[9, 10, "MethodName", "LSTM"], [26, 27, "MethodName", "Transformer"]]}
{"text": "MR is a popular QA task that identifies an answer string in a paragraph or a text of limited size for a question .", "entities": [[0, 1, "DatasetName", "MR"]]}
{"text": "Moreover , the joint modeling aspect of MR regards sentences from the same paragraphs .", "entities": [[7, 8, "DatasetName", "MR"]]}
{"text": "Jin et al ( 2020 ) use the relation between candidates in Multi - task learning approach for AS2 .", "entities": [[12, 16, "TaskName", "Multi - task learning"]]}
{"text": "This makes our model even more unique ; it allows us to design innovative joint models , which are still not designed in any MR systems .", "entities": [[24, 25, "DatasetName", "MR"]]}
{"text": "Fact verification has become a social need given the massive amount of information generated daily .", "entities": [[0, 2, "TaskName", "Fact verification"]]}
{"text": "The problem has been explored in MR setting ( Wang et al , 2018 ) .", "entities": [[6, 7, "DatasetName", "MR"]]}
{"text": "The latter are retrieved based on similarity scores computed with both TF - IDF and sentence - embeddings from pre - trained BERT models .", "entities": [[22, 23, "MethodName", "BERT"]]}
{"text": "One simple and effective method to build an answer selector is to use a pre - trained Transformer model , adding a simple classification layer to it , and fine - tuning the model on the AS2 task .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "Each of them contains sublayers for multi - head attention , normalization and feed forward processing .", "entities": [[6, 10, "MethodName", "multi - head attention"]]}
{"text": "For example , a softmax can be used to model the probability of the question / candidate pair classification , as : p ( q , c )", "entities": [[4, 5, "MethodName", "softmax"]]}
{"text": "= sof tmax ( W \u00d7 tanh ( E ( q , c ) )", "entities": [[1, 2, "DatasetName", "sof"]]}
{"text": "Training the Transformer from scratch requires a large amount of labeled data , but it can be pre - trained using a masked language model , and the next sentence prediction tasks , for which labels can be automatically generated .", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "Several methods for pretraining Transformer - based language models have been proposed , e.g. , BERT ( Devlin et al , 2018 ) , RoBERTa ( Liu et al , 2019 ) , XLNet ( Yang et al , 2019 ) , AlBERT ( Lan et al , 2020 ) .", "entities": [[4, 5, "MethodName", "Transformer"], [15, 16, "MethodName", "BERT"], [24, 25, "MethodName", "RoBERTa"], [33, 34, "MethodName", "XLNet"]]}
{"text": "The first baseline is also a Transformer - based architecture : we concatenate the question with the top k + 1 answer can - didates , i.e. , ( q [ SEP ] c 1 [ SEP ] c 2 . . .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "[ SEP ] c k+1 ) , and provide this input to the same Transformer model used for pointwise reranking .", "entities": [[14, 15, "MethodName", "Transformer"]]}
{"text": "We use a transformer model fine - tuned with the TANDA - RoBERTa - base or large models , i.e. , RoBERTa models fine - tuned on ASNQ ( Garg et al , 2020 ) .", "entities": [[12, 13, "MethodName", "RoBERTa"], [21, 22, "MethodName", "RoBERTa"], [27, 28, "DatasetName", "ASNQ"]]}
{"text": "= sof tmax ( EW T ) .", "entities": [[1, 2, "DatasetName", "sof"]]}
{"text": "We concatenate the question with each c i to constitute the ( q , c i ) pairs , which are input to the Transformer , and we use the first input token [ CLS ] as the representation of each ( q , c i ) pair .", "entities": [[24, 25, "MethodName", "Transformer"]]}
{"text": "We train the model using a standard classification loss .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "It should be noted that to qualify for a pairwise approach , Joint Model Pairwise should use a ranking loss .", "entities": [[19, 20, "MetricName", "loss"]]}
{"text": "However , we always use standard cross - entropy loss as it is more efficient and the different is performance is negligible .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "Joint Model with KGAT Liu et al ( 2020 ) presented an interesting model , Kernel Graph Attention Network ( KGAT ) , for fact verification : given a claimed fact f , and a set of evidences", "entities": [[16, 19, "MethodName", "Graph Attention Network"], [24, 26, "TaskName", "fact verification"]]}
{"text": "More in detail , we initialize the node representation using the contextual embeddings obtained with two TANDA - RoBERTa - base models 1 : the first produces the embedding of ( q , t ) , while the second outputs the embedding of ( q , c i ) .", "entities": [[18, 19, "MethodName", "RoBERTa"]]}
{"text": "i , we use the Answer Support Classifier ( ASC ) , which classifies each ( t , c i ) in one of the following four classes : 0 : t and c i are both correct , 1 : t is correct while c i is not , 2 : vice versa , and 3 : both incorrect .", "entities": [[29, 30, "DatasetName", "0"]]}
{"text": "This multi - classifier , described in Figure 1b , is built on top a RoBERTa Transformer , which produced a PairWise Representation ( PWR ) .", "entities": [[15, 16, "MethodName", "RoBERTa"], [16, 17, "MethodName", "Transformer"]]}
{"text": "ASC is trained end - to - end with the rest of the network in a multi - task learning fashion , using its specific cross - entropy loss , computed with the labels above .", "entities": [[16, 20, "TaskName", "multi - task learning"], [28, 29, "MetricName", "loss"]]}
{"text": "We use RoBERTa to generate the [ CLS ] R d embedding of ( q , t )", "entities": [[2, 3, "MethodName", "RoBERTa"]]}
{"text": "We denote with\u00ca j the [ CLS ] output by another RoBERTa Transformer applied to answer pairs , i.e. , ( t , c j ) .", "entities": [[11, 12, "MethodName", "RoBERTa"], [12, 13, "MethodName", "Transformer"]]}
{"text": "Then , we use a standard feedforward network to implement a binary classification layer :", "entities": [[6, 8, "MethodName", "feedforward network"]]}
{"text": "= sof tmax ( V W T + B ) , where W R 2\u00d72d and B are parameters to transform the representation of the target answer t from dimension 2d to dimension 2 , which represents correct or incorrect labels .", "entities": [[1, 2, "DatasetName", "sof"]]}
{"text": "An alternative to the definition illustrated above is to use the following FEVER compatible encoding : 0 : t is correct , while c i can be any value , as also an incorrect c i may provide important context ( corresponding to FEVER Support label ) ; 1 : t is incorrect , c i correct , since c i can provide evidence that t is not similar to a correct answer ( corresponding to FEVER Refutal label ) ; and 2 : both are incorrect , in this case , nothing can be told ( corresponding to FEVER Neutral label ) .", "entities": [[12, 13, "DatasetName", "FEVER"], [16, 17, "DatasetName", "0"], [43, 44, "DatasetName", "FEVER"], [76, 77, "DatasetName", "FEVER"], [99, 100, "DatasetName", "FEVER"]]}
{"text": "A final multiclassifier and a softmax function , which scores each t from k + 1 embedding concatenation and selects the one with highest score .", "entities": [[5, 6, "MethodName", "softmax"]]}
{"text": "For training and testing , we select the t from the k + 1 candidates of q based on a softmax output at a time .", "entities": [[20, 21, "MethodName", "softmax"]]}
{"text": "The goal of MASR is to measure the relation between k + 1 target answers , t 0 , .. , t k .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "WikiQA is a QA dataset ( Yang et al , 2015 ) containing a sample of questions and answer - sentence candidates from Bing query logs over Wikipedia .", "entities": [[0, 1, "DatasetName", "WikiQA"]]}
{"text": "WQA The Web - based Question Answering is a dataset built by Alexa AI as part of the effort to improve understanding and benchmarking in QA systems .", "entities": [[5, 7, "TaskName", "Question Answering"]]}
{"text": "FEVER is a large - scale public corpus , proposed by Thorne et al ( 2018a ) for fact verification task , consisting of 185 , 455 annotated claims from 5 , 416 , 537 documents from the Wikipedia dump in June 2017 .", "entities": [[0, 1, "DatasetName", "FEVER"], [18, 20, "TaskName", "fact verification"]]}
{"text": "Metrics The performance of QA systems is typically measured with Accuracy in providing correct answers , i.e. , the percentage of correct responses .", "entities": [[10, 11, "MetricName", "Accuracy"]]}
{"text": "JOINT - MULTICLASSIFER JOINT - PAIR KGAT ASR 1 2 3 4 5 \u22123 \u22122 \u22121 0 1 2 3 4 5 k Improvement ( % ) Figure 2 : Impact of k on the WQA dev .", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Models We use the pre - trained RoBERTa - Base ( 12 layer ) and RoBERTa - Large - MNLI ( 24 layer ) models , which were released as checkpoints for use in downstream tasks 4 .", "entities": [[7, 8, "MethodName", "RoBERTa"], [15, 16, "MethodName", "RoBERTa"], [19, 20, "DatasetName", "MNLI"]]}
{"text": "We apply early stopping on the development set of the target corpus for both fine - tuning steps based on the highest MAP score .", "entities": [[2, 4, "MethodName", "early stopping"], [22, 23, "DatasetName", "MAP"]]}
{"text": "We set the maximum sequence length for RoBERTa to 128 tokens .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}
{"text": "We set the maximum sequence length for RoBERTa Base / Large to 130 tokens and the number of training epochs to 20 .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}
{"text": "We use two transformer models for ASR : a RoBERTa 4 https://github.com/pytorch/fairseq Base / Large for PR , and one for ASC .", "entities": [[9, 10, "MethodName", "RoBERTa"]]}
{"text": "We use the same configuration of the ASR training , including the optimizer type , learning rate , the number of epochs , GPU type , maximum sequence length , etc .", "entities": [[12, 13, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"], [19, 22, "HyperparameterName", "number of epochs"]]}
{"text": "Additionally , we design two different models MASR - F , using an ASC classifier targeting the FEVER labels , and MASR - FP , which initializes ASC with the data from FEVER .", "entities": [[17, 18, "DatasetName", "FEVER"], [32, 33, "DatasetName", "FEVER"]]}
{"text": "set , i.e. , we randomly choose a checkpoint after the standard three epochs of fine - tuning of RoBERTa transformer .", "entities": [[19, 20, "MethodName", "RoBERTa"]]}
{"text": "WikiQA and TREC - QA dev .", "entities": [[0, 1, "DatasetName", "WikiQA"], [2, 3, "DatasetName", "TREC"]]}
{"text": "All models use RoBERTa - Base pre - trained checkpoint and start from the same set of k candidates reranked by PR ( state - of - the - art model ) .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}
{"text": "The results confirm the statistically significant difference between ASR and all the baselines , with p < 0.05 for WikiQA , and between ASR and all models ( i.e. , including also KGAT ) on WQA .", "entities": [[19, 20, "DatasetName", "WikiQA"]]}
{"text": "As the state of the art for AS2 is obtained using RoBERTa Large , we trained KGAT and ASR using this pre - trained language model .", "entities": [[11, 12, "MethodName", "RoBERTa"]]}
{"text": "Again , our PR replicates the results of Garg et al ( 2020 ) , obtaining slightly lower performance on WikiQA but higher on TREC - QA .", "entities": [[20, 21, "DatasetName", "WikiQA"], [24, 25, "DatasetName", "TREC"]]}
{"text": "ASR establishes the new state of the art on WikiQA with an MAP of 92.80 vs. 92.00 .", "entities": [[9, 10, "DatasetName", "WikiQA"], [12, 13, "DatasetName", "MAP"]]}
{"text": "Also , on TREC - QA , ASR outperforms all models , being on par with PR regarding P@1 .", "entities": [[3, 4, "DatasetName", "TREC"], [18, 19, "MetricName", "P@1"]]}
{"text": "We manually checked these and found out that these were two annotation errors : ASR achieves perfect accuracy while PR only mistakes one answer .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "Of course , this just provides evidence that PR based on RoBERTa - Large solves the task of selecting the best answers ( i.e. , measuring P@1 on this dataset is not meaningful anymore ) .", "entities": [[11, 12, "MethodName", "RoBERTa"], [26, 27, "MetricName", "P@1"]]}
{"text": "We note that ASC in MASR - FP achieves the highest accuracy with respect to the average over all datasets .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "This happens since we pre - fine - tuned it with the FEVER data .", "entities": [[12, 13, "DatasetName", "FEVER"]]}
{"text": "Its ASC provides an average score for the category 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "However , PR and ASR both choose answer c 0 , which is correct but not natural , as it provides the requested information indirectly .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "ASR encodes the relation between the target answer and all the other candidates , using an additional Transformer model , and an Answer Support Classifier , while MASR jointly models the ASR representations for all target answers .", "entities": [[17, 18, "MethodName", "Transformer"]]}
{"text": "c3 : The colors in all rainbows are present in the same order : red , orange , yellow , green , blue , indigo , and violet .", "entities": [[0, 1, "DatasetName", "c3"]]}
{"text": "c4 : A rainbow occurs when white light bends and separates into red , orange , yellow , green blue , indigo and violet .", "entities": [[0, 1, "DatasetName", "c4"]]}
{"text": "Open - Domain Why - Question Answering with Adversarial Learning to Encode Answer Texts", "entities": [[5, 7, "TaskName", "Question Answering"]]}
{"text": "Through a series of experiments using Japanese why - QA datasets , we show that these representations improve the performance of our why - QA neural model as well as that of a BERT - based why - QA model .", "entities": [[33, 34, "MethodName", "BERT"]]}
{"text": "Why - question answering ( why - QA ) tasks retrieve from a text archive answers to such why - questions as \" Why does honey last such a long time ? \"", "entities": [[2, 4, "TaskName", "question answering"]]}
{"text": "This idea was inspired by Ishida et al ( 2018 ) , who used a seq2seq model to automatically generate such compact answers as C in Table 1 from the answer passages retrieved by a why - QA method .", "entities": [[15, 16, "MethodName", "seq2seq"]]}
{"text": "If we can use such automatically generated compact - answers to support a why - QA method in finding the exact reason of a whyquestion in these passages , why - QA accuracy may be improved .", "entities": [[32, 33, "MetricName", "accuracy"]]}
{"text": "We actually tried this idea in a preliminary study in which we generated a compact answer from a given question - passage pair by using the compact - answer generation method of Iida et al ( 2019 ) and used the generated compactanswer along with the given question - passage pair to find proper answer passages .", "entities": [[28, 30, "TaskName", "answer generation"]]}
{"text": "Inspired by the generative adversarial network ( GAN ) approach ( Goodfellow et al , 2014 ) , we developed an adversarial network called the Adversarial networks for Generating compact - answer Representation ( AGR ) .", "entities": [[0, 1, "DatasetName", "Inspired"], [3, 6, "MethodName", "generative adversarial network"], [7, 8, "MethodName", "GAN"]]}
{"text": "Like the original GAN , an AGR is composed of a generator and a discriminator : the generator network is trained for generating ( from answer passages ) fake representations to make it hard for the discriminator network to distinguish these fake representations from the true representations derived from manually created compact - answers .", "entities": [[3, 4, "MethodName", "GAN"]]}
{"text": "This combination also outperformed a vanilla BERT model , suggesting that the generator network in our AGR may be effective even if it is combined with many types of NN architectures .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Although the task was not our initial target ( why - QA ) and the answers in the DS - QA task were considerably shorter than those in the why - QA , experiments using three publicly available datasets ( Quasar - T ( Dhingra et al , 2017 ) , SearchQA", "entities": [[40, 43, "DatasetName", "Quasar - T"], [51, 52, "DatasetName", "SearchQA"]]}
{"text": "The probability ( the why - QA model 's final output ) is computed from these representations by our answer selection module , which is a logistic regression layer with dropout and softmax output .", "entities": [[19, 21, "TaskName", "answer selection"], [26, 28, "MethodName", "logistic regression"], [32, 33, "MethodName", "softmax"]]}
{"text": "LeCun et al , 1998 ) that ( 1 ) are augmented by two types of attention mechanisms , similarityattention and causality - attention , and ( 2 ) are given two types of word embeddings , general word embeddings computed by word2vec ( Mikolov et al , 2013 ) using Wikipedia and causal word embeddings ( Sharp et al , 2016 ) .", "entities": [[34, 36, "TaskName", "word embeddings"], [38, 40, "TaskName", "word embeddings"], [54, 56, "TaskName", "word embeddings"]]}
{"text": "= Encoder ( p ; \u03b8 F , q ) R ( c | q )", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "= Encoder ( c ; \u03b8 R , q ) Here \u03b8 F and \u03b8 R represent the parameters of networks F and R. The details of Encoder are described below .", "entities": [[5, 6, "HyperparameterName", "\u03b8"], [11, 12, "HyperparameterName", "\u03b8"], [14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "Discriminator D ( r ) takes as input r , either the output of F ( p | q ) or that of R ( c | q ) , and computes the probability that given representation r comes from a real compact - answer using a feedforward network with two hidden layers ( 100 nodes in the first layer and 50 in the second layer ) and a logistic regression layer on top of the hidden layers .", "entities": [[47, 49, "MethodName", "feedforward network"], [69, 71, "MethodName", "logistic regression"]]}
{"text": "We used sigmoid outputs by the logistic regression layer as the output probability .", "entities": [[6, 8, "MethodName", "logistic regression"]]}
{"text": "Encoder ( t ; \u03b8 , q ) first represents question q and passage / compact - answer t with pre - trained word embeddings , which are supplemented with attention mechanisms .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [23, 25, "TaskName", "word embeddings"]]}
{"text": "The resulting attention - weighted word embeddings are given to convolutional neural networks ( CNNs ) that generate a single feature vector , which is an output / value of Encoder ( t ; \u03b8 , q ) .", "entities": [[5, 7, "TaskName", "word embeddings"], [34, 35, "HyperparameterName", "\u03b8"]]}
{"text": "In the following , we give an overview of the word embeddings , the attention mechanisms , and the CNNs used in Encoder ( t ; \u03b8 , q ) .", "entities": [[10, 12, "TaskName", "word embeddings"], [26, 27, "HyperparameterName", "\u03b8"]]}
{"text": "General word embeddings are widely used embedding vectors ( 300 dimensions ) that were pretrained for about 1.65 million words by applying word2vec ( Mikolov et al , 2013 ) to about 35 million sentences from Japanese Wikipedia ( January 2015 version ) .", "entities": [[0, 1, "DatasetName", "General"], [1, 3, "TaskName", "word embeddings"]]}
{"text": "Causal word embeddings ( Sharp et al , 2016 ) were proposed for representing the causal associations between words .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "Then , following Sharp et al ( 2016 ) , we trained 300dimensional causal word embeddings for about 1.85 million words by applying the generalized skip - gram embedding model of Levy and Goldberg ( 2014 ) to the causality expressions .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "We also applied two types of attention mechanisms to the above word embeddings .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "[ a s 1 , , a s | t | ] and a c = [ a c 1 , , a c | t | ] , con - catenate them into a = [ a s ; a c ] R 2\u00d7 | t | , and produce attention - weighted word embedding t att of given text t , which is either an answer passage or a compact answer : t att = ReLU ( W t t + W a a ) where W t R 2d\u00d72d and W a R 2d\u00d72 are trainable parameters , t is the representation of text t , and ReLU represents the rectified linear units .", "entities": [[77, 78, "MethodName", "ReLU"], [110, 111, "MethodName", "ReLU"], [113, 116, "MethodName", "rectified linear units"]]}
{"text": "Convolutions are performed over the word embeddings using both multiple filters and multiple filter windows ( e.g. , sliding over 1 , 2 , or 3 word windows at a time and 100 filters for each window ) .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "An average pooling operation is applied to the convolution results to generate representation r t , which is the output / value of Encoder ( t ; \u03b8 , q ) ; r t = Encoder ( t ; \u03b8 , q ) .", "entities": [[1, 3, "MethodName", "average pooling"], [8, 9, "MethodName", "convolution"], [27, 28, "HyperparameterName", "\u03b8"], [39, 40, "HyperparameterName", "\u03b8"]]}
{"text": "For training the AGR , we used CmpAns , the training data set created in Ishida et al ( 2018 ) for compact - answer generation ; CmpAns consists of 15 , 130 triples of a why - question , an answer passage , and a manually - created compact answer .", "entities": [[24, 26, "TaskName", "answer generation"]]}
{"text": "CmpAns was created in the following manner : 1 ) human annotators manually came up with open - domain why - questions , 2 ) retrieved the top - 20 passages for each why - question using the open - domain why - QA module of a publicly available web - based QA system WISDOM X ( Mizuno et al , 2016 ; , and 3 ) three annotators created ( when possible ) a compact answer for each of the retrieved passages .", "entities": [[54, 55, "DatasetName", "WISDOM"]]}
{"text": "We used dropout ( Srivastava et al , 2014 ) with probability 0.5 on the final logistic regression layer .", "entities": [[16, 18, "MethodName", "logistic regression"]]}
{"text": "We optimized the learned parameters with the Adam stochastic gradient descent ( Kingma and Ba , 2015 ) .", "entities": [[7, 8, "MethodName", "Adam"], [8, 11, "MethodName", "stochastic gradient descent"]]}
{"text": "In the final scheme , we replaced the word embeddings for the passages given to fake - representation generator F with random vectors and used similarity - attention but not causality - attention .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "This scheme is more similar to the original GAN than the others because the fake - representation generator is given random noises .", "entities": [[8, 9, "MethodName", "GAN"]]}
{"text": "We implemented and evaluated the following four why - QA models in previous works as baselines , using the same dataset as ours : We also evaluated nine baseline neural models , four of which are BERT - based models ( BERT , BERT+AddTr , BERT+F OP , and BERT+F RV ) , to show the effectiveness of our why - QA model and AGR .", "entities": [[36, 37, "MethodName", "BERT"], [41, 42, "MethodName", "BERT"]]}
{"text": "BASE+AddTr BASE that used both W hySet and AddT r as its training data .", "entities": [[1, 2, "MethodName", "BASE"]]}
{"text": "On top of BASE , it additionally used real - representation generator R to encode compact answers , which were generated by the compactanswer generator of Iida et al ( 2019 ) .", "entities": [[3, 4, "MethodName", "BASE"]]}
{"text": "On top of BASE , it additionally used the encoder in the compact - answer generator of Iida et al ( 2019 ) to create compact - answer representation .", "entities": [[3, 4, "MethodName", "BASE"]]}
{"text": "BERT Same as BASE except that the CNNbased encoders for questions and passages were replaced with the BERT ( Devlin et al , 2019 ) .", "entities": [[0, 1, "MethodName", "BERT"], [3, 4, "MethodName", "BASE"], [17, 18, "MethodName", "BERT"]]}
{"text": "BERT+AddTr BERT , which used both W hySet and AddT r as its training data .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "On top of BERT , it additionally used compact - answer representation produced by FOP for answer selection .", "entities": [[3, 4, "MethodName", "BERT"], [16, 18, "TaskName", "answer selection"]]}
{"text": "To pre - train the BERT - based models , we used a combination of sentences extracted from Japanese Wikipedia articles ( August 2018 version ) and causality expressions automatically recognized from a causality recognizer ( Oh et al , 2013 ) .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "E Q E Q E P E 0 E 1 E N+M+1 E [ A BERT - based model , BERT , takes a questionpassage pair as input and computes the input representation using token , segment , position , and attention feature embeddings ( Fig . 3 ) .", "entities": [[7, 8, "DatasetName", "0"], [15, 16, "MethodName", "BERT"], [20, 21, "MethodName", "BERT"]]}
{"text": "For the input representation computation , the original BERT only used the token , segment , and position embeddings , while BERT additionally used the attention feature embeddings 5 to exploit the same similarity - attention and causalityattention features used in our proposed method .", "entities": [[8, 9, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"]]}
{"text": "We used the attention feature embeddings during the fine - tuning and testing , but not during the pretraining of the BERT - based model .", "entities": [[21, 22, "MethodName", "BERT"]]}
{"text": "MAP Oh et al ( 2013 ) 41.8 41.0 Sharp et al ( 2016 )", "entities": [[0, 1, "DatasetName", "MAP"]]}
{"text": "Our starting point , i.e. , BASE , was already superior to the methods in the previous works .", "entities": [[6, 7, "MethodName", "BASE"]]}
{"text": "Ours ( OP ) also outperformed all the BERTbased models but an interesting point is that fakerepresentation generator F boosted the performance of the BERT - based models ( statistically significant with p < 0.01 by the McNemar 's test ) .", "entities": [[24, 25, "MethodName", "BERT"]]}
{"text": "These results suggest that AGR is effective in both our why - QA model and our BERT - based model .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "We computed the following three representation sets from a gold set of 3 , 608 triples of why - questions , answer passages and manually created compact - answers that do not overlap with CmpAns : { r org i } : F RV 's output with the pairs of a whyquestion and an answer passage in the gold set as its input ; { r in i } : F RV 's output for the same input as { r org i } , where we replaced the word embeddings of all the content words in the answer passages that also appeared in the associated gold compact - answers with random vectors ; { r out i } : F RV 's output for the same input as { r org i } , where we replaced the word embeddings of all the content words in the answer passages that did not appear in the associated gold compact - answers with random vectors 6 .", "entities": [[89, 91, "TaskName", "word embeddings"], [139, 141, "TaskName", "word embeddings"]]}
{"text": "If F RV perfectly focuses on the gold standard compact - answers , for each question - passage pair , 6 For both r in i and r out i , we never replaced the word embeddings for the words that also appeared in the question .", "entities": [[35, 37, "TaskName", "word embeddings"]]}
{"text": "Note that we replaced the word embeddings for much more words with random vectors in the computation of { r out i } than those in the computation of { r in i } ( 38.1 words vs. 5.6 words ) .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "We tested our framework on another task , the distantly supervised open - domain question answering ( DS - QA ) task ( Chen et al , 2017 ) , to check its generalizability .", "entities": [[11, 16, "TaskName", "open - domain question answering"]]}
{"text": "The first three , Quasar - T , SearchQA , and TriviaQA provided by Lin et al ( 2018 ) , were used for training and evaluating DS - QA methods .", "entities": [[4, 7, "DatasetName", "Quasar - T"], [8, 9, "DatasetName", "SearchQA"], [11, 12, "DatasetName", "TriviaQA"]]}
{"text": "The training data of SQuAD v1.1 ( Rajpurkar et al , 2016 ) was used for training our AGR .", "entities": [[4, 5, "DatasetName", "SQuAD"]]}
{"text": "The SQuAD dataset consisted of the triples of a question , an answer , and a paragraph that includes the answer .", "entities": [[1, 2, "DatasetName", "SQuAD"]]}
{"text": "We trained our AGR with all the triples of a question , an answer , and a paragraph in the training data of SQuAD - v1.1 under the same settings for the AGR 's hyperparameters as in our why - QA experiment except that we use neither causal word embeddings nor causality - attention .", "entities": [[23, 24, "DatasetName", "SQuAD"], [48, 50, "TaskName", "word embeddings"]]}
{"text": "We used the 300 - dimensional GloVe word embeddings learned from 840 billion tokens in the web crawl data ( Pennington et al , 2014 ) , as general word embeddings .", "entities": [[6, 7, "MethodName", "GloVe"], [7, 9, "TaskName", "word embeddings"], [29, 31, "TaskName", "word embeddings"]]}
{"text": "In the original OpenQA , the paragraph selector and the reader use bidirectional stacked RNNs for encoding paragraphs , where word embeddings p i of a paragraph is used as the input .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "p j i = softmax j (", "entities": [[4, 5, "MethodName", "softmax"]]}
{"text": "p j i , where M R d\u00d7d is a trainable matrix , softmax j ( x ) denotes the j - th element of the softmaxed vector of x , and d", "entities": [[13, 14, "MethodName", "softmax"]]}
{"text": "Table 5 shows the performances of the four DS - QA methods : R 3 ( Wang et al , 2018 ) , OpenQA ( Lin et al , 2018 ) , Ours ( OP ) , and Ours ( RV ) evaluated against the Quasar - T , SearchQA and TriviaQA datasets .", "entities": [[45, 48, "DatasetName", "Quasar - T"], [49, 50, "DatasetName", "SearchQA"], [51, 52, "DatasetName", "TriviaQA"]]}
{"text": "EM measures the percentage of predictions that exactly match one of the ground - truth answers and F1 is a metric that loosely measures the average overlap between the prediction and ground - truth answer .", "entities": [[0, 1, "MetricName", "EM"], [17, 18, "MetricName", "F1"]]}
{"text": "We proposed a method for why - question answering ( why - QA ) that used an adversarial learning framework .", "entities": [[7, 9, "TaskName", "question answering"]]}
{"text": "Due to its potential applications , open - domain dialogue generation has become popular and achieved remarkable progress in recent years , but sometimes suffers from generic responses .", "entities": [[9, 11, "TaskName", "dialogue generation"]]}
{"text": "In recent years , open - domain dialogue generation has become a research hotspot in Natural Language Processing due to its broad application prospect , including chatbots , virtual personal assistants , etc .", "entities": [[7, 9, "TaskName", "dialogue generation"]]}
{"text": "Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic , persona modeling and emotion controlling ( Zhou et al , 2018b ) , most of these recent approaches are primarily built upon the sequence - to - sequence architecture Shang et al , 2015 ) which suffers from the \" safe \" response problem ( Li et al , 2016a ; Sato et al , 2017 ) .", "entities": [[24, 25, "DatasetName", "emotion"]]}
{"text": "This can be ascribed to modeling the response generation process as 1to - 1 mapping , which ignores the nature of 1 - to - n mapping of dialogue that multiple possible responses can correspond to the same query .", "entities": [[7, 9, "TaskName", "response generation"]]}
{"text": "al , , 2018a , Variational Autoencoders ( VAEs ) based models Serban et al , 2017 ) , etc .", "entities": [[6, 7, "MethodName", "Autoencoders"]]}
{"text": "In this paper , we propose a novel response generation model for open - domain conversation , which learns to generate multiple diverse responses with multiple references by considering the correlation of different responses .", "entities": [[8, 10, "TaskName", "response generation"]]}
{"text": "Inspired by this idea , we propose a two - step dialogue generation architecture as follows .", "entities": [[0, 1, "DatasetName", "Inspired"], [11, 13, "TaskName", "dialogue generation"]]}
{"text": "We consider the common and distinctive features of the response bag and propose a novel two - step dialogue generation architecture .", "entities": [[18, 20, "TaskName", "dialogue generation"]]}
{"text": "Along with the flourishing development of neural networks , the sequence - to - sequence framework has been widely used for conversation response generation ( Shang et al , 2015 ; Sordoni et al , 2015 ) where the mapping from a query x to a reply y is learned with the negative log likelihood .", "entities": [[22, 24, "TaskName", "response generation"]]}
{"text": "Besides , during the training of these models , response utterances are only used in the loss function and ignored when forward computing , which can confuse the model for pursuing multiple objectives simultaneously .", "entities": [[16, 17, "MetricName", "loss"]]}
{"text": "VAE is one of the most popular methods ( Bowman et al , 2016 ; Serban et al , 2017 ; Cao and Clark , 2017 ) , where the discourse - level diversity is modeled by a Gaussian distribution .", "entities": [[0, 1, "MethodName", "VAE"]]}
{"text": "However , it is observed that in the CVAE with a fixed Gaussian prior , the learned conditional posteriors tend to collapse to a single mode , resulting in a relatively simple scope ( Wang et al , 2017 ) .", "entities": [[8, 9, "MethodName", "CVAE"]]}
{"text": "In this paper , we propose a novel response generation model for short - text conversation , which models multiple valid responses for a given query jointly .", "entities": [[8, 10, "TaskName", "response generation"], [12, 16, "TaskName", "short - text conversation"]]}
{"text": "For a dialogue generation model , it aims to map from the input query x to the output response y { y } .", "entities": [[2, 4, "TaskName", "dialogue generation"]]}
{"text": "Inspired by multi - instance learning ( Zhou , 2004 ) , we start from the simple intuition that it is much easier for the model to fit multiple instances from their mid - point than a random start - point , as illustrated in Figure 1 .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "In practice , we first encode the input x with a bidirectional gated recurrent units ( GRU ) to obtain an input representation h x .", "entities": [[16, 17, "MethodName", "GRU"]]}
{"text": "Then , the common feature c is computed by a mapping network which is implemented by a feed - forward neural network whose trainable parameter is denoted as \u03b8 .", "entities": [[28, 29, "HyperparameterName", "\u03b8"]]}
{"text": "Then , the cosine similarity of the query and response embeddings is computed , denoted as D \u03b8 ( x , y ) , where \u03b8 represents trainable parameter in the discriminator .", "entities": [[17, 18, "HyperparameterName", "\u03b8"], [25, 26, "HyperparameterName", "\u03b8"]]}
{"text": "The objective of intermediate response y c is then to minimize the difference between D \u03b8 ( x , y c ) and D \u03b8 ( x , { y } ) : L disc = E x , { y } ,", "entities": [[15, 16, "HyperparameterName", "\u03b8"], [24, 25, "HyperparameterName", "\u03b8"]]}
{"text": "y c [ D \u03b8 ( x , y c )", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "\u2212 D \u03b8 ( x , { y } ) ]", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "( 2 ) where y c denotes the utterance produced by the decoder conditioned on the variable c. To overcome the discrete and non - differentiable problem , which breaks down gradient propagation from the discriminator , we adopt a \" soft \" continuous approximation ( Hu et al , 2017 ) : y ct \u223c softmax ( o t /\u03c4 ) ( 3 ) where o t is the logit vector as the inputs to the softmax function at time - step t and the temperature \u03c4 is set to \u03c4 0 as training proceeds for increasingly peaked distributions .", "entities": [[56, 57, "MethodName", "softmax"], [77, 78, "MethodName", "softmax"], [92, 93, "DatasetName", "0"]]}
{"text": "In practice , we adopt the CVAE architecture , while two prominent modifications remain .", "entities": [[6, 7, "MethodName", "CVAE"]]}
{"text": "Specifically , similar to the CVAE architecture , the overall objective for our model in the second generation phase is as below :", "entities": [[5, 6, "MethodName", "CVAE"]]}
{"text": "L cvae = E q \u03c6 ( z | x , y , c ) p \u03b8 ( c | x ) [ log p \u03c8 ( y | c , z ) ]", "entities": [[1, 2, "MethodName", "cvae"], [16, 17, "HyperparameterName", "\u03b8"]]}
{"text": "h x h y c \uf8f9 \uf8fb + b q ( 6 ) where h x and h y are the utterance representations of query and response got by GRU respectively , and the latent variable z \u223c N ( \u00b5 , \u03c3 2 I ) .", "entities": [[29, 30, "MethodName", "GRU"]]}
{"text": "One is the vanilla CVAE model where the prior p \u03d5 ( z | x , c ) is modeled by a another feed - forward network conditioned on the representations h x and c as follows , \u00b5 log \u03c3", "entities": [[4, 5, "MethodName", "CVAE"]]}
{"text": "To sample an instance , Gumble - Softmax reparametrization trick ( Kusner and Hern\u00e1ndez - Lobato , 2016 ) is utilized to normalize the coefficients .", "entities": [[7, 8, "MethodName", "Softmax"]]}
{"text": "Hence to distinguish the latent variable z for each separate response , we further introduce a multireference bag - of - word loss ( MBOW ) which requires the network to predict the current response y against the response bag : L mbow = E q \u03c6 ( z | x , y , c ) [ log p ( y bow |", "entities": [[22, 23, "MetricName", "loss"]]}
{"text": "As it shows , the MBOW loss penalizes the recognition networks if other complementary responses can be predicted from the distinctive variable z.", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "To overcome the vanishing latent variable problem ( Wang et al , 2017 ) of CVAE , we adopt the KL annealing strategy ( Bowman et al , 2016 ) , where the weight of the KL term is gradually increased during training .", "entities": [[15, 16, "MethodName", "CVAE"]]}
{"text": "The other technique employed is the MBOW loss which is able to sharpen the distribution of latent variable z for each specific response and alleviate the vanishing problem at the same time .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "Focusing on open - domain dialogue , we perform experiments on a large - scale single - turn conversation dataset Weibo ( Shang et al , 2015 ) , where each input post is generally associated with multiple response utterances 2 .", "entities": [[20, 21, "DatasetName", "Weibo"]]}
{"text": "Concretely , the Weibo dataset consists of short - text online chit - chat dialogues in Chinese , which is crawled from Sina Weibo 3 .", "entities": [[3, 4, "DatasetName", "Weibo"], [23, 24, "DatasetName", "Weibo"]]}
{"text": "We compare our model with representative dialogue generation approaches as listed below : S2S : the vanilla sequence - to - sequence model with attention mechanism where standard beam search is applied in testing to generate multiple different responses .", "entities": [[6, 8, "TaskName", "dialogue generation"]]}
{"text": "CVAE : the vanilla CVAE model with and without BOW ( bag - of - word ) loss ( CVAE+BOW and CVAE ) .", "entities": [[0, 1, "MethodName", "CVAE"], [4, 5, "MethodName", "CVAE"], [17, 18, "MetricName", "loss"], [21, 22, "MethodName", "CVAE"]]}
{"text": "WAE : the conditional Wasserstein autoencoder model for dialogue generation ( Gu et al , 2018 ) which models the distribution of data by training a GAN within the latent variable space .", "entities": [[5, 6, "MethodName", "autoencoder"], [8, 10, "TaskName", "dialogue generation"], [26, 27, "MethodName", "GAN"]]}
{"text": "Ours : we explore our model Ours and conduct various ablation studies : the model with only the second stage generation ( Ours - First ) , the model without the discriminator ( Ours - Disc ) and multireference BOW loss ( Ours - MBOW ) , and the model with GMM prior networks ( Ours+GMP ) .", "entities": [[40, 41, "MetricName", "loss"]]}
{"text": "Since multiple valid responses exist in this paper , we adopt multi - reference BLEU where the evaluated utterance is compared to provided multiple references simultaneously .", "entities": [[14, 15, "MetricName", "BLEU"]]}
{"text": "Embedding Similarity : Embedding - based metrics compute the cosine similarity between the sentence embedding of a ground - truth response and that of the generated one .", "entities": [[13, 15, "TaskName", "sentence embedding"]]}
{"text": "There are various ways to obtain the sentence - level embedding from the constituent word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "Table 3 : Case study for the generated responses from the testing set of Weibo , where the Chinese utterances are translated into English for the sake of readability .", "entities": [[14, 15, "DatasetName", "Weibo"]]}
{"text": "For each input query , we show four responses generated by each method and an additional intermediate utterance ( marked with underline ) for our model . dings ; and Extreme takes the most extreme value among all words for each dimension of word embeddings in a sentence .", "entities": [[43, 45, "TaskName", "word embeddings"]]}
{"text": "As to the CVAE and WAE models , with the latent variable to control the discourse - level diversity , diverse responses can be obtained .", "entities": [[3, 4, "MethodName", "CVAE"]]}
{"text": "With both obvious superiority ( readability for S2S and diversity for CVAE ) and inferiority ( diversity for S2S and relevance for CVAE ) , the baselines show limited overall performances , in contrast to which our method can output more diverse utterances while maintaining the relevance to the input query and achieve a high overall score .", "entities": [[11, 12, "MethodName", "CVAE"], [22, 23, "MethodName", "CVAE"]]}
{"text": "As the results of BLEU and embedding - based metrics show , the system can benefit from the common feature for better relevance to the query .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "Moreover , pairwise comparisons Ours - Disc vs. Ours and Ours - MBOW vs. Ours validate the effects of the discriminator and modified multireference bag - of - word loss ( MBOW ) .", "entities": [[29, 30, "MetricName", "loss"]]}
{"text": "The MBOW loss , similar to the effects of BOW loss in the CVAE , can lead to a more unique latent variable for each response and improve the final distinctness scores of generated utterances .", "entities": [[2, 3, "MetricName", "loss"], [10, 11, "MetricName", "loss"], [13, 14, "MethodName", "CVAE"]]}
{"text": "In the experiments , we also observed the KL vanishing problem when training our model and we overcame it with the KL weight annealing strategy and the MBOW loss described above .", "entities": [[28, 29, "MetricName", "loss"]]}
{"text": "Comparing the CVAE and Ours , we can find that although the CVAE model can generate diverse utterances , its responses tend to be irrelevant to the query and sometimes not grammatically formed , e.g. the words \" glowworm \" and \" robot \" in the sentences .", "entities": [[2, 3, "MethodName", "CVAE"], [12, 13, "MethodName", "CVAE"]]}
{"text": "This paper introduces SGNMT , our experimental platform for machine translation research .", "entities": [[9, 11, "TaskName", "machine translation"]]}
{"text": "SGNMT is actively being used by students in the MPhil program in Machine Learning , Speech and Language Technology at the University of Cambridge for course work and theses , as well as for most of the research work in our group .", "entities": [[23, 24, "DatasetName", "Cambridge"]]}
{"text": "We are developing an open source decoding framework called SGNMT , short for Syntactically Guided Neural Machine Translation .", "entities": [[16, 18, "TaskName", "Machine Translation"]]}
{"text": "The strict separation of scoring module and search strategy and the decoupling of scoring modules from each other makes SGNMT a very flexible decoding tool for neural and symbolic models which is applicable not only to machine translation .", "entities": [[36, 38, "TaskName", "machine translation"]]}
{"text": "SGNMT is based on the OpenFSTbased Cambridge SMT system ( Allauzen et al , 2014 ) .", "entities": [[6, 7, "DatasetName", "Cambridge"]]}
{"text": "In the 2015 - 16 academic year , two students on the Cambridge MPhil in Machine Learning , Speech and Language Technology used SGNMT for their dissertation projects .", "entities": [[12, 13, "DatasetName", "Cambridge"]]}
{"text": "Predictor Description nmt Attention - based neural machine translation following .", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "Tab . 1 summarizes the semantics of this interface for three very common predictors : the neural machine translation ( NMT ) predictor , the ( deterministic ) finite state transducer ( FST ) predictor for lattice rescoring , and the n - gram predictor for applying n - gram language models .", "entities": [[17, 19, "TaskName", "machine translation"]]}
{"text": "This decoder is able to translate the WMT English - French test sets news - test2012 to news - test2014 on a Titan X GPU with 911.6 words per second with the word - based NMT model described in Stahlberg et al ( 2016 ) .", "entities": [[22, 23, "DatasetName", "Titan"]]}
{"text": "This makes it possible to decode by combining scores from both a subword - unit ( BPE ) based NMT ( Sennrich et al , 2016 ) and a word - based NMT model with character - based NMT , masking the BPE - based and word - based NMT predictors with FSTs which transduce character sequences to BPE or word sequences .", "entities": [[16, 17, "MethodName", "BPE"], [42, 43, "MethodName", "BPE"], [58, 59, "MethodName", "BPE"]]}
{"text": "Joint decoding with different tokenization schemes has the potential of combining the benefits of the different schemes : character - and BPE - based models are able to address rare words , but word - based NMT can model long - range dependencies more efficiently .", "entities": [[21, 22, "MethodName", "BPE"]]}
{"text": "x 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "x 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Blue x 0 comes from the test dataset and its prediction can not be altered by the substitution film movie ( robust ) .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Yellow examplex 0 is slightly perturbed but remains natural .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "However , even if a model is first - order robust on an input sentence x 0 , it is possible that the model is not robust on a natural sentencex 0 that is slightly modified from x 0 .", "entities": [[16, 17, "DatasetName", "0"], [31, 32, "DatasetName", "0"], [38, 39, "DatasetName", "0"]]}
{"text": "Throughout this paper , we callx 0 a vulnerable example .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "2 as an example , the model is first - order robust on the input sentence x 0", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "( the prediction can not be altered ) , but it is not second - order robust due to the existence of the vulnerable examplex 0 .", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "Our framework is designed to identifyx 0 .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "We experiment with English sentiment classification on the SST - 2 dataset ( Socher et al , 2013 ) across various model architectures .", "entities": [[8, 9, "DatasetName", "SST"]]}
{"text": "Surprisingly , although robustly trained CNN ( Jia et al , 2019 ) and Transformer ( Xu et", "entities": [[14, 15, "MethodName", "Transformer"]]}
{"text": "( 2 ) We propose two second - order attacks to quantify the stronger notion of second - x 0x 0 x 0 x 1 negative positive Figure 2 : An illustration of the decision boundary .", "entities": [[20, 21, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "Blue x 0 is a robust input example ( the entire diamond is green ) .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Yellowx 0 is a vulnerable example in the neighborhood of x 0 .", "entities": [[1, 2, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "Redx 0 is an adversarial example tox 0 .", "entities": [[1, 2, "DatasetName", "0"], [7, 8, "DatasetName", "0"]]}
{"text": "Note : x 0 is not an adversarial example to x 0 since they have different meanings to human ( outside the diamond ) .", "entities": [[3, 4, "DatasetName", "0"], [11, 12, "DatasetName", "0"]]}
{"text": "V 2 be a pair of synonyms ( called patch words ) , X p \u2286 X denotes sentences with a single occurrence of p ( 1 ) ( for simplicity we skip other sentences ) , x 0 X p be an input sentence , then x 0", "entities": [[38, 39, "DatasetName", "0"], [48, 49, "DatasetName", "0"]]}
{"text": "p means \" substitute p ( 1 ) p ( 2 ) in x 0 \" .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "The result after substitution is : x 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "= x 0", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "where p = ( film , movie ) and x 0 = a deep and meaningful film , the perturbed sentence is x 0", "entities": [[10, 11, "DatasetName", "0"], [23, 24, "DatasetName", "0"]]}
{"text": "We limit the neighborhood sentences within a small 0 norm ball ( regarding the test instance ) to ensure syntactic similarity , and empirically ensure the naturalness through a language model .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "The neighborhood of an input sentence x 0 X is : Neighbor k ( x 0 ) \u2286", "entities": [[7, 8, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}
{"text": "Ball k ( x 0 ) \u2229", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "x 0 )", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "x \u2212 x 0 0 \u2264 k , x X } is the 0 norm ball around x 0", "entities": [[3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [18, 19, "DatasetName", "0"]]}
{"text": "We construct neighborhood sentences from x 0 by substituting at most k tokens .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "For each recursion , the algorithm first masks each token of the input sentence ( may be the original x 0 or thex from last recursion ) separately and predicts likely replacements with a masked language model ( e.g. , DistilBERT , Sanh et al 2019 ) .", "entities": [[20, 21, "DatasetName", "0"], [40, 41, "MethodName", "DistilBERT"]]}
{"text": "x ( i ) 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "( x ) : X { 0 , 1 } as an example to describe our framework .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "= y 0 .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "\u03b4", "entities": [[0, 1, "HyperparameterName", "\u03b4"]]}
{"text": "Blue x 0 is the input sentence and yellowx 0 is our constructed vulnerable example ( the prediction can be altered by substituting film movie ) .", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}
{"text": "X \u00d7 V 2 { \u22121 , 0 , 1 } by : 3 F ( x ; p ) :", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "1 as an example , the prediction difference forx 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "on p is F ( x 0 ; p )", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "Given an input sentence x 0 , we want to find patch words p and a vulnerable examplex 0 such that f ( x 0 p )", "entities": [[5, 6, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [24, 25, "DatasetName", "0"]]}
{"text": "( x 0 ) .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "X [ 0 , 1 ] denotes probability output ( e.g. , after the softmax layer but before the final argmax ) , f soft ( p ( 1 ) ) and f soft ( p ( 2 ) ) denote the predictions for the single word , and we enumerate through all possible p for x 0 .", "entities": [[2, 3, "DatasetName", "0"], [14, 15, "MethodName", "softmax"], [57, 58, "DatasetName", "0"]]}
{"text": "Let k be the neighborhood distance , then the attack is equivalent to solving : x 0", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "Neighbor k ( x 0 ) | F ( x ; p ) | .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "( 3 ) is to enumerate through Neighbor k ( x 0 ) .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "Neighbor k ( x 0 ) : L ( x ; p ) :", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "= \u2212 log ( 1 \u2212 f min ) \u2212 log ( f max ) , ( 4 ) where f min and f max are the smaller and the larger output probability between f soft ( x ) and f soft ( x 3 We assume a binary classification task , but our framework is general and can be extended to multi - class classification . p ) , respectively .", "entities": [[62, 66, "TaskName", "multi - class classification"]]}
{"text": "( 4 ) effectively leads to f min 0 and f max 1 , and we use a beam search to find the best x. At each iteration , we construct sentences through Neighbor 1 ( x ) and only keep the top 20 sentences with the smallest L ( x ; p ) .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "= 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "then returnx0 ; 8 Xnew SortIncreasing ( Xnew , L ) ; 9 Xbeam { X ( 0 ) new , . . .", "entities": [[17, 18, "DatasetName", "0"]]}
{"text": "We train the binary sentiment classifiers on the SST - 2 dataset with bag - ofwords ( BoW ) , CNN , LSTM , and attention - based Original : 70 % Negative Input Example : in its best moments , resembles a bad high school production of grease , without benefit of song .", "entities": [[8, 9, "DatasetName", "SST"], [22, 23, "MethodName", "LSTM"]]}
{"text": "For BoW , CNN , and LSTM , all models use pre - trained GloVe embeddings ( Pennington et al , 2014 ) , and have one hidden layer of the corresponding type with 100 hidden size .", "entities": [[6, 7, "MethodName", "LSTM"], [14, 16, "MethodName", "GloVe embeddings"]]}
{"text": "For attention - based models , we train a 3 - layer Transformer ( the largest size in ) and fine - tune a pre - trained bertbase - uncased from HuggingFace ( Wolf et al , 2020 ( Morris et al , 2020 ) .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "The average running time per example ( in seconds ) on base LSTM is 31.9 for Genetic , 1.1 for BAE , 7.0 for SO - Enum", "entities": [[12, 13, "MethodName", "LSTM"]]}
{"text": "k \u2248 0 , whereas a positive or negative B p , k indicates that the model shows preference or against to p ( 2 ) , respectively .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "For instance , let x 0 be a sentence containing he , even though it is possible for Neighbor 1 ( x 0 ) to contain many stereotyping sentences ( e.g. , contains tokens such as doctor and driving ) that affect the distribution of f soft ( x ) , but it does not bias Eq .", "entities": [[5, 6, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "We evaluate counterfactual token bias on the SST - 2 dataset with both the base and debiased models .", "entities": [[7, 8, "DatasetName", "SST"]]}
{"text": "We train a single layer LSTM with pre - trained GloVe embeddings and 75 hidden size ( from TextAttack , Morris et al 2020 ) .", "entities": [[5, 6, "MethodName", "LSTM"], [10, 12, "MethodName", "GloVe embeddings"]]}
{"text": "Furthermore , we observe mitigated biases in the debiased model , which demonstrates the effectiveness of data augmentation .", "entities": [[16, 18, "TaskName", "data augmentation"]]}
{"text": "Similar to other existing adversarial attack methods ( Ebrahimi et al , 2018 ; Jin et al , 2019 ; Zhao et al , 2018b ) , our second - order attacks can be used for finding vulnerable examples to a NLP system .", "entities": [[4, 6, "TaskName", "adversarial attack"]]}
{"text": "We perform the attack on the same models as Table 2 , and the attack success rates on robustly trained BoW , CNN , LSTM , and Transformers are 18.8 % , 22.3 % , 15.2 % , and 25.1 % , respectively .", "entities": [[24, 25, "MethodName", "LSTM"]]}
{"text": "Semantic similarity after the synonym substitution .", "entities": [[0, 2, "TaskName", "Semantic similarity"]]}
{"text": "We use the same base LSTM as described in 4.2 .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "This is probably caused by tuples like ( him , his , her ) which can not be swapped perfectly , and requires additional processing such as part - of - speech resolving ( Zhao et al , 2018a ) .", "entities": [[27, 30, "DatasetName", "part - of"]]}
{"text": "Better Feature Integration for Named Entity Recognition", "entities": [[4, 7, "TaskName", "Named Entity Recognition"]]}
{"text": "It has been shown that named entity recognition ( NER ) could benefit from incorporating the long - distance structured information captured by dependency trees .", "entities": [[5, 8, "TaskName", "named entity recognition"], [9, 10, "TaskName", "NER"]]}
{"text": "However , existing approaches largely focused on stacking the LSTM and graph neural networks such as graph convolutional networks ( GCNs ) for building improved NER models , where the exact interaction mechanism between the two different types of features is not very clear , and the performance gain does not appear to be significant .", "entities": [[9, 10, "MethodName", "LSTM"], [25, 26, "TaskName", "NER"]]}
{"text": "In this work , we propose a simple and robust solution to incorporate both types of features with our Synergized - LSTM ( Syn - LSTM ) , which clearly captures how the two types of features interact .", "entities": [[21, 22, "MethodName", "LSTM"], [25, 26, "MethodName", "LSTM"]]}
{"text": "Named entity recognition ( NER ) is one of the most fundamental and important tasks in natural language processing ( NLP ) .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"]]}
{"text": "While the literature ( Peters et al , 2018 ; Akbik et al , 2018 ; Devlin et al , 2019 ) largely focuses on training deep language models to improve the contextualized word representations , previous studies show that the structured information such as interactions between non - adjacent words can also be important for NER ( Finkel et", "entities": [[56, 57, "TaskName", "NER"]]}
{"text": "However , sequence models such as bidirectional LSTM ( Hochreiter and Schmidhuber , 1997 ) are not able to fully capture the long - range dependencies ( Bengio , 2009 ) .", "entities": [[6, 8, "MethodName", "bidirectional LSTM"]]}
{"text": "For instance , Figure 1 ( top ) shows one type of structured information in NER .", "entities": [[15, 16, "TaskName", "NER"]]}
{"text": "The words \" Precision Castparts Corp. \" can be easily inferred as ORGANIZATION by its context ( i.e. , Corp. ) .", "entities": [[3, 4, "MetricName", "Precision"]]}
{"text": "However , the second entity \" PCP \" could be misclassified as a PRODUCT entity if a model relies more on the context \" begin trading with \" but ignores the hidden information that \" PCP \" is the symbol of \" Precision Castparts Corp. \" .", "entities": [[42, 43, "MetricName", "Precision"]]}
{"text": "However , simply stacking the LSTM and GCN architectures for NER can only provide us with modest improvements ; sometimes , it decreases performance ( Jie and Lu , 2019 ) .", "entities": [[5, 6, "MethodName", "LSTM"], [7, 8, "MethodName", "GCN"], [10, 11, "TaskName", "NER"]]}
{"text": "Based on the depen - dency path in Figure 1 , it requires a 5 - layer GCN to capture the connections between these two entities .", "entities": [[17, 18, "MethodName", "GCN"]]}
{"text": "However , deep GCN architectures often face training difficulties , which cause a performance drop ( Hamilton et al , 2017b ; Kipf and Welling , 2017 ) .", "entities": [[3, 4, "MethodName", "GCN"]]}
{"text": "Directly stacking GCN and LSTM has difficulties in modeling the interaction between dependency trees and contextual information .", "entities": [[2, 3, "MethodName", "GCN"], [4, 5, "MethodName", "LSTM"]]}
{"text": "To address the above limitations , we propose the Synergized - LSTM ( Syn - LSTM ) , a new recurrent neural network architecture that considers an additional graph - encoded representation to update the memory and hidden states , as shown in Figure 2 .", "entities": [[11, 12, "MethodName", "LSTM"], [15, 16, "MethodName", "LSTM"]]}
{"text": "Our proposed Syn - LSTM allows the cell to receive the structured information from the graph - encoded representation .", "entities": [[4, 5, "MethodName", "LSTM"]]}
{"text": "Our contributions can be summarized as : We propose a simple and robust Syn - LSTM model to better incorporate the structured information conveyed by dependency trees .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "The output of the Syn - LSTM cell is jointly determined by both contextual and structured information .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "We adopt the classic conditional random fields ( CRF ) ( Lafferty et al , 2001 ) on top of the Syn - LSTM for NER .", "entities": [[8, 9, "MethodName", "CRF"], [23, 24, "MethodName", "LSTM"], [25, 26, "TaskName", "NER"]]}
{"text": "2 Synergized - LSTM", "entities": [[3, 4, "MethodName", "LSTM"]]}
{"text": "To incorporate the long - range dependencies , we consider an additional graph - encoded representation g t ( Figure 2 ) as the model input to integrate \u03c3 \u03c3 \u03c3 tanh tanh \u03c3 \u00d7 + \u00d7 + \u00d7 \u00d7 tanh c t - 1 Previous Cell contextual and structured information .", "entities": [[46, 47, "DatasetName", "Cell"]]}
{"text": "The graphencoded representation g t can be derived from Graph Neural Networks ( GNNs ) such as GCN ( Kipf and Welling , 2017 ) , which are capable of bringing in structured information through graph structure ( Hamilton et al , 2017a ) .", "entities": [[17, 18, "MethodName", "GCN"]]}
{"text": "A straightforward solution is to integrate both structured and contextual information via LSTM .", "entities": [[12, 13, "MethodName", "LSTM"]]}
{"text": "The input to the LSTM can simply be the concatenation of word representation x t and g t at each position ( Jie and Lu , 2019 ) 2 .", "entities": [[4, 5, "MethodName", "LSTM"]]}
{"text": "We propose the Synergized - LSTM ( Syn - LSTM ) to better integrate the contextual and structured information to address the above limitations .", "entities": [[5, 6, "MethodName", "LSTM"], [9, 10, "MethodName", "LSTM"]]}
{"text": "The inputs of the Syn - LSTM cell include previous cell state c t\u22121 , previous hidden state h t\u22121 , current cell input x t , and an additional graph - encoded representation g t .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "The outputs of the Syn - LSTM cell include current cell state c t and current hidden state h t .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "With the proposed Syn - LSTM , the structured information captured by the dependency trees can be passed to each cell , and the additional gate m t is able to control how much structured information can be incorporated .", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "The additional gate enables the model to feed the contextual and structured information into the LSTM cell separately .", "entities": [[15, 16, "MethodName", "LSTM"]]}
{"text": "Syn - LSTM Syn - LSTM Syn - LSTM Syn - LSTM", "entities": [[2, 3, "MethodName", "LSTM"], [5, 6, "MethodName", "LSTM"], [8, 9, "MethodName", "LSTM"], [11, 12, "MethodName", "LSTM"]]}
{"text": "g 0", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "t - 1 g 0 t g 0 t+1 g 0 t+2 Graph Convolutional Network c t = f t c t\u22121", "entities": [[4, 5, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [12, 15, "MethodName", "Graph Convolutional Network"]]}
{"text": "( i j t k = j+1 f k )", "entities": [[4, 6, "HyperparameterName", "k ="]]}
{"text": "The value of the two terms are in the range from 0 to 1 .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "The goal of named entity recognition is to predict the label sequence y", "entities": [[3, 6, "TaskName", "named entity recognition"]]}
{"text": "Our model is mainly constructed with three layers : input representation layer , bi - directional Syn - LSTM layer , and CRF layer .", "entities": [[18, 19, "MethodName", "LSTM"], [22, 23, "MethodName", "CRF"]]}
{"text": "The architecture of our Syn - LSTM - CRF is shown in Figure 3 .", "entities": [[6, 7, "MethodName", "LSTM"], [8, 9, "MethodName", "CRF"]]}
{"text": ", our input representation also includes the character embeddings , which are the hidden states of character - based BiLSTM .", "entities": [[19, 20, "MethodName", "BiLSTM"]]}
{"text": "For experiments with the contextualized representations ( e.g. , BERT ( Devlin et al , 2019 ) ) , we further concatenate the contextual word representation to x t .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "For our task , we employ the graph convolutional network ( Kipf and Welling , 2017 ; Zhang et al , 2018b ) to get the graph - encoded representation g t .", "entities": [[7, 10, "MethodName", "graph convolutional network"]]}
{"text": "The input and output representations of the l - th layer GCN at t - th position are denoted as g l\u22121 t and g l t respectively .", "entities": [[11, 12, "MethodName", "GCN"]]}
{"text": "The GCN operation is defined as : g l t = ReLU ( n j=1 A t , j W l g l\u22121 t /d t + b l ) ( 13 ) where W l is a linear transformation and b l is a bias .", "entities": [[1, 2, "MethodName", "GCN"], [11, 12, "MethodName", "ReLU"]]}
{"text": "The initial g 0 t is the concatenation of word embedding v t , character embedding e t , and dependency relation embedding r t : \u2212 h t from backward Syn - LSTM to form the contextual representation of t - th token : g 0", "entities": [[3, 4, "DatasetName", "0"], [33, 34, "MethodName", "LSTM"], [46, 47, "DatasetName", "0"]]}
{"text": "CRF Layer The CRF ( Lafferty et al , 2001 ) is widely used in NER tasks as it is capable of capturing the structured correlations between adjacent output labels .", "entities": [[0, 1, "MethodName", "CRF"], [3, 4, "MethodName", "CRF"], [15, 16, "TaskName", "NER"]]}
{"text": "We learn the model parameters by minimizing the negative log - likelihood and employ the Viterbi algorithm to obtain the best label sequence during evaluation .", "entities": [[9, 12, "MetricName", "log - likelihood"]]}
{"text": "Catalan and Spanish datasets , and OntoNotes 5.0 ( Weischedel et al , 2013 )", "entities": [[6, 8, "DatasetName", "OntoNotes 5.0"]]}
{"text": "For OntoNotes 5.0 datasets , there are 18 entity types in total .", "entities": [[1, 3, "DatasetName", "OntoNotes 5.0"]]}
{"text": "Experimental Setup For Catalan , Spanish , and Chinese , we use the FastText ( Grave et al , 2018 ) 300 dimensional embeddings to initialize the word embeddings .", "entities": [[13, 14, "MethodName", "FastText"], [27, 29, "TaskName", "word embeddings"]]}
{"text": "For OntoNotes 5.0 English , we adopt the publicly available GloVE ( Pennington et al , 2014 ) 100 dimensional embeddings to initialize the word embeddings .", "entities": [[1, 3, "DatasetName", "OntoNotes 5.0"], [24, 26, "TaskName", "word embeddings"]]}
{"text": "For experiments with the contextualized representation , we adopt the pre - trained language model BERT ( Devlin et al , 2019 ) for the four datasets .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "Following Luo et al ( 2020 ) , we use the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data .", "entities": [[14, 15, "MethodName", "BERT"], [22, 24, "DatasetName", "OntoNotes 5.0"]]}
{"text": "We use the cased version of BERT base model for the experiments on the other three datasets .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "For the character embedding , we randomly initialize the character embeddings and set the dimension as 30 , and set the hidden size of character - level BiLSTM as 50 .", "entities": [[27, 28, "MethodName", "BiLSTM"]]}
{"text": "The hidden size of GCN and Syn - LSTM is set as 200 , the number of GCN layer is 2 .", "entities": [[4, 5, "MethodName", "GCN"], [8, 9, "MethodName", "LSTM"], [17, 18, "MethodName", "GCN"]]}
{"text": "The first one is BERT - CRF , where we apply a CRF layer on top of BERT ( Devlin et al , 2019 ) .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "CRF"], [12, 13, "MethodName", "CRF"], [17, 18, "MethodName", "BERT"]]}
{"text": "Secondly , we compare with the BERT implementation by HuggingFace ( Wolf et al , 2019 ) .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "( Peters et al , 2018 ) , but we also implement it with BERT .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "Our Syn - LSTM - CRF model outperforms all existing models with F 1 82.76 and 85.09 ( p < 10 \u22125 ) compared to DGLSTM - CRF on Catalan and Spanish datasets when FastText word embeddings are used .", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"], [27, 28, "MethodName", "CRF"], [34, 35, "MethodName", "FastText"], [35, 37, "TaskName", "word embeddings"]]}
{"text": "Our model outperforms the BiLSTM - CRF model by 13.25 and 11.22 F 1 points , and outperforms BiLSTM - GCN - CRF ( Jie and Lu , 2019 ) model by 4.64 and 3.16 on Catalan and Spanish .", "entities": [[4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "CRF"], [18, 19, "MethodName", "BiLSTM"], [20, 21, "MethodName", "GCN"], [22, 23, "MethodName", "CRF"]]}
{"text": "The large performance gap between BiLSTM - GCN - CRF and our model indicates that Syn - LSTM - CRF shows better compatibility with GCN , and this confirms that simply stacking GCN on top of the BiLSTM does not perform well .", "entities": [[5, 6, "MethodName", "BiLSTM"], [7, 8, "MethodName", "GCN"], [9, 10, "MethodName", "CRF"], [17, 18, "MethodName", "LSTM"], [19, 20, "MethodName", "CRF"], [24, 25, "MethodName", "GCN"], [32, 33, "MethodName", "GCN"], [37, 38, "MethodName", "BiLSTM"]]}
{"text": "Our method outperforms GCN - BiLSTM - CRF model by 5.33 and 3.24 F 1 points on Catalan and Spanish .", "entities": [[3, 4, "MethodName", "GCN"], [5, 6, "MethodName", "BiLSTM"], [7, 8, "MethodName", "CRF"]]}
{"text": "Furthermore , our proposed method brings 1.12 and 1.62 F 1 points improvement on Catalan and Spanish datasets compare to the DGLSTM - CRF ( Jie and Lu , 2019 ) .", "entities": [[23, 24, "MethodName", "CRF"]]}
{"text": "The DGLSTM - CRF employs 2 - layer dependency guided BiLSTM to capture grandchild dependencies , which leads to longer training time and more model parameters .", "entities": [[3, 4, "MethodName", "CRF"], [10, 11, "MethodName", "BiLSTM"]]}
{"text": "However , our Syn - LSTM - CRF is able to get better performance with fewer model parameters and shorter training time because of the fewer LSTM layers .", "entities": [[5, 6, "MethodName", "LSTM"], [7, 8, "MethodName", "CRF"], [26, 27, "MethodName", "LSTM"]]}
{"text": "Such results demonstrate that our proposed Syn - LSTM - CRF manages to capture structured information effectively .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"]]}
{"text": "Furthermore , with the contextualized word representation , the Syn - LSTM - CRF + BERT achieves much higher performance improvement than any other method .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "CRF"], [15, 16, "MethodName", "BERT"]]}
{"text": "Our model outperforms the strong baseline model DGLSTM - CRF + ELMO by 4.83 and 2.54 in terms of F 1 ( p < 10 \u22125 ) on Catalan and Spanish , respectively .", "entities": [[9, 10, "MethodName", "CRF"], [11, 12, "MethodName", "ELMO"]]}
{"text": "OntoNotes 5.0 English To understand the generalizability of our model , we evaluate the proposed Syn - LSTM - CRF model on large scale OntoNotes 5.0 datasets .", "entities": [[0, 2, "DatasetName", "OntoNotes 5.0"], [17, 18, "MethodName", "LSTM"], [19, 20, "MethodName", "CRF"], [24, 26, "DatasetName", "OntoNotes 5.0"]]}
{"text": "Our Syn - LSTM - CRF model outperforms all existing methods with 89.04 in terms of F 1 score ( p < 0.01 ) compared to DGLSTM - CRF , when GloVE word embeddings are used .", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"], [28, 29, "MethodName", "CRF"], [32, 34, "TaskName", "word embeddings"]]}
{"text": "Our model outperforms the BiLSTM - CRF model by 1.97 in F 1 , BiLSTM - GCN - CRF ( Jie and Lu , 2019 ) model by 0.86 .", "entities": [[4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "CRF"], [14, 15, "MethodName", "BiLSTM"], [16, 17, "MethodName", "GCN"], [18, 19, "MethodName", "CRF"]]}
{"text": "Note that our implemented GCN - BiLSTM - CRF outperforms the previous DGLSTM - CRF ( Jie and Lu , 2019 ) by 0.14 in F 1 .", "entities": [[4, 5, "MethodName", "GCN"], [6, 7, "MethodName", "BiLSTM"], [8, 9, "MethodName", "CRF"], [14, 15, "MethodName", "CRF"]]}
{"text": "Our Syn - LSTM - CRF further brings the improvement to 0.52 .", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "CRF"]]}
{"text": "Moreover , with the contextualized word representation BERT , our method achieves an F 1 score of 90.85 ( p < 10 \u22125 ) compared to DGLSTM - CRF + ELMO .", "entities": [[7, 8, "MethodName", "BERT"], [28, 29, "MethodName", "CRF"], [30, 31, "MethodName", "ELMO"]]}
{"text": "This shows that the proposed Syn - LSTM - CRF is able to extract more entities .", "entities": [[7, 8, "MethodName", "LSTM"], [9, 10, "MethodName", "CRF"]]}
{"text": "forms the baseline models , specifically by 2.04 in F 1 compared to BiLSTM - CRF , by 2.39 compared to BiLSTM - GCN - CRF , by 1.86 compared to GCN - BILSTM - CRF and by 1.11 ( p < 10 \u22125 ) compared to DGLSTM - CRF when FastText is used .", "entities": [[13, 14, "MethodName", "BiLSTM"], [15, 16, "MethodName", "CRF"], [21, 22, "MethodName", "BiLSTM"], [23, 24, "MethodName", "GCN"], [25, 26, "MethodName", "CRF"], [31, 32, "MethodName", "GCN"], [33, 34, "MethodName", "BILSTM"], [35, 36, "MethodName", "CRF"], [49, 50, "MethodName", "CRF"], [51, 52, "MethodName", "FastText"]]}
{"text": "Note that the baseline BiLSTM - GCN - CRF model is 0.35 points worse than BiLSTM - CRF .", "entities": [[4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "GCN"], [8, 9, "MethodName", "CRF"], [15, 16, "MethodName", "BiLSTM"], [17, 18, "MethodName", "CRF"]]}
{"text": "Such results further confirm the effectiveness of our proposed Syn - LSTM - CRF for incorporating structured information .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "CRF"]]}
{"text": "We find a similar behavior when the contextualized word representation BERT is used .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "Table 8 presents the comparisons between Syn - LSTM - CRF + BERT and DGLSTM - CRF + ELMO with given , predicted and random dependency trees .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [12, 13, "MethodName", "BERT"], [16, 17, "MethodName", "CRF"], [18, 19, "MethodName", "ELMO"]]}
{"text": "Our performance differences with the given parse trees are relatively smaller than the corresponding differences in DGLSTM - CRF + ELMO .", "entities": [[18, 19, "MethodName", "CRF"], [20, 21, "MethodName", "ELMO"]]}
{"text": "It is worthwhile to note that , with the predicted dependencies , our proposed Syn - LSTM - CRF + BERT is still able to outperform the strong baseline DGLSTM - CRF + ELMO even with the given parse trees on Catalan , English , and Chinese datasets .", "entities": [[16, 17, "MethodName", "LSTM"], [18, 19, "MethodName", "CRF"], [20, 21, "MethodName", "BERT"], [31, 32, "MethodName", "CRF"], [33, 34, "MethodName", "ELMO"]]}
{"text": "when 0 - 0 .4 0 .4 - 0 .5 0 .5 - 0 .6 0 .6 - 0 .7 0 .7 - 0 .8 0 .8 - 0 .9 0", "entities": [[1, 2, "DatasetName", "0"], [3, 4, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [20, 21, "DatasetName", "0"], [23, 24, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}
{"text": "We compare the performance of our Syn - LSTM - CRF + BERT with BiLSTM - CRF + BERT and DGLSTM - CRF + ELMO models with respect to sentence length , and the results are shown in Figure 5 .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "BiLSTM"], [16, 17, "MethodName", "CRF"], [18, 19, "MethodName", "BERT"], [22, 23, "MethodName", "CRF"], [24, 25, "MethodName", "ELMO"]]}
{"text": "We observe that the Syn - LSTM - CRF + BERT model consistently outperforms the two baseline models on the four languages 8 .", "entities": [[6, 7, "MethodName", "LSTM"], [8, 9, "MethodName", "CRF"], [10, 11, "MethodName", "BERT"]]}
{"text": "This confirms that the proposed Syn - LSTM - CRF + BERT is able to effectively incorporate structured information .", "entities": [[7, 8, "MethodName", "LSTM"], [9, 10, "MethodName", "CRF"], [11, 12, "MethodName", "BERT"]]}
{"text": "Note that our 2 - layer GCN is computed based on the dependency trees , which include both short - range dependencies and long - range dependencies .", "entities": [[6, 7, "MethodName", "GCN"]]}
{"text": "With the graph - encoded representation and the proposed Syn - LSTM - CRF + BERT , the individual word representation is enhanced by both contextual and structured information .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "CRF"], [15, 16, "MethodName", "BERT"]]}
{"text": "The significant performance improvements on the four datasets show the capability of our Syn - LSTM - CRF to capture the structured information despite the sentence length .", "entities": [[15, 16, "MethodName", "LSTM"], [17, 18, "MethodName", "CRF"]]}
{"text": "We conduct another evaluation on BiLSTM - CRF + BERT , DGLSTM - CRF + ELMO , and Syn - LSTM - CRF + BERT models with respect to entity length { 1 , 2 , 3 , 4 , 5 , \u2265 6 } on the four languages .", "entities": [[5, 6, "MethodName", "BiLSTM"], [7, 8, "MethodName", "CRF"], [9, 10, "MethodName", "BERT"], [13, 14, "MethodName", "CRF"], [15, 16, "MethodName", "ELMO"], [20, 21, "MethodName", "LSTM"], [22, 23, "MethodName", "CRF"], [24, 25, "MethodName", "BERT"]]}
{"text": "With the structured information , both DGLSTM - CRF + ELMO and Syn - LSTM - CRF + BERT achieve better performance compared to BiLSTM - CRF + BERT .", "entities": [[8, 9, "MethodName", "CRF"], [10, 11, "MethodName", "ELMO"], [14, 15, "MethodName", "LSTM"], [16, 17, "MethodName", "CRF"], [18, 19, "MethodName", "BERT"], [24, 25, "MethodName", "BiLSTM"], [26, 27, "MethodName", "CRF"], [28, 29, "MethodName", "BERT"]]}
{"text": "When the length of entity is \u2264 3 , Syn - LSTM - CRF + BERT achieves better results compared to DGLSTM - CRF + ELMO .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "CRF"], [15, 16, "MethodName", "BERT"], [23, 24, "MethodName", "CRF"], [25, 26, "MethodName", "ELMO"]]}
{"text": "Our model consistently outperforms BiLSTM - CRF + BERT , and the performance tends to have more improvements when entities are getting longer except on the Chinese dataset .", "entities": [[4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "CRF"], [8, 9, "MethodName", "BERT"]]}
{"text": "As mentioned by Jie and Lu ( 2019 ) , the percentage of entities that are able to perfectly form a sub - tree is only 92.9 % for OntoNotes Chinese , as compared to 98.5 % , 100 % , 100 % for OntoNotes English , SemEval Catalan and Spanish .", "entities": [[29, 30, "DatasetName", "OntoNotes"], [44, 45, "DatasetName", "OntoNotes"]]}
{"text": "To fully explore the impact of the number of GCN layers , we conduct another experiment on Syn - LSTM - CRF + BERT model with the number of GCN layers { 1 , 2 , 3 } , and Figure 6 shows the performance on the dev set of the four languages .", "entities": [[9, 10, "MethodName", "GCN"], [19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"], [23, 24, "MethodName", "BERT"], [29, 30, "MethodName", "GCN"]]}
{"text": "We observe that the overall performance is better when the number of GCN layers equals 2 .", "entities": [[12, 13, "MethodName", "GCN"]]}
{"text": "Note that similar behavior can also be found in the work by Kipf and Welling ( 2017 ) for document classification and node classification .", "entities": [[19, 21, "TaskName", "document classification"], [22, 24, "TaskName", "node classification"]]}
{"text": "Therefore , we evaluate our proposed Syn - LSTM - CRF model with 2 - layer GCN .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [16, 17, "MethodName", "GCN"]]}
{"text": "To understand the contribution of each component , we conduct an ablation study on the OntoNotes 5.0 English dataset , and Table 7 presents the detailed results of our model with contextualized representation .", "entities": [[15, 17, "DatasetName", "OntoNotes 5.0"]]}
{"text": "We find that the performance drops by 0.24 F 1 score when we only use 1 - layer GCN .", "entities": [[18, 19, "MethodName", "GCN"]]}
{"text": "Without GCN at all , the score drops by 1.13 F 1 .", "entities": [[1, 2, "MethodName", "GCN"]]}
{"text": "LSTM LSTM has demonstrated its great effectiveness in many NLP tasks and becomes a standard module for many state - of - the - art models ( Wen et al , 2015 ; Ma and Hovy , 2016 ; Dozat and Manning , 2017 ) .", "entities": [[0, 1, "MethodName", "LSTM"], [1, 2, "MethodName", "LSTM"]]}
{"text": "However , the sequential nature of the LSTM makes it challenging to capture long - range dependencies .", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "Zhang et al ( 2018a ) propose the S - LSTM model to include a sentence state to allow both local and global information exchange simultaneously .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "Mogrifier LSTM ( Melis et al , 2020 ) mutually gates the current input and the previous output to enhance the interaction between the input and the context .", "entities": [[0, 2, "MethodName", "Mogrifier LSTM"]]}
{"text": "These two works do not consider structured information for the LSTM design .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "Since natural language is usually structured , Shen et al ( 2018 ) propose ON - LSTM to add a hierarchical bias to allow the neurons to be updated by following certain order .", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "While the ON - LSTM is learning the latent constituency parse trees , we focus on incorporating the explicit structured information conveyed by the dependency parse trees .", "entities": [[4, 5, "MethodName", "LSTM"]]}
{"text": "NER Early work ( Sasano and Kurohashi , 2008 ) uses syntactic dependency features to improve the SVM performance on Japanese NER task .", "entities": [[0, 1, "TaskName", "NER"], [17, 18, "MethodName", "SVM"], [21, 22, "TaskName", "NER"]]}
{"text": "The pre - trained language models ( e.g. , BERT ( Devlin et al , 2019 ) , ELMO ( Peters et al , 2018 ) ) further improve neuralbased approaches with a good contextualized representation .", "entities": [[9, 10, "MethodName", "BERT"], [18, 19, "MethodName", "ELMO"]]}
{"text": "In language model BERT ( Devlin et al , 2019 ) for the four datasets .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "Following Luo et al ( 2020 ) , we select the 18 th layer of the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data .", "entities": [[19, 20, "MethodName", "BERT"], [27, 29, "DatasetName", "OntoNotes 5.0"]]}
{"text": "We use the the 9 th layer of cased version of BERT base model for the experiments on the rest three datasets .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "For the character embedding , we randomly initialize the character embeddings and set the dimension as 30 , and set the hidden size of character - level BiLSTM as 50 .", "entities": [[27, 28, "MethodName", "BiLSTM"]]}
{"text": "The hidden size of GCN and Syn - LSTM is set as 200 .", "entities": [[4, 5, "MethodName", "GCN"], [8, 9, "MethodName", "LSTM"]]}
{"text": "Note that we only use one layer of bi - directional Syn - LSTM for our experiments .", "entities": [[13, 14, "MethodName", "LSTM"]]}
{"text": "Dropout is set to 0.5 for input embeddings and hidden states .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "Table 9 shows the statistics of the number of entities with respect to entity length for OntoNotes 5.0 English and Chinese , SemEval 2010 Task 1 Catalan and Spanish datasets .", "entities": [[16, 18, "DatasetName", "OntoNotes 5.0"]]}
{"text": "Figure 7 shows the comparisons of the models of using random trees and given trees on OntoNotes 5.0 English and Chinese datasets .", "entities": [[16, 18, "DatasetName", "OntoNotes 5.0"]]}
{"text": "We compare the performance of our Syn - LSTM - CRF + BERT with BiLSTM - CRF + BERT and DGLSTM - CRF + ELMO models with respect to sentence length , and the results are shown in Figure 8 .", "entities": [[8, 9, "MethodName", "LSTM"], [10, 11, "MethodName", "CRF"], [12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "BiLSTM"], [16, 17, "MethodName", "CRF"], [18, 19, "MethodName", "BERT"], [22, 23, "MethodName", "CRF"], [24, 25, "MethodName", "ELMO"]]}
{"text": "The example is selected from OntoNotes 5.0 English dataset .", "entities": [[5, 7, "DatasetName", "OntoNotes 5.0"]]}
{"text": "Even though the DGLSTM - CRF ( Jie and Lu , 2019 ) model is able to recognize \" Tianshui \" as a named entity , it predicts a wrong entity type as PERSON while the true type is GPE .", "entities": [[5, 6, "MethodName", "CRF"]]}
{"text": "The average run time for Syn - LSTM is 52 sec / epoch , 55 sec / epoch , 290 sec / epoch , 350 sec / epoch for Catalan , Spanish , Chinese and English datasets respectively .", "entities": [[7, 8, "MethodName", "LSTM"]]}
{"text": "For hyper - parameter , we use the FastText ( Grave et al , 2018 ) 300 dimensional embeddings to initialize the word embeddings for Catalan , Spanish , and Chinese .", "entities": [[8, 9, "MethodName", "FastText"], [22, 24, "TaskName", "word embeddings"]]}
{"text": "For OntoNotes 5.0 English , we adopt the publicly available GloVE ( Pennington et al , 2014 ) 100 dimensional embeddings to initialize the word embeddings .", "entities": [[1, 3, "DatasetName", "OntoNotes 5.0"], [24, 26, "TaskName", "word embeddings"]]}
{"text": "With the non - local information from the graph - encoded representation , our Syn - LSTM - CRF successfully predicts the right entity type .", "entities": [[16, 17, "MethodName", "LSTM"], [18, 19, "MethodName", "CRF"]]}
{"text": "In recent years , we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks , such as similarity , entailment and sentiment analysis .", "entities": [[33, 35, "TaskName", "sentiment analysis"]]}
{"text": "Here we introduce a new task : understanding of mental health concepts derived from Cognitive Behavioural Therapy ( CBT ) .", "entities": [[18, 19, "DatasetName", "CBT"]]}
{"text": "We define a mental health ontology based on the CBT principles , annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations .", "entities": [[5, 6, "MethodName", "ontology"], [9, 10, "DatasetName", "CBT"]]}
{"text": "Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non - deep - learning models in this difficult task .", "entities": [[12, 14, "TaskName", "word embeddings"], [15, 17, "TaskName", "sentence embeddings"]]}
{"text": "In this paper we introduce a new task : understanding of mental health concepts derived from Cognitive Behavioural Therapy ( CBT ) .", "entities": [[20, 21, "DatasetName", "CBT"]]}
{"text": "We present an ontology that is formulated according to Cognitive Behavioural Therapy principles .", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "We then investigate two approaches for classifying the user input according to the defined ontology .", "entities": [[14, 15, "MethodName", "ontology"]]}
{"text": "The second involves a gated recurrent network ( GRU ) operating over distributed representation of sentences .", "entities": [[8, 9, "MethodName", "GRU"]]}
{"text": "In Section 2 we give a brief background of the statistical approach to dialogue modelling , focusing on dialogue ontology and natural language understanding .", "entities": [[19, 20, "MethodName", "ontology"], [21, 24, "TaskName", "natural language understanding"]]}
{"text": "The sections that follow represent the main contribution of this work : a CBT ontology in Section 4 , a labelled dataset in Section 5 , and models for language understanding in Section 6 .", "entities": [[13, 14, "DatasetName", "CBT"], [14, 15, "MethodName", "ontology"]]}
{"text": "A structured representation of the database , the ontology is a central element of a dialogue system .", "entities": [[8, 9, "MethodName", "ontology"]]}
{"text": "Another critical component is the natural language understanding unit , which takes textual user input and detects presence of the ontology concepts in the text .", "entities": [[5, 8, "TaskName", "natural language understanding"], [20, 21, "MethodName", "ontology"]]}
{"text": "There has been a significant amount of work in spoken language understanding focused on exploiting large knowledge graphs in order to improve coverage ( T\u00fcr et al , 2012 ; Heck et al , 2013 ) .", "entities": [[9, 12, "TaskName", "spoken language understanding"], [16, 18, "TaskName", "knowledge graphs"]]}
{"text": "Available medical ontologies follow a symptom - treatment categorisation and are not suitable for dialogue or natural language understanding ( Bluhm , 2017 ; Hofmann , 2014 ; Wang et al , 2018 ) .", "entities": [[16, 19, "TaskName", "natural language understanding"]]}
{"text": "Within a dialogue system , a natural language understanding unit extracts meaning from user sentences .", "entities": [[6, 9, "TaskName", "natural language understanding"]]}
{"text": "Deep learning architectures that exploit distributed word - vector representations have been successfully applied to different tasks in natural language understanding , such as semantic role labelling , semantic parsing , spoken language un - derstanding , sentiment analysis or dialogue belief tracking ( Collobert et al , 2011 ; Kim , 2014 ; Kalchbrenner et al , 2014 ; Le and Mikolov , 2014a ; Rojas Barahona et", "entities": [[18, 21, "TaskName", "natural language understanding"], [28, 30, "TaskName", "semantic parsing"], [37, 39, "TaskName", "sentiment analysis"]]}
{"text": "There is no language understanding : the agent simply asks questions and the user selects answers from a given list .", "entities": [[7, 8, "DatasetName", "agent"]]}
{"text": "The agent is however able to interpret hand gestures , posture shifts , and facial expressions .", "entities": [[1, 2, "DatasetName", "agent"]]}
{"text": "The measurements contribute to the indicator analysis of affect , gesture , emotion and engagement .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "The Stanford Woebot chat - bot proposed by ( Fitzpatrick et al , 2017 ) is designed for delivering CBT to young adults with depression and anxiety .", "entities": [[19, 20, "DatasetName", "CBT"]]}
{"text": "It has been shown that the interaction with this chat - bot can significantly reduce the symptoms of depression when compared to a group of people directed to a read a CBT manual .", "entities": [[31, 32, "DatasetName", "CBT"]]}
{"text": "The conversational agent appears to be effective in engaging the users .", "entities": [[2, 3, "DatasetName", "agent"]]}
{"text": "Still , one of the main deficiencies reported by the trial participants in ( Fitzpatrick et al , 2017 ) was the inability to converse naturally .", "entities": [[24, 25, "DatasetName", "converse"]]}
{"text": "Here we address this problem by performing statistical natural language understanding .", "entities": [[8, 11, "TaskName", "natural language understanding"]]}
{"text": "To define the ontology we draw from principles of Cognitive Behavioural Therapy ( CBT ) .", "entities": [[3, 4, "MethodName", "ontology"], [13, 14, "DatasetName", "CBT"]]}
{"text": "There is evidence that CBT is more effective than other forms of psychotherapy ( Tolin , 2010 ) .", "entities": [[4, 5, "DatasetName", "CBT"]]}
{"text": "Unlike other , longer - term , forms of therapy such as psychoanalysis , CBT can have a positive effect on the client within a few sessions .", "entities": [[14, 15, "DatasetName", "CBT"]]}
{"text": "This is why we adopted CBT as the basis of our work .", "entities": [[5, 6, "DatasetName", "CBT"]]}
{"text": "A major underlying principle of CBT is the idea of cognitive distortions , and the value in challenging them .", "entities": [[5, 6, "DatasetName", "CBT"]]}
{"text": "In CBT , clients are helped to test their assumptions and views of the world in order to check if they fit with reality .", "entities": [[1, 2, "DatasetName", "CBT"]]}
{"text": "Within the realm of cognitive distortion , CBT identifies a number of specific self - defeating thought processes , or thinking errors .", "entities": [[7, 8, "DatasetName", "CBT"]]}
{"text": "A strong component of CBT is teaching clients to be able to recognize and identify the thinking errors themselves , and ultimately discard the negative thought processes and 're - think ' their problems .", "entities": [[4, 5, "DatasetName", "CBT"]]}
{"text": "We consider the main analytical step in this therapy : an adequate decoding of these ' thinking error ' concepts , and the identification of the key emotion ( s ) and the situational context of a particular problem .", "entities": [[27, 28, "DatasetName", "emotion"]]}
{"text": "Therefore , our ontology consists of think - ing errors , emotions , and situations .", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "Notwithstanding slight variations in number and terminology , the list of thinking errors is fairly well standardised in the CBT literature .", "entities": [[19, 20, "DatasetName", "CBT"]]}
{"text": "In CBT , emotions tend to be divided into positive and negative , or helpful / healthy and unhelpful/ unhealthy emotions ( Branch and Willson , 2010 ) .", "entities": [[1, 2, "DatasetName", "CBT"]]}
{"text": "Although we initally agreed to focus on ' unhealthy ' emotions , as defined by CBT , there seemed also to be a place for the ' healthy ' emotion Grief / sadness .", "entities": [[15, 16, "DatasetName", "CBT"], [29, 30, "DatasetName", "emotion"]]}
{"text": "Overall , the list of emotions used was drawn from a number of sources , including CBT literature , the annotators ' own knowledge of what they work with in psychological therapy , and the common emotions that were seen emerging from the data early on in the process .", "entities": [[16, 17, "DatasetName", "CBT"]]}
{"text": "Note that more than one emotion might be expressed within an individual problem - for example Depression and Loneliness .", "entities": [[5, 6, "DatasetName", "emotion"]]}
{"text": "Initially , any first - time Koko user would be given a short introductory tutorial in the art of 're - thinking'/'re - framing ' problems ( based on CBT principles ) , before being able to use the platform .", "entities": [[29, 30, "DatasetName", "CBT"]]}
{"text": "This however changed over time , as the age of the users decreased , and a different tutorial , emphasizing empathy and optimism , was used ( less CBT - based than the 're - thinking ' ) .", "entities": [[28, 29, "DatasetName", "CBT"]]}
{"text": "These were used to define the ontology .", "entities": [[6, 7, "MethodName", "ontology"]]}
{"text": "The inter - annotator agreement was calculated using a contingency table for thinking error , emotion and situation , showing agreement and disagreement between the two annotators .", "entities": [[15, 16, "DatasetName", "emotion"]]}
{"text": "The task of decoding thinking errors and emotions is closely related to the task of sentiment analysis .", "entities": [[15, 17, "TaskName", "sentiment analysis"]]}
{"text": "In sentiment analysis we are concerned with positive or negative sentiment expressed in a sentence .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}
{"text": "We utilise GloVe ( Pennington et al , 2014 ) word vectors , which have previously achieved competitive results in a similarity task .", "entities": [[2, 3, "MethodName", "GloVe"]]}
{"text": "An approach that represents vectors by weighted averages of word vectors and then modifies them using PCA and SVD outperforms skipthought vectors ( Arora et al , 2017 ) .", "entities": [[16, 17, "MethodName", "PCA"], [18, 19, "DatasetName", "SVD"]]}
{"text": "This method however does not do well on a sentiment analysis task due to down - weighting of words like \" not \" .", "entities": [[9, 11, "TaskName", "sentiment analysis"]]}
{"text": "We then train a gated recurrent unit ( GRU ) network using the skip - thoughts as input .", "entities": [[4, 7, "MethodName", "gated recurrent unit"], [8, 9, "MethodName", "GRU"]]}
{"text": "The convolutional neural network ( CNN ) used in this work is inspired by ( Kim , 2014 ) and operates over pre - trained GloVe embeddings of dimensionality d.", "entities": [[25, 27, "MethodName", "GloVe embeddings"]]}
{"text": "We use the gated recurrent unit ( GRU ) model to process skip - thought sentence vectors , for two reasons .", "entities": [[3, 6, "MethodName", "gated recurrent unit"], [7, 8, "MethodName", "GRU"]]}
{"text": "Second , since our corpus only comprises very limited labelled data , a GRU should perform better than a long short - term memory ( LSTM ) network as it has less parameters .", "entities": [[13, 14, "MethodName", "GRU"], [19, 24, "MethodName", "long short - term memory"], [25, 26, "MethodName", "LSTM"]]}
{"text": "First , we use an already trained GRU to extract skip - thought embeddings e t from the sentences s t .", "entities": [[7, 8, "MethodName", "GRU"]]}
{"text": "Then , taking the sequence of sentence vectors { e 1 , e 2 , ... , e t , ... } as input , another GRU is used as follows : z t = \u03c3 ( W z h", "entities": [[26, 27, "MethodName", "GRU"]]}
{"text": "We first train 100 and 300 dimensions for both GloVe embeddings and skip - thought embeddings using the same mechanism as in ( Pennington et al , 2014 ; Kiros et al , 2015 ) .", "entities": [[9, 11, "MethodName", "GloVe embeddings"]]}
{"text": "We do not treat the problem separately from the negative take as the GRU will anyway put more importance on the information that comes last .", "entities": [[13, 14, "MethodName", "GRU"]]}
{"text": "We split the labelled data in a 8 : 1 : 1 ratio for training , validation and testing in a 10 - fold cross validation for both GRU and CNN training .", "entities": [[28, 29, "MethodName", "GRU"]]}
{"text": "The non - recurrent weights with a truncated normal distribution ( 0 , 0.01 ) , and the recurrent weights with orthogonal initialisation ( Saxe et al , 2013 ) .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "Both models were trained with Adam algorithm and implemented in Tensorflow ( Girija , 2016 ) .", "entities": [[5, 6, "MethodName", "Adam"]]}
{"text": "In addition , we trained two non - deep - learning models , the logistic regression ( LR ) model and the Support Vector Machine ( SVM ) .", "entities": [[14, 16, "MethodName", "logistic regression"], [22, 25, "MethodName", "Support Vector Machine"], [26, 27, "MethodName", "SVM"]]}
{"text": "It shows that GloVe word vectors with CNN achieves the best performance both in 100 and 300 dimensions .", "entities": [[3, 4, "MethodName", "GloVe"]]}
{"text": "The results show that both models outperform SVM - BOW in larger embedding dimensions .", "entities": [[7, 8, "MethodName", "SVM"]]}
{"text": "We also find that CNN - GloVe on average works better than GRU - Skip - thought , which is expected as the space of words is smaller in comparison to the space of sentences so the word vectors can be more accurately trained .", "entities": [[6, 7, "MethodName", "GloVe"], [12, 13, "MethodName", "GRU"]]}
{"text": "While the CNN operating on 100 dimensional word vectors is comparable to the CNN operating on 300 dimensional word vectors , the GRU - Skip - thought tends to be worse on 100 dimensional skip - thoughts , suggesting that sentence vectors generally need to be of a higher dimension to represent the meaning more accurately than word vectors .", "entities": [[22, 23, "MethodName", "GRU"]]}
{"text": "Table 7 shows a more detailed analysis of the 300 dimensional CNN - GloVe performance , where both precision and recall are presented , indicating that oversampling mechanism can help overcome the data bias problem .", "entities": [[13, 14, "MethodName", "GloVe"]]}
{"text": "While oversampling is essential for both models , GRU - Skip - thought is less sensitive to lower oversampling ratios , suggesting that skip - thoughts can already capture sentiment on the sentence level .", "entities": [[8, 9, "MethodName", "GRU"]]}
{"text": "We presented an ontology based on the principles of Cognitive Behavioural Therapy .", "entities": [[3, 4, "MethodName", "ontology"]]}
{"text": "We trained GloVe word embeddings and skip - thought embeddings on 500 K posts in an unsupervised fashion and generated distributed representations both of words and of sentences .", "entities": [[2, 3, "MethodName", "GloVe"], [3, 5, "TaskName", "word embeddings"]]}
{"text": "We then used the GloVe word vectors as input to a CNN and the skip - thought sentence vectors as input to a GRU .", "entities": [[4, 5, "MethodName", "GloVe"], [23, 24, "MethodName", "GRU"]]}
{"text": "The results suggest that both models significantly outperform a chance classifier for all thinking errors , emotions and situations with CNN - GloVe on average achieving better results .", "entities": [[22, 23, "MethodName", "GloVe"]]}
{"text": "We also plan to extend the current ontology with its focus on thinking errors , emotions and situations to include a much lager number of concepts .", "entities": [[7, 8, "MethodName", "ontology"]]}
{"text": "A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings", "entities": [[9, 11, "TaskName", "Word Embeddings"]]}
{"text": "Our metric ( Relative Negative Sentiment Bias , RNSB ) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups .", "entities": [[13, 15, "TaskName", "word embeddings"]]}
{"text": "We show that our framework and metric enable useful analysis into the bias in word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "Word embeddings have established themselves as an integral part of Natural Language Processing ( NLP ) applications .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "Unfortunately word embeddings have also introduced unintended biases that could cause downstream NLP systems to be unfair .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "Recent studies have shown that word embeddings exhibit unintended gender and stereotype biases inherent in the training corpus .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "In Figure 1 : 2 - D PCA embeddings for positive / negative sentiment words and a set of national origin identity terms .", "entities": [[7, 8, "MethodName", "PCA"]]}
{"text": "our work , we restrict our definition of bias to unequal distributions of negative sentiment among demographic identity terms in word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "Sentiment analysis makes up a large portion of current NLP systems .", "entities": [[0, 2, "TaskName", "Sentiment analysis"]]}
{"text": "Therefore , preventing negative sentiment from mixing with sensitive attributes ( i.e. race , gender , religion ) in word embeddings is needed to prevent discrimination in ML models using the embeddings .", "entities": [[19, 21, "TaskName", "word embeddings"]]}
{"text": "As studied in ( Packer et al , 2018 ) , unintentionally biased word embeddings can have adverse consequences when deployed in applications , such as movie sentiment analyzers or messaging apps .", "entities": [[13, 15, "TaskName", "word embeddings"]]}
{"text": "Negative sentiment can be unfairly entangled in the word embeddings , and detecting this unintended bias is a difficult problem .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "To demonstrate this need for clear signals of bias in word embeddings , we look at Figure 1 .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "Our framework enables transparent insights into word embedding bias by instead viewing the output of a simple logistic regression algorithm trained on an unbiased positive / negative word sentiment dataset initialized with biased word vectors .", "entities": [[17, 19, "MethodName", "logistic regression"]]}
{"text": "We use this framework to create a clear metric for unintended demographic bias in word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "More specifically , word embeddings has been an area of focus for evaluating unintended bias .", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "We present our framework for understanding and evaluating unintentional demographic bias in word embeddings .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "Figure 2 : We isolate unintended bias to the word embeddings by training a logistic regression classifier on a unbiased positive / negative word sentiment dataset ( initialized with the biased word embeddings ) .", "entities": [[9, 11, "TaskName", "word embeddings"], [14, 16, "MethodName", "logistic regression"], [31, 33, "TaskName", "word embeddings"]]}
{"text": "Our framework enables the evaluation of unintended bias in word embeddings through the results of negative sentiment predictions .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "Neutral identity terms that are unfairly entangled with negative sentiment in the word embeddings will be classified like their neighboring sentiment words from the sentiment dataset .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "We evaluate three pretrained embedding models : GloVe ( Pennington et al , 2014 ) , Word2vec ( Mikolov et al , 2013 ) ( trained on the large Google News corpus ) , and ConceptNet .", "entities": [[7, 8, "MethodName", "GloVe"], [29, 30, "DatasetName", "Google"], [35, 36, "DatasetName", "ConceptNet"]]}
{"text": "GloVe and Word2vec embeddings have been shown to contain unintended bias in ( Bolukbasi et al , 2016a ; Caliskan - Islam et al , 2016 ) .", "entities": [[0, 1, "MethodName", "GloVe"]]}
{"text": "ConceptNet has been shown to be less biased than these models ( Speer , 2017 ) due to the mixture of curated corpora used for training .", "entities": [[0, 1, "DatasetName", "ConceptNet"]]}
{"text": "We trust these labels to be unbiased so that we may isolate the unintended biases entering our system to the word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "Finally , we use a simple logistic regression algorithm to predict negative sentiment .", "entities": [[6, 8, "MethodName", "logistic regression"]]}
{"text": "Although the choice of ML model can have an impact on fairness for sentiment applications as shown in ( Kiritchenko and Mohammad , 2018 ) , we choose a simple ML model to limit the possible unintended biases introduced downstream from our word embeddings .", "entities": [[42, 44, "TaskName", "word embeddings"]]}
{"text": "= \u03c3 ( w T x i ) , for the logistic loss , l , and learned weights , w. min w R d n", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "+ \u03bb w 2 , \u03bb > 0 Then for a set", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "This type of discrimination can show up in many downstream sentiment analysis applications .", "entities": [[10, 12, "TaskName", "sentiment analysis"]]}
{"text": "First , we compare the RNSB metric for 3 pretrained word embeddings , showing that our metric is consistent with other word embedding analysis like WEAT ( Caliskan - Islam et al , 2016 ) .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "We vary the word embeddings used in our framework and calculate the RNSB metric for each embedding .", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "For both case studies , the bias is largest in GloVe , as shown by the largest RNSB metric .", "entities": [[10, 11, "MethodName", "GloVe"]]}
{"text": "As mentioned earlier , ConceptNet is a state of the art model that mixes models like GloVe and Word2vec , creating fairer word embeddings .", "entities": [[4, 5, "DatasetName", "ConceptNet"], [16, 17, "MethodName", "GloVe"], [22, 24, "TaskName", "word embeddings"]]}
{"text": "Through the RNSB metric , one can see that the unintended demographic bias of these word embeddings is an order of magnitude lower than GloVe or Word2vec .", "entities": [[15, 17, "TaskName", "word embeddings"], [24, 25, "MethodName", "GloVe"]]}
{"text": "The WEAT score shows that word embeddings like Word2vec and GloVe are biased with respect to national origin because European - American names are more correlated with positive sentiment than African - American names .", "entities": [[5, 7, "TaskName", "word embeddings"], [10, 11, "MethodName", "GloVe"]]}
{"text": "The left histogram is computed using the GloVe word embeddings , and the right histogram is computed using the fairer Concept - Net embeddings .", "entities": [[7, 8, "MethodName", "GloVe"], [8, 10, "TaskName", "word embeddings"]]}
{"text": "The ConceptNet distribution seems to equalize much of this disparity .", "entities": [[1, 2, "DatasetName", "ConceptNet"]]}
{"text": "We compute this matrix for the same two cases : GloVe word embeddings ( top ) and ConceptNet word embeddings ( bottom ) shown in Figure 4 .", "entities": [[10, 11, "MethodName", "GloVe"], [11, 13, "TaskName", "word embeddings"], [17, 18, "DatasetName", "ConceptNet"], [18, 20, "TaskName", "word embeddings"]]}
{"text": "The GloVe word embedding correlation matrix contains a lot of dark low correlations between identities , as a lot of identities contain small amounts of negative sentiment .", "entities": [[1, 2, "MethodName", "GloVe"]]}
{"text": "For the ConceptNet word embeddings , we see a much more colorful heat map , indicating there are higher correlations between more identity terms .", "entities": [[2, 3, "DatasetName", "ConceptNet"], [3, 5, "TaskName", "word embeddings"]]}
{"text": "This hints that ConceptNet contains less targeted discrimination via negative sentiment .", "entities": [[3, 4, "DatasetName", "ConceptNet"]]}
{"text": "We used our framework to evaluate unintended bias with respect to sentiment , but there exists many other types of unintended demographic bias to create clear signals for in word embeddings .", "entities": [[29, 31, "TaskName", "word embeddings"]]}
{"text": "We presented a transparent framework for evaluating unintended demographic bias in word embeddings .", "entities": [[11, 13, "TaskName", "word embeddings"]]}
{"text": "In our framework , we train a classifier on an unbiased positive / negative word sentiment dataset initialized with biased word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "This way , we can observe the unfairness in the word embeddings at the ML prediction level .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "Previous metrics and analysis into unintended bias in word embeddings rely on vector space arguments for only two demographics at a time , which does not lend itself well to evaluating real world discrimination .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "Finally , our framework and metric reveal transparent analysis of the unintended bias hidden in word embeddings .", "entities": [[15, 17, "TaskName", "word embeddings"]]}
{"text": "Recent models tackling consistency often train with additional Natural Language Inference ( NLI ) labels or attach trained extra modules to the generative agent for maintaining consistency .", "entities": [[8, 11, "TaskName", "Natural Language Inference"], [23, 24, "DatasetName", "agent"]]}
{"text": "Inspired by social cognition and pragmatics , we endow existing dialogue agents with public self - consciousness on the fly through an imaginary listener .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "Literal Agent : !", "entities": [[1, 2, "DatasetName", "Agent"]]}
{"text": "Interlocutor Self - Conscious Agent : \"", "entities": [[4, 5, "DatasetName", "Agent"]]}
{"text": "While a literal dialogue agent ( S 0 ) fails to deliver a consistent persona , our self - conscious agent ( S 1 ) does so , by modeling an imaginary listener .", "entities": [[4, 5, "DatasetName", "agent"], [7, 8, "DatasetName", "0"], [20, 21, "DatasetName", "agent"]]}
{"text": "Next , methods with NLI models for rating the agent 's consistency also need to train them separately with those labels .", "entities": [[9, 10, "DatasetName", "agent"]]}
{"text": "Inspired by this , our intuition is that self - consciousness through an imaginary listener will let dialogue agents better maintain consistency .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "As the agent tries to generate tokens that help the imaginary listener identify the agent 's persona , it can lastly generate more consistent utterances .", "entities": [[2, 3, "DatasetName", "agent"], [14, 15, "DatasetName", "agent"]]}
{"text": "Along with large reduction in contradiction , the utterance accuracy significantly increases too .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "Madotto et al ( 2019 ) use meta - learning to adapt to new personas with few dialogue samples .", "entities": [[7, 10, "TaskName", "meta - learning"]]}
{"text": "The NLI approach is applied for coherence evaluation ( Dziri et al , 2019 ) , rewards to reinforcement learning agents ( Song et al , 2019 ) , finding inconsistent words ( Song et al , 2020 ) , and unlikelihood training ( Li et al , 2020 ) .", "entities": [[6, 8, "TaskName", "coherence evaluation"]]}
{"text": "Our approach belongs to the general family of Bayesian Rational Speech Acts ( Andreas and Klein , 2016 ) , image captioning ( Mao et al , 2016 ;", "entities": [[20, 22, "TaskName", "image captioning"]]}
{"text": "Vedantam et al , 2017 ; Cohn - Gordon et al , 2018 ) , instruction following ( Fried et al , 2017 ) , navigating ( Fried et al , 2018 ) , translation ( Cohn - Gordon and Goodman , 2019 ) , summarization ( Shen et al , 2019 ) and referring expression generation ( Zarrie\u00df and Schlangen , 2019 ) .", "entities": [[45, 46, "TaskName", "summarization"], [54, 56, "TaskName", "referring expression"]]}
{"text": "The models rank the candidates by perplexity scores .", "entities": [[6, 7, "MetricName", "perplexity"]]}
{"text": "Table 1 reports language metrics between the selected candidates and the given persona sentences using SPICE ( Anderson et al , 2016 ) and ROUGE ( Lin , 2004 ) .", "entities": [[15, 16, "DatasetName", "SPICE"]]}
{"text": "SPICE metric measures semantic similarity and ROUGE metric measures n - gram overlaps between two sentences .", "entities": [[0, 1, "DatasetName", "SPICE"], [3, 5, "TaskName", "semantic similarity"]]}
{"text": "Contradict@1 - Utt shows lower SPICE scores and higher ROUGE scores than other utterances , implying that it may be different in semantics but similar in syntax to the given persona .", "entities": [[5, 6, "DatasetName", "SPICE"]]}
{"text": "To take a closer look , we extract the contradicting words from Contradict@1 - Utt and their counterparts from GT utterances to compare their average perplexity scores .", "entities": [[25, 26, "MetricName", "perplexity"]]}
{"text": "If properly conditioned with the given persona , models should show lower perplexity for the words in the persona .", "entities": [[12, 13, "MetricName", "perplexity"]]}
{"text": "However , their perplexity scores are significantly higher than those of contradictory words .", "entities": [[3, 4, "MetricName", "perplexity"]]}
{"text": "Since the imaginary listener arises from the plain dialogue - agent , separate training is not needed .", "entities": [[10, 11, "DatasetName", "agent"]]}
{"text": "We seek to build a dialogue agent who is selfconscious about its consistency without the need for training on NLI labels or rating consistency with NLI models .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "To apply the framework to sequence generation for dialogues , we extend the incremental approach proposed for image captioning ( Cohn - Gordon et al , 2018 ) .", "entities": [[17, 19, "TaskName", "image captioning"]]}
{"text": "To generate an utterance , the agent computes the distribution of every next token u t at timestep t in Bayesian fashion as follows .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "Base Speaker S 0 .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "Base Speaker : # \" \" , \u210e , & \" ) Figure 3 : The proposed self - conscious agent S 1 consists of base speaker S 0 and imaginary listener L 0 .", "entities": [[20, 21, "DatasetName", "agent"], [28, 29, "DatasetName", "0"], [33, 34, "DatasetName", "0"]]}
{"text": "The base speaker S t 0 returns a distribution over the next token at timestep t : S t 0", "entities": [[5, 6, "DatasetName", "0"], [19, 20, "DatasetName", "0"]]}
{"text": "Any conditional dialogue agent can be used as a base speaker .", "entities": [[3, 4, "DatasetName", "agent"]]}
{"text": "Imaginary Listener L 0 .", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "The imaginary listener L t 0 is the posterior distribution of the speaker 's persona in terms of the base speaker and the world prior p t ( i ) over personas as follows , L t 0", "entities": [[5, 6, "DatasetName", "0"], [37, 38, "DatasetName", "0"]]}
{"text": "( i | h , u \u2264t , p t ) \u221d S t 0", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "( u t | i , h , u < t ) \u03b2 \u00d7 p t ( i )", "entities": [[12, 13, "HyperparameterName", "\u03b2"]]}
{"text": "i I S t 0", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "( u t | i , h , u < t ) \u03b2 \u00d7 p t ( i ) .", "entities": [[12, 13, "HyperparameterName", "\u03b2"]]}
{"text": "L 0 returns a probability distribution over the personas in world I , which is a finite set ( | I | = 3 ) comprising the given persona i and distractor personas .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "With S t 0 and L t 0 , the self - conscious speaker S t 1 is defined as S t 1 ( u t | i , h , u < t )", "entities": [[3, 4, "DatasetName", "0"], [7, 8, "DatasetName", "0"]]}
{"text": "\u221d L t 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "Especially , the agent seeks to be perceived as the given persona", "entities": [[3, 4, "DatasetName", "agent"]]}
{"text": "Updating the world prior with L 0 .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "Starting from a uniform distribution as the initial prior p 0", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "= L t 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "( 3 ) Hence , p t ( i ) represents the cumulative state of the partial utterance up to t. Cohn - Gordon et al ( 2018 ) report the prior update with L 1 \u221d S t 0", "entities": [[39, 40, "DatasetName", "0"]]}
{"text": "( u t | i , h , u < t ) \u00d7 L t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "We thus propose to learn distractor selection , especially based on the life - long memory network ( Kaiser et al , 2017 ) .", "entities": [[15, 17, "MethodName", "memory network"]]}
{"text": "The life - long memory network is capable of implicitly clustering similar dialogue contexts into a few slots with associated persona .", "entities": [[4, 6, "MethodName", "memory network"]]}
{"text": "In Appendix , we experiment that our approach outperforms other models including BERT - based algorithms .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "We select the persona that have the best Hits@1 as the distractor label per training dialogue .", "entities": [[8, 9, "MetricName", "Hits@1"]]}
{"text": "The Hits@1 is the score for favoring the ground - truth next utterance ( consistent and context - relevant ) over other candidate utterances which are just being consistent ( i.e. entailing ) or contradictory to the given persona .", "entities": [[1, 2, "MetricName", "Hits@1"]]}
{"text": "Thus , such distractors can help the self - conscious agent to generate responses which are context - relevant and allow the imaginary listener to identify the speaker 's persona .", "entities": [[10, 11, "DatasetName", "agent"]]}
{"text": "We construct the query vector q for each datapoint with the BERT - Uncased - Base ( Devlin et al , 2019 ) model .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "We use the output embedding of BERT 's [ CLS ] token , and normalize it to a unit length to build q R d .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Then , the loss is computed as L = max ( q K [ n b ] \u2212 q K [ n p ]", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "This loss maximizes the cosine similarity between the query q and the positive key K [ n p ] , while minimizing the similarity to the negative key K [ n b ] .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "We finetune the query network BERT with this loss .", "entities": [[5, 6, "MethodName", "BERT"], [8, 9, "MetricName", "loss"]]}
{"text": "After computing the loss , memory M is updated differently for two cases .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "n ] q , v [ n ] l , a [ n ] 0 .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "In our Distractor Memory network , training corresponds to updating the memory and the parameters of the query network .", "entities": [[3, 5, "MethodName", "Memory network"]]}
{"text": "At inference , given a test example , we obtain the query by encoding the dialogue context and the persona using BERT .", "entities": [[21, 22, "MethodName", "BERT"]]}
{"text": "We show that our self - conscious framework can significantly improve consistency and accuracy of state - of - the - art persona - based agents on two benchmark datasets .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "This task was the subject of the ConvAI2 competition ( Dinan et al , 2019 ) at NeurIPS 2018 .", "entities": [[7, 8, "DatasetName", "ConvAI2"]]}
{"text": "We experiment on three pretrained models including ControlSeq2Seq ( See et al , 2019 ) , TransferTransfo ( Wolf et al , 2019b ) , and Blender as base speakers ( S 0 ) for our self - conscious agents ( S 1 ) .", "entities": [[26, 27, "MethodName", "Blender"], [32, 33, "DatasetName", "0"]]}
{"text": "The ControlSeq2Seq is a Seq2Seq model with attention trained on Twitter dataset ( Miller et al , 2017 ) and finetuned on PersonaChat .", "entities": [[4, 5, "MethodName", "Seq2Seq"]]}
{"text": "TranferTransfo based on GPT ( Radford et al , 2018 ) is the winner of the ConvAI2 competition in automatic evaluation .", "entities": [[3, 4, "MethodName", "GPT"], [16, 17, "DatasetName", "ConvAI2"]]}
{"text": "Blender , a recently released generative dialogue model , is the state - of - the - art open - domain chatbot .", "entities": [[0, 1, "MethodName", "Blender"], [21, 22, "TaskName", "chatbot"]]}
{"text": "High scores in Hits@1 , Entail@1 and low scores in Contradict@1 imply better consistency . granting them the sense of self - consciousness .", "entities": [[3, 4, "MetricName", "Hits@1"]]}
{"text": "For Dialogue NLI , we report three ranking metrics introduced in the original paper : Hits@1 , Entail@1 , and Contradict@1 .", "entities": [[15, 16, "MetricName", "Hits@1"]]}
{"text": "For PersonaChat , we report Hits@1 , standard F1 score , perplexity and C score , following the Con - vAI2 protocol .", "entities": [[5, 6, "MetricName", "Hits@1"], [8, 10, "MetricName", "F1 score"], [11, 12, "MetricName", "perplexity"]]}
{"text": "The NLI model returns 1 , 0 , - 1 for entailment , neutrality , and contradiction , respectively .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "For TransferTransfo , we use the generative version to calculate Hits@1 .", "entities": [[10, 11, "MetricName", "Hits@1"]]}
{"text": "Since the posterior update of our self - conscious agent revises the distribution learned by the base speaker , the increase in perplexity is natural due to the effect of regularization .", "entities": [[9, 10, "DatasetName", "agent"], [22, 23, "MetricName", "perplexity"]]}
{"text": "Nevertheless , our approach improves the F1 score for TransferTransfo and Blender .", "entities": [[6, 8, "MetricName", "F1 score"], [11, 12, "MethodName", "Blender"]]}
{"text": "Another notable result is that our agents without NLI ( S 1 + DM in Table 3 ) for ControlSeq2Seq and TransferTransfo even outperform the base agents with NLI ( S 0", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "+ NLI ) on Hits@1 .", "entities": [[4, 5, "MetricName", "Hits@1"]]}
{"text": "That is , our self - conscious agents achieve better GT accuracy even without the help of an NLI model trained on consistency labels .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "For consistency , we follow Madotto et al ( 2019 ) and ask judges to assign 1 , 0 , \u22121 to the utterance for consistency , neutrality , and contradiction , respectively .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "The agent with our self - consciousness method S 1 is rated as more consistent than the base agent S 0 while maintaining a similar level of engagingness .", "entities": [[1, 2, "DatasetName", "agent"], [18, 19, "DatasetName", "agent"], [20, 21, "DatasetName", "0"]]}
{"text": "While it can be trivial to increase consistency at the cost of engagingness ( e.g. perfect consistency can by generating boring utterances with very little variance ) , it is not the case for our agent .", "entities": [[35, 36, "DatasetName", "agent"]]}
{"text": "Since our agent seeks to be heard as the given persona to the listener , self - distinctive words tend to meld into generated responses ( see Figure 6 ) .", "entities": [[2, 3, "DatasetName", "agent"]]}
{"text": "We demonstrate that our self - conscious agent can be generalized to generate context - consistent utterances beyond persona .", "entities": [[7, 8, "DatasetName", "agent"]]}
{"text": "We condition the agent with its previous responses in the dialogue history ; that is , i in Eq .", "entities": [[3, 4, "DatasetName", "agent"]]}
{"text": "( 2 ) is the agent 's past responses instead of persona sentences .", "entities": [[5, 6, "DatasetName", "agent"]]}
{"text": "Hence , tokens that are inconsistent to the agent 's past response would be less favored by the model .", "entities": [[8, 9, "DatasetName", "agent"]]}
{"text": "Our S 1 agent outperforms other literal agents on all three datasets in terms of consistency .", "entities": [[3, 4, "DatasetName", "agent"]]}
{"text": "We compare it with the base speaker ( S 0 ) of TransferTransfo ( Wolf et al , 2019b ) and the human response ( Human ) .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "To further analyze our self - conscious agent , we conduct experiments by controlling three features of our agent : world prior updates p t ( i ) , listener rationality \u03b2 and speaker rationality \u03b1 .", "entities": [[7, 8, "DatasetName", "agent"], [18, 19, "DatasetName", "agent"], [31, 32, "HyperparameterName", "\u03b2"], [35, 36, "HyperparameterName", "\u03b1"]]}
{"text": "In the self - conscious agent , the world prior acts as a cumulative state over personas .", "entities": [[5, 6, "DatasetName", "agent"]]}
{"text": "We remind that we propose to update the world prior with L t 0 instead of L t 1 in Eq .", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "However , our approach with L t 0 makes significant difference , as shown in Figure 5 .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "The reason is that the pragmatic listener L t 1 \u221d S t 0", "entities": [[13, 14, "DatasetName", "0"]]}
{"text": "( u t | i , h , u < t ) \u00d7 L t 0", "entities": [[15, 16, "DatasetName", "0"]]}
{"text": "( i | h , u \u2264t , p t ) reflects the current S t 0 twice ( i.e. in L t 0 and in itself ) per time step .", "entities": [[16, 17, "DatasetName", "0"], [23, 24, "DatasetName", "0"]]}
{"text": "On the other hand , L t 0 moderately combines the information from both S t 0 and p t ( i ) , preserving better cumulative information .", "entities": [[7, 8, "DatasetName", "0"], [16, 17, "DatasetName", "0"]]}
{"text": "Listener Rationality \u03b2 .", "entities": [[2, 3, "HyperparameterName", "\u03b2"]]}
{"text": "Speaker Rationality \u03b1 .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "As \u03b1 increases , the self - conscious agent reflects the listener 's distribution ( i.e. the likelihood ) more into the posterior .", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [8, 9, "DatasetName", "agent"]]}
{"text": "When \u03b1 is too large , the posterior distribution is overwhelmed by the likelihood of the persona .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "Hence , \u03b1 can control the degree of copying the given condition text .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "An appropriate \u03b1 value allows the given persona condition to blend smoothly in the utterance .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "We compare our proposed Distractor Memory ( DM ) with three heuristic methods , and two variants of the pretrained BERT model ( Devlin et al , 2019 ) .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "Second , we test the k - nearest search by speaker 's persona , denoted by Nearest ; for a given persona descriptions , we find its closest training persona embedding using cosine similarity on average pooled BERT features .", "entities": [[37, 38, "MethodName", "BERT"]]}
{"text": "We also compare with two variants of the BERT model .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "The first variant is BERT - Classifier , which takes dialogue context as input and returns the index of persona from training set as output .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "The second variant is bi - encoder ranking model of Miller et al ( 2017 ) , denoted by BERT - Ranker .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "It encodes dialogue context and candidate persona with separate BERT encoders measuring its ranking with cosine similarity .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "The BERT - Ranker performs the best among baselines , but not as good as ours , which validates that memorization capability is effective for selecting useful distractors .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "We train our DM on one NVIDIA TITAN Xp GPU up to 7 epochs .", "entities": [[7, 8, "DatasetName", "TITAN"]]}
{"text": "We choose the hyper - parameter configuration showing the best performance in Hits@1 for Dialogue NLI and F1 score for PersonaChat .", "entities": [[12, 13, "MetricName", "Hits@1"], [17, 19, "MetricName", "F1 score"]]}
{"text": "Following Madotto et al ( 2019 ) , 2 https://parl.ai/ 3 https://huggingface.co/transformers/ we use the finetuned BERT - based NLI model 4 to compute the C score .", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "In each set , we show given persona , dialogue context , human responses , and generated responses by our self - conscious agent and the base speaker .", "entities": [[23, 24, "DatasetName", "agent"]]}
{"text": "( S 0 )", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "Figure 7 : Examples of generated responses by our self - conscious agent with Distractor Memory ( S 1 + DM ) on the PersonaChat dataset ( Zhang et al , 2018 ) .", "entities": [[12, 13, "DatasetName", "agent"]]}
{"text": "We compare it with the base speaker ( S 0 ) of TransferTransfo ( Wolf et al , 2019b ) and the human response ( Human ) .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "With the experiments , CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings .", "entities": [[15, 17, "TaskName", "word embeddings"]]}
{"text": "Analogical reasoning has become a reliable evaluation method for word embeddings .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "Although it has been widely used in evaluation of word embeddings ( Yang and Sun , 2015 ;", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "Since there are no explicit word boundaries in Chinese , we take dictionaries and word segmentation specifications as references to confirm the inclusion of each word Levy and Goldberg ( 2014b ) unifies SGNS and PPMI in a framework , which share the same hyper - parameter settings .", "entities": [[35, 36, "DatasetName", "PPMI"]]}
{"text": "Digits and punctuations are remained .", "entities": [[0, 1, "DatasetName", "Digits"]]}
{"text": "Conduct Chinese word segmentation with HanLP ( v_1.5.3 ) 3 .", "entities": [[1, 4, "TaskName", "Chinese word segmentation"]]}
{"text": "SGNS ( skipgram model with negative sampling ) ( Mikolov et al , 2013 ) and PPMI ( Positive Pointwise Mutual Information ) ( Levy and Goldberg , 2014a ) are respectively typical methods for learning dense and sparse word vectors .", "entities": [[16, 17, "DatasetName", "PPMI"]]}
{"text": "We can observe that on CA8 dataset , SGNS representations perform better in analogical reasoning of morphological relations and PPMI representations show great advantages in semantic relations .", "entities": [[19, 20, "DatasetName", "PPMI"]]}
{"text": "This result is consistent with performance of English dense and sparse vectors on MSR ( morphology - only ) , SemEval ( semanticonly ) , and Google ( mixed ) analogy datasets ( Levy and Goldberg , 2014b ;", "entities": [[26, 27, "DatasetName", "Google"]]}
{"text": "Levy et al , 2015 966 .603 .117 .162 .181 .389 .222 .414 .345 .236 .223 .327 word+ngram .715 .977 .640 .143 .184 .197 .429 .250 .449 .308 .276 .310 .368 word+char .676 .966 .548 .358 .540 .326 .612 .455 .468 .226 .296 .305 .368 PPMI word .925 .920 .548 .103 .139 .138 .464 .226 .627 .501 .300 .515 .522 word+ngram .943 .960 .658 .102 .129 .168 .456 .230 .680 .535 .371 .626 .586 word+char .913 .886 .614 .106 .190 .173 .505 .260 .638 .502 .288 .515 .524 probably because the reasoning on morphological relations relies more on common words in context , and the training procedure of SGNS favors frequent word pairs .", "entities": [[45, 46, "DatasetName", "PPMI"], [99, 102, "DatasetName", "words in context"]]}
{"text": "Meanwhile , PPMI model is more sensitive to infrequent and specific word pairs , which are beneficial to semantic relations .", "entities": [[2, 3, "DatasetName", "PPMI"]]}
{"text": "SGNS model integrating with character features even doubles the accuracy in morphological questions .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "Also , CA8 is proved to be a reliable benchmark for evaluation of Chinese word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "The benchmark and embedding sets we release could also serve as a solid basis for Chinese NLP tasks .", "entities": [[0, 2, "DatasetName", "The benchmark"]]}
{"text": "The Fact Extraction and VERification ( FEVER ) Shared Task", "entities": [[6, 7, "DatasetName", "FEVER"]]}
{"text": "We present the results of the first Fact Extraction and VERification ( FEVER ) Shared Task .", "entities": [[12, 13, "DatasetName", "FEVER"]]}
{"text": "The best performing system achieved a FEVER score of 64.21 % .", "entities": [[6, 7, "DatasetName", "FEVER"]]}
{"text": "Information extraction is a well studied domain and the outputs of such systems enable many natural language technologies such as question answering and text summarization .", "entities": [[20, 22, "TaskName", "question answering"], [23, 25, "TaskName", "text summarization"]]}
{"text": "For this purpose , we hosted the first Fact Extraction and VERification ( FEVER ) shared task to raise interest in and awareness of the task of automatic information verificationa research domain that is orthogonal to information extraction .", "entities": [[13, 14, "DatasetName", "FEVER"]]}
{"text": "The systems participating in the FEVER shared task were required to label claims with the correct class and also return the sentence ( s ) forming the necessary evidence for the assigned label .", "entities": [[5, 6, "DatasetName", "FEVER"]]}
{"text": "A key difference between this task and other textual entailment and natural language inference tasks ( Dagan et al , 2009 ; Bowman et al , 2015 ) is the need to identify the evidence from a large textual corpus .", "entities": [[11, 14, "TaskName", "natural language inference"]]}
{"text": "Furthermore , in comparison to large - scale question answering tasks ( Chen et al , 2017 ) , systems must reason about information that is not present in the claim .", "entities": [[8, 10, "TaskName", "question answering"]]}
{"text": "We hope that research in these fields will be stimulated by the challenges present in FEVER .", "entities": [[15, 16, "DatasetName", "FEVER"]]}
{"text": "As part of the FEVER shared task , any evidence retrieved by participating systems that was not contained in the original dataset was annotated and used to augment the evidence in the test set .", "entities": [[4, 5, "DatasetName", "FEVER"]]}
{"text": "In this paper , we present a short description of the task and dataset , present a summary of the submissions and the leader board , and highlight future research directions .", "entities": [[12, 14, "DatasetName", "and dataset"]]}
{"text": "Candidate systems for the FEVER shared task", "entities": [[4, 5, "DatasetName", "FEVER"]]}
{"text": "Training and development data was released through the FEVER website .", "entities": [[8, 9, "DatasetName", "FEVER"]]}
{"text": "The FEVER shared task requires submission of evidence to justify the labeling of a claim .", "entities": [[1, 2, "DatasetName", "FEVER"]]}
{"text": "The primary scoring metric for the task is the label accuracy conditioned on providing at least one complete set of evidence , referred to as the FEVER score .", "entities": [[10, 11, "MetricName", "accuracy"], [26, 27, "DatasetName", "FEVER"]]}
{"text": "Correctly labeled claims with no or only partial evidence received no points for the FEVER score .", "entities": [[14, 15, "DatasetName", "FEVER"]]}
{"text": "Where multiple sets of evidence was annotated in the data , only one set was required for the claim to be considered correct for the FEVER score .", "entities": [[25, 26, "DatasetName", "FEVER"]]}
{"text": "This performance level can also be achieved for the FEVER score by predicting NOTENOUGHINFO for every claim .", "entities": [[9, 10, "DatasetName", "FEVER"]]}
{"text": "However , as the FEVER score requires evidence for SUP - PORTED and REFUTED claims , a random baseline is expected to score lower on this metric .", "entities": [[4, 5, "DatasetName", "FEVER"]]}
{"text": "The recall point is awarded , as is the case for the FEVER score , only by providing a complete set of evidence for the claim .", "entities": [[12, 13, "DatasetName", "FEVER"]]}
{"text": "The FEVER shared task was hosted as a competition on Codalab 3 which allowed submissions to be scored against the blind test set without the need to publish the correct labels .", "entities": [[1, 2, "DatasetName", "FEVER"]]}
{"text": "The system with the highest score was submitted by UNC - NLP ( FEVER score : 64.21 % ) .", "entities": [[13, 14, "DatasetName", "FEVER"]]}
{"text": "This consisted of three stages : document selection , sentence selection and natural language inference .", "entities": [[12, 15, "TaskName", "natural language inference"]]}
{"text": "These were used either as inputs to a search API ( i.e. Wikipedia Search or Google Search ) , search server ( e.g. Lucene 5 or Solr 6 ) or as keywords for matching against Wikipedia page titles or article bodies .", "entities": [[15, 16, "DatasetName", "Google"]]}
{"text": "BUPT - NLPer report using S - MART for entity linking ( Yang and Chang , 2015 ) and the highest scor - ing team , UNC - NLP , report using page viewership statistics to rank the candidate pages .", "entities": [[9, 11, "TaskName", "entity linking"]]}
{"text": "The teams which scored highest on evidence recall were Athene UKP TU Darmstadt ( recall = 85.19 % ) and UCL Machine Reading Group ( recall = 82.84 % ) 7 8 Athene report extracting nounphrases from the claim and using these to query the Wikipedia search API .", "entities": [[10, 11, "DatasetName", "UKP"]]}
{"text": "A similar approach was used by Columbia NLP who query the Wikipedia search API using named entities extracted from the claim as a query string , all the text before the first lowercase verb phrase as a query string and also combine this result with Wikipedia pages identified with Google search using the entire claim .", "entities": [[49, 50, "DatasetName", "Google"]]}
{"text": "UNC - NLP , Athene UKP TU Darmstadt and Columbia NLP modeled the task as supervised binary classification , using architectures such as Enhanced LSTM ( Chen et al , 2016 ) , Decomposable Attention ( Parikh et al , 2016 ) or similar to them .", "entities": [[5, 6, "DatasetName", "UKP"], [24, 25, "MethodName", "LSTM"]]}
{"text": "SWEEPer and BUPT - NLPer present jointly learned models for sentence selection and natural language inference .", "entities": [[13, 16, "TaskName", "natural language inference"]]}
{"text": "Other teams report scoring based on sentence similarity using Word Mover 's Distance ( Kusner et al , 2015 ) or cosine similarity over smooth inverse frequency weightings ( Arora et al , 2017 ) , ELMo embeddings ( Peters et al , 2018 ) and TF - IDF ( Salton et al , 1983 ) .", "entities": [[36, 37, "MethodName", "ELMo"]]}
{"text": "While many different approaches were used for sentence pair classification , e.g. Enhanced LSTM ( Chen et al , 2016 ) , Decomposable Attention ( Parikh et al , 2016 ) , Transformer Model ( Radford and Salimans , 2018 ) , Random Forests ( Svetnik et al , 2003 ) and ensembles thereof , these are not specific to the task and it is difficult to assess their impact due to the differences in the processing preceding this stage .", "entities": [[13, 14, "MethodName", "LSTM"], [32, 33, "MethodName", "Transformer"]]}
{"text": "Evidence Combination : UNC - NLP ( the highest scoring team ) concatenate the evidence sentences into a single string for classification ; UCL Machine Reading Group classify each evidenceclaim pair individually and aggregate the results using a simple multilayer perceptron ( MLP ) ; Columbia NLP perform majority voting ; and finally , Athene - UKP TU Darmstadt encode each evidence - claim pair individually using an Enhanced LSTM , pool the resulting vectors and use an MLP for classification .", "entities": [[42, 43, "DatasetName", "MLP"], [56, 57, "DatasetName", "UKP"], [69, 70, "MethodName", "LSTM"], [78, 79, "DatasetName", "MLP"]]}
{"text": "Both Ohio State and UNC - NLP report alternative token encodings : UNC - NLP report using ELMo ( Peters et al , 2018 ) and WordNet", "entities": [[17, 18, "MethodName", "ELMo"]]}
{"text": "Training : BUPT - NLPer and SWEEPer model the evidence selection and claim verification using a multi - task learning model under the hypothesis that information from each task supplements the other .", "entities": [[16, 20, "TaskName", "multi - task learning"]]}
{"text": "There were 18 , 846 claims where at least one system returned an incorrect label , according to the FEVER score , i.e. taking evidence into account .", "entities": [[19, 20, "DatasetName", "FEVER"]]}
{"text": "The evidence sentences returned by each system for each claim was sampled further with a probability proportional to the system 's FEVER score in an attempt to focus annotation efforts towards higher quality candidate evidence .", "entities": [[21, 22, "DatasetName", "FEVER"]]}
{"text": "Finally , the claim verifier is a state - of - theart 3 - way neural natural language inference ( NLI ) classifier ( with WordNet and ELMo features ) that takes the concatenation of all selected evidence as the premise and the claim as the hypothesis , and labels each such evidences - claim pair as one of ' support ' , ' refute ' , or ' not enough info ' .", "entities": [[16, 19, "TaskName", "natural language inference"], [27, 28, "MethodName", "ELMo"]]}
{"text": "The UCLMR system is a four stage model consisting of document retrieval , sentence retrieval , natural language inference and aggregation .", "entities": [[16, 19, "TaskName", "natural language inference"]]}
{"text": "A natural language inference model is then applied to each of these sentences paired with the claim , giving a prediction for each potential evidence .", "entities": [[1, 4, "TaskName", "natural language inference"]]}
{"text": "These predictions are then aggregated using a simple MLP , and the sentences are reranked to keep only the evidence consistent with the final prediction .", "entities": [[8, 9, "DatasetName", "MLP"]]}
{"text": "Sentence selection The hinge loss with negative sampling is applied to train the enhanced LSTM .", "entities": [[4, 5, "MetricName", "loss"], [14, 15, "MethodName", "LSTM"]]}
{"text": "RTE We combine the 5 sentences from sentence selection and the claim to form 5 pairs and then apply enhanced LSTM for each pair .", "entities": [[0, 1, "DatasetName", "RTE"], [20, 21, "MethodName", "LSTM"]]}
{"text": "We combine the resulting representations using average and max pooling and feed the resulting vector through an MLP for classification .", "entities": [[8, 10, "MethodName", "max pooling"], [17, 18, "DatasetName", "MLP"]]}
{"text": "We develop a system for the FEVER fact extraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with language modeling ( Radford and Salimans , 2018 ) , to classify a broad set of potential evidence .", "entities": [[6, 7, "DatasetName", "FEVER"]]}
{"text": "Our model for fact checking and verification consists of two stages : 1 ) identifying relevant documents using lexical and syntactic features from the claim and first two sentences in the Wikipedia article and 2 ) jointly modeling sentence extraction and verification .", "entities": [[3, 5, "TaskName", "fact checking"]]}
{"text": "As the tasks of fact checking and finding evidence are dependent on each other , an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that supports / refutes the position of the claim .", "entities": [[4, 6, "TaskName", "fact checking"]]}
{"text": "We thus jointly model the second stage by using a pointer network with the claim and evidence sentence represented using the ESIM module .", "entities": [[10, 12, "MethodName", "pointer network"], [21, 22, "MethodName", "ESIM"]]}
{"text": "For stage 2 , we first train both components using multi - task learning over a larger memory of extracted sentences , then tune parameters using re - inforcement learning to first extract sentences and predict the relation over only the extracted sentences .", "entities": [[10, 14, "TaskName", "multi - task learning"]]}
{"text": "For Sentence Selection we used the modified document retrieval component of DrQA to get the top 5 sentences and then further extracted the top 3 sentences using cosine similarity between vectors obtained from Elmo ( Peters et al , 2018 ) sentence embeddings of the claim and the evidence .", "entities": [[33, 34, "MethodName", "Elmo"], [41, 43, "TaskName", "sentence embeddings"]]}
{"text": "For RTE we used the same model as outlined by ( Conneau et al , 2017 ) in their work for recognizing textual entailment and learning universal sentence representations .", "entities": [[1, 2, "DatasetName", "RTE"]]}
{"text": "We prepared a pipeline system which composes of document selection , a sentence retrieval , and a recognizing textual entailment ( RTE ) components .", "entities": [[21, 22, "DatasetName", "RTE"]]}
{"text": "A simple entity linking approach with text match is used as the document selection component , this component identifies relevant documents for a given claim by using mentioned entities as clues .", "entities": [[2, 4, "TaskName", "entity linking"]]}
{"text": "Finally , the RTE component selects evidence sentences by ranking the sentences and classifies the claim as SUPPORTED , REFUTED , or NOTENOUGH - INFO simultaneously .", "entities": [[3, 4, "DatasetName", "RTE"]]}
{"text": "As the RTE component , we adopted DEISTE ( Deep Explorations of Inter - Sentence interactions for Textual Entailment )", "entities": [[2, 3, "DatasetName", "RTE"]]}
{"text": "( Yin et al , 2018 ) model that is the state - of - the - art in RTE task .", "entities": [[19, 20, "DatasetName", "RTE"]]}
{"text": "Then we use two neural networks , one for named entity recognition and the other for constituency parsing , and also the Stanford dependency parser to create the keywords used inside the Lucene queries .", "entities": [[9, 12, "TaskName", "named entity recognition"], [16, 18, "TaskName", "constituency parsing"]]}
{"text": "We use the Standford POS - Tagger to generate POS - Tags for the claim and candidate sentences which are then used in a handcrafted scoring script to assign a score on a 0 to 15 scale .", "entities": [[33, 34, "DatasetName", "0"]]}
{"text": "In this paper , we describe the system we designed for the FEVER 2018 Shared Task .", "entities": [[12, 13, "DatasetName", "FEVER"]]}
{"text": "We NER tagged the claim using SpaCy and used the Named Entities as candidate page IDs .", "entities": [[1, 2, "TaskName", "NER"]]}
{"text": "This article presents the SIRIUS - LTG system for the Fact Extraction and VERification ( FEVER ) Shared Task .", "entities": [[15, 16, "DatasetName", "FEVER"]]}
{"text": "First we extract the entities in the claim , then we find potential Wikipedia URI candidates for each of the entities using the SPARQL query over DBpedia ( Khot et al , 2018 ) and a Gradient - Boosted Decision Trees ( TalosTree ) model ( Baird et al , 2017 ) for this task .", "entities": [[26, 27, "DatasetName", "DBpedia"]]}
{"text": "We introduce an end - to - end multi - task learning model for fact extraction and verification with bidirection attention .", "entities": [[8, 12, "TaskName", "multi - task learning"]]}
{"text": "We propose a multi - task learning framework for the evidence extraction and claim verification because these two tasks can be accomplished at the same time .", "entities": [[3, 7, "TaskName", "multi - task learning"]]}
{"text": "For each claim , our system firstly uses the entity linking tool S - MART to retrieve relative pages from Wikipedia .", "entities": [[9, 11, "TaskName", "entity linking"]]}
{"text": "Many approaches to automatically recognizing entailment relations have employed classifiers over hand engineered lexicalized features , or deep learning models that implicitly capture lexicalization through word embeddings .", "entities": [[25, 27, "TaskName", "word embeddings"]]}
{"text": "For example , such a system trained in the news domain may learn that a sentence like \" Palestinians recognize Texas as part of Mexico \" tends to be unsupported , a fact which has no value in say a scientific domain .", "entities": [[20, 21, "DatasetName", "Texas"]]}
{"text": "In its current implementation , this model does not perform well on the FEVER dataset , due to two reasons .", "entities": [[13, 14, "DatasetName", "FEVER"]]}
{"text": "First , for the information retrieval part of the task we used the baseline system provided , since this was not the aim of our project .", "entities": [[4, 6, "TaskName", "information retrieval"]]}
{"text": "Second , this is work in progress and we still are in the process of identifying more features and gradually increasing the accuracy of our model .", "entities": [[22, 23, "MetricName", "accuracy"]]}
{"text": "We describe the UMBC - FEVER system that we used in the 2018 FEVER shared task .", "entities": [[5, 6, "DatasetName", "FEVER"], [13, 14, "DatasetName", "FEVER"]]}
{"text": "The system employed a frame - based information retrieval approach to select Wikipedia sentences providing evidence and used a two - layer multilayer perceptron ( MLP ) for classification .", "entities": [[7, 9, "TaskName", "information retrieval"], [25, 26, "DatasetName", "MLP"]]}
{"text": "The work reported was partly conducted while James Thorne was at Amazon Research Cambridge .", "entities": [[13, 14, "DatasetName", "Cambridge"]]}
{"text": "Social chatbots , also known as chit - chat chatbots , evolve rapidly with large pretrained language models .", "entities": [[15, 18, "TaskName", "pretrained language models"]]}
{"text": "We show that speakers ' personas can be inferred through a simple neural network with high accuracy .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "With recent progress in large pretrained language models ( Radford et al , 2019 ; Yang et al , 2019 ) , some attempts ( Wolf et al , 2019 ;", "entities": [[5, 8, "TaskName", "pretrained language models"]]}
{"text": "For example , as Figure 1 shows , when using a fine - tuned GPT - 2 as the encoder and decoder of an LM - based social chatbot , if the learned representation of each utterance can be obtained by an adversary , then the adversary can build a classifier to predict the persona information based on the representation .", "entities": [[14, 15, "MethodName", "GPT"], [28, 29, "TaskName", "chatbot"]]}
{"text": "Third , there has been no existing LM - based chatbot that can defend against persona inference attacks , and no study shows how to protect both known and unknown persona attributes .", "entities": [[10, 11, "TaskName", "chatbot"]]}
{"text": "In this paper , to address the above challenges , we use the fine - tuned GPT - 2 as our chatbot .", "entities": [[16, 17, "MethodName", "GPT"], [21, 22, "TaskName", "chatbot"]]}
{"text": "We build a single external multi - layer perception ( MLP ) attacker model to perform black - box persona inference attacks on the utterance - level embeddings .", "entities": [[10, 11, "DatasetName", "MLP"]]}
{"text": "The high accuracy of the attacker model implies that the utterance - level embeddings have potential vulnerabilities to reveal \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "Every representation of the utterance , which is based on the last hidden state of GPT - 2 , is attacked without defense ( column of \" Attacks on LM \" ) and with defense ( column of \" Attacks on the defensed LM \" ) .", "entities": [[15, 16, "MethodName", "GPT"]]}
{"text": "Finally , we apply defense learning strategies on the GPT - 2 to prevent such black - box attacks .", "entities": [[9, 10, "MethodName", "GPT"]]}
{"text": "To the best of our knowledge , we are the first to disclose and analyze the persona inference attack for LM - based chatbots and treat it as a privacy risk . 2 ) :", "entities": [[17, 19, "TaskName", "inference attack"]]}
{"text": "al ( 2021 ) performed black - box model inversion attack on GPT - 2 through descriptive prompts with beam search .", "entities": [[12, 13, "MethodName", "GPT"]]}
{"text": "Lehman et al ( 2021 ) examined BERT pretrained on Electronic Health Records via blank filling and model probing to recover Personal Health Information .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "What is worse , for an LM - based chatbot , its training conversations are prone to include more private attributes than other commonly - used corpora for language modeling like BooksCorpus ( Zhu et al , 2015 ) and Wikipedia .", "entities": [[9, 10, "TaskName", "chatbot"]]}
{"text": "Tigunova et al ( 2019 ) proposed Hidden Attribute Model ( HAM ) to extract professions and genders of speakers from various dialog datasets .", "entities": [[11, 12, "DatasetName", "HAM"]]}
{"text": "In this section , we illustrate black - box persona inference attacks on GPT - 2 and our defense strategies .", "entities": [[13, 14, "MethodName", "GPT"]]}
{"text": "We assume that there is a GPT - 2 based chatbot f pretrained on private conversations D. Only language modeling is used to train the chatbot :", "entities": [[6, 7, "MethodName", "GPT"], [10, 11, "TaskName", "chatbot"], [25, 26, "TaskName", "chatbot"]]}
{"text": "L f ( u ; \u03b8 f )", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "= \u2212 | u | i=1 log ( Pr ( wi | c , w0 , w1 , ... , wi\u22121 ) ) , ( 1 ) where f refers to the LM - based chatbot with given utterance u = { w 0 , w 1 , ... ,", "entities": [[35, 36, "TaskName", "chatbot"], [43, 44, "DatasetName", "0"]]}
{"text": "Each persona s kj is an integer that can be mapped to its persona according to a predefined dictionary and 0", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "The persona inference attack can be viewed as a supervised classification task .", "entities": [[2, 4, "TaskName", "inference attack"]]}
{"text": "For the black - box attack setup , the adversary can only query the target dialog model f with access to embeddings of adversary 's inputs and can not access or modify model parameters \u03b8 f .", "entities": [[34, 35, "HyperparameterName", "\u03b8"]]}
{"text": "Its loss function L A exploits cross - entropy between the predicted distribution and ground truth distribution that can be formulated as : LA ( u kj , s kj ; \u03b8A )", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "Ideally , to achieve an optimal privacypreserving chatbot against persona inference attacks , the probability distribution of the attacker model", "entities": [[7, 8, "TaskName", "chatbot"]]}
{"text": "That is , the adversary can not improve its inference accuracy from posterior estimation A ( f ( u ) ) and the accuracy is no better than making random guesses on the persona attributes .", "entities": [[10, 11, "MetricName", "accuracy"], [23, 24, "MetricName", "accuracy"]]}
{"text": "KL loss aims to minimize the Kullback - Leibler divergence between A ( f ( u ) ) and the uniform distribution .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "For optimization , we can leave out constant terms and the logarithm ( Mireshghallah et al , 2021 ) \u20dd and the attacking stage is marked by 2 \u20dd. Both language modeling and defender objectives are jointly trained for the defense to optimize the GPT - 2 model .", "entities": [[44, 45, "MethodName", "GPT"]]}
{"text": "After GPT - 2 's training stage 1 \u20dd is finished , parameters of GPT - 2 are all frozen and then the attacking stage 2 \u20dd starts .", "entities": [[1, 2, "MethodName", "GPT"], [14, 15, "MethodName", "GPT"]]}
{"text": "the following loss function : LD ( u ; \u03b8A )", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "Then the KL loss becomes : L kl ( u ; \u03b8A p , \u03b8 f )", "entities": [[3, 4, "MetricName", "loss"], [14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "where parameters of the chatbot \u03b8 f and the fake attacker \u03b8 Ap are updated via KL loss .", "entities": [[4, 5, "TaskName", "chatbot"], [5, 6, "HyperparameterName", "\u03b8"], [11, 12, "HyperparameterName", "\u03b8"], [17, 18, "MetricName", "loss"]]}
{"text": "The intuition is to train the chatbot together with a fake attacker to prevent model overlearning by flattening the attacker model 's distribution .", "entities": [[6, 7, "TaskName", "chatbot"]]}
{"text": "min \u03b8 f I ( f ( u ) ; s ) .", "entities": [[1, 2, "HyperparameterName", "\u03b8"]]}
{"text": "A p with softmax activation to learn p \u03a8 , we obtain the final objective for the defender : min \u03b8 Ap max \u03b8 f CE", "entities": [[3, 4, "MethodName", "softmax"], [20, 21, "HyperparameterName", "\u03b8"], [23, 24, "HyperparameterName", "\u03b8"]]}
{"text": "= CE ( A p ( f ( u kj ) ) , s kj ) and L mi2 ( u kj , s kj ; \u03b8 f )", "entities": [[26, 27, "HyperparameterName", "\u03b8"]]}
{"text": "= \u2212CE ( A p ( f ( u kj ) ) , s kj ) for the fake adversary and the chatbot respectively .", "entities": [[22, 23, "TaskName", "chatbot"]]}
{"text": "A p and the defensed chatbot f .", "entities": [[5, 6, "TaskName", "chatbot"]]}
{"text": "The right part of Figure 2 illustrates how the chatbot is trained to address the black - box attack .", "entities": [[9, 10, "TaskName", "chatbot"]]}
{"text": "The loss function for the defender combines KL loss , MI loss and LM loss .", "entities": [[1, 2, "MetricName", "loss"], [8, 9, "MetricName", "loss"], [11, 12, "MetricName", "loss"], [14, 15, "MetricName", "loss"]]}
{"text": "Notice that the fake adversary objective in MI loss violates KL loss which tries to make the distribution of A p flatten .", "entities": [[8, 9, "MetricName", "loss"], [11, 12, "MetricName", "loss"]]}
{"text": "Our proposed loss assigns more weights to the KL loss : L = L f + \u03bb1L kl", "entities": [[2, 3, "MetricName", "loss"], [9, 10, "MetricName", "loss"]]}
{"text": "Though the chatbot trained with overall loss L still can not interfere training process of A during black - box attacks , L aims to mitigate persona overlearning issues of f to address such persona inference attacks .", "entities": [[2, 3, "TaskName", "chatbot"], [6, 7, "MetricName", "loss"]]}
{"text": "To train the GPT - 2 as our chatbot , we use the DialoGPT pretrained on Reddit comment chains .", "entities": [[3, 4, "MethodName", "GPT"], [8, 9, "TaskName", "chatbot"], [16, 17, "DatasetName", "Reddit"]]}
{"text": "Then we use PersonaChat dataset ( Zhang et al , 2018 ) to fine - tune the GPT - 2 .", "entities": [[17, 18, "MethodName", "GPT"]]}
{"text": "The attacker model exploits the final layer embedding of the last token \" < | endof - text | > \" from the GPT - 2 as model input .", "entities": [[23, 24, "MethodName", "GPT"]]}
{"text": "We also try other attacker model architectures ( transformer block based attackers ) and input embeddings ( average of all embeddings in the final layer of GPT - 2 ) , but the attacking performance is worse than the 2 - layer model mentioned above .", "entities": [[26, 27, "MethodName", "GPT"]]}
{"text": "For privacy , we use persona inference accuracy and weighted F1score to evaluate the attacker 's performance .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "( Gu et al , 2021a ) to quantify the attacker 's privacy loss for the estimated persona distribution .", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "Top - k accuracy is reported in the Appendix .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "BERTScore and BLEU measure similarity between generated outputs and ground truth while Distinct ( Dist ) focuses on diversity .", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "Perplexity shows the uncertainty when the LM model fits the data .", "entities": [[0, 1, "MetricName", "Perplexity"]]}
{"text": "We list the attacking performance of A in multiple scenarios shown in Acc refers to test persona inference accuracy .", "entities": [[12, 13, "MetricName", "Acc"], [18, 19, "MetricName", "accuracy"]]}
{"text": "F1 uses weighted average F1 - score .", "entities": [[0, 1, "MetricName", "F1"], [4, 7, "MetricName", "F1 - score"]]}
{"text": "LM indicates the attacker performance that only language modeling objective is applied to train the chatbot without any defense mechanism .", "entities": [[15, 16, "TaskName", "chatbot"]]}
{"text": "That is , the black - box persona inference attack has 52\u00d7 the accuracy of guessing .", "entities": [[8, 10, "TaskName", "inference attack"], [13, 14, "MetricName", "accuracy"]]}
{"text": "As a result , the black - box persona prediction attack becomes useless after applying the defenses for the chatbot .", "entities": [[19, 20, "TaskName", "chatbot"]]}
{"text": "After applying KL loss and MI loss , the attacker model tends to make predictions on a single label .", "entities": [[3, 4, "MetricName", "loss"], [6, 7, "MetricName", "loss"]]}
{"text": "To show the effectiveness of proposed KL loss and MI loss and how they affect the performance of black - box persona inference attacks , we consider the inclusion and exclusion of proposed defense objectives .", "entities": [[7, 8, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}
{"text": "LM+MI applies language modeling and MI loss .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "From the table , it can be seen that LM+KL , LM+MI and LM+KL+MI are all able to reduce the test accuracy of the attacks .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "The KL loss is weaker from the perspective of defense , but it tends to flatten the estimated persona distribution with much smaller Max - Ratio .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "This suggests that MI loss causes the attacker model to predict all labels on a single persona attribute .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "Besides privacy , utility is another key objective to train a chatbot .", "entities": [[11, 12, "TaskName", "chatbot"]]}
{"text": "Acc and Max - Ratio are measured in % .", "entities": [[0, 1, "MetricName", "Acc"]]}
{"text": "BP u corresponds to Bayesian Privacy loss on the uniform distribution .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "Due to the one - to - many nature of chit - chats , the BLEU is not adequate to compare generated responses with ground truth .", "entities": [[15, 16, "MetricName", "BLEU"]]}
{"text": "Hence , we adapt Precision , Recall and Precision of BERTScore to measure the similarity in the embedding space .", "entities": [[4, 5, "MetricName", "Precision"], [6, 7, "MetricName", "Recall"], [8, 9, "MetricName", "Precision"]]}
{"text": "Though the uncertainty increases after applying KL loss and MI loss , it does no harm to the quality of generation .", "entities": [[7, 8, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}
{"text": "The defender owns 8 , 031 conversations with persona labels ranging from 500 to 4 , 331 while the adversary holds 2 , 376 dialogues with persona labels ranging from 0 to 4 , 331 .", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "For testing , 500 conversations with persona labels ranging from 0 to 4 , 331 are used .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "The persona inference accuracy is still very low and the attacker model tends to predict more on a single persona label than the balanced data distribution setup .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "This result shows that the proposed overall loss can also prevent black - box persona inference attacks on unseen personas .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "It also verifies previous suggestions that combining LM loss with MI loss may fool the attacker model to make wrong predictions .", "entities": [[8, 9, "MetricName", "loss"], [11, 12, "MetricName", "loss"]]}
{"text": "Besides , to better evaluate privacy loss for the estimated distribution , a smaller label space is preferred .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "We use Sentence - BERT ( Reimers and Gurevych , 2020 ) to embed all persona sentences and perform k - means clustering on the embeddings to obtain 8 clusters .", "entities": [[4, 5, "MethodName", "BERT"], [19, 23, "MethodName", "k - means clustering"]]}
{"text": "Here , the defender owns 6 , 654 conversations with persona labels ranging from 3 to 7 while the adversary holds 3 , 753 dialogues with persona labels ranging from 0 to 7 .", "entities": [[30, 31, "DatasetName", "0"]]}
{"text": "For testing , 500 conversations with persona labels ranging from 0 to 7 are used .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "( F 0 | |", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "A ( f ( u ) ) ) where F 0 refers to uniform distribution .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "For imbalanced data distribution with a small label space , our proposed defenses can still achieve much lower attack accuracy than LM on both Unseen and Overall .", "entities": [[19, 20, "MetricName", "accuracy"]]}
{"text": "However , for Overall , LM+KL+MI has higher accuracy with a lower F1 - score compared with two baselines .", "entities": [[8, 9, "MetricName", "accuracy"], [12, 15, "MetricName", "F1 - score"]]}
{"text": "Thus the effectiveness of the KL loss is verified .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "In Figure 3 , we give an example of the persona inference attack , where conversations are generated between the chatbot and the user with the given context .", "entities": [[11, 13, "TaskName", "inference attack"], [20, 21, "TaskName", "chatbot"]]}
{"text": "This attack example illustrates that our defense objectives can prevent the black - box persona inference attack from inferring relevant personas .", "entities": [[15, 17, "TaskName", "inference attack"]]}
{"text": "In this paper , we show that LM - based chatbots tend to reveal personas of speakers and propose effective defense objectives to prevent GPT - 2 from overlearning .", "entities": [[24, 25, "MethodName", "GPT"]]}
{"text": "We declare that all authors of this paper acknowledge the ACM Code of Ethics and honor the code of conduct .", "entities": [[10, 11, "DatasetName", "ACM"], [13, 14, "DatasetName", "Ethics"]]}
{"text": "Previous experiments mainly consider accuracy as the evaluation metric .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "In this section , we use top - k accuracy for the black - box persona inference attacks to measure privacy protection .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "For each conversation , the utterances are concatenated by the special token \" < | endoftext | > \" to train the GPT - 2 .", "entities": [[22, 23, "MethodName", "GPT"]]}
{"text": "To decode outputs from GPT - 2 , we apply the Nucleus Sampling ( Holtzman et al , 2020 ) method .", "entities": [[4, 5, "MethodName", "GPT"]]}
{"text": "We set top - p = 0.9 with a temperature coefficient 0.9 to sample words from the GPT - 2 .", "entities": [[17, 18, "MethodName", "GPT"]]}
{"text": "For optimization , we set 2 AdamW optimizers ( Loshchilov and Hutter , 2019 ) for the chatbot and the persona predictor respectively .", "entities": [[6, 7, "MethodName", "AdamW"], [17, 18, "TaskName", "chatbot"]]}
{"text": "For hyper - parameters , we set \u03bb 0 = 1 , \u03bb 1 = 10 and \u03bb 2 = 1 .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "Figure 4 shows the data distribution of the test set and average distribution after softmax activation over the 8 labels for attacker A and defender A p .", "entities": [[14, 15, "MethodName", "softmax"]]}
{"text": "This behavior conforms to the KL loss 's objective that aims to flatten the distribution and deviate from the ground truth distribution .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "Translation performance is affected by nonparallel texts and non - literal translations ( Carpuat et al , 2017 ) .", "entities": [[0, 1, "TaskName", "Translation"]]}
{"text": "Proposed approaches for translating towards lowresource and morphologically - rich languages have included transfer learning ( Zoph et al , 2016 ) as well as multilingual and multi - way NMT ( Rikters et al , 2018 ) .", "entities": [[13, 15, "TaskName", "transfer learning"]]}
{"text": "In addition ensembles of factored NMT models have been proposed for automatic post - editing and quality estimation ( for example Hokamp , 2017 ) .", "entities": [[11, 15, "TaskName", "automatic post - editing"]]}
{"text": "For each model , different optimization options from Marian - NMT during the validation phase are used to create three NMT variants of each model , namely optimizing with ( i ) BLEU , ( ii ) entropy and ( iii ) word - wise normalized crossentropy ( denoted as \" ce - mean \" and representing the default optimization for Marian - NMT ) .", "entities": [[32, 33, "MetricName", "BLEU"]]}
{"text": "All cells used both in the encoder and decoder side are gated recurrent units ( GRU ) .", "entities": [[15, 16, "MethodName", "GRU"]]}
{"text": "To reduce the lexicon size , a total of 85 , 000 merge operations are allowed using the BPE ( Byte Pair Encoding ) method proposed in ( Sennrich et al , 2016 ) , this being the default setting for marian - nmt applications .", "entities": [[18, 19, "MethodName", "BPE"], [20, 23, "MethodName", "Byte Pair Encoding"]]}
{"text": "Typically , for a single - GPU system ( equipped with an NVIDIA Titan XP GTX1080 GPU card driven by an Intel i - 9700 K CPU ) , 24 hours are required for training the transformer , 130 hours for amun and 308 hours for s2s .", "entities": [[13, 14, "DatasetName", "Titan"]]}
{"text": "The experiments aim to improve the translation accuracy of an NMT system , taking into account limited training data and constrained computing resources .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "In order to investigate translation into a lesser - used and highly inflectional language , we Common to all 3 models layer - normalization yes exponential smoothing yes beam - size 6 normalize 0.6 early - stopping 5 Transformer - specific transformer - dropout 0.1 transformer - dropout - attention 0.1 transformer - dropout - ffn 0.1 Amun and s2s - specific dropout - rnn 0.2 dropout - src 0.1 dropout - trg 0.1 have chosen the English - to - Greek language pair .", "entities": [[38, 39, "MethodName", "Transformer"]]}
{"text": "To improve translation accuracy , the main errors need to be identified in an automated manner .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "The idea is that a poor alignment between source text and translation indicates substantial loss of meaning during translation .", "entities": [[14, 15, "MetricName", "loss"]]}
{"text": "The establishment of representative alignment scores allows in turn the combination of multiple NMT models , using AVM to evaluate the accuracy of each candidate translation and thus select the best translation on a sentence - by - sentence basis .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "This module , handling sentence pairs from this parallel corpus , identifies the correspondence of words and phrases from SL to TL , to determine the translation accuracy .", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "To determine the quality of the NMT - based translations ( amun - , s2s - and transformer - based models ) , two widely used MT evaluation metrics are utilised , namely BLEU ( Papineni et", "entities": [[33, 34, "MetricName", "BLEU"]]}
{"text": "The last two rows report the accuracy of ensembles using PAM with ( i ) Uscore and ( ii ) Wscore , respectively .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "The PAM - Wscore ensemble gives a higher accuracy than the Marian - NMT ensemble , whilst the accuracy of PAM - Uscore is lower than PAM - Wscore .", "entities": [[8, 9, "MetricName", "accuracy"], [18, 19, "MetricName", "accuracy"]]}
{"text": "On the whole , it is Marian - ensemble and PAM - Wscore that generate the best NIST and BLEU scores .", "entities": [[19, 20, "MetricName", "BLEU"]]}
{"text": "To that end , the BLEU and NIST scores of all the independent sentences are assembled , forming two populations of scores ( one for BLEU and one for NIST ) for each experimental run .", "entities": [[5, 6, "MetricName", "BLEU"], [25, 26, "MetricName", "BLEU"]]}
{"text": "PAM - Wscore achieves consistently higher translation scores than Marian - ensemble for both BLEU and NIST .", "entities": [[14, 15, "MetricName", "BLEU"]]}
{"text": "Emphasis has been placed on improving the translation accuracy of NMT models that can be trained more rapidly and cost - effectively ( in terms of CPU processing power ) and rendering this performance comparable to that of more complex models .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "The proposed hybrid approach has resulted in higher BLEU and NIST scores , compared to those of single NMT models .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "Another short term activity involves using the proposed method with sacreBLEU instead of the BLEU and NIST metrics provided by mt - eval .", "entities": [[10, 11, "MetricName", "sacreBLEU"], [14, 15, "MetricName", "BLEU"]]}
{"text": "One area of interest would be to determine the effectiveness of the PAM - based method when very limited dictionaries are available as well as the limitations when the accuracy of the parser used is relatively low .", "entities": [[29, 30, "MetricName", "accuracy"]]}
{"text": "The authors wish to acknowledge the contribution of NVIDIA who donated for research purposes in the area of Machine Translation a Titan XP GPU card under the NVIDIA Academic Support Programme to the MT group of ILSP / Athena R.C.", "entities": [[18, 20, "TaskName", "Machine Translation"], [21, 22, "DatasetName", "Titan"]]}
{"text": "KorNLI and KorSTS : New Benchmark Datasets for Korean Natural Language Understanding", "entities": [[0, 1, "DatasetName", "KorNLI"], [2, 3, "DatasetName", "KorSTS"], [9, 12, "TaskName", "Natural Language Understanding"]]}
{"text": "Natural language inference ( NLI ) and semantic textual similarity ( STS ) are key tasks in natural language understanding ( NLU ) .", "entities": [[0, 3, "TaskName", "Natural language inference"], [7, 10, "TaskName", "semantic textual similarity"], [11, 12, "TaskName", "STS"], [17, 20, "TaskName", "natural language understanding"]]}
{"text": "Although several benchmark datasets for those tasks have been released in English and a few other languages , there are no publicly available NLI or STS datasets in the Korean language .", "entities": [[25, 26, "TaskName", "STS"]]}
{"text": "Motivated by this , we construct and release new datasets for Korean NLI and STS , dubbed KorNLI and KorSTS , respectively .", "entities": [[14, 15, "TaskName", "STS"], [17, 18, "DatasetName", "KorNLI"], [19, 20, "DatasetName", "KorSTS"]]}
{"text": "To accelerate research on Korean NLU , we also establish baselines on KorNLI and KorSTS .", "entities": [[12, 13, "DatasetName", "KorNLI"], [14, 15, "DatasetName", "KorSTS"]]}
{"text": "Natural language inference ( NLI ) and semantic textual similarity ( STS ) are considered as two of the central tasks in natural language understanding ( NLU ) .", "entities": [[0, 3, "TaskName", "Natural language inference"], [7, 10, "TaskName", "semantic textual similarity"], [11, 12, "TaskName", "STS"], [22, 25, "TaskName", "natural language understanding"]]}
{"text": "They are not only featured in GLUE ( Wang et al , 2018 ) and SuperGLUE ( Wang et al , 2019 ) , which are two popular benchmarks for NLU , but also known to be useful for supplementary training of pre - trained language models ( Phang et al , 2018 ) as well as for building and evaluating fixedsize sentence embeddings ( Reimers and Gurevych , 2019 ) .", "entities": [[6, 7, "DatasetName", "GLUE"], [15, 16, "DatasetName", "SuperGLUE"], [62, 64, "TaskName", "sentence embeddings"]]}
{"text": "Accordingly , several benchmark datasets have been released for both NLI ( Bowman et al , 2015 ; and STS ( Cer et al , 2017 ) in the English language .", "entities": [[19, 20, "TaskName", "STS"]]}
{"text": "When it comes to the Korean language , however , benchmark datasets for NLI and STS do not exist .", "entities": [[15, 16, "TaskName", "STS"]]}
{"text": "Popular benchmark datasets for Korean NLU typically involve question answering 12 and sentiment analysis 3 , but not NLI or STS .", "entities": [[8, 10, "TaskName", "question answering"], [12, 14, "TaskName", "sentiment analysis"], [20, 21, "TaskName", "STS"]]}
{"text": "We believe that the lack of publicly available benchmark datasets for Korean NLI and STS has led to the lack of interest for building Korean NLU models suited for these key understanding tasks .", "entities": [[14, 15, "TaskName", "STS"]]}
{"text": "Motivated by this , we construct and release Ko - rNLI and KorSTS , two new benchmark datasets for NLI and STS in the Korean language .", "entities": [[12, 13, "DatasetName", "KorSTS"], [21, 22, "TaskName", "STS"]]}
{"text": "We then establish baselines for both KorNLI and KorSTS to facilitate research on Korean NLU .", "entities": [[6, 7, "DatasetName", "KorNLI"], [8, 9, "DatasetName", "KorSTS"]]}
{"text": "Bowman et al ( 2015 ) introduced the Stanford NLI ( SNLI ) dataset , which consists of 570 K English sentence pairs based on image captions .", "entities": [[11, 12, "DatasetName", "SNLI"]]}
{"text": "introduced the Multi - Genre NLI ( MNLI ) dataset , which consists of 455 K English sentence pairs from ten genres .", "entities": [[7, 8, "DatasetName", "MNLI"]]}
{"text": "Conneau et al ( 2018 ) released the Cross - lingual NLI ( XNLI ) dataset by extending the development and test data of the MNLI corpus to 15 languages .", "entities": [[13, 14, "DatasetName", "XNLI"], [25, 26, "DatasetName", "MNLI"]]}
{"text": "Note that Korean is not one of the 15 languages in XNLI .", "entities": [[11, 12, "DatasetName", "XNLI"]]}
{"text": "MT and PE indicate machine translation and post - editing , respectively .", "entities": [[4, 6, "TaskName", "machine translation"]]}
{"text": "For development and test data , the machine translation outputs are further post - edited by human experts .", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "STS is a task that assesses the gradations of semantic similarity between two sentences .", "entities": [[0, 1, "TaskName", "STS"], [9, 11, "TaskName", "semantic similarity"]]}
{"text": "The similarity score ranges from 0 ( completely dissimilar ) to 5 ( completely equivalent ) .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "It is commonly used to evaluate either how well a model grasps the closeness of two sentences in meaning , or how well a sentence embedding embodies the semantic representation of the sentence .", "entities": [[24, 26, "TaskName", "sentence embedding"]]}
{"text": "The STS - B dataset consists of 8 , 628 English sentence pairs selected from the STS tasks organized in the context of SemEval between 2012 and 2017 ( Agirre et al , 2012 ( Agirre et al , , 2013 ( Agirre et al , , 2014 ( Agirre et al , , 2015 ( Agirre et al , , 2016 .", "entities": [[1, 4, "DatasetName", "STS - B"], [16, 17, "TaskName", "STS"]]}
{"text": "We explain how we develop two new Korean language understanding datasets : KorNLI and Ko - rSTS .", "entities": [[12, 13, "DatasetName", "KorNLI"]]}
{"text": "The KorNLI dataset is derived from three different sources : SNLI , MNLI , and XNLI , while the KorSTS dataset stems from the STS - B dataset .", "entities": [[1, 2, "DatasetName", "KorNLI"], [10, 11, "DatasetName", "SNLI"], [12, 13, "DatasetName", "MNLI"], [15, 16, "DatasetName", "XNLI"], [19, 20, "DatasetName", "KorSTS"], [24, 27, "DatasetName", "STS - B"]]}
{"text": "First , we translate the training sets of the SNLI , MNLI , and STS - B datasets , as well as the development and test sets of the XNLI 4 and STS - B datasets , into Korean using an internal neural machine translation engine .", "entities": [[9, 10, "DatasetName", "SNLI"], [11, 12, "DatasetName", "MNLI"], [14, 17, "DatasetName", "STS - B"], [29, 30, "DatasetName", "XNLI"], [32, 35, "DatasetName", "STS - B"], [43, 45, "TaskName", "machine translation"]]}
{"text": "Rather , it refers to human translation based on the prior machine translation results , which serve as first drafts .", "entities": [[11, 13, "TaskName", "machine translation"]]}
{"text": "In this section , we provide baselines for the Korean NLI and STS tasks using our newly created benchmark datasets .", "entities": [[12, 13, "TaskName", "STS"]]}
{"text": "As illustrated with BERT ( Devlin et al , 2019 ) and many of its variants , the de facto standard approach for NLU tasks is to pre - train a large language model and fine - tune it on each task .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "In the cross - encoding Examples Score A : \u1112 \u1161 \u11ab \u1102 \u1161 \u11b7\u110c \u1161\u1100 \u1161 \u110b", "entities": [[6, 7, "MetricName", "Score"]]}
{"text": "For both KorNLI and KorSTS , we consider two pre - trained language models .", "entities": [[2, 3, "DatasetName", "KorNLI"], [4, 5, "DatasetName", "KorSTS"]]}
{"text": "We first pre - train a Korean RoBERTa ( Liu et al , 2019 ) , both base and large versions , on a collection of internally collected Korean corpora ( 65 GB ) .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}
{"text": "We construct a byte pair encoding ( BPE ) ( Gage , 1994 ; Sennrich et al , 2016 ) dictionary of 32 K tokens using Sen - tencePiece ( Kudo and Richardson , 2018 ) .", "entities": [[3, 6, "MethodName", "byte pair encoding"], [7, 8, "MethodName", "BPE"]]}
{"text": "We also use XLM - R ( Conneau and Lample , 2019 ) , a publicly available cross - lingual language model that was pre - trained on 2.5 TB of Common Crawl corpora in 100 languages including Korean ( 54 GB ) .", "entities": [[3, 4, "MethodName", "XLM"], [31, 33, "DatasetName", "Common Crawl"]]}
{"text": "Note that the base and large architectures of XLM - R are identical to those of RoBERTa , except that the vocabulary size is significantly larger ( 250 K ) , making the embedding and output layers that much larger .", "entities": [[8, 9, "MethodName", "XLM"], [16, 17, "MethodName", "RoBERTa"]]}
{"text": "For KorNLI , we additionally include results for XLM - R models fine - tuned on the original MNLI training set ( also known as cross - lingual transfer in XNLI ) .", "entities": [[1, 2, "DatasetName", "KorNLI"], [8, 9, "MethodName", "XLM"], [18, 19, "DatasetName", "MNLI"], [25, 29, "TaskName", "cross - lingual transfer"], [30, 31, "DatasetName", "XNLI"]]}
{"text": "To ensure comparability across settings , we only train on the MNLI portion when fine - tuning on KorNLI .", "entities": [[11, 12, "DatasetName", "MNLI"], [18, 19, "DatasetName", "KorNLI"]]}
{"text": "Overall , the Korean RoBERTa models outperform the XLM - R models , regardless of whether they are fine - tuned on Korean or English training sets .", "entities": [[4, 5, "MethodName", "RoBERTa"], [8, 9, "MethodName", "XLM"]]}
{"text": "The large version of Korean RoBERTa performs the best for both KorNLI ( 83.67 % ) and KorSTS ( 85.27 % ) among all models tested .", "entities": [[5, 6, "MethodName", "RoBERTa"], [11, 12, "DatasetName", "KorNLI"], [17, 18, "DatasetName", "KorSTS"]]}
{"text": "Among the XLM - R models for KorNLI , those fine - tuned on the Korean training set consistently outperform the cross - lingual transfer variants .", "entities": [[2, 3, "MethodName", "XLM"], [7, 8, "DatasetName", "KorNLI"], [21, 25, "TaskName", "cross - lingual transfer"]]}
{"text": "We also report the KorSTS scores of bi - encoding models .", "entities": [[4, 5, "DatasetName", "KorSTS"]]}
{"text": "Here , we first provide two baselines that do not use pre - trained language models : Korean fastText and the multilingual universal sentence encoder ( M - USE ) .", "entities": [[18, 19, "MethodName", "fastText"], [21, 25, "MethodName", "multilingual universal sentence encoder"], [28, 29, "MethodName", "USE"]]}
{"text": "Korean fastText ( Bojanowski et al , 2017 ) is a pre - trained word embedding model 6 trained on Korean text from Common Crawl .", "entities": [[1, 2, "MethodName", "fastText"], [23, 25, "DatasetName", "Common Crawl"]]}
{"text": "To produce sentence embeddings , we take the average of fastText word embeddings for each sentence .", "entities": [[2, 4, "TaskName", "sentence embeddings"], [10, 11, "MethodName", "fastText"], [11, 13, "TaskName", "word embeddings"]]}
{"text": "M - USE 7 ( Yang et al , 2019 ) , is a CNN - based sentence encoder model trained for NLI , questionanswering , and translation ranking across 16 languages including Korean .", "entities": [[2, 3, "MethodName", "USE"]]}
{"text": "For both Korean fastText and M - USE , we compute the cosine similarity between two input sentence embeddings to make an unsupervised STS prediction .", "entities": [[3, 4, "MethodName", "fastText"], [7, 8, "MethodName", "USE"], [17, 19, "TaskName", "sentence embeddings"], [23, 24, "TaskName", "STS"]]}
{"text": "Pre - trained language models can also be used as bi - encoding models following the approach of Sen - tenceBERT ( Reimers and Gurevych , 2019 ) , which involves fine - tuning a BERT - like model with a Siamese network structure on NLI and/or STS .", "entities": [[35, 36, "MethodName", "BERT"], [41, 43, "MethodName", "Siamese network"], [47, 48, "TaskName", "STS"]]}
{"text": "We use the SentenceBERT approach for both Korean RoBERTa ( \" Korean SRoBERTa \" ) and XLM - R ( \" SXLM - R \" ) .", "entities": [[8, 9, "MethodName", "RoBERTa"], [16, 17, "MethodName", "XLM"]]}
{"text": "We categorize each result based on whether the model was additionally trained on KorNLI and/or KorSTS .", "entities": [[13, 14, "DatasetName", "KorNLI"], [15, 16, "DatasetName", "KorSTS"]]}
{"text": "Note that models that are not fine - tuned at all or only fine - tuned to KorNLI can be considered as unsupervised w.r.t .", "entities": [[17, 18, "DatasetName", "KorNLI"]]}
{"text": "KorSTS .", "entities": [[0, 1, "DatasetName", "KorSTS"]]}
{"text": "Also note that M - USE is trained on a machinetranslated version of SNLI , which is a subset of KorNLI , as part of its pre - training step .", "entities": [[5, 6, "MethodName", "USE"], [13, 14, "DatasetName", "SNLI"], [20, 21, "DatasetName", "KorNLI"]]}
{"text": "6 https://dl.fbaipublicfiles.com/ fasttext / vectors - crawl / cc.ko.300.bin.gz 7 https://tfhub.dev/google/ universal - sentence - encoder - multilingual/ 3 First , given each model , we find that supplementary training on KorNLI consistently improves the KorSTS scores for both unsupervised and supervised settings , as was the case with English models ( Conneau et al , 2017 ; Reimers and Gurevych , 2019 ) .", "entities": [[2, 3, "MethodName", "fasttext"], [31, 32, "DatasetName", "KorNLI"], [35, 36, "DatasetName", "KorSTS"]]}
{"text": "This shows that the KorNLI dataset can be an effective intermediate training source for biencoding approaches .", "entities": [[4, 5, "DatasetName", "KorNLI"]]}
{"text": "When comparing the baseline models in each setting , we find that both M - USE and the SentenceBERT - based models trained on KorNLI achieve competitive unsupervised Ko - rSTS scores .", "entities": [[15, 16, "MethodName", "USE"], [24, 25, "DatasetName", "KorNLI"]]}
{"text": "Both models significantly outperform the average of fastText embeddings model and the Korean SRoBERTa and SXLM - R models without fine - tuning .", "entities": [[7, 8, "MethodName", "fastText"]]}
{"text": "Among our baselines , large SXLM - R trained on KorNLI followed by KorSTS achieves the best score ( 81.84 ) .", "entities": [[10, 11, "DatasetName", "KorNLI"], [13, 14, "DatasetName", "KorSTS"]]}
{"text": "In addition , our baseline experiments in Section 4.1 show that supplementary training on KorNLI improves KorSTS performance ( +1 % for RoBERTa and +4 - 11 % for XLM - R ) , suggesting that the labels of KorNLI are still meaningful .", "entities": [[14, 15, "DatasetName", "KorNLI"], [16, 17, "DatasetName", "KorSTS"], [22, 23, "MethodName", "RoBERTa"], [29, 30, "MethodName", "XLM"], [39, 40, "DatasetName", "KorNLI"]]}
{"text": "Another quantitative evidence is that the performance of XLM - R fine - tuned on KorNLI ( 80.3 % with cross - lingual transfer ) is within a comparable range of the model 's performance on other XNLI languages ( 80.1 % on average ) .", "entities": [[8, 9, "MethodName", "XLM"], [15, 16, "DatasetName", "KorNLI"], [20, 24, "TaskName", "cross - lingual transfer"], [37, 38, "DatasetName", "XNLI"]]}
{"text": "For example , there were cases in which the two input sentences for KorSTS were so similar ( with 4 + similarity scores ) that upon translation , the two inputs simply became identical .", "entities": [[13, 14, "DatasetName", "KorSTS"]]}
{"text": "\u1162 \u11bc\u1102 \u1175 \u11b7 , which is a correct word translation but is a gender - neutral noun , because there is no gender - specific counterpart to the word in Korean .", "entities": [[9, 11, "TaskName", "word translation"]]}
{"text": "We introduced KorNLI and KorSTS - new datasets for Korean natural language understanding .", "entities": [[2, 3, "DatasetName", "KorNLI"], [4, 5, "DatasetName", "KorSTS"], [10, 13, "TaskName", "natural language understanding"]]}
{"text": "Using these datasets , we also established baselines for Korean NLI and STS with both cross - encoding and bi - encoding approaches .", "entities": [[12, 13, "TaskName", "STS"]]}
{"text": "For the Korean RoBERTa baselines used in 4 , we pre - train a RoBERTa ( Liu et al , 2019 ) model on an internal Korean corpora of size 65 GB , consisting of online news articles ( 56 GB ) , encyclopedia ( 7 GB ) , movie subtitles ( \u223c1 GB ) , and the Sejong corpus 8 ( \u223c0.5 GB ) .", "entities": [[3, 4, "MethodName", "RoBERTa"], [14, 15, "MethodName", "RoBERTa"]]}
{"text": "We use fairseq , which includes the official implementation for RoBERTa .", "entities": [[10, 11, "MethodName", "RoBERTa"]]}
{"text": "In compared to the original RoBERTa ( English ) , the model architectures are identical except for the token embedding layer , as we use different vocabularies ( 32 K sentencepiece vocab instead of 50 K byte - level BPE ) .", "entities": [[5, 6, "MethodName", "RoBERTa"], [30, 31, "MethodName", "sentencepiece"], [39, 40, "MethodName", "BPE"]]}
{"text": "To fine - tune Korean RoBERTa and XLM - R models using the cross - encoding approach ( 4.1 ) , we follow the fine - tuning procedures of RoBERTa ( Liu et al , 2019 )", "entities": [[5, 6, "MethodName", "RoBERTa"], [7, 8, "MethodName", "XLM"], [29, 30, "MethodName", "RoBERTa"]]}
{"text": "For each dataset and model size , we choose the hyperparameter configurations that are used in the corresponding English version of the dataset and model size ( except for the XLM - R cross - lingual transfer using MNLI , where we also use the same hyperparameters as RoBERTa and XLM - R on KorNLI ) .", "entities": [[30, 31, "MethodName", "XLM"], [33, 37, "TaskName", "cross - lingual transfer"], [38, 39, "DatasetName", "MNLI"], [48, 49, "MethodName", "RoBERTa"], [50, 51, "MethodName", "XLM"], [54, 55, "DatasetName", "KorNLI"]]}
{"text": "We observe that the XLM - R models fine - tuned on KorNLI and KorSTS achieve the highest scores on the development set , although the Korean RoBERTa models perform better on the test set ( Table 5", "entities": [[4, 5, "MethodName", "XLM"], [12, 13, "DatasetName", "KorNLI"], [14, 15, "DatasetName", "KorSTS"], [27, 28, "MethodName", "RoBERTa"]]}
{"text": "To fine - tune Korean RoBERTa and XLM - R models using the bi - encoding approach ( 4.2 ) , we train Korean Sentence RoBERTa ( \" Korean SRoBERTa \" ) and Sentence XLM - R ( \" SXLM - R \" ) , following the fine - tuning procedure of SentenceBERT ( Reimers and Gurevych , 2019 ) .", "entities": [[5, 6, "MethodName", "RoBERTa"], [7, 8, "MethodName", "XLM"], [25, 26, "MethodName", "RoBERTa"], [34, 35, "MethodName", "XLM"]]}
{"text": "For each model size , we manually search among learning rates { 2e - 5 , 1e - 5 } for training on KorNLI , { 1e - 5 , 2e - 6 } for training on KorSTS , and { 1e - 5 , 2e - 6 } for training on KorSTS after KorNLI .", "entities": [[23, 24, "DatasetName", "KorNLI"], [37, 38, "DatasetName", "KorSTS"], [52, 53, "DatasetName", "KorSTS"], [54, 55, "DatasetName", "KorNLI"]]}
{"text": "After training until convergence , we choose the learning rate that lead to the highest KorSTS score on the development set .", "entities": [[8, 10, "HyperparameterName", "learning rate"], [15, 16, "DatasetName", "KorSTS"]]}
{"text": "Korean SRoBERTa ( large ) achieves the best development set performance on both supervised settings , but SXLM - R ( large ) achieves the best performance for the KorNLI KorSTS setting on test set .", "entities": [[29, 30, "DatasetName", "KorNLI"], [30, 31, "DatasetName", "KorSTS"]]}
{"text": "DialPort ( Zhao et al , 2016 ) began with a central agent and the Let'sForecast weather information system .", "entities": [[12, 13, "DatasetName", "agent"]]}
{"text": "The Cambridge restaurant system ( Gasic et al , 2015 ) and a general restaurant system ( Let 's Eat , that handles cities that Cambridge does not cover ) joined the portal .", "entities": [[1, 2, "DatasetName", "Cambridge"], [25, 26, "DatasetName", "Cambridge"]]}
{"text": "A chatbot , Qubot , was developed to deal with out - of - domain requests .", "entities": [[1, 2, "TaskName", "chatbot"]]}
{"text": "Originally envisioned as a website with a list of the urls of systems a user could try , the portal has become easier to use , more closely resembling what users might expect , given their exposure to the Amazon ECHO 1 and Google HOME 2 , etc .", "entities": [[43, 44, "DatasetName", "Google"]]}
{"text": "By the end of March 2017 , in addition to the above systems , the portal also included Mr. Clue , a word game from USC ( Pincus and Traum , 2016 ) , a restaurant opinion bot ( Let 's Discuss , CMU ) , and a bus information system derived from Let 's Go ( Raux et al , 2005 ) .", "entities": [[25, 26, "DatasetName", "USC"]]}
{"text": "The portal offers users the option of typing or talking and of seeing an agent or just hearing it .", "entities": [[14, 15, "DatasetName", "agent"]]}
{"text": "And it challenged the nature of the agent : some users prefer no visual agent ; others could n't use speech with the system .", "entities": [[7, 8, "DatasetName", "agent"], [14, 15, "DatasetName", "agent"]]}
{"text": "It quickly evolved into one userfriendly portal where , all of the SDS are accessed through one central agent , users being seamlessly transferred from one system to another .", "entities": [[18, 19, "DatasetName", "agent"]]}
{"text": "The system was tried out informally ( Lee et al , 2017 ) to determine whether the portal fulfilled criteria such as : timely response , correct transfer ( to what the user wanted ) , and correct recommendation of systems ( not saying for example , you can ask me about restaurants in Cambridge just after the user has finished talking to that system ) .", "entities": [[54, 55, "DatasetName", "Cambridge"]]}
{"text": "Let'sForecast , Cambridge SDS on restaurants , Lets Eat ; Mr Clue word game ; and Qubot chatbot handling out of domain requests .", "entities": [[2, 3, "DatasetName", "Cambridge"], [17, 18, "TaskName", "chatbot"]]}
{"text": "Since then , Let 's Go and Let'sDiscuss , a chatbot that gives restaurant reviews , have joined .", "entities": [[10, 11, "TaskName", "chatbot"]]}
{"text": "Cambridge The Cambridge restaurant information system helps users find a restaurant in Cambridge , UK based on the area , the price range or the food type .", "entities": [[0, 1, "DatasetName", "Cambridge"], [2, 3, "DatasetName", "Cambridge"], [12, 13, "DatasetName", "Cambridge"]]}
{"text": "The system runs a trained dialogue policy based on the GP - SARSA algorithm ( Ga\u0161i\u0107 et al , 2010 ) .", "entities": [[12, 13, "MethodName", "SARSA"]]}
{"text": "Second , since Di - alPort has multiple users in parallel , Mr. Clue was updated to launch a new agent instance for each new HTTP session ( user ) that is directed to the game from the main DialPort system .", "entities": [[20, 21, "DatasetName", "agent"]]}
{"text": "For each new session , the agent chooses 1 of 77 different pre - compiled clue lists ( each with 10 unique target - words ) at random .", "entities": [[6, 7, "DatasetName", "agent"]]}
{"text": "Other Systems QuBot , a chatbot from Pohang University and CMU , is used for out - of - domain handling .", "entities": [[5, 6, "TaskName", "chatbot"]]}
{"text": "Let 's Eat from CMU is based on Yelp , finding restaurants for cities that Cambridge does not cover and for Cambridge if that system is down .", "entities": [[15, 16, "DatasetName", "Cambridge"], [21, 22, "DatasetName", "Cambridge"]]}
{"text": "This included the use of speech ( as opposed to typing ) , the use of a visual agent , the absence of both graphical and speech response , feedback and portal behavior .", "entities": [[18, 19, "DatasetName", "agent"]]}
{"text": "Starting with few ESes , each on very different topics , the agent selection policy simply tried to detect the topic in the users ' request and select the corresponding ES .", "entities": [[12, 13, "DatasetName", "agent"]]}
{"text": "As more ESes connect to the portal , non - trivial relationships among ESes emerge : 1 ) Dialog context sensitive agent selection : The optimal choice of ES may depend on discourse history .", "entities": [[21, 22, "DatasetName", "agent"]]}
{"text": "For example , Let'sForecast , Cambridge restaurant and Let 's Eat : after the user has weather information for city X , they say , recommend a place to have lunch .", "entities": [[5, 6, "DatasetName", "Cambridge"]]}
{"text": "Choosing between Let 's Eat and Cambridge restaurant depends on the value of city X , because Cambridge restaurant covers places to eat in Cambridge UK and Let 's Eat covers other places .", "entities": [[6, 7, "DatasetName", "Cambridge"], [17, 18, "DatasetName", "Cambridge"], [24, 25, "DatasetName", "Cambridge"]]}
{"text": "2 ) Discourse Obligation for Agent selection : Users have various ways to make requests : request ( tell me xxx ) , WH - question ( what 's the weather in xx )", "entities": [[5, 6, "DatasetName", "Agent"]]}
{"text": "Finally , the NLU has been extended to support multi - intent multi - domain identification by reducing the problem to a multi - label classification task using a one - vs - all strategy .", "entities": [[22, 26, "TaskName", "multi - label classification"]]}
{"text": "The first advertising attempt using Google AdWords 3 attracted few explorers and no real users .", "entities": [[5, 6, "DatasetName", "Google"]]}
{"text": "Left - to - Right Dependency Parsing with Pointer Networks", "entities": [[5, 7, "TaskName", "Dependency Parsing"]]}
{"text": "Similarly to the recent stack - pointer parser by Ma et al ( 2018 ) , we use the pointer network framework that , given a word , can directly point to a position from the sentence .", "entities": [[19, 21, "MethodName", "pointer network"]]}
{"text": "Dependency parsing , the task of automatically obtaining the grammatical structure of a sentence expressed as a dependency tree , has been widely studied by natural language processing ( NLP ) researchers in the last decades .", "entities": [[0, 2, "TaskName", "Dependency parsing"]]}
{"text": "Many attempts have been made to alleviate the impact of error propagation in transition - based dependency parsing , but the latest and most successful approach was developed by Ma et al ( 2018 ) .", "entities": [[13, 18, "TaskName", "transition - based dependency parsing"]]}
{"text": "In particular , they make use of pointer networks ( Vinyals et al , 2015 ) to implement a new neural network architecture called stack - pointer network .", "entities": [[26, 28, "MethodName", "pointer network"]]}
{"text": "We also take advantage of pointer network capabilities and use the neural network architecture introduced by Ma et al ( 2018 ) to design a nonprojective left - to - right transition - based algorithm , where the position value pointed by the network has the opposite meaning : it denotes the index that corresponds to the head node of the current focus word .", "entities": [[5, 7, "MethodName", "pointer network"]]}
{"text": "Apart from increasing the parsing speed twofold ( while keeping the same quadratic time complexity ) , it achieves the best accuracy to date among fully - supervised single - model dependency parsers on the PTB - SD , and obtains competitive accuracies on twelve different languages in comparison to the original top - down version .", "entities": [[21, 22, "MetricName", "accuracy"], [35, 36, "DatasetName", "PTB"]]}
{"text": "2 Preliminaries Ma et al ( 2018 ) propose a novel neural network architecture whose main backbone is a pointer network ( Vinyals et al , 2015 ) .", "entities": [[19, 21, "MethodName", "pointer network"]]}
{"text": "w i on top of the stack , as the pointer network just returns a position p from a given sentence , they proceed as follows to determine which transition should be applied :", "entities": [[10, 12, "MethodName", "pointer network"]]}
{"text": "More in detail , for each parsing configuration c t , the decoder ( implemented as a uni - directional LSTM ) receives the encoder hidden state e i of the word", "entities": [[20, 21, "MethodName", "LSTM"]]}
{"text": "i = score ( d t , s i ) ( 1 ) a t = sof tmax ( v t ) ( 2 )", "entities": [[16, 17, "DatasetName", "sof"]]}
{"text": "They prove that these additions improve final accuracy , especially when children are attached in an inside - out fashion .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Thus , the conditional probability P \u03b8 ( y | x ) of the dependency tree y for an input sentence x can be factorized according to this top - down structure as : P \u03b8 ( y | x )", "entities": [[6, 7, "HyperparameterName", "\u03b8"], [35, 36, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b8 ( p i |", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "k i=1 l i j=1 P \u03b8 ( w i , j |", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "w i , < j , p < i , x ) where \u03b8 represents model parameters , p < i stands for previous paths already explored , w i , j denotes the jth word in path", "entities": [[13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "For more thorough details of the stack - pointer network architecture and the top - down transition system , please read the original work by Ma et al ( 2018 ) .", "entities": [[8, 10, "MethodName", "pointer network"]]}
{"text": "Note that , in our algorithm , p can equal 0 , attaching , in that case , w i to the dummy root node .", "entities": [[10, 11, "DatasetName", "0"]]}
{"text": "In dependency parsing , a tree for an input sentence of length n can be represented as a set of n directed and binary links l 1 , . . .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}
{"text": "Therefore , to train this novel variant , we factorize the conditional probability P \u03b8 ( y | x ) to a set of head - dependent pairs as follows : P \u03b8 ( y | x )", "entities": [[14, 15, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b8"]]}
{"text": "n i=1 P \u03b8 ( l i |", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "n i=1 P \u03b8 ( w h", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "We use the same implementation as Ma et al ( 2018 ) and conduct experiments on the Stanford Dependencies ( de Marneffe and Manning , 2008 ) conversion ( using the Stanford parser v3.3.0 ) 3 of the English Penn Treebank ( Marcus et al , 1993 ) , with standard splits and predicted PoS tags .", "entities": [[39, 41, "DatasetName", "Penn Treebank"]]}
{"text": "In addition , we compare our approach to the original top - down parser on the same twelve languages from the Universal Dependency Treebanks 4 ( UD ) that were used by Ma et al ( 2018 ) .", "entities": [[26, 27, "DatasetName", "UD"]]}
{"text": "5 Following standard practice , we just exclude punctuation for evaluating on PTB - SD and , for each experiment , we report the average Labelled and Unlabelled Attachment Scores ( LAS and UAS ) over 3 and 5 repetitions for UD and PTB - SD , respectively .", "entities": [[12, 13, "DatasetName", "PTB"], [41, 42, "DatasetName", "UD"], [43, 44, "DatasetName", "PTB"]]}
{"text": "3 https://nlp.stanford.edu/software/ lex - parser.shtml 4 http://universaldependencies.org 5 Please note that , since they used a former version of UD datasets , we reran also the top - down algorithm on the latest treebank version ( 2.2 ) in order to perform a fair comparison .", "entities": [[19, 20, "DatasetName", "UD"]]}
{"text": "Finally , we use the same hyper - parameter values , pre - trained word embeddings and beam size ( 10 for PTB - SD and 5 for UD ) as Ma et al ( 2018 ) .", "entities": [[14, 16, "TaskName", "word embeddings"], [22, 23, "DatasetName", "PTB"], [28, 29, "DatasetName", "UD"]]}
{"text": "By outperforming the two current state - of - theart graph - based ( Dozat and Manning , 2016 ) and transition - based ( Ma et al , 2018 ) models on the PTB - SD , our approach becomes the most accurate fully - supervised dependency parser developed so far , as shown in Table 1 . 6", "entities": [[34, 35, "DatasetName", "PTB"]]}
{"text": "Finally , in spite of requiring a cycle - checking procedure , our approach proves to be twice as fast as the top - down alternative in decoding time , achieving , under the exact same conditions , a 23.08 - sentences - per - second speed on the PTB - SD compared to 10.24 of the original system .", "entities": [[49, 50, "DatasetName", "PTB"]]}
{"text": "There is previous work that proposes to implement dependency parsing by independently selecting the head of each word in a sentence , using neural networks .", "entities": [[8, 10, "TaskName", "dependency parsing"]]}
{"text": "In particular , Zhang et al ( 2017 ) make use of a BiLSTM - based neural architecture to compute the probability of attaching each word to one of the other input words , in a similar way as pointer networks do .", "entities": [[13, 14, "MethodName", "BiLSTM"]]}
{"text": "Our approach does not need this postprocessing , as cycles are forbidden during parsing instead , and achieves a higher accuracy thanks to the pointer network architecture and the use of information about previous dependencies .", "entities": [[20, 21, "MetricName", "accuracy"], [24, 26, "MethodName", "pointer network"]]}
{"text": "Before Ma et al ( 2018 ) presented their topdown parser , Chorowski et al ( 2017 ) had already employed pointer networks ( Vinyals et al , 2015 ) for dependency parsing .", "entities": [[31, 33, "TaskName", "dependency parsing"]]}
{"text": "Concretely , they developed a pointer - network - based neural architecture with multitask learning able to perform preprocessing , tagging and dependency parsing exclusively by reading tokens from an input sen - tence , without needing POS tags or pre - trained word embeddings .", "entities": [[22, 24, "TaskName", "dependency parsing"], [43, 45, "TaskName", "word embeddings"]]}
{"text": "Apart from doubling their system 's speed , our approach proves to be a competitive alternative on a variety of languages and achieves the best accuracy to date on the PTB - SD .", "entities": [[25, 26, "MetricName", "accuracy"], [30, 31, "DatasetName", "PTB"]]}
{"text": "Automatic Fake News Detection : Are current models \" fact - checking \" or \" gut - checking \" ?", "entities": [[1, 4, "TaskName", "Fake News Detection"]]}
{"text": "Automatic fake news detection models are ostensibly based on logic , where the truth of a claim made in a headline can be determined by supporting or refuting evidence found in a resulting web query .", "entities": [[1, 4, "TaskName", "fake news detection"]]}
{"text": "This implies that other signals are contained within the examined evidence , and could be based on manipulable factors such as emotion , sentiment , or part - of - speech ( POS ) frequencies , which are vulnerable to adversarial inputs .", "entities": [[21, 22, "DatasetName", "emotion"], [26, 29, "DatasetName", "part - of"]]}
{"text": "We neutralize some of these signals through multiple forms of both neural and non - neural pre - processing and style transfer , and find that this flattening of extraneous indicators can induce the models to actually require both claims and evidence to perform well .", "entities": [[20, 22, "TaskName", "style transfer"]]}
{"text": "We conclude with the construction of a model using emotion vectors built off a lexicon and passed through an \" emotional attention \" mechanism to appropriately weight certain emotions .", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "This finding indicates a problem with current automatic fake news detection , signaling that the models rely on features in the evidence typical to fake news , rather than using entailment .", "entities": [[8, 11, "TaskName", "fake news detection"]]}
{"text": "Since most automated fact - checking research is primarily concerned with the accuracy of the results , rather than addressing how the results are achieved , we propose a novel investigation into these models and their evidence .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "Style transfer neural models using the Styleformer model to perform informal - to - formal and formal - to - informal paraphrasing methods ( Li et al , 2018 ; Schmidt , 2020 ) .", "entities": [[0, 2, "TaskName", "Style transfer"]]}
{"text": "We also develop our own BERT - based model as an extension of the EmoCred system ( Giachanou Figure 1 : An example of why evidence alone does not suffice in identifying fake news , despite the evidence being conditioned on the claim as a search - engine query .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "Current fake news detection models that use a claim 's search engine results as evidence may unintentionally use hidden signals that are not attributed to the claim ( Hansen et al , 2021 ) .", "entities": [[1, 4, "TaskName", "fake news detection"]]}
{"text": "Using the claim as a query , the top ten results from Google News ( \" snippets \" ) constitute the evidence ( Hansen et al , 2021 ) .", "entities": [[12, 13, "DatasetName", "Google"]]}
{"text": "PolitiFact and Snopes use five labels ( False , Mostly False , Mixture , Mostly True , True ) , which we collapse to True , Mixture , and False .", "entities": [[0, 1, "DatasetName", "PolitiFact"], [2, 3, "DatasetName", "Snopes"]]}
{"text": "To construct the emotion vectors for our EmoAttention system , we use the NRC Affect Intensity Lexicon , which maps approximately 6 , 000 terms to values between 0 and 1 , representing the term 's intensity along 8 different emotions ( Mohammad , 2017 ) .", "entities": [[3, 4, "DatasetName", "emotion"], [28, 29, "DatasetName", "0"]]}
{"text": "The most common automatic fact - checking NLP models are based on term frequency , word embeddings , and contextualized word embeddings , using Random Forests , LSTMs , and BERT ( Hassan et al , 2017 ) .", "entities": [[15, 17, "TaskName", "word embeddings"], [20, 22, "TaskName", "word embeddings"], [30, 31, "MethodName", "BERT"]]}
{"text": "We limit our experimentation to the BERT model , as it is the highest performing state - of - the - art model and was thoroughly tested in ( Hansen et al , 2021 ) .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "This BERT model with no pre - processing is our baseline model .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "For the style transfer model we use the Styleformer model ( Li et al , 2018 ; Schmidt , 2020 ) , a Transformer - based seq2seq model .", "entities": [[2, 4, "TaskName", "style transfer"], [23, 24, "MethodName", "Transformer"], [26, 27, "MethodName", "seq2seq"]]}
{"text": "We also develop our own BERT - based model using the EmoLexi and EmoInt implementation of the EmoCred system by adding an emotional attention layer to emphasize certain emotion representations for a given claim and its evidence ( Giachanou et al , 2019 ) .", "entities": [[5, 6, "MethodName", "BERT"], [28, 29, "DatasetName", "emotion"]]}
{"text": "These steps were chosen as they have been shown to facilitate affective tasks such as sentiment analysis , emotion classification , and sarcasm detection ( Babanejad et al , 2020 ) .", "entities": [[15, 17, "TaskName", "sentiment analysis"], [18, 20, "TaskName", "emotion classification"], [22, 24, "TaskName", "sarcasm detection"]]}
{"text": "We use the adversarial technique of generating paraphrases for all the claims and evidence through style transfer .", "entities": [[15, 17, "TaskName", "style transfer"]]}
{"text": "These eight emotions can be used to create an emotion vector for a sentence , where each index corresponds to a score : [ anger , anticipation , disgust , fear , joy , sadness , surprise , trust ] .", "entities": [[9, 10, "DatasetName", "emotion"]]}
{"text": "We create the vector of length eight , and for each word associated with an emotion , the emotion 's indexed value is either : ( 1 ) incremented by one for EmoLexi ; or , ( 2 ) incremented by its intensity for EmoInt .", "entities": [[15, 16, "DatasetName", "emotion"], [18, 19, "DatasetName", "emotion"]]}
{"text": "Thus , the sentence \" He had an affection for suffering \" would have an EmoLexi emotion vector of [ 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 ] and an EmoInt emotion vector of [ 0 , 0 , 0 , 0 , 0.647 , 0.844 , 0 , 0 ] We build on this EmoCred framework , adding an attention system for emotion that gives a weight to each emotion vector , just as the attention layer for each snippet gives a weight to each snippet .", "entities": [[16, 17, "DatasetName", "emotion"], [20, 21, "DatasetName", "0"], [22, 23, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [32, 33, "DatasetName", "0"], [34, 35, "DatasetName", "0"], [39, 40, "DatasetName", "emotion"], [43, 44, "DatasetName", "0"], [45, 46, "DatasetName", "0"], [47, 48, "DatasetName", "0"], [49, 50, "DatasetName", "0"], [55, 56, "DatasetName", "0"], [57, 58, "DatasetName", "0"], [71, 72, "DatasetName", "emotion"], [78, 79, "DatasetName", "emotion"]]}
{"text": "Surprisingly , the four top - performing models with the Snopes dataset include two non - neural models and two neural models .", "entities": [[10, 11, "DatasetName", "Snopes"]]}
{"text": "POS and STOP yield the biggest delta between S C&E vs. S E , followed by EmoInt and Informal Style Transfer .", "entities": [[19, 21, "TaskName", "Style Transfer"]]}
{"text": "However , EmoInt yields the highest F1 Macro , followed by POS , Informal , and STOP .", "entities": [[6, 8, "MetricName", "F1 Macro"]]}
{"text": "In PolitiFact , none of the pre - processing steps achieve a delta greater than zero for S C&E versus S E .", "entities": [[1, 2, "DatasetName", "PolitiFact"]]}
{"text": "For the best F1 Macro scores overall , EmoAttention 's two forms ( i.e. , EmoInt and EmoLexi ) were the two best , followed by STOP Macro scores and deltas are in red .", "entities": [[3, 5, "MetricName", "F1 Macro"]]}
{"text": "With the exception of EmoLexi tying for the lowest delta , the best pre - processing steps outperform the baseline BERT model from Hansen et al ( 2021 ) . and POS .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "All of these pre - processing steps achieve higher F1 Macro scores than the baseline BERT model .", "entities": [[9, 11, "MetricName", "F1 Macro"], [15, 16, "MethodName", "BERT"]]}
{"text": "Many pre - processing steps increase both the model 's F1 scores and its need for claims and evidence , validating our hypothesis that signals in style and tone have become a crutch for factchecking models .", "entities": [[10, 11, "MetricName", "F1"]]}
{"text": "Rather than doing entailment , they are leveraging other signals - perhaps similar to sentiment analysis - and relying on a \" gut feeling \" .", "entities": [[14, 16, "TaskName", "sentiment analysis"]]}
{"text": "Automatic Fake News detection remains a challenging problem , and unfortunately , current fact - checking models can be subverted by adversarial techniques that exploit emotionally charged writing .", "entities": [[1, 4, "TaskName", "Fake News detection"]]}
{"text": "Figure 4 : The full table of results for all pre - processing steps for the Snopes ( SNES ) and PolitiFact ( POMT ) datasets .", "entities": [[16, 17, "DatasetName", "Snopes"], [21, 22, "DatasetName", "PolitiFact"]]}
{"text": "Due to the high compute requirements of the formal and informal style transfer models , these datasets were only prepared for the Snopes dataset .", "entities": [[11, 13, "TaskName", "style transfer"], [22, 23, "DatasetName", "Snopes"]]}
{"text": "Currently , the tool takes into account only lexical information and defines the CEFR level of each particular lemma based on CEFR vocabulary lists ( see Chapter 3.1 ) .", "entities": [[18, 19, "DatasetName", "lemma"]]}
{"text": "The program runs on EstNLTK v 1.6 , which offers functionality to lemmatise and perform morphological analysis .", "entities": [[15, 17, "TaskName", "morphological analysis"]]}
{"text": "\" Teacher 's Tools \" as a resource can be used for different CALL tasks , including auto - matic CEFR - related vocabulary and grammar exercise generation or lexical simplification tasks .", "entities": [[29, 31, "TaskName", "lexical simplification"]]}
{"text": "Arguably , spoken dialogue systems are most often used not in hands / eyes - busy situations , but rather in settings where a graphical display is also available , such as a mobile phone .", "entities": [[2, 5, "TaskName", "spoken dialogue systems"]]}
{"text": "By visualising the current dialogue state and possible continuations of it as a simple tree , and allowing interaction with that visualisation ( e.g. , for confirmations or corrections ) , the system provides both feedback on past user actions and guidance on possible future ones , and it can span the continuum from slot filling to full prediction of user intent ( such as GoogleNow ) .", "entities": [[54, 56, "TaskName", "slot filling"]]}
{"text": "This section introduces and describes our SDS , which is modularised into four main components : ASR , natural language understanding ( NLU ) , dialogue management ( DM ) , and the graphical user interface ( GUI ) which , as explained below , is visualised as a right - branching tree .", "entities": [[18, 21, "TaskName", "natural language understanding"], [25, 27, "TaskName", "dialogue management"]]}
{"text": "The IUs themselves are interconnected via so - called same level links ( SLL ) and groundedin links ( GRIN ) , the former allowing the linking of IUs as a growing sequence , the latter allowing that sequence to convey what IUs directly affect it ( see Figure 2 for an example of incremental ASR ) .", "entities": [[19, 20, "MethodName", "GRIN"]]}
{"text": "We opt for Google ASR for its vocabulary coverage of our evaluation language ( German ) .", "entities": [[3, 4, "DatasetName", "Google"]]}
{"text": "Following , Baumann et al ( 2016 ) , we package output from the Google service into IUs which are passed to the NLU module , which we now explain .", "entities": [[14, 15, "DatasetName", "Google"]]}
{"text": "For all other w , C ( w | r ) = 0 .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "It is currently implemented as a web - based interface ( using the JavaScript D3 library ) , allowing it to be usable as a web application on any machine or mobile device .", "entities": [[14, 15, "DatasetName", "D3"]]}
{"text": "We used the Mint tools data collection framework ( Kousidis et al , 2012 ) to log the interactions .", "entities": [[3, 4, "DatasetName", "Mint"]]}
{"text": "The adaptivity moves our system from one extreme of the continuum - simple slot filling - closer towards the extreme that is fully predictive , with the additional benefit of being able to easily correct mistakes in the predictions .", "entities": [[13, 15, "TaskName", "slot filling"]]}
{"text": "The following questions were used on the general questionnaire : I regularly use personal assistants such as Siri , Cortana , Google now or Amazon Echo : Yes / No I have never used a speech - based personal assistant :", "entities": [[21, 22, "DatasetName", "Google"]]}
{"text": "Evaluating Word Embeddings for Language Acquisition", "entities": [[1, 3, "TaskName", "Word Embeddings"], [4, 6, "TaskName", "Language Acquisition"]]}
{"text": "Continuous vector word representations ( or word embeddings ) have shown success in capturing semantic relations between words , as evidenced by evaluation against behavioral data of adult performance on semantic tasks ( Pereira et al , 2016 ) .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Adult semantic knowledge is the endpoint of a language acquisition process ; thus , a relevant question is whether these models can also capture emerging word representations of young language learners .", "entities": [[8, 10, "TaskName", "language acquisition"]]}
{"text": "In this paper , we propose to bridge this gap by using Age of Acquisition norms to evaluate word embeddings learnt from child - directed input .", "entities": [[18, 20, "TaskName", "word embeddings"]]}
{"text": "We present two methods that evaluate word embeddings in terms of ( a ) the semantic neighbourhood density of learnt words , and ( b ) convergence to adult word associations .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Word embeddings have a long tradition in Computational Linguistics .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "There exist a range of methods to derive word embeddings based on the distributional paradigm , such that words with similar embeddings are semantically related .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "Adult semantic knowledge is the culmination of a language acquisition process ; therefore , a relevant question is whether these models can also capture emerging word representations of language learners .", "entities": [[8, 10, "TaskName", "language acquisition"]]}
{"text": "A capacity for distributional analysis is a basic assumption of all theories of language acquisition : children are capable of performing distributional analyses over their input from a young age ( Saffran et al , 1996 ) , motivating the use of word embeddings for modelling language acquisition .", "entities": [[13, 15, "TaskName", "language acquisition"], [42, 44, "TaskName", "word embeddings"], [46, 48, "TaskName", "language acquisition"]]}
{"text": "This paper presents two methods for evaluating word embeddings for language acquisition .", "entities": [[7, 9, "TaskName", "word embeddings"], [10, 12, "TaskName", "language acquisition"]]}
{"text": "Bag - of - words models offer a good starting point to evaluate word representations in the context of language acquisition , given their minimal assumptions on knowledge of word order : once the context of a word is determined , the order in which words appear in this context is ignored by these type of models .", "entities": [[19, 21, "TaskName", "language acquisition"]]}
{"text": "We explore a range of hyperparameter configurations of two models : a ' contextcounting ' model involving a PPMI matrix compressed with Singular Value Decomposition ( SVD ) , and the Skipgram with Negative Sampling ( SGNS ) version of word2vec ( Mikolov et al , 2013 ) .", "entities": [[18, 19, "DatasetName", "PPMI"], [26, 27, "DatasetName", "SVD"]]}
{"text": "Note that , although these models have been found to implicitly optimize the same shifted - PPMI matrix ( Levy and Goldberg , 2014 ) , they are unlikely to obtain the same results without careful parameter alignment .", "entities": [[16, 17, "DatasetName", "PPMI"]]}
{"text": "The hyperparameters we explore include : window size [ 1 , 2 , 3 , 4 , 5 , 7 , 10 ] , minimum frequency threshold [ 10 , 50 , 100 ] , dynamic window ( for SGNS ) , negative sampling in SGNS [ 0 , 15 ] ( and its equivalents as shifted - PPMI ) , eigenvalue in SVD [ 0 , 0.5 , 1 ] .", "entities": [[47, 48, "DatasetName", "0"], [58, 59, "DatasetName", "PPMI"], [63, 64, "DatasetName", "SVD"], [65, 66, "DatasetName", "0"]]}
{"text": "We extracted the child - directed speech data from the CHILDES database ( MacWhinney , 2000 ) , for all the varieties of English , for ages ranging from 0 to 60 months .", "entities": [[29, 30, "DatasetName", "0"]]}
{"text": "Word tokens were coded at the lemma level .", "entities": [[6, 7, "DatasetName", "lemma"]]}
{"text": "In their work , the authors modeled the emerging network of semantic associations that children build during language acquisition .", "entities": [[17, 19, "TaskName", "language acquisition"]]}
{"text": "This index has been repeatedly shown to predict language acquisition phenomena , such as the age of acquisition of words in different syntactic categories ( Hills et al , 2010 ;", "entities": [[8, 10, "TaskName", "language acquisition"]]}
{"text": "Because of the binarization of the co - occurrence matrix , the CD index is an indicator of type co - occurrences , and is therefore agnostic to co - occurrence frequency .", "entities": [[3, 4, "TaskName", "binarization"]]}
{"text": "Among the hyperparameters of these models , one that is particularly relevant to language acquisition is the window size , as this reveals the amount of context that children most likely attend to in the analyzed ages .", "entities": [[13, 15, "TaskName", "language acquisition"]]}
{"text": "Our first evaluation method above focused on the structure of the semantic spaces provided by the learnt word embeddings .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "Instead , we computed the Spearman rank correlation between the number of overlaps and the AoA norms , in order to quantify whether word embeddings corresponding to words learned earlier by children are also those that are converging faster to adult semantic knowledge .", "entities": [[23, 25, "TaskName", "word embeddings"]]}
{"text": "The figure also shows whether this metric favours a model that did not perform well in our previous evaluation metric ( SVD with window size 4 , shift 15 , frequency threshold 10 ) .", "entities": [[21, 22, "DatasetName", "SVD"]]}
{"text": "We proposed two methods to evaluate word embeddings for language acquisition .", "entities": [[6, 8, "TaskName", "word embeddings"], [9, 11, "TaskName", "language acquisition"]]}
{"text": "The main feature of these methods is the use of AoA norms for assessing whether the semantic organization of the word embeddings support the developmental trajectory of word learning .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "Tag Assisted Neural Machine Translation of Film Subtitles", "entities": [[3, 5, "TaskName", "Machine Translation"]]}
{"text": "We implemented a neural machine translation system that uses automatic sequence tagging to improve the quality of translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}
{"text": "Neural machine translation ( NMT ) uses neural networks to translate unannotated text between a source and target language , but without additional linguistic information certain ambiguous inputs may be translated incorrectly .", "entities": [[1, 3, "TaskName", "machine translation"]]}
{"text": "In addition to the bare token sequences , part - of - speech or named entity annotation of each token , provided manually or automatically , could provide additional information to improve the quality of translation .", "entities": [[8, 11, "DatasetName", "part - of"]]}
{"text": "NLP tools include part - of - speech ( POS ) taggers , identifying the syntactic function of each input token , and named entity recognition systems .", "entities": [[3, 6, "DatasetName", "part - of"], [23, 26, "TaskName", "named entity recognition"]]}
{"text": "Named entity recognition ( NER ) identifies which tokens refer to named entities , including proper nouns such as people , place names , organizations , or dates .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"]]}
{"text": "Recently , automatic named entity recognition ( NER ) systems have seen much development and refinement with the same deep learning tools used for NMT ( Li et al , 2020 ) .", "entities": [[3, 6, "TaskName", "named entity recognition"], [7, 8, "TaskName", "NER"]]}
{"text": "NER tags produced by these systems are useful in many other natural language processing contexts , such as coreference resolution , entity linking , or entity extraction ( Ferreira Cruz et", "entities": [[0, 1, "TaskName", "NER"], [18, 20, "TaskName", "coreference resolution"], [21, 23, "TaskName", "entity linking"]]}
{"text": "Adding tag information to the predic - tion and corresponding training loss encourages the model to incorporate this information into its latent representations to improve outputs .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "Later , especially with neural machine translation ( NMT ) systems , source - side feature augmentation research studied the inclusion of linguistic feature information into the source - side token embeddings , usually by adding in or concatenating additional learned feature vectors to the token embedding vectors , as we do in this work ( Sennrich and Haddow , 2016 ; Hoang et al , 2016b ; Ugawa et al , 2018 ; Modrzejewski , 2020 ; Armengol - Estap\u00e9 et al , 2020 ) .", "entities": [[5, 7, "TaskName", "machine translation"]]}
{"text": "Factored translation systems , under both statistical and neural machine translation , instead explore the addition of externally supplied linguistic features to the raw text at both input and output .", "entities": [[9, 11, "TaskName", "machine translation"]]}
{"text": "These features include part - of - speech ( POS ) tags , word lemmatizations , morphological analysis , and semantic analysis ( Koehn and Hoang , 2007 ;", "entities": [[3, 6, "DatasetName", "part - of"], [16, 18, "TaskName", "morphological analysis"]]}
{"text": "Factored translation models map feature - augmented input into feature - augmented output , however outputs include only an underlying lemma together with the predicted features .", "entities": [[20, 21, "DatasetName", "lemma"]]}
{"text": "This work showed negative results for various syntactic tag types on IWSLT'14 shared task data ( Cettolo et al , 2014 ) , whereas this work presents NER and POS tags on film subtitles data .", "entities": [[27, 28, "TaskName", "NER"]]}
{"text": "We implemented two extensions to the standard seq2seq encoder - decoder architecture for neural machine translation to use token - level tags to improve translation results .", "entities": [[7, 8, "MethodName", "seq2seq"], [14, 16, "TaskName", "machine translation"]]}
{"text": "We used a Transformer encoder and decoder for the base seq2seq model ( Vaswani et al , 2017 ) .", "entities": [[3, 4, "MethodName", "Transformer"], [10, 11, "MethodName", "seq2seq"]]}
{"text": "The Opus project provided a parallel German to English subtitles corpus from OpenSubtitles ( Tiedemann , 2012 ; Aulamo et al , 2020 ) .", "entities": [[12, 13, "DatasetName", "OpenSubtitles"]]}
{"text": "Around 3 % of words in the OpenSubtitles corpus were tagged as named entities ( non O ) .", "entities": [[7, 8, "DatasetName", "OpenSubtitles"]]}
{"text": "We used a Transformer encoder and decoder ( Vaswani et al , 2017 ) for the base seq2seq system , each with 6 layers and 8 attention heads , and layer and embedding dimensions 512 .", "entities": [[3, 4, "MethodName", "Transformer"], [17, 18, "MethodName", "seq2seq"]]}
{"text": "There are two ways to understand this in comparison with NER tags .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "First , POS tags carry a significant amount of information about the sentence , not only helping to disambiguate between different word senses by part - of - speech , but also assisting the model with encoding the sentence 's syntactic structure .", "entities": [[24, 27, "DatasetName", "part - of"]]}
{"text": "Compared to NER tags , this amount of structural information might be difficult to model with the same decoder architecture used for token prediction .", "entities": [[2, 3, "TaskName", "NER"]]}
{"text": "Second , POS tags tend to carry the same amount of information for each tag at each position , compared to NER tags only conveying most of their information at the named entity spans which are few and far between .", "entities": [[21, 22, "TaskName", "NER"]]}
{"text": "This also lends itself to the idea that POS tags have a higher information content that is less easily modeled by the decoder , leading to worse results than NER tagging .", "entities": [[29, 30, "TaskName", "NER"]]}
{"text": "For both NER and POS tagged results , the baseline was the same Transformer architecture trained only on untagged data ( without adding tag embeddings or predicting tags from the decoder ) .", "entities": [[2, 3, "TaskName", "NER"], [13, 14, "MethodName", "Transformer"]]}
{"text": "For NER tagging this was an improvement in BLEU scores , but for POS tagging scores decreased when adding target tagging .", "entities": [[1, 2, "TaskName", "NER"], [8, 9, "MetricName", "BLEU"]]}
{"text": "Whereas source - side tag information is added into the embeddings without any modification to the training objective , target - side tag predictions are a part of the modified training loss , so that it is the target - side tag prediction that pushes the model to incorporate accurate knowledge of the tags into its learning representations .", "entities": [[31, 32, "MetricName", "loss"]]}
{"text": "That NER tag modeling improved results while POS tag modeling did not is consistent with our earlier observation that POS tag modeling seems to be more difficult than NER tag modeling , and is not done effectively by the current architecture .", "entities": [[1, 2, "TaskName", "NER"], [28, 29, "TaskName", "NER"]]}
{"text": "Looking only at tag - level cross - entropy , it 's interesting to notice that the POS tagging loss is significantly higher than the NER tagging loss .", "entities": [[19, 20, "MetricName", "loss"], [25, 26, "TaskName", "NER"], [27, 28, "MetricName", "loss"]]}
{"text": "While this could be simply because the lower - bound inherent entropy is higher ( POS tags naturally contain more information , being more uniformly distributed than NER tags ) , this could also be consistent with the idea that POS tag modeling is more difficult , explaining the decreased translation scores observed with POS tag prediction .", "entities": [[27, 28, "TaskName", "NER"]]}
{"text": "It should not go unnoticed that the typical inference algorithms for sequence labeling , particularly the BiLSTM - CRF inference employed by most NER systems , are incompatible with the autoregressive sequence decoding algorithms ( greedy decoding and beam search ) used for inference by seq2seq models .", "entities": [[16, 17, "MethodName", "BiLSTM"], [18, 19, "MethodName", "CRF"], [23, 24, "TaskName", "NER"], [45, 46, "MethodName", "seq2seq"]]}
{"text": "We also imagine that the design of the underlying seq2seq architecture may lend itself to certain types of sequence labeling .", "entities": [[9, 10, "MethodName", "seq2seq"]]}
{"text": "For example , the bidirectional context modeled by a BiLSTM - based translation model may be more suitable for certain types of sequence labeling tasks than the Transformer 's attentional activations .", "entities": [[9, 10, "MethodName", "BiLSTM"], [27, 28, "MethodName", "Transformer"]]}
{"text": "Because our contributions are agnostic to the type of sequence labeling ( NER or part - of - speech tagging or any other kind ) as well as to the design of the encoder and decoder , future experiments should also explore these possibilities .", "entities": [[12, 13, "TaskName", "NER"], [14, 20, "TaskName", "part - of - speech tagging"]]}
{"text": "We implemented extensions to existing neural machine translation models that allow the use of offthe - shelf token - level tagging systems to improve translation accuracy .", "entities": [[6, 8, "TaskName", "machine translation"], [25, 26, "MetricName", "accuracy"]]}
{"text": "Translation inputs and training outputs were tagged with pre - trained sequence labeling systems .", "entities": [[0, 1, "TaskName", "Translation"]]}
{"text": "At model output , the final decoder layer used separate softmax layers to predict tokens and tags .", "entities": [[10, 11, "MethodName", "softmax"]]}
{"text": "During training , a combined loss function encouraged the model to learn token and tag information jointly .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "The models we tested were 1 ) Multinomial Naive Bayes ( McCallum et al , 1998 ) with unigram attributes and 2 ) L2regularised logistic regression models with different word n - gram features , as implemented in LIB - LINEAR 2 .", "entities": [[24, 26, "MethodName", "logistic regression"]]}
{"text": "Overall , logistic regression with unigrams performed the worst , yielding ( slightly ) lower values for all three measures .", "entities": [[2, 4, "MethodName", "logistic regression"]]}
{"text": "We will train popular word embeddings algorithms on the MLT Corpus , such as Word2Vec ( Mikolov et al , 2013 ) and FastText ( Bojanowski et al , 2017 ) , and identify words that are close to our loanwords in the semantic space .", "entities": [[4, 6, "TaskName", "word embeddings"], [23, 24, "MethodName", "FastText"]]}
{"text": "Finally , we hope to extrapolate these findings by deploying our trained classifier on other online discourse sources , such as Reddit posts .", "entities": [[21, 22, "DatasetName", "Reddit"]]}
{"text": "For disambiguation , the parser associates words with BiLSTM vectors and utilizes these vectors to assign scores to candidate dependencies .", "entities": [[8, 9, "MethodName", "BiLSTM"]]}
{"text": "We conduct experiments on the data sets from Se - mEval 2015 as well as Chinese CCGBank .", "entities": [[16, 17, "DatasetName", "CCGBank"]]}
{"text": "Semantic Dependency Parsing ( SDP ) is defined as the task of recovering sentence - internal bilexical semantic dependency structures , which encode predicate - argument relationships for all content words .", "entities": [[1, 3, "TaskName", "Dependency Parsing"]]}
{"text": "There are two key dimensions of the data - driven dependency parsing approach : decoding and disambiguation .", "entities": [[10, 12, "TaskName", "dependency parsing"]]}
{"text": "Based on an efficient first - order Maximum Subgraph decoder , we implement a data - driven parser that scores arcs based on stacked bidirectional - LSTM ( BiLSTM ) together with a multi - layer perceptron .", "entities": [[26, 27, "MethodName", "LSTM"], [28, 29, "MethodName", "BiLSTM"]]}
{"text": "Using the benchmark data sets from the SemEval 2015 Task 18 ( Oepen et al , 2015 ) , our parser gives very competitive results for English semantic parsing .", "entities": [[1, 3, "DatasetName", "the benchmark"], [27, 29, "TaskName", "semantic parsing"]]}
{"text": "To test the ability for crosslingual parsing , we also conduct experiments on the Chinese CCGBank ( Tse and Curran , 2010 ) and Enju HPSGBank ( Yu et al , 2010 ) data .", "entities": [[15, 16, "DatasetName", "CCGBank"]]}
{"text": "Most studies on semantic parsing focused on the in - domain setting , meaning that both training and testing data are drawn from the same domain .", "entities": [[3, 5, "TaskName", "semantic parsing"]]}
{"text": "Figure 1 shows an Minimal Recursion Semantics ( MRS ; Copestake et al , 2005 ) reduced semantic dependency analysis ( Ivanova et al , 2012 ) .", "entities": [[8, 9, "DatasetName", "MRS"]]}
{"text": "Dependency parsing , thus , can be formulated as the search for a maximum spanning tree ( MST ) from an arcweighted ( complete ) graph .", "entities": [[0, 2, "TaskName", "Dependency parsing"]]}
{"text": "In general , dependency parsing is formulated as the search for Maximum Subgraph regarding to a particular graph class , viz .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}
{"text": "Inspired by Costa - juss\u00e0 and Fonollosa ( 2016 ) 's work , we utilize character - based embedding for low - frequency words , i.e. , words that appear more than k times in the training data , and word - based embeddings for other words .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "c n ) within a character - based BiLSTM : x 1 :", "entities": [[8, 9, "MethodName", "BiLSTM"]]}
{"text": "x n = BiLSTM ( c 1 : c n )", "entities": [[3, 4, "MethodName", "BiLSTM"]]}
{"text": "i r 1 : r n = BiLSTM ( a 1 : a n )", "entities": [[7, 8, "MethodName", "BiLSTM"]]}
{"text": "= W 2 ReLU ( W 1 , 1 r i + W 1 , 2 r j + b )", "entities": [[3, 4, "MethodName", "ReLU"]]}
{"text": "= arg max W 2 ReLU ( W 1 , 1 r i + W 1 , 2 r j + b ) + b 2 We can see here the two local score functions explicitly utilize the positions of a semantic head and a semantic dependent .", "entities": [[5, 6, "MethodName", "ReLU"]]}
{"text": "In order to update graphs which achieve high model scores but are actually wrong , we use a margin - based approach to compute loss from the gold graph G * and the best prediction\u011c under current model .", "entities": [[24, 25, "MetricName", "loss"]]}
{"text": "Different from data - driven syntactic parsing , semantic parsing for the first type of annotation can leverage a precision grammar - guided model .", "entities": [[8, 10, "TaskName", "semantic parsing"]]}
{"text": "For example , using ERG ( Flickinger , 2000 ) , which provides precise linguistic analyses for a broad range of phenomena , as the the core engine , PET 2 ( Callmeier , 2000 ) and ACE 3 produce better results than all existing datadriven semantic parsers for sentences that can be parsed by ERG .", "entities": [[29, 30, "DatasetName", "PET"]]}
{"text": "Even for treebanking on the newswire data , i.e. , the Wall Street Journal data from Penn TreeBank , ERG lacks analyses for c.a . 11 % sentences ( Oepen et al , 2015 ) .", "entities": [[16, 18, "DatasetName", "Penn TreeBank"]]}
{"text": "Zhang and Wang ( 2009 ) proposed to derive features from syntactic parses generated by PET to assist a data - driven dependency tree parser and observed some encouraging results for cross - domain evaluation .", "entities": [[15, 16, "DatasetName", "PET"]]}
{"text": "A considerable number of sentences can not benefit from ERG since PET may produce no analysis .", "entities": [[11, 12, "DatasetName", "PET"]]}
{"text": "When processing an imperfect sentence , we do not take a loss into consideration if the loss of this sentence is too small .", "entities": [[11, 12, "MetricName", "loss"], [16, 17, "MetricName", "loss"]]}
{"text": "( 2 ) Chinese CCGBank", "entities": [[4, 5, "DatasetName", "CCGBank"]]}
{"text": "Ensemble methods have been shown very helpful to boost the accuracy of neural network based parsing .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "Redwoods also includes ( re ) treebanking results of the first 22 sections of the venerable Wall Street Journal ( WSJ ) text and the section of Brown Corpus in the Penn Treebank ( Marcus et al , 1993 ) .", "entities": [[31, 33, "DatasetName", "Penn Treebank"]]}
{"text": "The CCG grounded analysis is from Chinese CCGBank .", "entities": [[7, 8, "DatasetName", "CCGBank"]]}
{"text": "Both data sets are transformed from Chinese TreeBank with two rich sets of heuristic rules ( Yu et al , 2010 ; Tse and Curran , 2010 ) .", "entities": [[6, 8, "DatasetName", "Chinese TreeBank"]]}
{"text": "In this paper , we consider two POS taggers : a symbol - refined generative HMM tagger ( SR - HMM ) ( Huang et al , 2009 ) and a BiLSTM - CRF model when assisting Chinese SDG .", "entities": [[31, 32, "MethodName", "BiLSTM"], [33, 34, "MethodName", "CRF"]]}
{"text": "For the neural tagging model , in addition to a BiL - STM layer for encoding words , we set a BiLSTM layer for encoding characters , which supports us to derive character - level representations for all words .", "entities": [[21, 22, "MethodName", "BiLSTM"]]}
{"text": "In particular , vectors from the characterlevel LSTM is concatenated with the pre - trained word embedding before feeding into the other word - level BiLSTM network to capture contextual information .", "entities": [[7, 8, "MethodName", "LSTM"], [25, 26, "MethodName", "BiLSTM"]]}
{"text": "The final module of our CRF tagger is a linear chain CRF which scores the output sequence by factoring it in local tag bi - grams .", "entities": [[5, 6, "MethodName", "CRF"], [11, 12, "MethodName", "CRF"]]}
{"text": "This phenomenon is consist with Chinese syntactic parsing , including both constituency and dependency parsing .", "entities": [[13, 15, "TaskName", "dependency parsing"]]}
{"text": "\" ZDSW \" is the system that obtained the best parsing accuracy on the Chinese CCGBank data in the literature .", "entities": [[11, 12, "MetricName", "accuracy"], [15, 16, "DatasetName", "CCGBank"]]}
{"text": "Parsing sentences to linguistically - rich semantic representations is a key goal of Natural Language Understanding .", "entities": [[13, 16, "TaskName", "Natural Language Understanding"]]}
{"text": "We think this method can be re - used for other types of datadriven semantic parsing models .", "entities": [[14, 16, "TaskName", "semantic parsing"]]}
{"text": "Open Relation Extraction : Relational Knowledge Transfer from Supervised Data to Unsupervised Data", "entities": [[1, 3, "TaskName", "Relation Extraction"]]}
{"text": "Open relation extraction ( OpenRE ) aims to extract relational facts from the open - domain corpus .", "entities": [[1, 3, "TaskName", "relation extraction"]]}
{"text": "Relation extraction ( RE ) aims to extract relational facts between two entities from plain texts .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}
{"text": "To lower the level of supervision , several semi - supervised approaches have been developed , including bootstrapping , active learning , label propagation ( Pawar et al , 2017 ) .", "entities": [[19, 21, "TaskName", "active learning"]]}
{"text": "Open relation extraction ( OpenRE ) aims to extract relational facts on the open - domain corpus , where the relation types may not be predefined .", "entities": [[1, 3, "TaskName", "relation extraction"]]}
{"text": "We conduct experiments on real - world RE datasets , FewRel and FewRel - distant , by splitting relations into seen and unseen set , and evaluate our models in supervised , semi - supervised , and distantly - supervised scenarios .", "entities": [[10, 11, "DatasetName", "FewRel"], [12, 13, "DatasetName", "FewRel"]]}
{"text": "The experimental results show that all these RSN models achieve significant improvements in F - measure compared with state - of - the - art baselines .", "entities": [[13, 16, "MetricName", "F - measure"]]}
{"text": "Open Relation Extraction .", "entities": [[1, 3, "TaskName", "Relation Extraction"]]}
{"text": "Relation extraction ( RE ) is an important task in NLP .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}
{"text": "To solve this problem , recently many efforts have been invested in exploring methods for open relation extraction ( OpenRE ) , which aims to discover new relation types from unsupervised open - domain corpora .", "entities": [[16, 18, "TaskName", "relation extraction"]]}
{"text": "Marcheggiani ( 2016 ) proposes a reconstructionbased model discrete - state variational autoencoder for OpenRE via unlabeled instances .", "entities": [[11, 13, "MethodName", "variational autoencoder"]]}
{"text": "Few - shot Learning .", "entities": [[0, 4, "TaskName", "Few - shot Learning"]]}
{"text": "Few - shot learning aims to classify instances with a handful of labeled samples .", "entities": [[0, 4, "TaskName", "Few - shot learning"]]}
{"text": "Many efforts are devoted to few - shot image classification ( Koch et al , 2015 ) and relation classification ( Yuan et al , 2017 ; Han et al , 2018 ) .", "entities": [[5, 10, "TaskName", "few - shot image classification"], [18, 20, "TaskName", "relation classification"]]}
{"text": "Notably , ( Koch et al , 2015 ) introduces Convolu - tional Siamese Neural Network for image metric learning , which inspires us to learn relational similarity metrics for OpenRE .", "entities": [[18, 20, "TaskName", "metric learning"]]}
{"text": "Semi - supervised clustering aims to cluster semantic patterns given instance seeds of target categories ( Bair , 2013 ; Hongtao Lin , 2019 ) .", "entities": [[11, 12, "DatasetName", "seeds"]]}
{"text": "The embedding layer transforms the words in a sentence x and the positions of entities e head and e tail into pre - trained word embeddings and random - initialized position embeddings .", "entities": [[24, 26, "TaskName", "word embeddings"]]}
{"text": "Finally , an FC layer with sigmoid activation maps features into a relational vector v. To summarize , we obtain a vector representation v for a relational sentence with our CNN module : v = CNN ( s ) , ( 1 ) in which we denote the joint information of a sentence x and two entities in it e head and e tail as a data sample s.", "entities": [[6, 8, "MethodName", "sigmoid activation"]]}
{"text": "p [ 0 , 1 ] .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "The layer is a one - dimensionaloutput FC layer with sigmoid activation : p = \u03c3 ( kvd + b ) , ( 4 ) in which \u03c3 denotes the sigmoid function , k and b denote the weights and bias .", "entities": [[10, 12, "MethodName", "sigmoid activation"]]}
{"text": "Thus , we can use binary labels q and binary cross entropy loss to train our RSN : L l = E d l \u223cD l [ q ln ( p \u03b8 ( d l ) )", "entities": [[12, 13, "MetricName", "loss"], [31, 32, "HyperparameterName", "\u03b8"]]}
{"text": "We try to achieve this goal with several additional loss functions .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "Lu = E du\u223cDu [ p \u03b8 ( du ) ln ( p \u03b8 ( du ) )", "entities": [[6, 7, "HyperparameterName", "\u03b8"], [13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "For labeled data , we have L vl = E d l \u223cD l [ DKL ( p \u03b8 ( d l )", "entities": [[18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "And for unlabeled data , we have Lvu = E du\u223cDu [ DKL ( p \u03b8 ( du ) |", "entities": [[15, 16, "HyperparameterName", "\u03b8"]]}
{"text": "| p \u03b8 ( du , t1 , t2 ) )", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "] , ( 8 ) in which the perturbations t 1 and t 2 are added to word embeddings rather than the words themselves .", "entities": [[17, 19, "TaskName", "word embeddings"]]}
{"text": "To summarize , we use the following loss function to train Semi - supervised RSN , which learns from both labeled and unlabeled data :", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "Specifically , we use the following loss function :", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "The reason to remove the loss is simple : virtual adversarial training on auto - labeled data can amplify the noise from false labels .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "Indeed , we do find that the virtual adversarial loss on autolabeled data can harm our model 's performance in experiments .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "We do not use more denoising methods , since we think RSN has some inherent advantages of tolerating such noise .", "entities": [[5, 6, "TaskName", "denoising"]]}
{"text": "To construct the graph , we use the binary approximation of RSN 's output , with 0 indicating an edge between two nodes .", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "However , the relation vectors in our model are high - dimensional , and the distance metric described by RSN is non - linear .", "entities": [[15, 17, "HyperparameterName", "distance metric"]]}
{"text": "In experiments , we use FewRel ( Han et al , 2018 ) as our first dataset .", "entities": [[5, 6, "DatasetName", "FewRel"]]}
{"text": "FewRel is a human - annotated dataset containing 80 types of relations , each with 700 instances .", "entities": [[0, 1, "DatasetName", "FewRel"]]}
{"text": "An advantage of FewRel is that every instance contains a unique entity pair , so RE models can not choose the easy way to memorize the entities .", "entities": [[3, 4, "DatasetName", "FewRel"]]}
{"text": "We use the original train set of FewRel , which contains 64 relations , as labeled set with predefined relations , and the original validation set of FewRel , which contains 16 new relations , as the unlabeled set with novel relations to extract .", "entities": [[7, 8, "DatasetName", "FewRel"], [27, 28, "DatasetName", "FewRel"]]}
{"text": "The second dataset we use is FewRel - distant , which contains the distantly - supervised data obtained by the authors of FewRel before human an - notation .", "entities": [[6, 7, "DatasetName", "FewRel"], [22, 23, "DatasetName", "FewRel"]]}
{"text": "We follow the split of FewRel to obtain the auto - labeled train set and unlabeled train set .", "entities": [[5, 6, "DatasetName", "FewRel"]]}
{"text": "For evaluation , we use the human - annotated test set of FewRel with 1 , 600 instances .", "entities": [[12, 13, "DatasetName", "FewRel"]]}
{"text": "Unlabeled instances already existing in this test set are removed from the unlabeled train set of FewRel - distant .", "entities": [[16, 17, "DatasetName", "FewRel"]]}
{"text": "However , it has several shortcomings compared with FewRel - distant .", "entities": [[8, 9, "DatasetName", "FewRel"]]}
{"text": "Therefore , we think the results on FewRel - distant are convincing enough for Distantly - supervised OpenRE .", "entities": [[7, 8, "DatasetName", "FewRel"]]}
{"text": "This phenomenon suggests a potential application of our model in hierarchical relation extraction .", "entities": [[11, 13, "TaskName", "relation extraction"]]}
{"text": "For word embeddings , we use pre - trained 50 - dimensional Glove ( Pennington et al , 2014 ) word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"], [20, 22, "TaskName", "word embeddings"]]}
{"text": "The activation function after the max - pooling layer is ReLU , and the activation functions after FC layers are sigmoid .", "entities": [[1, 3, "HyperparameterName", "activation function"], [10, 11, "MethodName", "ReLU"]]}
{"text": "We do not use F1 during model validation because the clustering steps are timeconsuming .", "entities": [[4, 5, "MetricName", "F1"]]}
{"text": "To demonstrate the effectiveness of our RSN models , we compare our models with two state - of - the - art models : ( 1 ) HAC with re - weighted word embeddings ( RW - HAC )", "entities": [[32, 34, "TaskName", "word embeddings"]]}
{"text": "The model first extracts KB types and NER tags of entities as well as re - weighted word embeddings from sentences , then adopts principal component analysis ( PCA ) to reduce feature dimensionality , and finally uses HAC to cluster the concatenation of reduced feature representations .", "entities": [[7, 8, "TaskName", "NER"], [17, 19, "TaskName", "word embeddings"], [28, 29, "MethodName", "PCA"]]}
{"text": "( 2 ) Discrete - state variational autoencoder ( VAE ) ( Marcheggiani and Titov , 2016 ) :", "entities": [[6, 8, "MethodName", "variational autoencoder"], [9, 10, "MethodName", "VAE"]]}
{"text": "VAE is the state - of - the - art reconstruction - based model for OpenRE via unlabeled instances .", "entities": [[0, 1, "MethodName", "VAE"]]}
{"text": "RW - HAC and VAE both rely on external linguistic tools to extract rich features from plain texts .", "entities": [[4, 5, "MethodName", "VAE"]]}
{"text": "Next , we preprocess the instances with part - of - speech ( POS ) tagging , named - entity recognition ( NER ) , and dependency parsing with Stanford CoreNLP .", "entities": [[7, 10, "DatasetName", "part - of"], [22, 23, "TaskName", "NER"], [26, 28, "TaskName", "dependency parsing"]]}
{"text": "RW - HAC uses word embeddings , but integrates them in a rulebased way .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "To study this influence , we use FewRel for evaluation and change the number of relations in the labeled train set from 40 to 64 while fixing the total num - ber of labeled instances to 25 , 000 , and report the clustering results in Figure 5 .", "entities": [[7, 8, "DatasetName", "FewRel"]]}
{"text": "In this paper , we propose a new model Relational Siamese Network ( RSN ) for OpenRE .", "entities": [[10, 12, "MethodName", "Siamese Network"]]}
{"text": "Second , we propose Semi / Distantly - supervised RSN , to further perform semi - supervised and distantlysupervised transfer learning .", "entities": [[19, 21, "TaskName", "transfer learning"]]}
{"text": "For future research , we plan to explore the following directions : ( 1 ) Besides CNN , there are some other popular sentence encoder structures like piecewise convolutional neural network ( PCNN ) and Long Short - Term Memory ( LSTM ) for RE .", "entities": [[35, 40, "MethodName", "Long Short - Term Memory"], [41, 42, "MethodName", "LSTM"]]}
{"text": "Pre - trained language models ( PLMs ) have achieved remarkable success on various natural language understanding tasks .", "entities": [[14, 17, "TaskName", "natural language understanding"]]}
{"text": "Moreover , adaptive pre - training can harm the PLM 's performance on the downstream task by causing catastrophic forgetting of its general knowledge .", "entities": [[22, 24, "TaskName", "general knowledge"]]}
{"text": "We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains .", "entities": [[8, 10, "TaskName", "question answering"], [11, 14, "TaskName", "named entity recognition"]]}
{"text": "( Devlin et al , 2019 ; Brown et al , 2020 ) have shown to be effective on various Natural Language Understanding ( NLU ) tasks .", "entities": [[20, 23, "TaskName", "Natural Language Understanding"]]}
{"text": "Although PLMs aim to address diverse downstream tasks from various data sources , there have been considerable efforts to adapt the PLMs to specific domains - distributions over the language characterizing a given topic or genre ( Gururangan et al , 2020 ) - for which the acquisition of domain knowledge is required to accurately solve the downstream tasks ( e.g. , Biomedical Named Entity Recognition ( Dogan et al , 2014 ) ) .", "entities": [[63, 66, "TaskName", "Named Entity Recognition"]]}
{"text": "This problem , known as Language Model Adaptation , can be viewed as a transfer learning problem ( Yosinski et al , 2014 ; Ruder , 2019 ) under domain shift , where the model is pre - trained on the general domain and the labeled distribution is available for the target domain - specific task .", "entities": [[14, 16, "TaskName", "transfer learning"]]}
{"text": "The most prevalent approach to this problem is adaptive pre - training ( Figure 2a ) which further updates all parameters of the PLM on a large domain - specific or curated task - specific corpus , with the same pretraining strategy ( e.g. , masked language modeling ) before fine - tuning it on the downstream task ( Beltagy et al , 2019 ; Gururangan et al , 2020 ) .", "entities": [[45, 48, "TaskName", "masked language modeling"]]}
{"text": "Besides , it is difficult to adapt the PLM to a new domain without forgetting the general knowledge it obtained from the initial pretraining step , since all pre - trained parameters are continually updated to fit the domain - specific corpus during adaptive pre - training .", "entities": [[16, 18, "TaskName", "general knowledge"]]}
{"text": "general knowledge may lead to the performance degradation on the downstream tasks .", "entities": [[0, 2, "TaskName", "general knowledge"]]}
{"text": "Then , to represent the domain knowledge consisting of entities and relations , we introduce the Entity Memory , which is the source of entity embeddings but independent of the PLM parameters ( See Entity Memory in Figure 2b ) .", "entities": [[24, 26, "TaskName", "entity embeddings"]]}
{"text": "Question Answering ( QA ) and Named Entity Recognition ( NER ) .", "entities": [[0, 2, "TaskName", "Question Answering"], [6, 9, "TaskName", "Named Entity Recognition"], [10, 11, "TaskName", "NER"]]}
{"text": "We show that our KALA significantly enhances the model 's performance on domain - specific QA and NER tasks , while being significantly more efficient over existing LM adaptation methods .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "Language Model Adaptation Nowadays , transfer learning ( Howard and Ruder , 2018 ) is a dominant approach for solving Natural Language Understanding ( NLU ) tasks .", "entities": [[5, 7, "TaskName", "transfer learning"], [20, 23, "TaskName", "Natural Language Understanding"]]}
{"text": "For instance , ERNIE ( Zhang et al , 2019 ) and KnowBERT ( Peters et al , 2019 ) incorporate entities as additional inputs in the pretraining stage to obtain a knowledge - aware LM , wherein a pre - trained knowledge graph embedding from Wikidata ( Vrandecic and Kr\u00f6tzsch , 2014 ) is used to represent entities .", "entities": [[42, 45, "TaskName", "knowledge graph embedding"]]}
{"text": "Our goal is to solve Natural Language Understanding ( NLU ) tasks for a specific domain , with a knowledge - augmented Language Model ( LM ) .", "entities": [[5, 8, "TaskName", "Natural Language Understanding"]]}
{"text": "Then , given a training dataset D = { ( x ( i ) , y ( i ) ) } N i=1 , the objective is to maximize the log - likelihood as follows : max \u03b8 L ( \u03b8 ) : = max \u03b8 ( x , y ) \u223cD log p", "entities": [[30, 33, "MetricName", "log - likelihood"], [37, 38, "HyperparameterName", "\u03b8"], [40, 41, "HyperparameterName", "\u03b8"], [45, 46, "HyperparameterName", "\u03b8"]]}
{"text": "x ; \u03b8 ) , p ( y", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "x ; \u03b8 )", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "g ( H ; \u03b8 g ) , H = f", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "( x ; \u03b8 f ) , where f is an encoder of the PLM which outputs contextualized representation H from x , and g is a decoder which models the probability distribution p of the label y , with trainable parameters \u03b8 = ( \u03b8 f , \u03b8 g ) .", "entities": [[3, 4, "HyperparameterName", "\u03b8"], [42, 43, "HyperparameterName", "\u03b8"], [45, 46, "HyperparameterName", "\u03b8"], [48, 49, "HyperparameterName", "\u03b8"]]}
{"text": "[ f 0 , . .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "1 Knowledge - Augmented Language Model The conventional learning objective defined above might be sufficient for understanding the texts if the tasks require only the general knowledge stored in PLMs .", "entities": [[25, 27, "TaskName", "general knowledge"]]}
{"text": "However , it is suboptimal for tackling domain - specific tasks since the general knowledge captured by the parameters \u03b8 f may not include the knowledge required for solving the domain - specific tasks .", "entities": [[13, 15, "TaskName", "general knowledge"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "Formally , the objective for a NLU task with our knowledge - augmented LM is given as follows : max \u03b8 , \u03c6 L ( \u03b8 , \u03c6 ) : = max \u03b8 , \u03c6 ( x , y ) \u223cD log p ( y |", "entities": [[20, 21, "HyperparameterName", "\u03b8"], [25, 26, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b8"]]}
{"text": "x ; \u03b8 , \u03c6 ) , p ( y", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "x ; \u03b8 , \u03c6 ) =", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "g ( H ; \u03b8 g ) , H l = f l ( H l\u22121 , h l ( H l\u22121 , E , M , G ; \u03c6 ) ; \u03b8 f l ) , where \u03c6 is parameters for the function h , E is the set of entities , M is the set of corresponding mentions , and G is a knowledge graph .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b8"]]}
{"text": "w | x | ] , let E be a set of entities in x. Then an entity e E is composed of one or multiple adjacent tokens within the input text : [ w m \u03b1 , . . .", "entities": [[36, 37, "HyperparameterName", "\u03b1"]]}
{"text": "Given a set of all entities E train \u222a { e } , we represent them in the continuous vector ( feature ) space to learn meaningful entity embeddings .", "entities": [[27, 29, "TaskName", "entity embeddings"]]}
{"text": "Before describing our interleaving method in detail , we first describe the Transformer architecture .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "Transformer Given | x | token representations H l\u22121 =", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "Linear Modulation on Transformer An effective yet efficient way to fuse knowledge from different sources without modifying the original model architecture is to scale and shift the features of one source with respect to the data from another source .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "This scheme of feature - wise affine transformation is effective on various tasks , such as language - conditioned image reasoning or style - transfer in image generation ( Huang and Belongie , 2017 Motivated by them , we propose to linearly transform the intermediate features after the layer normalization of the transformer - based PLM , conditioned on the knowledge sources E , M ,", "entities": [[26, 28, "TaskName", "image generation"], [48, 50, "MethodName", "layer normalization"]]}
{"text": "G. We term this method as the Knowledge - conditioned Feature Modulation ( KFM ) , described as follows : \u0393 , B , \u0393 , B = h l ( H l\u22121 , E , M , G ; \u03c6 ) ,", "entities": [[20, 21, "HyperparameterName", "\u0393"], [24, 25, "HyperparameterName", "\u0393"]]}
{"text": "H l = \u0393 LN ( H l\u22121 + Attn ( H l\u22121 ) )", "entities": [[3, 4, "HyperparameterName", "\u0393"]]}
{"text": "where H l\u22121 R | x | \u00d7d is the matrix of hidden representations from the previous layer , denotes the hadamard ( element - wise ) product , and \u0393", "entities": [[30, 31, "HyperparameterName", "\u0393"]]}
{"text": ", \u03b3 | x | ] R | x | \u00d7d , B =", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "\u03b2 | x | ] R | x | \u00d7d .", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "\u0393 and B are learnable modulation parameters from the function h , which are conditioned by the entity representation .", "entities": [[0, 1, "HyperparameterName", "\u0393"]]}
{"text": "Although the simple access to the entity memory can retrieve the necessary entity embeddings for the modulation , this approach has obvious drawbacks as it not only fails to reflect the relations with other entities , but also regards unseen entities as the same null entity e .", "entities": [[12, 14, "TaskName", "entity embeddings"]]}
{"text": "Motivated by this observation , we propose Relational Retrieval which leverages a KG G to retrieve entity embeddings from the memory , according to the relations defined in the given KG ( See Figure 3 , right ) .", "entities": [[16, 18, "TaskName", "entity embeddings"]]}
{"text": "e j N ( e i ; G ) with softmax : \u03b1 ij = softmax ( \u03c8 ( e i , r ij , e j ) )", "entities": [[10, 11, "MethodName", "softmax"], [12, 13, "HyperparameterName", "\u03b1"], [15, 16, "MethodName", "softmax"]]}
{"text": "Then , we update the entity embedding with a weighted average of the neighboring nodes with \u03b1 as an attention coefficient , denoted as follows : v = UPDATE e j N ( e i ; G )", "entities": [[16, 17, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 ij e j .", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "i = m \u03b1 h l\u22121", "entities": [[3, 4, "HyperparameterName", "\u03b1"]]}
{"text": "Question Answering ( QA ) and Named Entity Recognition ( NER ) .", "entities": [[0, 2, "TaskName", "Question Answering"], [6, 9, "TaskName", "Named Entity Recognition"], [10, 11, "TaskName", "NER"]]}
{"text": "For QA , we use three domain - specific datasets : NewsQA ( News , Trischler et al , 2017 ) and two subsets ( Relation , Medication ) of EMRQA ( Clinical , Pampari et", "entities": [[11, 12, "DatasetName", "NewsQA"], [30, 31, "DatasetName", "EMRQA"]]}
{"text": "We use the Exact - Match ( EM ) and the F1 score as evaluation metrics .", "entities": [[7, 8, "MetricName", "EM"], [11, 13, "MetricName", "F1 score"]]}
{"text": "For NER , we use three datasets from different domains , namely CoNLL - 2003 ( News , Sang andMeulder , 2003 ) , WNUT - 17 ( Social Media , Derczynski et al , 2017 ) and NCBI - Disease ( Biomedical , Dogan et al , 2014 ) .", "entities": [[1, 2, "TaskName", "NER"], [31, 32, "DatasetName", "Derczynski"], [38, 41, "DatasetName", "NCBI - Disease"]]}
{"text": "We use the F1 score as the evaluation metric .", "entities": [[3, 5, "MetricName", "F1 score"]]}
{"text": "A variant of KALA that only uses the entity memory and does not use the knowledge graphs .", "entities": [[15, 17, "TaskName", "knowledge graphs"]]}
{"text": "We use the uncased BERT - base ( Devlin et al , 2019 ) as the base PLM for all our experiments on QA and NER tasks .", "entities": [[4, 5, "MethodName", "BERT"], [25, 26, "TaskName", "NER"]]}
{"text": "Performance on QA and NER tasks On both extractive QA and NER tasks , our KALA outperforms all baselines , including TAPT and TAPT+RedcAdam ( Gururangan et al , 2020 ; , as shown in Table 1 and 2 .", "entities": [[4, 5, "TaskName", "NER"], [11, 12, "TaskName", "NER"]]}
{"text": "This result confirms that the performance improvement of KALA is not due to the increased number of parameters .", "entities": [[15, 18, "HyperparameterName", "number of parameters"]]}
{"text": "Specifically , we couple the following five components with KALA : Entity - as - Experts ( F\u00e9vry et al , 2020 ) , Adapter ( Houlsby et al , 2019 ) , KT - Net ( Yang et al , 2019 ) , ERNIE ( Zhang et al , 2019 ) , and ERICA", "entities": [[24, 25, "MethodName", "Adapter"]]}
{"text": "Although we believe our experimental results on more params on NewsQA ) .", "entities": [[8, 9, "MetricName", "params"], [10, 11, "DatasetName", "NewsQA"]]}
{"text": "Figure 1 illustrates the performance and training FLOPs of KALA against baselines on the NewsQA dataset .", "entities": [[14, 15, "DatasetName", "NewsQA"]]}
{"text": "We observe that the performance of TAPT decreases with the increased number of iterations , which could be due to forgetting of the knowledge from the PLM .", "entities": [[11, 14, "HyperparameterName", "number of iterations"]]}
{"text": "We validated KALA on various domains of QA and NER tasks , on which KALA significantly outperforms relevant baselines while being computationally efficient .", "entities": [[9, 10, "TaskName", "NER"]]}
{"text": "Our KALA contributes to this problem by augmenting domain knowledge graphs for PLMs .", "entities": [[9, 11, "TaskName", "knowledge graphs"]]}
{"text": "Relation extraction is the way how we obtain the factual knowledge from the text of the target dataset .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}
{"text": "In the example , \" Text \" indicates the entity mention within the input x , the \" start \" and \" end \" indicates its mention position denoted as ( m \u03b1 , m \u03c9 ) , and \" i d \" indicates the wikidata i d for the entity identification used in the next step .", "entities": [[32, 33, "HyperparameterName", "\u03b1"]]}
{"text": "To extract the relation among entities that we obtained above , we use the scheme of Relation Extraction ( RE ) .", "entities": [[16, 18, "TaskName", "Relation Extraction"]]}
{"text": "Specifically , we first fine - tune the BERT - base model ( Devlin et al , 2019 ) for 2 epochs with 600k distantly supervised data used in Qin et al ( 2021 ) , where the Wikipedia document and the Wikidata triplets are aligned .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "Then , we use the fine - tuned BERT model to extract the relations between entity pairs in the text .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "We use the model with a simple bilinear layer on top of it , which is widely used scheme in the relation extraction literature ( Yao et al , 2019 ) .", "entities": [[21, 23, "TaskName", "relation extraction"]]}
{"text": "In the relation extraction , the model returns the categorical distribution over the top 100 frequent relations .", "entities": [[2, 4, "TaskName", "relation extraction"]]}
{"text": "For relational retrieval , we implement the novel GNN model based on GATv2 ( Brody et al , 2021 ) provided by the torch - geometric package ( Fey and Lenssen , 2019 ) .", "entities": [[12, 13, "MethodName", "GATv2"]]}
{"text": "However , we empirically found that inserting one to three KFM layers at the end of the PLM ( i.e. , after the 9th - 11th layers of the BERT - base language model ) is beneficial to the performance ( See Appendix C.4 for experiments on diverse layer locations ) .", "entities": [[29, 30, "MethodName", "BERT"]]}
{"text": "Here we describe the dataset details with its statistics for two different tasks : extractive question answering ( QA ) and named entity recognition ( NER ) .", "entities": [[15, 17, "TaskName", "question answering"], [21, 24, "TaskName", "named entity recognition"], [25, 26, "TaskName", "NER"]]}
{"text": "Question Answering We evaluate models on three domain - specific datasets : NewsQA , Relation , and Medication .", "entities": [[0, 2, "TaskName", "Question Answering"], [12, 13, "DatasetName", "NewsQA"]]}
{"text": "Notably , NewsQA ( Trischler et al , 2017 ) is curated from CNN news articles .", "entities": [[2, 3, "DatasetName", "NewsQA"]]}
{"text": "Relation and Medication are originally part of the emrQA ( Pampari et al , 2018 ) , which is an automatically constructed question answering dataset based on the electrical medical record from n2c2 challenges 9 .", "entities": [[8, 9, "DatasetName", "emrQA"], [22, 24, "TaskName", "question answering"]]}
{"text": "Since the original emrQA is automatically generated based on templates , the quality is poor - it means that the original emrQA dataset was inappropriate to evaluate the ability of the model to reason over the clinical text since the most of questions can be answered by the simple text matching .", "entities": [[3, 4, "DatasetName", "emrQA"], [21, 22, "DatasetName", "emrQA"], [49, 51, "TaskName", "text matching"]]}
{"text": "Second , they suggest replacing medical terminologies in the question of the test set into synonyms to avoid the trivial question which can be solvable with a simple text matching .", "entities": [[28, 30, "TaskName", "text matching"]]}
{"text": "Named Entity Recognition", "entities": [[0, 3, "TaskName", "Named Entity Recognition"]]}
{"text": "We use three different domain - specific datasets for evaluating our KALA on NER tasks : CoNLL - 2003 ( Sang andMeulder , 2003 ) ( News ) , WNUT - 17 ( Derczynski et", "entities": [[13, 14, "TaskName", "NER"], [33, 34, "DatasetName", "Derczynski"]]}
{"text": "In the following three paragraphs , we explain the setting of fine - tuning for QA , NER , and generative QA tasks .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "Specifically , reported TAPT result on NewsQA , Relation , and Medication are obtained by 1 epoch of further pre - training .", "entities": [[6, 7, "DatasetName", "NewsQA"]]}
{"text": "Also , the masking ratio for the pre - training objective is set to 0.15 , following the existing strategy introduced in the original BERT paper ( Devlin et al , 2019 ) .", "entities": [[24, 25, "MethodName", "BERT"]]}
{"text": "For NER tasks , we further pre - train the PLM for 3 epochs across all datasets .", "entities": [[1, 2, "TaskName", "NER"]]}
{"text": "Specifically , as the source of corpus for the News domain , we use the sampled set of 10 million News from the RealNews dataset used in Gururangan et al ( 2021 ) .", "entities": [[23, 24, "DatasetName", "RealNews"]]}
{"text": "As the source of corpus for the Medical domain , we use the set of approximately 100k passages from the Medical textbook provided in Jin et al ( 2020 ) .", "entities": [[7, 9, "DatasetName", "Medical domain"]]}
{"text": "In other words , for experiments on NewsQA , TAPT only uses fine - tuning contexts containing 5.8 million words from the NewsQA training dataset , while DAPT uses more than a hundred times larger data - enormous contexts containing about 618 million words from the RealNews database .", "entities": [[7, 8, "DatasetName", "NewsQA"], [22, 23, "DatasetName", "NewsQA"], [46, 47, "DatasetName", "RealNews"]]}
{"text": "v = softmax ( h e E ) E , where h e represents the average of token representations of the entity mention m = ( m \u03b1 , m \u03c9 ) .", "entities": [[2, 3, "MethodName", "softmax"], [27, 28, "HyperparameterName", "\u03b1"]]}
{"text": "Adapter ( Houlsby et al , 2019 ) is introduced to fine - tune the PLM only with a few trainable parameters , instead of fine - tuning the whole parameters of the PLM .", "entities": [[0, 1, "MethodName", "Adapter"]]}
{"text": "To adapt this original implementation into our KALA framework , we replace our Knowledge - conditioned Feature Modulation with it , where the Adapter is used as the knowledge integration module .", "entities": [[23, 24, "MethodName", "Adapter"]]}
{"text": "We interleave the layer of Adapter after the feed - forward layer ( F F ) and before the residual connection of the transformer block .", "entities": [[5, 6, "MethodName", "Adapter"], [19, 21, "MethodName", "residual connection"]]}
{"text": "Note that we fine - tune the whole parameters following our KALA setting , unlike fine - tuning the parameters of only Adapter layers in Houlsby et al ( 2019 ) .", "entities": [[22, 23, "MethodName", "Adapter"]]}
{"text": "Therefore , instead of using a Graph Neural Network ( GNN ) layer , we use a multi - head self - attention layer to contextualize the entity embeddings .", "entities": [[27, 29, "TaskName", "entity embeddings"]]}
{"text": "( Qin et al , 2021 ) uses contrastive learning in LM pre - training to reflect the relational knowledge into the language model .", "entities": [[8, 10, "MethodName", "contrastive learning"]]}
{"text": "In other words , contrastive learning can not reflect the entity and relation in the test dataset .", "entities": [[4, 6, "MethodName", "contrastive learning"]]}
{"text": "We majorly follow the script from the ELECTRA ( Clark et al , 2020 ) repository to compute the approximated FLOPs for all models including ours .", "entities": [[7, 8, "MethodName", "ELECTRA"]]}
{"text": "Note that , in NewsQA training data , the average number of nodes is 57 , the average number of edges for each node is 0.64 , and the average number of mentions in the context is 92.68 .", "entities": [[4, 5, "DatasetName", "NewsQA"]]}
{"text": "In Figure 8 , we plot the performance of KALA on the NewsQA dataset by varying the number of entity elements in the memory .", "entities": [[12, 13, "DatasetName", "NewsQA"]]}
{"text": "Thus , the results are obtained by only considering the entities that appear more than [ 1000 , 100 , 10 , 5 , 0 ] times , e.g. , 0 means the model with full entity memory .", "entities": [[24, 25, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}
{"text": "Finally , we would like to note that , in Figure 1 , we report the performance of our KALA in the case of [ 1000 , 5 , 0 ] ( i.e. , considering entities appearing more than [ 1000 , 5 , 0 ] times ) .", "entities": [[29, 30, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}
{"text": "In this subsection , we aim to analyze which numbers of entities and facts per context are appropriate to achieve good performance in NER tasks .", "entities": [[23, 24, "TaskName", "NER"]]}
{"text": "Therefore , in this section , we analyze where we obtain the best performance in various locations of the KFM layer on the NewsQA dataset .", "entities": [[23, 24, "DatasetName", "NewsQA"]]}
{"text": "Specifically , in Figure 10 , we show the performance of our KALA with varying the location of the KFM layer insider the BERT - base model .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "The results demonstrate that the model with the KFM on the last layer of the BERT - base outperforms all the other choices .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "This might be because , as the final layer of the PLM is generally considered as the most task - specific layer , our KFM interleaved in the latest layer of BERT expressively injects the task - specific information from the entity memory and KGs , to such a task - specific layer .", "entities": [[31, 32, "MethodName", "BERT"]]}
{"text": "Specifically , we measure the computational cost on the NewsQA dataset with BERT - base , where we use the single Geforce RTX 2080", "entities": [[9, 10, "DatasetName", "NewsQA"], [12, 13, "MethodName", "BERT"]]}
{"text": "However , by reducing the number of entities in the memory , we can achieve better efficiency than more params in terms of GPU memory and FLOPs .", "entities": [[19, 20, "MetricName", "params"]]}
{"text": "Also , we observe that the training cost ( i.e. , Wall Time and FLOPs ) of TAPT and DAPT is high , especially on DAPT , thus we verify that our KALA is more efficient to train for domain adaptation settings .", "entities": [[39, 41, "TaskName", "domain adaptation"]]}
{"text": "While we already show the contextualized representations of seen and unseen entities in the latent 5163 space in Figure 2 right , we further visualize them including the missing baselines of Figure 2 , such as Fine - tuning or TAPT , in Figure 12 on the NCBI - Disease dataset .", "entities": [[47, 50, "DatasetName", "NCBI - Disease"]]}
{"text": "As shown in Figure 13 and 14 of QA and NER datasets , the entity frequency follows the long - tail distribution , where most entities appear a few times .", "entities": [[10, 11, "TaskName", "NER"]]}
{"text": "For instance , in the NewsQA dataset , more than 20k entities among entire 60k entities appear only once in the training dataset , whereas one entity ( CNN 10 ) appears approximately 20k times .", "entities": [[5, 6, "DatasetName", "NewsQA"]]}
{"text": "In addition to the case study in Figure 5 , we further show the case on the question answering task in Figure 15 , like in Section 5.5 , With this example , we explain how the factual knowledge in KGs could be utilized to solve the task via our KALA .", "entities": [[17, 19, "TaskName", "question answering"]]}
{"text": "ZYJ123@DravidianLangTech - EACL2021 : Offensive Language Identification based on XLM - RoBERTa with DPCNN", "entities": [[5, 7, "TaskName", "Language Identification"], [9, 10, "MethodName", "XLM"], [11, 12, "MethodName", "RoBERTa"]]}
{"text": "This paper describes our work on the task of Offensive Language Identification in Dravidian language - EACL 2021 .", "entities": [[10, 12, "TaskName", "Language Identification"]]}
{"text": "To complete this task , we propose a system based on the multilingual model XLM - Roberta and DPCNN .", "entities": [[14, 15, "MethodName", "XLM"]]}
{"text": "Our team takes part in the shared task of Offensive Language Identification in Dravidian Languages - EACL 2021 ( Chakravarthi et al , , 2020aHande et al , 2020 ) .", "entities": [[10, 12, "TaskName", "Language Identification"]]}
{"text": "In our approach , the multilingual model XLM - RoBERTa and DPCNN are combined to carry out the classification task .", "entities": [[7, 8, "MethodName", "XLM"], [9, 10, "MethodName", "RoBERTa"]]}
{"text": "In the second part , we introduce the relevant work in this field , which involves offensive language detection and text classification methods .", "entities": [[20, 22, "TaskName", "text classification"]]}
{"text": "In contrast to the start - to - end training model , Howard and Ruder ( 2018 ) proposed an effective transfer learning method , Universal Language Model Tuning ( ULMFIT ) , which can be applied to any task in natural language processing , and has shown significant results on six text classification tasks .", "entities": [[21, 23, "TaskName", "transfer learning"], [30, 31, "MethodName", "ULMFIT"], [52, 54, "TaskName", "text classification"]]}
{"text": "Subsequently , Abdellatif and Elgammal ( 2020 ) used the ULMFiT transfer learning method to train forward and backward models on Arabic datasets and ensemble the results to perform an offensive language detection task .", "entities": [[10, 11, "MethodName", "ULMFiT"], [11, 13, "TaskName", "transfer learning"]]}
{"text": "Pitenis et al ( 2020 ) tested the performance of several traditional machine learning models and deep learning models on an offensive language dataset of Greek , and the best results were achieved with the attention model of LSTM and GRU .", "entities": [[38, 39, "MethodName", "LSTM"], [40, 41, "MethodName", "GRU"]]}
{"text": "Ozdemir and Yeniterzi ( 2020 ) ensembled CNN - LSTM , BILSTM - Attention , and BERT three models , combined with pre - trained word embedding on Twitter to complete the identification task of offensive Turkish language , and achieved a good result .", "entities": [[9, 10, "MethodName", "LSTM"], [11, 12, "MethodName", "BILSTM"], [16, 17, "MethodName", "BERT"]]}
{"text": "A key challenge in automatically detecting hate speech on social media is to separate hate speech from other offensive languages .", "entities": [[6, 8, "DatasetName", "hate speech"], [14, 16, "DatasetName", "hate speech"]]}
{"text": "Davidson et al ( 2017 ) used the crowd - sourced hate speech lexicon to collect tweets containing hate speech keywords .", "entities": [[11, 13, "DatasetName", "hate speech"], [18, 20, "DatasetName", "hate speech"]]}
{"text": "They trained a multi - class classifier to reliably distinguish hate speech from other offensive languages , and found that racist and homophobic tweets were more likely to be classified as hate speech , but sexist tweets were generally classified as offensive .", "entities": [[10, 12, "DatasetName", "hate speech"], [31, 33, "DatasetName", "hate speech"]]}
{"text": "The system leverages a variety of statistical models and rule - based patterns , combined with an auxiliary weighted pattern library , to improve accuracy by matching text with its graded entries .", "entities": [[24, 25, "MetricName", "accuracy"]]}
{"text": "Sun et al ( 2019 ) proposed a Hierarchical Attention Prototype Network ( HAPN ) for fewshot text classification , which designed multiple cross - concerns of a feature layer , word layer , and instance layer for the model to enhance the expressive power of semantic space .", "entities": [[17, 19, "TaskName", "text classification"]]}
{"text": "The model was validated on two standard reference text classification datasets , Fewrel and CSID .", "entities": [[8, 10, "TaskName", "text classification"], [12, 13, "DatasetName", "Fewrel"]]}
{"text": "Prettenhofer and Stein ( 2010 ) built on structural correspondence learning , using untagged documents and simple word translation to induce task - specific , cross - language word correspondence .", "entities": [[17, 19, "TaskName", "word translation"]]}
{"text": "Using English data , Ranasinghe and Zampieri ( 2020 ) trained the model by applying cross - language contextual word embedding and transfer learning methods , and then predicted the effect of cross - language contextual embedding and transfer learning on this task in less resourceintensive languages such as Bengali , Hindi , and Spanish .", "entities": [[22, 24, "TaskName", "transfer learning"], [38, 40, "TaskName", "transfer learning"]]}
{"text": "Compared with the original BERT model , XLM - RoBERTa increases the number of languages and the number of training data sets .", "entities": [[4, 5, "MethodName", "BERT"], [7, 8, "MethodName", "XLM"], [9, 10, "MethodName", "RoBERTa"]]}
{"text": "This enables XLM - RoBERTa to achieve state - of - the - art results in cross - language benchmarks while exceeding the performance of the single - language BERT model for each language .", "entities": [[2, 3, "MethodName", "XLM"], [4, 5, "MethodName", "RoBERTa"], [29, 30, "MethodName", "BERT"]]}
{"text": "The XLM - RoBERTa parameter changes include up - sampling of low - resource languages during training and vocabulary building , generating a larger shared vocabulary , and increasing the overall model to 550 million parameters .", "entities": [[1, 2, "MethodName", "XLM"], [3, 4, "MethodName", "RoBERTa"]]}
{"text": "In this task , we combined XLM - RoBERTa with DPCNN ( Johnson and Zhang , 2017 ) to make the whole model more suitable for the downstream classification task .", "entities": [[6, 7, "MethodName", "XLM"], [8, 9, "MethodName", "RoBERTa"]]}
{"text": "DPCNN simply stacks the convolution module and negative sampling layer .", "entities": [[4, 5, "MethodName", "convolution"]]}
{"text": "The computation volume of the whole model is limited to less than two times the number of convolution blocks .", "entities": [[17, 18, "MethodName", "convolution"]]}
{"text": "In a common classification task , the last hidden state of the first token of the sequence ( CLS token ) , namely the original output of XLM - Roberta ( Pooler output ) , is further processed through the linear layer and the tanh activation function for classification purposes .", "entities": [[27, 28, "MethodName", "XLM"], [40, 42, "MethodName", "linear layer"], [44, 46, "MethodName", "tanh activation"]]}
{"text": "To obtain richer semantic information features of the model and improve the performance of the model , we first processed the output of the last three layers of XLM - RoBERTa through DPCNN , and then concatenate it with the original output of XLM - RoBERTa ( Pooler output ) to get a new and more effective feature vector , and then input this feature vector into the classifier for classification .", "entities": [[28, 29, "MethodName", "XLM"], [30, 31, "MethodName", "RoBERTa"], [43, 44, "MethodName", "XLM"], [45, 46, "MethodName", "RoBERTa"]]}
{"text": "In this experiment , the pre - training model I used was XLM - RoBERTa - base .", "entities": [[12, 13, "MethodName", "XLM"], [14, 15, "MethodName", "RoBERTa"]]}
{"text": "In this paper , we describe our system in the task of offensive language identification for Tamil , Malayalam , and Kannada language .", "entities": [[13, 15, "TaskName", "language identification"]]}
{"text": "In this model , the XLM - RoBERTa pre - training model is used to extract semantic information features of the text , and DPCNN is used to further process the output features .", "entities": [[5, 6, "MethodName", "XLM"], [7, 8, "MethodName", "RoBERTa"]]}
{"text": "First , we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for INFOTABS , a tabular NLI benchmark .", "entities": [[17, 18, "DatasetName", "INFOTABS"]]}
{"text": "Two recently created tabular Natural language Inference ( NLI ) datasets , TabFact ( Chen et al , 2020b ) on Wikipedia relational tables and INFOTABS on Wikipedia Infoboxes help study the question of inferential reasoning over semi - structured tables .", "entities": [[4, 7, "TaskName", "Natural language Inference"], [12, 13, "DatasetName", "TabFact"], [25, 26, "DatasetName", "INFOTABS"]]}
{"text": "Most of Breakfast in America was recorded 3 in the last month of 1978 3 .", "entities": [[2, 3, "DatasetName", "Breakfast"]]}
{"text": "Breakfast in America was released 4 the same month recording ended 4 . Figure 1 : A semi - structured premise ( the table ' Breakfast in America ' ) example from .", "entities": [[0, 1, "DatasetName", "Breakfast"], [25, 26, "DatasetName", "Breakfast"]]}
{"text": "To illustrate this task , consider an example from the INFOTABS dataset in Figure 1 , which shows a premise table and three hypotheses .", "entities": [[10, 11, "DatasetName", "INFOTABS"]]}
{"text": "We explore several unsupervised evidence extraction approaches for INFOTABS .", "entities": [[8, 9, "DatasetName", "INFOTABS"]]}
{"text": "For supervised evidence extraction , we annotate the IN - FOTABS training set ( 17 K table - hypothesis pairs with 1740 unique tables ) with relevant rows following the methodology of , and then train a RoBERTa LARGE classifier .", "entities": [[37, 38, "MethodName", "RoBERTa"]]}
{"text": "We enrich the INFOTABS training set with evidence rows , and develop a supervised extractor that has near - human performance .", "entities": [[3, 4, "DatasetName", "INFOTABS"]]}
{"text": "Indeed , INFOTABS has multiple neutral hypotheses that are partly entailed by the table ; if any part of a hypothesis contradicts the table , then the inference label should be CONTRADICT .", "entities": [[2, 3, "DatasetName", "INFOTABS"]]}
{"text": "TabFact , INFOTABS , and the Se - mEval'21 Task 9 ( Wang et al , 2021b ) and the FEVEROUS'21 shared task ( Aly et al , 2021 ) datasets .", "entities": [[0, 1, "DatasetName", "TabFact"], [2, 3, "DatasetName", "INFOTABS"]]}
{"text": "We use the INFOTABS data in this work .", "entities": [[3, 4, "DatasetName", "INFOTABS"]]}
{"text": "It contains finer - grained annotation ( e.g. , TabFact lacks NEUTRAL hypotheses ) and more complex reasoning than the others 3 .", "entities": [[9, 10, "DatasetName", "TabFact"]]}
{"text": "This section describes the process of using Amazon MTurk to annotate evidence rows for the 16 , 538 premise - hypothesis pairs that make the training set of INFOTABS .", "entities": [[28, 29, "DatasetName", "INFOTABS"]]}
{"text": "The quality control details are provided in the Appendix B. In total , we collected 81 , 282 annotations from 3 As per , 33 % of examples in INFOTABS involve multiple rows .", "entities": [[29, 30, "DatasetName", "INFOTABS"]]}
{"text": "The dataset covers all the reasoning types present in the Glue and SuperGlue Choice of Semi - structured Data .", "entities": [[12, 13, "DatasetName", "SuperGlue"]]}
{"text": "Inspired by the Distracting Row Removal ( DRR ) heuristic of Neeraja et al ( 2021 ) , we propose DRR ( Re - Rank + Top - S \u03c4 ) , which uses fastText ( Joulin et al , 2016 ; Mikolov et al , 2018 ) based static embeddings to measure sentence similarity .", "entities": [[0, 1, "DatasetName", "Inspired"], [34, 35, "MethodName", "fastText"]]}
{"text": "We observed that the raw similarity scores ( i.e. , using only fastText ) for some valid evidence rows could be low , despite exact wordlevel lexical matching with the row 's key and values .", "entities": [[12, 13, "MethodName", "fastText"]]}
{"text": "We augmented the scores by \u03b4 for each exact match to incentivize precise matches .", "entities": [[5, 6, "HyperparameterName", "\u03b4"], [8, 10, "MetricName", "exact match"]]}
{"text": "After alignment , we normalize the sum of cosine similarities of RoBERTa LARGE token embeddings 8 to derive the relevance score .", "entities": [[11, 12, "MethodName", "RoBERTa"]]}
{"text": "As an alternative , we can directly compute similarities between the contextualised sentence embeddings of rows and the hypothesis .", "entities": [[12, 14, "TaskName", "sentence embeddings"]]}
{"text": "Sentence Transformer : We use Sentence - BERT ( Reimers and Gurevych , 2019 ) and its variants ( Reimers and Gurevych , 2020 ;", "entities": [[1, 2, "MethodName", "Transformer"], [7, 8, "MethodName", "BERT"]]}
{"text": "SimCSE : SimCSE ( Gao et al , 2021 ) uses a contrastive learning to train sentence embeddings in both unsupervised and supervised settings .", "entities": [[0, 1, "MethodName", "SimCSE"], [2, 3, "MethodName", "SimCSE"], [12, 14, "MethodName", "contrastive learning"], [16, 18, "TaskName", "sentence embeddings"]]}
{"text": "The latter uses example pairs from the MNLI dataset ( Williams et al , 2018 ) with entailments serving as positive examples and contradiction serving as hard negatives for contrastive learning .", "entities": [[7, 8, "DatasetName", "MNLI"], [29, 31, "MethodName", "contrastive learning"]]}
{"text": "We give the row sentences directly to SimCSE to get their embeddings .", "entities": [[7, 8, "MethodName", "SimCSE"]]}
{"text": "We then use the cosine similarity between SimCSE sentence embeddings to compute the final relevance score .", "entities": [[7, 8, "MethodName", "SimCSE"], [8, 10, "TaskName", "sentence embeddings"]]}
{"text": "In the study , we refer to this method as SimCSE ( Hypo - Title - Swap + Re - rank + Top - K \u03c4 ) .", "entities": [[10, 11, "MethodName", "SimCSE"]]}
{"text": "We train a relevant - vsirrelevant row classifier using RoBERTa LARGE 's two sentence classifier .", "entities": [[9, 10, "MethodName", "RoBERTa"]]}
{"text": "We use RoBERTa LARGE because of its superior performance over other models in preliminary experiments , and also the fact that it is also used for the NLI classifier .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}
{"text": "Our choice of S is based on the observation that in INFOTABS most ( 92 % ) instances have only one ( 54 % ) or two ( 38 % ) relevant rows .", "entities": [[11, 12, "DatasetName", "INFOTABS"]]}
{"text": "For the Sentence Transformer , we used the paraphrase - mpnet - base v2 model ( Reimers and Gurevych , 2019 ) which is a pre - trained with the mpnet - base architecture using several existing paraphrase datasets .", "entities": [[3, 4, "MethodName", "Transformer"], [10, 11, "MethodName", "mpnet"], [30, 31, "MethodName", "mpnet"]]}
{"text": "Both the supervised and unsupervised SimCSE models use the same parameters as DRR ( Re - Rank + Top - K \u03c4 ) .", "entities": [[5, 6, "MethodName", "SimCSE"]]}
{"text": "We refer to the supervised and unsupervised variants as SimCSE - Supervised and SimCSE - Unsupervised respectively .", "entities": [[9, 10, "MethodName", "SimCSE"], [13, 14, "MethodName", "SimCSE"]]}
{"text": "For the NLI task , we use the BPR representation over extracted evidence T R with the RoBERTa LARGE two sentence classification model .", "entities": [[17, 18, "MethodName", "RoBERTa"], [20, 22, "TaskName", "sentence classification"]]}
{"text": "We see that the contextual embedding method , SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( \u03c4 = 1 ) ) , performs the best .", "entities": [[8, 9, "MethodName", "SimCSE"]]}
{"text": "Previously used approaches such as DRR and WMD have low F1 - score , because of poor precision .", "entities": [[10, 13, "MetricName", "F1 - score"]]}
{"text": "Using Re - Rank based on exact match improves the evidence extraction recall .", "entities": [[6, 8, "MetricName", "exact match"]]}
{"text": "Furthermore , the zero weighting of title matches using the Hypo - Title - Swap heuristic , benefits contextualized embedding models such as SimCSE 12 . SimCSE - supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( \u03c4 = 1 ) )", "entities": [[23, 24, "MethodName", "SimCSE"], [26, 27, "MethodName", "SimCSE"]]}
{"text": "This highlights directions for future improvement via domain adaptation .", "entities": [[7, 9, "TaskName", "domain adaptation"]]}
{"text": "This gap exists because , in addition to extracting evidence , the INFOTABS hypotheses require inference with the evidence involving common - sense and knowledge , which the NLI component does not adequately perform .", "entities": [[12, 13, "DatasetName", "INFOTABS"]]}
{"text": "Tabular Reasoning Many recent studies investigate various NLP tasks on semi - structured tabular data , including tabular NLI and fact verification ( Chen et al , 2020b ; , various question answering and semantic parsing tasks ( Zhang and Balog , 2020 ; Pasupat and Liang , 2015 ;", "entities": [[20, 22, "TaskName", "fact verification"], [31, 33, "TaskName", "question answering"], [34, 36, "TaskName", "semantic parsing"]]}
{"text": "Krishnamurthy et al , 2017 ; Abbas et al , 2016 ; Sun et al , 2016 ; Chen et al , 2020c ; Lin et al , 2020 ; Zayats et al , 2021 ; Oguz et al , 2020 ; Chen et al , 2021 , inter alia ) , andtable - to - text generation ( e.g. , Parikh et al , 2020 ; Nan et al , 2021 ; Yoran et al , 2021 ; Chen et al , 2020a ) .", "entities": [[56, 58, "TaskName", "text generation"]]}
{"text": "Several strategies for representing Wikipedia relational tables are proposed , such as Ta - ble2vec ( Deng et al , 2019 ) , TAPAS ( Herzig et al , 2020 ) , TaBERT ( Yin et al , 2020 ) , TabStruc ( Zhang et al , 2020a ) , TABBIE ( Iida et al , 2021 ) , TabGCN ( Pramanick and Bhattacharya , 2021 ) and RCI ( Glass et al , 2021 ) .", "entities": [[16, 19, "DatasetName", "Deng et al"], [23, 24, "MethodName", "TAPAS"], [32, 33, "MethodName", "TaBERT"], [50, 51, "MethodName", "TABBIE"]]}
{"text": "Recently , Kumar and Talukdar ( 2020 ) introduced Natural - language Inference over Label - specific Explanations ( NILE ) , an NLI approach for generating labels and accompanying faithful explanations using auto - generated label - specific natural language explanations .", "entities": [[2, 3, "DatasetName", "Kumar"]]}
{"text": "In this work , we focus on item evidence extraction for non - scientific Wikipedia Infobox entity tables , proposed a twostage sequential approach , and used the INFOTABS dataset which has complex reasoning and multiple adversarial tests for robust evaluation .", "entities": [[28, 29, "DatasetName", "INFOTABS"]]}
{"text": "Furthermore , we are only concerned with entity tables rather than relational tables or unstructured text , while the FEVEROUS data has relational tables , unstructured text , and fewer entity tables .", "entities": [[19, 20, "DatasetName", "FEVEROUS"]]}
{"text": "To investigate this , we use Hard Negative ( 3x ) with RoBERTa LARGE model as our evidence extraction classifier , which is similar to the full supervision method .", "entities": [[12, 13, "MethodName", "RoBERTa"]]}
{"text": "It would be worthwhile to explore if this volatility could be reduced by strategic sampling using an unsupervised extraction model , an active learning framework , and strategic diversity maximizing sampling , which is left as future work .", "entities": [[22, 24, "TaskName", "active learning"]]}
{"text": "4 Wikipedia data was processed with GloVe ( Pennington et al , 2014 ) to create a semantic vector space model of topics , based on word co - occurrences .", "entities": [[6, 7, "MethodName", "GloVe"]]}
{"text": "In short : ( 1 ) whether a given haiku makes sense and how well it fits the topic , ( 2 ) whether it fits the form , i.e. , is it a valid haiku ? , and ( 3 ) , the beauty of the writing , the emotion it evokes .", "entities": [[50, 51, "DatasetName", "emotion"]]}
{"text": "Form : the haiku - generating subsystem guarantees that the requirements of a grammatical skeleton are met , and the 5/7/5 syllable pattern is guaranteed ( up to the accuracy of the CMU Pronouncing Dictionary ) .", "entities": [[29, 30, "MetricName", "accuracy"]]}
{"text": "Mohammad ( 2016 ) surveys more recent work in NLP on modelling emotion , which could be exploited in future work .", "entities": [[12, 13, "DatasetName", "emotion"]]}
{"text": "Word similarities can be found using GloVe : this would presumably produce links with more coherent meanings , compared to the edit distance - based measure we used .", "entities": [[6, 7, "MethodName", "GloVe"]]}
{"text": "Distributional properties of political dogwhistle representations in Swedish BERT", "entities": [[4, 5, "DatasetName", "dogwhistle"], [8, 9, "MethodName", "BERT"]]}
{"text": "We take the result of a word - replacement survey of the Swedish population intended to reveal how dogwhistles are understood , and we show that the difficulty of annotating dogwhistles is reflected in the separability of the space of a sentence - transformer Swedish BERT trained on general data .", "entities": [[45, 46, "MethodName", "BERT"]]}
{"text": "We explore whether contemporary vector - space sentence representation techniques also provide a structured representation of the different messages in \" dogwhistle \" political communication .", "entities": [[21, 22, "DatasetName", "dogwhistle"]]}
{"text": "A dogwhistle refers to a word or phrase used in manipulative communication , usually in a political context .", "entities": [[1, 2, "DatasetName", "dogwhistle"]]}
{"text": "We ask the question : are the responses clearly partitioned in the semantic space , and does the \" sharpness \" of this partitioning reflect the ease of dogwhistle identification by expert annotators ?", "entities": [[28, 29, "DatasetName", "dogwhistle"]]}
{"text": "Therefore , we contribute the following : We present a preliminary dataset of a word replacement task by members of the Swedish population as part of a survey of political attitudes , including a manual annotation for dogwhistle identification with inter - annotator agreement ( IAA ; Krippendorff 's \u03b1 ) scores .", "entities": [[37, 38, "DatasetName", "dogwhistle"], [49, 50, "HyperparameterName", "\u03b1"]]}
{"text": "We use a transformer - based model to represent the responses in a semantic space and apply classification ( SVM ) and clustering techniques ( K - means ) to the vectors .", "entities": [[19, 20, "MethodName", "SVM"]]}
{"text": "We then conclude that a Swedish BERT variant already represents important aspects of the underlying semantics of dogwhistles .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Dogwhistle politics has become increasingly salient in the current mass and social media environment .", "entities": [[0, 1, "DatasetName", "Dogwhistle"]]}
{"text": "Then we manually annotated these responses for whether they identified a dogwhistle use or not .", "entities": [[11, 12, "DatasetName", "dogwhistle"]]}
{"text": "Each item therefore contains the substitution of participant - provided words or phrases for the original dogwhistle in the full context of the corresponding stimulus sentence .", "entities": [[16, 17, "DatasetName", "dogwhistle"]]}
{"text": "Others would replace \" globalists \" with , e.g. , \" people concerned with international affairs \" thus not showing an understanding of the dogwhistle as having any associations with the aforementioned groups .", "entities": [[24, 25, "DatasetName", "dogwhistle"]]}
{"text": "The replacements for each dogwhistle was manually labeled depending on a person picking up on the dogwhistle meaning or not .", "entities": [[4, 5, "DatasetName", "dogwhistle"], [16, 17, "DatasetName", "dogwhistle"]]}
{"text": "An inter - annotator score was then calculated for the labeling of each dogwhistle .", "entities": [[13, 14, "DatasetName", "dogwhistle"]]}
{"text": "The goal of the annotation and the computation of IAA is to determine whether or not the annotation task can be designed with the following criterion in mind : that a panel of trained annotators with access to the guidelines can reliably distinguish between participant responses that did pick up on the \" ingroup \" dogwhistle meaning from those that did not .", "entities": [[55, 56, "DatasetName", "dogwhistle"]]}
{"text": "The identification and interpretation of a dogwhistle is an inherently subjective task which stems directly from one of the reasons to use a dogwhistle in the first place : to take advantage of the ambiguity of interpretation based on the standpoint of the individual recipients of the message .", "entities": [[6, 7, "DatasetName", "dogwhistle"], [23, 24, "DatasetName", "dogwhistle"]]}
{"text": "However , in this case , the annotation guidelines were developed in an iterative process to be presented in future publications that ensured that Swedish - speaking annotators informed about Swedish politics could consistently identify the dogwhistle interpretations of survey participants .", "entities": [[36, 37, "DatasetName", "dogwhistle"]]}
{"text": "The focus of this work is to explore the extent to which the intuitions behind the annotation guidelines are reflected in a Swedish BERT model trained on a multi - genre corpus .", "entities": [[23, 24, "MethodName", "BERT"]]}
{"text": "Sentence transformers ( Reimers and Gurevych , 2019 ) are based on BERT ( Devlin et al , 2018 ) and produce state of the art semantic representations of entire sentences and paragraphs .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "A high performing sentence model returns semantic representations of sentences , with a cosine distance that correlates with their semantic similarity .", "entities": [[19, 21, "TaskName", "semantic similarity"]]}
{"text": "As we were interested in the semantic representations given by the sentence replacements for each dogwhistle response , we did the following : we input each of the sentences containing the replaced dogwhistle from the dataset into a sentence transformer in order to get dense 768 - dimensional vector representations .", "entities": [[15, 16, "DatasetName", "dogwhistle"], [32, 33, "DatasetName", "dogwhistle"]]}
{"text": "Then in order to visualize the semantic clustering of these sentence representations we used Principal Component Analysis ( PCA ; Abdi and Williams , 2010 ) to reduce the vectors to 3 dimensions .", "entities": [[18, 19, "MethodName", "PCA"]]}
{"text": "A more compact cluster further apart from the other cluster will result in a lower score , with 0 indicating two very distinct clusters .", "entities": [[18, 19, "DatasetName", "0"]]}
{"text": "In addition , we trained a linear - kernel support vector machine ( SVM ) .", "entities": [[9, 12, "MethodName", "support vector machine"], [13, 14, "MethodName", "SVM"]]}
{"text": "When training the SVM , we randomly sampled the sentence representations and labels , and split the data into training and testing ( 70 % - 30 % ) .", "entities": [[3, 4, "MethodName", "SVM"]]}
{"text": "Given the distance in the semantic space between the two groups , it should be possible to separate the space with a linear SVM trained on a subset of the data .", "entities": [[23, 24, "MethodName", "SVM"]]}
{"text": "The dogwhistle replacements might vary widely enough to not cluster well while still being separatable using a hyperplane to a high de - gree of accuracy .", "entities": [[1, 2, "DatasetName", "dogwhistle"], [25, 26, "MetricName", "accuracy"]]}
{"text": "Similarly , \" suburban gang \" had the highest IAA ( 1/1 ) and very high F1 scores as well ( 0.98/0.97 ) .", "entities": [[16, 17, "MetricName", "F1"]]}
{"text": "An explanation for this might be that some dogwhistle clusterings are spread over a wider semantic space , while still being linearly separatable ( with an SVM ) from other clusterings .", "entities": [[8, 9, "DatasetName", "dogwhistle"], [26, 27, "MethodName", "SVM"]]}
{"text": "The SVM was generally able to separate the two clusters well , even given fairly small amounts of training data .", "entities": [[1, 2, "MethodName", "SVM"]]}
{"text": "The general correlation with IAA scores were higher with PCA dimensionalityreduced vector representations .", "entities": [[9, 10, "MethodName", "PCA"]]}
{"text": "Possible reasons for the performance of the SVM might be that the SVM does not take into account the separation of the data from its cluster centroid in the opposite di - rection of the other cluster or the dispersion of the datapoints along an axis orthogonal to the separating plane .", "entities": [[7, 8, "MethodName", "SVM"], [12, 13, "MethodName", "SVM"]]}
{"text": "The SVM measurement only takes into account the overlapping of the semantic meanings of the sentences , represented in the space .", "entities": [[1, 2, "MethodName", "SVM"]]}
{"text": "Our evaluations show that easily identified dogwhistle interpretations are partitioned well enough in the vector space given by SOTA sentence models that they are linearly separable using a simple SVM .", "entities": [[6, 7, "DatasetName", "dogwhistle"], [29, 30, "MethodName", "SVM"]]}
{"text": "Dice Loss for Data - imbalanced NLP Tasks", "entities": [[0, 2, "MethodName", "Dice Loss"]]}
{"text": "Many NLP tasks such as tagging and machine reading comprehension ( MRC ) are faced with the severe data imbalance issue : negative examples significantly outnumber positive ones , and the huge number of easy - negative examples overwhelms training .", "entities": [[7, 10, "TaskName", "machine reading comprehension"]]}
{"text": "The most commonly used cross entropy criteria is actually accuracy - oriented , which creates a discrepancy between training and test .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "At training time , each training instance contributes equally to the objective function , while at test time F1 score concerns more about positive examples .", "entities": [[18, 20, "MetricName", "F1 score"]]}
{"text": "In this paper , we propose to use dice loss in replacement of the standard cross - entropy objective for data - imbalanced NLP tasks .", "entities": [[8, 10, "MethodName", "dice loss"]]}
{"text": "Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training .", "entities": [[12, 14, "MetricName", "F1 score"], [18, 20, "MethodName", "dice loss"]]}
{"text": "Notably , we are able to achieve SOTA results on CTB5 , CTB6 and UD1.4 for the part of speech tagging task , and competitive or even better results on CoNLL03 , OntoNotes5.0 , MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks .", "entities": [[30, 31, "DatasetName", "CoNLL03"], [39, 42, "TaskName", "named entity recognition"], [46, 49, "TaskName", "machine reading comprehension"], [50, 52, "TaskName", "paraphrase identification"]]}
{"text": "( Rajpurkar et al , 2016 ) 10.3 M 175 K 55.9 SQuAD 2.0 ( Rajpurkar et al , 2018 ) 15.4 M 188 K 82.0 QUOREF ( Dasigi et al , 2019 ) 6.52 M 38.6 K 169", "entities": [[12, 13, "DatasetName", "SQuAD"], [26, 27, "DatasetName", "QUOREF"]]}
{"text": "Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension .", "entities": [[16, 19, "TaskName", "machine reading comprehension"]]}
{"text": "Table 1 gives concrete examples : for the Named Entity Recognition ( NER ) task Nadeau and Sekine , 2007 ) , most tokens are backgrounds with", "entities": [[8, 11, "TaskName", "Named Entity Recognition"], [12, 13, "TaskName", "NER"]]}
{"text": "tagging class O. Specifically , the number of tokens with tagging class O is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset ; Dataimbalanced issue is more severe for MRC tasks ( Rajpurkar et al , 2016 ;", "entities": [[25, 26, "DatasetName", "CoNLL03"]]}
{"text": "The S\u00f8rensen - Dice coefficient , dice loss for short , is the harmonic mean of precision and recall .", "entities": [[3, 4, "MetricName", "Dice"], [6, 8, "MethodName", "dice loss"]]}
{"text": "Tversky index extends dice loss by using a weight that trades precision and recall , which can be thought as the approximation of the F \u03b2 score , and thus comes with more flexibility .", "entities": [[3, 5, "MethodName", "dice loss"], [25, 26, "HyperparameterName", "\u03b2"]]}
{"text": "Therefore , we use dice loss or Tversky index to replace CE loss to address the first issue .", "entities": [[4, 6, "MethodName", "dice loss"], [12, 13, "MetricName", "loss"]]}
{"text": "Only using dice loss or Tversky index is not enough since they are unable to address the dominating influence of easy - negative examples .", "entities": [[2, 4, "MethodName", "dice loss"]]}
{"text": "This is intrinsically because dice loss is actually a soft version of the F1 score .", "entities": [[4, 6, "MethodName", "dice loss"], [13, 15, "MetricName", "F1 score"]]}
{"text": "Taking the binary classification task as an example , at test time , an example will be classified as negative as long as its probability is smaller than 0.5 , but training will push the value to 0 as much as possible .", "entities": [[37, 38, "DatasetName", "0"]]}
{"text": "The background - object label imbalance issue is severe and thus well studied in the field of object detection ( Li et al , 2015 ; Girshick , 2015 ;", "entities": [[17, 19, "TaskName", "object detection"]]}
{"text": "The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP .", "entities": [[4, 6, "TaskName", "object detection"]]}
{"text": "They proposed to use the class re - balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks .", "entities": [[13, 15, "MethodName", "Dice Loss"]]}
{"text": "The mechanism can be easily extended to multi - class classification .", "entities": [[7, 11, "TaskName", "multi - class classification"]]}
{"text": "where y i0 , y i1 { 0 , 1 } , p i0 , p i1 [ 0 , 1 ] and p i1 + p", "entities": [[7, 8, "DatasetName", "0"], [18, 19, "DatasetName", "0"]]}
{"text": "Two strategies are normally used to address the the case where we wish that not all x i are treated equally : associating different classes with different weighting factor \u03b1 or resampling the datasets .", "entities": [[29, 30, "HyperparameterName", "\u03b1"]]}
{"text": "In this work , we use lg ( n\u2212nt nt + K ) to calculate the coefficient \u03b1 , where n t is the number of samples with class t and n is the total number of samples in the training set .", "entities": [[17, 18, "HyperparameterName", "\u03b1"], [24, 27, "HyperparameterName", "number of samples"], [35, 38, "HyperparameterName", "number of samples"]]}
{"text": "As can be seen , a negative example ( y i1 = 0 ) does not contribute to the objective .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "For smoothing purposes , it is common to add a \u03b3 factor to both the nominator and the denominator , making the form to be as follows ( we simply set \u03b3", "entities": [[10, 11, "HyperparameterName", "\u03b3"], [31, 32, "HyperparameterName", "\u03b3"]]}
{"text": "CE \u2212 j { 0 , 1 } y", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "i j { 0 , 1 } y", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "+ \u03b3 p i1 y i1", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b1 p i1", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "+ \u03b2", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "+ \u03b3 ( 1\u2212p i1 ) p i1", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b3 FL \u2212\u03b1", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b3 p i1", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "As can be seen , negative examples whose DSC is \u03b3 p i1", "entities": [[10, 11, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b3 , also contribute to the training .", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "Additionally , Milletari et al ( 2016 ) proposed to change the denominator to the square form for faster convergence , which leads to the following dice loss ( DL ) :", "entities": [[26, 28, "MethodName", "dice loss"]]}
{"text": "+ \u03b3", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "Tversky index ( TI ) , which can be thought as the approximation of the F \u03b2 score , extends dice coefficient to a more general case .", "entities": [[16, 17, "HyperparameterName", "\u03b2"]]}
{"text": "A \u2229 B | + \u03b1 | A\\B |", "entities": [[5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "It degenerates to DSC if \u03b1 =", "entities": [[5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "The Tversky loss ( TL ) is thus given as follows : TL =", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "( 11 ) Comparing Eq.5 with Eq.11 , we can see that Eq.5 is actually a soft form of F 1 , using a continuous p rather than the binary I ( p i1 > 0 .", "entities": [[35, 36, "DatasetName", "0"]]}
{"text": "For easy examples whose probability are approaching 0 or 1 , ( 1 \u2212 p i1 )", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "A close look at Eq.12 reveals that it actually mimics the idea of focal loss ( FL for short )", "entities": [[13, 15, "MethodName", "focal loss"]]}
{"text": "But for the other losses , the derivatives reach 0 only if the probability is exactly 1 , which means they will push p to 1 as much as possible .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "We evaluated the proposed method on four NLP tasks , part - of - speech tagging , named entity recognition , machine reading comprehension and paraphrase identification .", "entities": [[10, 16, "TaskName", "part - of - speech tagging"], [17, 20, "TaskName", "named entity recognition"], [21, 24, "TaskName", "machine reading comprehension"], [25, 27, "TaskName", "paraphrase identification"]]}
{"text": "More experiment details including datasets and hyperparameters are shown in supplementary material .", "entities": [[10, 12, "DatasetName", "supplementary material"]]}
{"text": "Settings Part - of - speech tagging ( POS ) is the task of assigning a part - of - speech label ( e.g. , noun , verb , adjective ) to each word in a given text .", "entities": [[1, 7, "TaskName", "Part - of - speech tagging"], [16, 19, "DatasetName", "part - of"]]}
{"text": "In this paper , we choose BERT ( Devlin et al , 2018 ) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank ( Xue et al , 2005 ) 5.0/6.0 and UD1.4 and English datasets including Wall Street Journal ( WSJ ) and the dataset proposed by Ritter et al ( 2011 ) .", "entities": [[6, 7, "MethodName", "BERT"], [28, 30, "DatasetName", "Chinese Treebank"]]}
{"text": "We report the span - level micro - averaged precision , recall and F1 for evaluation .", "entities": [[13, 14, "MetricName", "F1"]]}
{"text": "Settings Named entity recognition ( NER ) is the task of detecting the span and semantic category of entities within a chunk of text .", "entities": [[1, 4, "TaskName", "Named entity recognition"], [5, 6, "TaskName", "NER"]]}
{"text": "Our implementation uses the current state - of - the - art model proposed by as the backbone , and changes the MLE loss to DSC loss .", "entities": [[23, 24, "MetricName", "loss"], [26, 27, "MetricName", "loss"]]}
{"text": "Datasets that we use include OntoNotes4.0 ( Pradhan et al , 2011 ) , MSRA ( Levow , 2006 ) , CoNLL2003 ( Sang and Meulder , 2003 and OntoNotes5.0", "entities": [[21, 22, "DatasetName", "CoNLL2003"]]}
{"text": "We report span - level micro - averaged precision , recall and F1 .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "Baselines We use the following baselines : ELMo : a tagging model with pretraining from Peters et al ( 2018 ) .", "entities": [[7, 8, "MethodName", "ELMo"]]}
{"text": "Lattice - LSTM :", "entities": [[2, 3, "MethodName", "LSTM"]]}
{"text": "Settings The task of machine reading comprehension ( MRC ) ( Seo et al , 2016 ; Wang and Jiang , 2016 ; Shen et al , 2017 ; predicts the answer span in the passage given a question and the passage .", "entities": [[4, 7, "TaskName", "machine reading comprehension"]]}
{"text": "We report Extract Match ( EM ) as well as F1 score on validation set .", "entities": [[5, 6, "MetricName", "EM"], [10, 12, "MetricName", "F1 score"]]}
{"text": "We use three datasets on this task : SQuAD v1.1 , SQuAD v2.0 ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 and Quoref ( Dasigi et al , 2019 ) .", "entities": [[8, 9, "DatasetName", "SQuAD"], [11, 12, "DatasetName", "SQuAD"], [27, 28, "DatasetName", "Quoref"]]}
{"text": "With either BERT or XLNet , our proposed DSC loss obtains significant performance boost on both EM and F1 .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "XLNet"], [9, 10, "MetricName", "loss"], [16, 17, "MetricName", "EM"], [18, 19, "MetricName", "F1"]]}
{"text": "Settings Paraphrase identification ( PI ) is the task of identifying whether two sentences have the same meaning or not .", "entities": [[1, 3, "TaskName", "Paraphrase identification"]]}
{"text": "We conduct experiments on the two widely - used datasets : MRPC ( Dolan and Brockett , 2005 ) and QQP .", "entities": [[11, 12, "DatasetName", "MRPC"], [20, 21, "DatasetName", "QQP"]]}
{"text": "F1 score is reported for comparison .", "entities": [[0, 2, "MetricName", "F1 score"]]}
{"text": "We use BERT ( Devlin et al , 2018 ) and XLNet", "entities": [[2, 3, "MethodName", "BERT"], [11, 12, "MethodName", "XLNet"]]}
{"text": "We find that replacing the training objective with DSC introduces performance boost for both settings , +0.58 for MRPC and +0.73 for QQP .", "entities": [[18, 19, "DatasetName", "MRPC"], [22, 23, "DatasetName", "QQP"]]}
{"text": "We use the paraphrase identification dataset QQP ( 37 % positive and 63 % negative ) for studies .", "entities": [[3, 5, "TaskName", "paraphrase identification"], [6, 7, "DatasetName", "QQP"]]}
{"text": "To construct datasets with different imbalance degrees , we used the original QQP dataset to construct synthetic training sets with different positive - negative ratios .", "entities": [[12, 13, "DatasetName", "QQP"]]}
{"text": "Despite the fact that - negative creates a balanced dataset , the number of training data decreases , resulting in inferior performances . DSC achieves the highest F1 score across all datasets .", "entities": [[27, 29, "MetricName", "F1 score"]]}
{"text": "We argue that the cross - entropy objective is actually accuracy - oriented , whereas the proposed losses perform as a soft version of F1 score .", "entities": [[10, 11, "MetricName", "accuracy"], [24, 26, "MetricName", "F1 score"]]}
{"text": "To These results verify that the proposed dice loss is not accuracy - oriented , and should not be used for accuracy - oriented tasks .", "entities": [[7, 9, "MethodName", "dice loss"], [11, 12, "MetricName", "accuracy"], [21, 22, "MetricName", "accuracy"]]}
{"text": "In this subsection , we explore the effect of hyperparameters ( i.e. , \u03b1 and \u03b2 ) in TI to test how they manipulate the tradeoff .", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [15, 16, "HyperparameterName", "\u03b2"]]}
{"text": "We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset .", "entities": [[7, 8, "TaskName", "NER"], [11, 12, "DatasetName", "QuoRef"]]}
{"text": "In addition , we can observe that the performance varies a lot as \u03b1 changes in distinct datasets , which shows that the hyperparameters \u03b1 , \u03b2 acturally play an important role in TI .", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b1"], [26, 27, "HyperparameterName", "\u03b2"]]}
{"text": "We set \u03b2", "entities": [[2, 3, "HyperparameterName", "\u03b2"]]}
{"text": "Datasets For the NER task , we consider both Chinese datasets , i.e. , OntoNotes4.0 5 and MSRA 6 , and English datasets , i.e. , CoNLL2003 7 and OntoNotes5.0 8 .", "entities": [[3, 4, "TaskName", "NER"], [26, 27, "DatasetName", "CoNLL2003"]]}
{"text": "CoNLL2003 is an English dataset with 4 entity types : Location , Organization , Person and Miscellaneous .", "entities": [[0, 1, "DatasetName", "CoNLL2003"], [16, 17, "TaskName", "Miscellaneous"]]}
{"text": "SQuAD v1.1 and SQuAD v2.0 are the most widely used QA benchmarks .", "entities": [[0, 1, "DatasetName", "SQuAD"], [3, 4, "DatasetName", "SQuAD"]]}
{"text": "SQuAD1.1 is a collection of 100 K crowdsourced question - answer pairs , and SQuAD2.0 extends SQuAD1.1 allowing no short answer exists in the provided passage .", "entities": [[0, 1, "DatasetName", "SQuAD1.1"], [14, 15, "DatasetName", "SQuAD2.0"], [16, 17, "DatasetName", "SQuAD1.1"]]}
{"text": "MRPC is a corpus of sentence pairs automatically extracted from online news sources , with human annotations of whether the sentence pairs are semantically equivalent .", "entities": [[0, 1, "DatasetName", "MRPC"]]}
{"text": "The MRPC dataset has imbalanced classes ( 6800 pairs in total , and 68 % for positive , 32 % for negative ) .", "entities": [[1, 2, "DatasetName", "MRPC"]]}
{"text": "QQP is a collection of question pairs from the community question - answering website Quora .", "entities": [[0, 1, "DatasetName", "QQP"]]}
{"text": "The class distribution in QQP is also unbalanced ( over 400 , 000 question pairs in total , and 37 % for positive , 63 % for negative ) .", "entities": [[4, 5, "DatasetName", "QQP"]]}
{"text": "In addition to its utility for improving highstakes exams , the problem of modeling response process complexity is interesting from an NLP perspective because it requires the modeling of cognitive processes beyond reading comprehension .", "entities": [[32, 34, "TaskName", "reading comprehension"]]}
{"text": "This is especially relevant for the data used here because , as we explain in Section 3 below , the items in our bank assess expert - level clinical knowledge and are written to a common reading level using standardized language .", "entities": [[28, 30, "TaskName", "clinical knowledge"]]}
{"text": "We use unsupervised clustering to define classes of high and low responseprocess complexity from a large sample of items and test - takers in a high - stakes medical exam ; ii ) the study provides empirical evidence that linguistic characteristics carry signal relevant to an item 's response process complexity ; iii ) the most predictive features are identified through several feature selection methods and their potential relationship to response process complexity is discussed ; iv ) the errors made by the model and their implications for predicting response process complexity are analysed .", "entities": [[62, 64, "MethodName", "feature selection"]]}
{"text": "Most NLP studies modeling the difficulty of test questions for humans have been conducted in the domain of reading comprehension , where the readability of reading passages is associated with the difficulty of their corresponding comprehension questions ( Huang et al , 2017 ; Beinborn et al , 2015 ; Loukina et al , 2016 ) .", "entities": [[18, 20, "TaskName", "reading comprehension"]]}
{"text": "Continuations of this study include the use of transfer learning to predict difficulty and response time ( Xue et al , 2020 ) , as well as using predicted difficulty for filtering out items that are too easy or too difficult for the intended examinee population .", "entities": [[8, 10, "TaskName", "transfer learning"]]}
{"text": "The data 1 used in this study comprises 18 , 961 Step 2 Clinical Knowledge items from the United States Medical Licensing Examination ( USMLE \u00ae ) , a large - scale high - stakes medical assessment .", "entities": [[13, 15, "TaskName", "Clinical Knowledge"]]}
{"text": "i , U n is the 0 - 1 score ( incorrect - correct ) on item i earned by examinee n , and N is the total number of examinees in the sample .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "Thus , difficulty measured in this way ranges from 0 to 1 and higher values correspond to easier items .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "To assign items to classes , p - value and mean response time are rescaled such that each variable has a mean of 0 and a standard deviation of 1 .", "entities": [[23, 24, "DatasetName", "0"]]}
{"text": "Moreover , we use two quantitative methods to categorize items and retain only those items where there was agreement between the two methods . Method 1 : Items were classified by applying a K - means clustering algorithm via the kmeans function in Python 's Scikit - learn ( Pedregosa et al , 2011 ) .", "entities": [[33, 37, "MethodName", "K - means clustering"]]}
{"text": "This group of features relates to the medical content of the items by mapping terms and phrases in the text to medical concepts contained in the Unified Medical Language System ( UMLS ) Metathesaurus ( Schuyler et al , 1993 )", "entities": [[31, 32, "DatasetName", "UMLS"]]}
{"text": "the item contains ( note that a given term found in the items can refer to multiple UMLS concepts ) .", "entities": [[17, 18, "DatasetName", "UMLS"]]}
{"text": "This information is captured by UMLS Terms Count , indicating the number of terms in an item that appear in the UMLS wherein each instance of a given term contributes to the total count , as well as UMLS Distinct Terms Count : the number of terms in an item that appear in the UMLS wherein multiple instances of a given term contribute only once to the total count .", "entities": [[5, 6, "DatasetName", "UMLS"], [21, 22, "DatasetName", "UMLS"], [38, 39, "DatasetName", "UMLS"], [54, 55, "DatasetName", "UMLS"]]}
{"text": "The same kinds of counts are done for medical phrases - UMLS Phrases Count refers to the number of phrases in an item .", "entities": [[11, 12, "DatasetName", "UMLS"]]}
{"text": "These include Average Number of Competing UMLS Concepts Per Term Count , which captures the average number of UMLS concepts that a term could be referring to , averaged for all terms in an item , and weighted by the number of times Metamap returns the term .", "entities": [[6, 7, "DatasetName", "UMLS"], [18, 19, "DatasetName", "UMLS"]]}
{"text": "A similar version of this feature but without weighting by the number of times Metamap returns the term is Average Number of UMLS Concepts Per Term Count .", "entities": [[22, 23, "DatasetName", "UMLS"]]}
{"text": "This metric is then computed at the level of sentences and items , resulting in : Average Number of UMLS Concepts per Sentence , which measures the medical ambigu - ity of sentences and UMLS Concept Count , which measures item medical ambiguity through the total number of UMLS concepts all terms in an item could refer to .", "entities": [[19, 20, "DatasetName", "UMLS"], [34, 35, "DatasetName", "UMLS"], [48, 49, "DatasetName", "UMLS"]]}
{"text": "Finally , UMLS concept incidence refers to the number of UMLS concepts per 1000 words .", "entities": [[2, 3, "DatasetName", "UMLS"], [10, 11, "DatasetName", "UMLS"]]}
{"text": "This section describes three baseline models ( Section 4.5 ) , the training of classifiers using the full feature set ( Section 4.6 ) , and the feature selection procedures ( Section 4.7 ) .", "entities": [[27, 29, "MethodName", "feature selection"]]}
{"text": "After scaling the features , two models were fit using Python 's scikit - learn library and the full set of features : a logistic regression model and a random forests one ( 400 trees ) .", "entities": [[24, 26, "MethodName", "logistic regression"]]}
{"text": "Feature selection was undertaken to better understand which features were most strongly associated with class differences .", "entities": [[0, 2, "MethodName", "Feature selection"]]}
{"text": "After applying feature selection to the training set , the predictive performance of the selected features is evaluated on the test set and compared to the performance of the full feature set and the baseline models outlined above .", "entities": [[2, 4, "MethodName", "feature selection"]]}
{"text": "Table 3 presents the classification results for the baselines , the full feature set , and the selected features for both logistic regression and random forests .", "entities": [[21, 23, "MethodName", "logistic regression"]]}
{"text": "Results are reported using a weighted F1 score , which is a classification accuracy measure based on the mean between the precision and recall after adjusting for class imbalance .", "entities": [[6, 8, "MetricName", "F1 score"], [13, 14, "MetricName", "accuracy"]]}
{"text": "UMLS phrases count , Unique word count , Polysemic word count , Average noun phrase length , Automated readability index , Prepositional phrases , UMLS distinct terms count , and Concreteness ratio .", "entities": [[0, 1, "DatasetName", "UMLS"], [24, 25, "DatasetName", "UMLS"]]}
{"text": "As can be seen in Figure 2 , high - complexity items contain a slightly higher number of UMLS phrases and ( distinct ) medical terms , as well as a higher number of unique words .", "entities": [[18, 19, "DatasetName", "UMLS"]]}
{"text": "2016 Task 1 : Multiple Approaches to Measuring Semantic Textual Similarity", "entities": [[8, 11, "TaskName", "Semantic Textual Similarity"]]}
{"text": "This paper describes our participation in the SemEval - 2016 Task 1 : Semantic Textual Similarity ( STS ) .", "entities": [[13, 16, "TaskName", "Semantic Textual Similarity"], [17, 18, "TaskName", "STS"]]}
{"text": "We developed three methods for the English subtask ( STS Core ) .", "entities": [[9, 10, "TaskName", "STS"]]}
{"text": "The third method uses word2vec and LDA with regression splines .", "entities": [[6, 7, "MethodName", "LDA"]]}
{"text": "Measuring semantic textual similarity ( STS ) is the task of determining the similarity between two different text passages .", "entities": [[1, 4, "TaskName", "semantic textual similarity"], [5, 6, "TaskName", "STS"]]}
{"text": "The task is important for various natural language processing tasks like topic detection or automated text summarization because languages are versatile and authors can express similar content or even the same content with different words .", "entities": [[15, 17, "TaskName", "text summarization"]]}
{"text": "Predicting semantic textual similarity has been a recurring task in SemEval challenges ( Agirre et al , 2015 ; Agirre et al , 2014 ; Agirre et al , 2013 ; Agirre et al , 2012 ) .", "entities": [[1, 4, "TaskName", "semantic textual similarity"]]}
{"text": "As in previous years , the purpose of the STS task is the development of systems that automatically predict the semantic similarity of two sentences in the continuous interval [ 0 , 5 ] where 0 represents a complete dissimilarity and 5 denotes a complete semantic equivalence between the sentences ( Agirre et al , 2015 ) .", "entities": [[9, 10, "TaskName", "STS"], [20, 22, "TaskName", "semantic similarity"], [30, 31, "DatasetName", "0"], [35, 36, "DatasetName", "0"]]}
{"text": "The quality of a system is determined by calculating the Pearson correlation between the predicted values and a human gold standard that has been created by crowdsourcing .", "entities": [[10, 12, "MetricName", "Pearson correlation"]]}
{"text": "The data from previous STS tasks can be used for training supervised methods .", "entities": [[4, 5, "TaskName", "STS"]]}
{"text": "In this year 's shared task , the systems are tested on five different categories with different topics and varying textual characteristics like text length or spelling errors : answer - answer , plagiarism , postediting , headlines , and question - question The remainder of the paper is structured as follows : Section 2 discusses related approaches to automatically determining semantic textual similarity .", "entities": [[61, 64, "TaskName", "semantic textual similarity"]]}
{"text": "In the last shared tasks , most of the teams used natural languages processing techniques like tokenization , part - of - speech tagging , lemmatization , named entity recognition and word embeddings .", "entities": [[18, 24, "TaskName", "part - of - speech tagging"], [25, 26, "TaskName", "lemmatization"], [27, 30, "TaskName", "named entity recognition"], [31, 33, "TaskName", "word embeddings"]]}
{"text": "In ( Vu et al , 2015 ) , the similarity between LDA vectors calculated from documents is used together with syntactic and lexical similarity measures to compute the similarity between text fragments .", "entities": [[12, 13, "MethodName", "LDA"]]}
{"text": "This idea is also incorporated in our Deep LDA method .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "If the WordnetStemmer can not provide a lemma for a token , we use the predicted lemma from the Stanford CoreNLP .", "entities": [[7, 8, "DatasetName", "lemma"], [16, 17, "DatasetName", "lemma"]]}
{"text": "We look up both words ( or their lemmas if the words are not present in the model ) and calculate the cosine similarity of their word embeddings .", "entities": [[26, 28, "TaskName", "word embeddings"]]}
{"text": "= = t 2 .lemma 1 if t 1 and t 2 have the same most common synset 0.5 if t 1 and t 2 share any synset d ( t 1 , t 2 ) if t 1 and t 2 have word embeddings default otherwise where d ( t 1 , t 2 ) denotes the cosine similarity between the two word embeddings of the tokens .", "entities": [[43, 45, "TaskName", "word embeddings"], [63, 65, "TaskName", "word embeddings"]]}
{"text": "STS ( s 1 , s 2 ) : = 5 ssim ( s 1 , s 2 )", "entities": [[0, 1, "TaskName", "STS"], [11, 12, "TaskName", "ssim"]]}
{"text": "We observed that some samples in the STS 2016 test data consist almost entirely of stopwords .", "entities": [[7, 8, "TaskName", "STS"]]}
{"text": "For example , the STS 2016 evaluation data contained a sample with the sentences \" I think you should do both . \"", "entities": [[4, 5, "TaskName", "STS"]]}
{"text": "We train a neural network with 3 layers and a sigmoid activation function in Accord .", "entities": [[10, 12, "MethodName", "sigmoid activation"]]}
{"text": "We use the Levenberg - Marquardt algorithm ( Levenberg , 1944 ; Marquardt , 1963 ) to train our network on the STS Core test data from 2015 and 2014 .", "entities": [[22, 23, "TaskName", "STS"]]}
{"text": "For each lemma l , we calculate the minimum number of times l occurs in each sentence and the delta between the minimum and the maximum :", "entities": [[2, 3, "DatasetName", "lemma"]]}
{"text": "Table i Lemma bow s 1 bow s 2 min", "entities": [[2, 3, "DatasetName", "Lemma"]]}
{"text": "i | 1 tim 1 1 1 0 2 play 1 0 0 1 3 guitar 1 1 1 0 4 like 0 1 0 1 5 song 0 1 0 1 2 3", "entities": [[7, 8, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [22, 23, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}
{"text": "We represent the semantic similarity between two documents s 1 and s 2 by means of a vector F", "entities": [[3, 5, "TaskName", "semantic similarity"]]}
{"text": "= [ f 1 , f 2 , f 3 , f 4 ] R 4 , where each component of F is responsible for modelling a different aspect of the semantic similarity , namely the surface - level similarity , context similarity , and the topical similarity .", "entities": [[31, 33, "TaskName", "semantic similarity"]]}
{"text": "The surface - level similarity can to some extent ( although not entirely ) capture the semantic similarity between documents .", "entities": [[16, 18, "TaskName", "semantic similarity"]]}
{"text": "al , 2002 ) metric ( standard machine translation metric ) where the brevity penalty is eliminated .", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "In order to model the context similarity between documents , we use word embeddings that learn semantically meaningful representations for words from local co - occurrences in sentences .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "To model the topical similarity between two documents , we use Latent Dirichlet Allocation ( LDA ) ( Blei et al , 2003 ) to train models on the English Wikipedia .", "entities": [[15, 16, "MethodName", "LDA"]]}
{"text": "In order to predict the semantic similarity between two documents , we use a combination of k - NN and Multivariate Adaptive Regression Splines ( MARS ) ( Friedman , 1991 ) .", "entities": [[5, 7, "TaskName", "semantic similarity"], [16, 19, "MethodName", "k - NN"], [25, 26, "DatasetName", "MARS"]]}
{"text": ", ( s m , s m , gs m ) } be the training set consisting of m document pairs together with their corresponding gold standard semantic similarity and ( s i , s i ) /", "entities": [[27, 29, "TaskName", "semantic similarity"]]}
{"text": "T be a document pair for which the semantic similarity has to be computed .", "entities": [[8, 10, "TaskName", "semantic similarity"]]}
{"text": ", ( F m , gs m ) } where each F j is the four - dimensional vector representation of the semantic similarity between s j and s j .", "entities": [[22, 24, "TaskName", "semantic similarity"]]}
{"text": "Moreover , we Sentence 1 Sentence 2 gs STS", "entities": [[8, 9, "TaskName", "STS"]]}
{"text": "Next , we construct a set F k containing the k - nearest neighbors to the vector F i .", "entities": [[10, 14, "MethodName", "k - nearest neighbors"]]}
{"text": "Finally , we construct a vector gs k containing the gold standard similarity values of the k - nearest neighbors and feed it into a MARS model to predict the semantic similarity of the pair ( s i , s i ) .", "entities": [[16, 20, "MethodName", "k - nearest neighbors"], [25, 26, "DatasetName", "MARS"], [30, 32, "TaskName", "semantic similarity"]]}
{"text": "The choice of MARS is due to its capability to automatically model non - linearities between variables .", "entities": [[3, 4, "DatasetName", "MARS"]]}
{"text": "We report the results of our three approaches for the STS Core test from 2016 and 2015 .", "entities": [[10, 11, "TaskName", "STS"]]}
{"text": "In this year 's run , our best result was the Overlap method , followed by the Same Word Neural Network method and the Deep LDA approach .", "entities": [[25, 26, "MethodName", "LDA"]]}
{"text": "From a semantic point of view , the most obvious value for the default value in our Overlap method is 0 .", "entities": [[20, 21, "DatasetName", "0"]]}
{"text": "However , we have discovered that a default value 0.15 returned better results on the STS Core test data from 2015 and also chose this default value for our submission .", "entities": [[15, 16, "TaskName", "STS"]]}
{"text": "In the Deep LDA approach , we set the parameter N = 2 , although the use of unigrams did not show any significant statistical difference in the results .", "entities": [[3, 4, "MethodName", "LDA"]]}
{"text": "We choose the number of topics in the LDA model to be 300 .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "It is interesting to see that the Deep LDA method performed best out of our three systems on 2015 .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "For the training phase , the Same Word Neural Network method used the STS Core test from 2014 .", "entities": [[13, 14, "TaskName", "STS"]]}
{"text": "The Deep LDA method was trained on the data from 2012 to 2014 .", "entities": [[2, 3, "MethodName", "LDA"]]}
{"text": "We have presented three approaches to measure textual semantic similarity .", "entities": [[8, 10, "TaskName", "semantic similarity"]]}
{"text": "Using Large Pretrained Language Models for Answering User Queries from Product Specifications", "entities": [[2, 5, "TaskName", "Pretrained Language Models"]]}
{"text": "From the perspective of both the e - commerce service provider as well as the customers , there must be an effective question answering system to provide immediate answers to the user queries .", "entities": [[22, 24, "TaskName", "question answering"]]}
{"text": "We utilize recently proposed XLNet and BERT architectures for this problem and find that they provide much better performance than the Siamese model , previously applied for this problem ( Lai et al , 2018 ) .", "entities": [[4, 5, "MethodName", "XLNet"], [6, 7, "MethodName", "BERT"]]}
{"text": "1 Flipkart Pvt Ltd. is an e - commerce company based in Bangalore , India .", "entities": [[2, 3, "MethodName", "Pvt"]]}
{"text": "( ii ) We utilize a method to automatically create a large training dataset using a semisupervised approach , that was used to fine - tune XLNet and other models .", "entities": [[26, 27, "MethodName", "XLNet"]]}
{"text": "In recent years , e - commerce product question answering ( PQA ) has received a lot of attention .", "entities": [[8, 10, "TaskName", "question answering"]]}
{"text": "It assumes that a candidate answer set containing the correct answers is available for answer selection .", "entities": [[14, 16, "TaskName", "answer selection"]]}
{"text": "Cui et al ( 2017 ) develop a chatbot for e - commerce sites known as SuperAgent .", "entities": [[8, 9, "TaskName", "chatbot"]]}
{"text": "Language representation models like BERT ( Devlin et al , 2019 ) and XLNet", "entities": [[4, 5, "MethodName", "BERT"], [13, 14, "MethodName", "XLNet"]]}
{"text": "The resulting models have achieved state of the art in many natural language processing tasks including question answering .", "entities": [[16, 18, "TaskName", "question answering"]]}
{"text": "Dzendzik et al ( 2019 ) employ BERT to answer binary questions by utilizing customer reviews .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "We leverage the product specifications to answer user queries by using BERT and XLNet .", "entities": [[11, 12, "MethodName", "BERT"], [13, 14, "MethodName", "XLNet"]]}
{"text": "We fine - tune BERT and XLNet for this classification task .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "XLNet"]]}
{"text": "In the Siamese model , the question and specification is passed through a Siamese Bi - LSTM layer .", "entities": [[16, 17, "MethodName", "LSTM"]]}
{"text": "Finally , the softmax layer gives the relevance score .", "entities": [[3, 4, "MethodName", "softmax"]]}
{"text": "BERT and XLNet : The architecture we use for fine - tuning BERT and XLNet is the same .", "entities": [[0, 1, "MethodName", "BERT"], [2, 3, "MethodName", "XLNet"], [12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "XLNet"]]}
{"text": "We begin with the pre - trained BERT Base and XLNet Base model .", "entities": [[7, 8, "MethodName", "BERT"], [10, 11, "MethodName", "XLNet"]]}
{"text": "During fine - tuning , we optimize the entire model end - to - end , with the additional softmax classifier parameters W R K\u00d7H , where H is the dimen - sion of the hidden state vectors and K is the number of classes .", "entities": [[19, 20, "MethodName", "softmax"]]}
{"text": "We consider ( q i , s j ) pair positive if DU AL ( q i , s j ) \u2265 \u03b8 and negative if DU AL ( q i , s j ) <", "entities": [[22, 23, "HyperparameterName", "\u03b8"]]}
{"text": "j ( 1 ) where s j = 1 | s j | ts s j t s t s", "entities": [[13, 14, "MethodName", "ts"]]}
{"text": "The fine - tuning of BERT and XL - Net is done with the same experimental settings as given in the original papers .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "In all the models , we minimize the cross - entropy loss while training .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "BERT - 380 and XLNet - 380 models are fine - tuned on the 380 labeled validation dataset that was used for creating the training dataset in Section 5.1 .", "entities": [[0, 1, "MethodName", "BERT"], [4, 5, "MethodName", "XLNet"], [16, 18, "DatasetName", "validation dataset"]]}
{"text": "BERT - 380 and XLNet - 380 perform very poorly , but when we use the train dataset created with DESM , there is a large boost in the models ' performance and it shows the effectiveness of our semi - supervised method in generating labeled dataset .", "entities": [[0, 1, "MethodName", "BERT"], [4, 5, "MethodName", "XLNet"]]}
{"text": "Both BERT and XLNet outperform the baseline Siamese model ( Lai et al , 2018 ) by a large margin , and retrieve the correct specification within top 3 results for most of the queries .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "MethodName", "XLNet"]]}
{"text": "For Backpack and AC , both BERT and XLNet are very competitive .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "XLNet"]]}
{"text": "XLNet outperforms BERT in Computer , Shoes , and Watches .", "entities": [[0, 1, "MethodName", "XLNet"], [2, 3, "MethodName", "BERT"]]}
{"text": "Only in HIT@1 of AC , BERT has surpassed XLNet with 0.07 points .", "entities": [[6, 7, "MethodName", "BERT"], [9, 10, "MethodName", "XLNet"]]}
{"text": "On the other hand , BERT and XLNet are able to retrieve the correct specifications .", "entities": [[5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "XLNet"]]}
{"text": "Error Analysis : We assume that for each question , there is only one correct specification , but the correct answer may span multiple specifications and our models can not provide a full answer .", "entities": [[0, 1, "MetricName", "Error"]]}
{"text": "We demonstrated that large pretrained language models such as BERT and XLNet can be fine - tuned successfully to obtain product specifications that can help answer user queries .", "entities": [[4, 7, "TaskName", "pretrained language models"], [9, 10, "MethodName", "BERT"], [11, 12, "MethodName", "XLNet"]]}
{"text": "In this paper , we propose MoKGE , a novel method that diversifies the generative reasoning by a mixture of expert ( MoE ) strategy on commonsense knowledge graphs ( KG ) .", "entities": [[27, 29, "TaskName", "knowledge graphs"]]}
{"text": "Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks , based on both automatic and human evaluations .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "Diversity in NLG has been extensively studied for various tasks in the past few years , such as machine translation ( Shen et al , 2019 ) and paraphrase Codes of our model and baselines are available at https://github.com/DM2 - ND / MoKGE .", "entities": [[18, 20, "TaskName", "machine translation"]]}
{"text": "Figure 1 shows an example in the commonsense explanation generation ( ComVE ) task .", "entities": [[8, 10, "TaskName", "explanation generation"]]}
{"text": "To improve the diversity in outputs for GCR tasks , we investigated the ComVE task and found that 75 % of the concepts ( nouns and verbs ) in human annotations were among 2 - hop neighbors of the concepts contained in the input sequence on the commonsense KG ConceptNet 1 .", "entities": [[49, 50, "DatasetName", "ConceptNet"]]}
{"text": "Then , each expert uses these representations to seek different yet relevant sets of concepts and sends them into a standard Transformer model to generate the corresponding output .", "entities": [[21, 22, "MethodName", "Transformer"]]}
{"text": "To encourage different experts to specialize in different reasoning abilities , we employ the stochastic hard - EM algorithm by assigning full responsibility of the largest joint probability to each expert .", "entities": [[17, 18, "MetricName", "EM"]]}
{"text": "We conducted experiments on two GCR benchmarks , i.e. , commonsense explanation generation and abductive commonsense reasoning .", "entities": [[11, 13, "TaskName", "explanation generation"]]}
{"text": "Generating multiple valid outputs given a source sequence has a wide range of applications , such as machine translation ( Shen et al , 2019 ) , paraphrase generation ( Gupta et al , 2018 ) , question generation ( Cho et al , 2019 ) , dialogue system ( Dou et al , 2021 ) , and story generation .", "entities": [[17, 19, "TaskName", "machine translation"], [27, 29, "TaskName", "paraphrase generation"], [37, 39, "TaskName", "question generation"], [58, 60, "TaskName", "story generation"]]}
{"text": "For example , in machine translation , there are often many plausible and semantically equivalent translations due to information asymmetry between different languages ( Lachaux et al , 2020 ) .", "entities": [[4, 6, "TaskName", "machine translation"]]}
{"text": "Shi et al ( 2018 ) employed an inverse reinforcement learning approach for unconditional diverse text generation .", "entities": [[15, 17, "TaskName", "text generation"]]}
{"text": "For example , Zhou et al ( 2018 ) enriched the context representations of the input sequence with neighbouring concepts on ConceptNet using graph attention .", "entities": [[21, 22, "DatasetName", "ConceptNet"]]}
{"text": "Recently , some work attempted to integrate external commonsense knowledge into generative pretrained language models ( Guan et al , 2020 ; Bhagavatula et al , 2020 ; Liu et al , 2021 ) .", "entities": [[12, 15, "TaskName", "pretrained language models"]]}
{"text": "In this paper , we focus on diversifying the outputs of generative commonsense reasoning ( GCR ) tasks , e.g. commonsense explanation generation and abductive commonsense reasoning .", "entities": [[21, 23, "TaskName", "explanation generation"]]}
{"text": "To model the relational information in the commonsen KG , we employ the relational graph convolutional network ( R - GCN ) ( Schlichtkrull et al , 2018 ) which generalizes GCN with relation specific weight matrices .", "entities": [[14, 17, "MethodName", "graph convolutional network"], [20, 21, "MethodName", "GCN"], [31, 32, "MethodName", "GCN"]]}
{"text": "Specifically , given the input subgraph G x = { V x , E x } and an R - GCN with L layers , we update the embedding of each node v V x at the ( l+1 ) - th layer by aggregating information from the embeddings of its neighbours in N ( v ) at the l - th layer : o", "entities": [[20, 21, "MethodName", "GCN"]]}
{"text": "( u , v , r ) E W l N \u03d5 ( h l u , h l r ) , ( 1 ) h l+1 v = ReLU", "entities": [[29, 30, "MethodName", "ReLU"]]}
{"text": "= h u \u2212h r inspired by the TransE ( Bordes et al , 2013 ) .", "entities": [[8, 9, "MethodName", "TransE"]]}
{"text": "For each concept v V x , we calculate its probability of being selected by taking a multilayer perception ( MLP ) on the top of graph encoder : p v = P r [ v is selected |", "entities": [[20, 21, "DatasetName", "MLP"]]}
{"text": "x ] = MLP ( h L v ) .", "entities": [[3, 4, "DatasetName", "MLP"]]}
{"text": "So , the concept selection loss ( here only for one expert , see MoE loss in Eq .", "entities": [[5, 6, "MetricName", "loss"], [15, 16, "MetricName", "loss"]]}
{"text": "We utilize a standard Transformer ( Vaswani et al , 2017 ) as our generation model .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "We adopt the cross - entropy loss , which can be written as : L generation = \u2212 log p", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "Note that since the selected concepts do not have a rigorous order , we only apply positional encodings ( used in Transformer ) to the input sequence x.", "entities": [[21, 22, "MethodName", "Transformer"]]}
{"text": "We jointly optimizes the following loss : L = L generation + \u03bb L concept .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "We minimize the loss function ( in Eq .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "Commonsense explanation generation .", "entities": [[1, 3, "TaskName", "explanation generation"]]}
{"text": "We use the benchmark dataset ComVE from SemEval - 2020 Task 4 ( Wang et al , 2020 ) .", "entities": [[2, 4, "DatasetName", "the benchmark"]]}
{"text": "VAE - based method .", "entities": [[0, 1, "MethodName", "VAE"]]}
{"text": "The variational auto - encoder ( VAE ) ( Kingma and Welling , 2014 ) is a deep generative latent variable model .", "entities": [[6, 7, "MethodName", "VAE"]]}
{"text": "VAE - based methods produce diverse outputs by sampling different latent variables from an approximate posterior distribution .", "entities": [[0, 1, "MethodName", "VAE"]]}
{"text": "CVAE - SVG ( SVG is short for sentence variant generation )", "entities": [[0, 1, "MethodName", "CVAE"]]}
{"text": "( Gupta et al , 2018 ) is a conditional VAE model that can produce multiple outputs based an original sentence as input .", "entities": [[10, 11, "MethodName", "VAE"]]}
{"text": "All baseline methods were built on the Transformer architecture with 6 - layer encoder and decoder , and initialized with pre - trained parameters from BARTbase ( Lewis et al , 2020 ) , which is one of the stateof - the - art pre - trained Transformer models for natural language generation ( Gehrmann et al , 2021 ) .", "entities": [[7, 8, "MethodName", "Transformer"], [47, 48, "MethodName", "Transformer"]]}
{"text": "In our MoKGE , the Transformer parameters were also initialized by BART - base , in order to make fair comparison with all baseline methods .", "entities": [[5, 6, "MethodName", "Transformer"], [11, 12, "MethodName", "BART"]]}
{"text": "The R - GCN parameters were random initialized .", "entities": [[3, 4, "MethodName", "GCN"]]}
{"text": "Our models were trained by one Tesla V100 GPU card with 32 GB memory , and implemented on PyTorch with the Huggingface 's Transformer ( Wolf et al , 2020 ) .", "entities": [[23, 24, "MethodName", "Transformer"]]}
{"text": "We evaluated the performance of different generation models from two aspects : quality ( or say accuracy ) and diversity .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "Referred as \" self - \" ( e.g. , self - BLEU ) , it measures the within - distribution similarity .", "entities": [[11, 12, "MetricName", "BLEU"]]}
{"text": "Automatic diversity evaluation ( e.g. , Self - BLEU , Distinct - k ) can not reflect the content - level diversity .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "The diversity and quality scores are normalized to the range from 0 to 3 .", "entities": [[11, 12, "DatasetName", "0"]]}
{"text": "meanings , e.g. , \" go to the zoo and see elephants \" and \" took him to the zoo and see elephants \" in the \u03b1 - NLG case .", "entities": [[26, 27, "HyperparameterName", "\u03b1"]]}
{"text": "Most of the existing diversity - promoting work has focused on improving syntactic and lexical diversity , such as different language style in machine translation ( Shen et al , 2019 ) and word variability in paraphrase generation ( Gupta et al , 2018 ) .", "entities": [[23, 25, "TaskName", "machine translation"], [36, 38, "TaskName", "paraphrase generation"]]}
{"text": "BERT - based distractor generation for Swedish reading comprehension questions using a small - scale dataset", "entities": [[0, 1, "MethodName", "BERT"], [3, 5, "TaskName", "distractor generation"], [7, 9, "TaskName", "reading comprehension"]]}
{"text": "An important part when constructing multiplechoice questions ( MCQs ) for reading comprehension assessment are the distractors , the incorrect but preferably plausible answer options .", "entities": [[11, 13, "TaskName", "reading comprehension"]]}
{"text": "Multiple - choice questions ( MCQs ) are widely used for student assessments , from high - stakes graduation tests to lower - stakes reading comprehension tests .", "entities": [[24, 26, "TaskName", "reading comprehension"]]}
{"text": "Given the challenges above , we attempt using machine learning ( ML ) to aid teachers in creating distractors for reading comprehension MCQs .", "entities": [[20, 22, "TaskName", "reading comprehension"]]}
{"text": "The key contributions of this work are : proposing a BERT - based method for generating distractors using only a small - scale dataset , releasing SweQUAD - MC 1 , a dataset of Swedish MCQs , and proposing a methodology for conducting human evaluation aimed at assessing the plausibility of distractors .", "entities": [[10, 11, "MethodName", "BERT"]]}
{"text": "2 Background 2.1 BERT for NLG Devlin et al ( 2019 ) introduced BERT as the first application of the Transformer architecture ( Vaswani et al , 2017 ) to language modelling .", "entities": [[3, 4, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"], [20, 21, "MethodName", "Transformer"], [30, 32, "TaskName", "language modelling"]]}
{"text": "BERT uses only Transformer 's encoder stacks ( with multihead self - attention , MHSA ) , while the NLG community relies more on Transformer 's decoder stacks ( with masked MHSA ) for text generation , e.g. , GPT ( Radford et al , 2018 ) .", "entities": [[0, 1, "MethodName", "BERT"], [3, 4, "MethodName", "Transformer"], [24, 25, "MethodName", "Transformer"], [34, 36, "TaskName", "text generation"], [39, 40, "MethodName", "GPT"]]}
{"text": "However , Wang and Cho ( 2019 ) showed that BERT is a Markov random field , meaning that BERT learns a joint probability distribution over all sentences of a fixed length , and one could use Gibbs sampling to generate a new sentence .", "entities": [[10, 11, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"]]}
{"text": "The authors compared samples generated autoregressively left - to - right by BERT and GPT , and found the perplexity of BERT samples to be higher than GPT 's ( BERT samples are of worse quality ) , but the n - gram overlap between the generated texts and texts from the dataset to be lower ( BERT samples are more diverse ) .", "entities": [[12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "GPT"], [19, 20, "MetricName", "perplexity"], [21, 22, "MethodName", "BERT"], [27, 28, "MethodName", "GPT"], [30, 31, "MethodName", "BERT"], [57, 58, "MethodName", "BERT"]]}
{"text": "Liao et al ( 2020 ) show a way to improve BERT 's generation capabilities via changing the masking scheme to a probabilistic one at training time .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "x \u00b1 y shows mean x and a standard deviation y thors proposed to train a PMLM with a uniform prior ( referred to as u - PMLM ) .", "entities": [[16, 17, "MethodName", "PMLM"], [27, 28, "MethodName", "PMLM"]]}
{"text": "The authors showed that the perplexity of the texts generated by u - PMLM is comparable to the ones by GPT .", "entities": [[5, 6, "MetricName", "perplexity"], [13, 14, "MethodName", "PMLM"], [20, 21, "MethodName", "GPT"]]}
{"text": "We propose to use convolution partial tree kernels ( CPTK ) for these purposes .", "entities": [[4, 5, "MethodName", "convolution"]]}
{"text": "Given the small scale of SweQUAD - MC we have decided to fine - tune a pretrained BERT 2 for Swedish ( Malmsten et al , 2020 ) on the task of distractor generation ( DG ) .", "entities": [[17, 18, "MethodName", "BERT"], [32, 34, "TaskName", "distractor generation"]]}
{"text": "For achieving this , we have added on top of BERT two linear layers with layer normalization ( Ba et al , 2016 ) in the middle to be trained from scratch ( see architecture in Figure 1 ) .", "entities": [[10, 11, "MethodName", "BERT"], [15, 17, "MethodName", "layer normalization"]]}
{"text": "The last linear layer is followed by a softmax activation giving probabilities over the tokens in the vocabulary for each position in the text .", "entities": [[2, 4, "MethodName", "linear layer"], [8, 9, "MethodName", "softmax"]]}
{"text": "We trained the model using cross - entropy loss only for tokens in masked positions .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "Recall that each MCQ consists of a base text T , the stem Q based on T , the key A and ( on average ) two distractors D1 and D2 .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "We provide all context components as input to the BERT model , separated from each other by the special separator token [ SEP ] .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "Given that BERT 's maximum input length is 512 tokens , we trim T to the first 384 tokens ( later referred to as T 384 ) , since that is the average text length of the training set .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "At generation time , the input to BERT consists of a context CTX ( T 384 , Q and A separated by [ SEP ] token ) , a [ SEP ] token , and a [ MASK ] token at the end .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "After a forward pass through BERT , the [ MASK ] token gets replaced by the word with the highest softmax score , which becomes the first word of the first distractor ( dubbed D11 ) .", "entities": [[5, 6, "MethodName", "BERT"], [20, 21, "MethodName", "softmax"]]}
{"text": "The second variant is inspired by u - PMLM , and aims at generating distractors autoregressively , but in an arbitrary word order .", "entities": [[8, 9, "MethodName", "PMLM"]]}
{"text": "At generation time , the input to BERT consists of a context CTX , a [ SEP ] token , and a predefined number of [ MASK ] tokens ( see Section 6.1 ) .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "The training procedure largely follows a masking scheme employed by u - PMLM by drawing the masking ratio from the uniform distribution ( see example training datapoints for one MCQ in Table 2 ) .", "entities": [[12, 13, "MethodName", "PMLM"]]}
{"text": "Each sampled masking ratio r for the u - PMLM variant means that each token in the distractors from the dataset has a probability r to be masked .", "entities": [[9, 10, "MethodName", "PMLM"]]}
{"text": "Our baseline uses NCPTK on Universal Dependencies ( UD ) trees ( Nivre et al , 2020 ) in the following way .", "entities": [[5, 7, "DatasetName", "Universal Dependencies"], [8, 9, "DatasetName", "UD"]]}
{"text": "For each given MCQ , we exclude the sentence containing the key from the base text and then parse each remaining sentence s i of the text , and the key using the UD parser for Swedish .", "entities": [[33, 34, "DatasetName", "UD"]]}
{"text": "With these settings , training took about 3.67h for the left - to - right and 3h for the u - PMLM variant .", "entities": [[21, 22, "MethodName", "PMLM"]]}
{"text": "UD trees for the baseline were obtained using Stanza package ( Qi et al , 2020 ) and convolution partial tree kernels on the UD trees were calculated using UDon2 library ( Kalpakchi and Boye , 2020 ) .", "entities": [[0, 1, "DatasetName", "UD"], [18, 19, "MethodName", "convolution"], [24, 25, "DatasetName", "UD"]]}
{"text": "The first stage is quantitative evaluation , which gives limited information about the model 's quality , but is sufficient for model selection .", "entities": [[21, 23, "TaskName", "model selection"]]}
{"text": "Automatic evaluation metrics , such as BLEU ( Papineni et", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "Based on these metrics , we performed a model selection on the development set and chose the models performing best on the most of these metrics .", "entities": [[8, 10, "TaskName", "model selection"]]}
{"text": "Baseline When using u - PMLM , shortest distractors were generated first .", "entities": [[5, 6, "MethodName", "PMLM"]]}
{"text": "In contrast , u - PMLM needs the lengths of the distractors to be decided in beforehand , which we set to be the lengths of the two reference distractors and the length of the key 4 .", "entities": [[5, 6, "MethodName", "PMLM"]]}
{"text": "Surprisingly , the order of distractors in terms of their length also matters for generation with u - PMLM , so we have tested three options : shortest first , longest first and random order .", "entities": [[18, 19, "MethodName", "PMLM"]]}
{"text": "According to the results of model selection on the development set ( presented in detail in Appendix C ) , u - PMLM models outperformed left - to - right models by a substantial margin .", "entities": [[5, 7, "TaskName", "model selection"], [22, 23, "MethodName", "PMLM"]]}
{"text": "The best u - PMLM model ( generating shortest distractors first ) and the baseline have been evaluated on the test set ( see Table 3 ) .", "entities": [[4, 5, "MethodName", "PMLM"]]}
{"text": "Interestingly , the similarity of syntactic structures between the key and distractors ( assessed by NCPTK ) is the same for both baseline ( that actually relies on NCPTK ) and u - PMLM .", "entities": [[33, 34, "MethodName", "PMLM"]]}
{"text": "At the same time , u - PMLM generates more distractors matching the reference ones compared to the baseline ( as seen from DisRecall and AnyDisRefMatch ) .", "entities": [[7, 8, "MethodName", "PMLM"]]}
{"text": "The baseline generates at least one empty string as a distractor 11.76 % of the time ( compared to no such cases for u - PMLM ) limiting possibilities of using the baseline in the real - life applications .", "entities": [[25, 26, "MethodName", "PMLM"]]}
{"text": "We have used distractors generated on the test set by the best u - PMLM model ( selected after quantitative evaluation in Section 6.1 ) to conduct human evaluation in 2 stages : from a perspective of a student and a teacher .", "entities": [[14, 15, "MethodName", "PMLM"]]}
{"text": "5 H 0 : N s = N r .", "entities": [[2, 3, "DatasetName", "0"]]}
{"text": "The appropriate statistical test in this case is one - sample two - tailed t - test with the aim of not being able to reject H 0 .", "entities": [[27, 28, "DatasetName", "0"]]}
{"text": "Following the calculations above , we have recruited 54 subjects on the Prolific platform 6 , and instructed them to choose the most plausible answer to a number of reading comprehension MCQs without providing the original texts .", "entities": [[29, 31, "TaskName", "reading comprehension"]]}
{"text": "To summarize , the chances of this sample to be collected are very low if H 0 were true .", "entities": [[16, 17, "DatasetName", "0"]]}
{"text": "The teachers were instructed to select those of generated distractors ( if any ) deemed suitable for testing reading comprehension .", "entities": [[18, 20, "TaskName", "reading comprehension"]]}
{"text": "In this paper we work on reading comprehension MCQs , which makes only 12 papers , dealing with factual questions , relevant .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}
{"text": "The former used an ontology and the latter employed event graphs containing information about coreferences to generate distractors .", "entities": [[4, 5, "MethodName", "ontology"]]}
{"text": "Offerijns et al ( 2020 ) trained a GPT - 2 model to generate 3 distractors for a given MCQ , and used BERT - based question answering model for quantitative evaluation ( along with human evaluation ) .", "entities": [[8, 9, "MethodName", "GPT"], [23, 24, "MethodName", "BERT"], [26, 28, "TaskName", "question answering"]]}
{"text": "Our method also relies on BERT , but has a number of differences beyond being applied to Swedish .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "We have collected SweQUAD - MC , the first dataset of Swedish MCQs , and showed the possibility of training usable BERT - based DG models , despite the small scale of the dataset .", "entities": [[21, 22, "MethodName", "BERT"]]}
{"text": "We have showed that a u - PMLM variant of the BERT - based DG model performs best on the dataset , and proposed a novel methodology of evaluating the plausibility of generated distractors .", "entities": [[7, 8, "MethodName", "PMLM"], [11, 12, "MethodName", "BERT"]]}
{"text": "Bearing in mind that the aim of the proposed method is to support ( not replace ) teachers , we deem that our method works well for MCQs in Swedish ( and potentially in other languages with a pretrained BERT and a dataset of a similar scale ) .", "entities": [[39, 40, "MethodName", "BERT"]]}
{"text": "Furthermore , we have presented a baseline applicable to any language with a UD treebank ( currently about 100 languages ) .", "entities": [[13, 14, "DatasetName", "UD"]]}
{"text": "Although its performance is nowhere near the u - PMLM variant , we believe that it can serve as a good point of comparison to emerging neural methods for other languages .", "entities": [[9, 10, "MethodName", "PMLM"]]}
{"text": "We have trained both left - to - right and u - PMLM variants for 6 epochs ( fixing a random seed for u - PMLM masking procedure to 42 ) .", "entities": [[12, 13, "MethodName", "PMLM"], [25, 26, "MethodName", "PMLM"]]}
{"text": "The best u - PMLM model ( i - 14000 ) outperformed the best left - to - right model ( i - 18000 ) on most of the quantitative metrics .", "entities": [[4, 5, "MethodName", "PMLM"]]}
{"text": "The next experiment concerned the order in which distractors are generated , which we tested only for the best u - PMLM model .", "entities": [[21, 22, "MethodName", "PMLM"]]}
{"text": "Descriptive statistics about the recruited sample of subjects Imagine that you are a teacher checking reading comprehension skills of your students .", "entities": [[15, 17, "TaskName", "reading comprehension"]]}
{"text": "Metric left - to - right u - PMLM i - 10000 i - 14000 i - 18000 i - 10000 i - 14000 i - 16000 e - 3 .", "entities": [[8, 9, "MethodName", "PMLM"]]}
{"text": "Outliers are typically defined in terms of the interquartile range ( IQR ) , which equals to Q3 - Q1 .", "entities": [[11, 12, "DatasetName", "IQR"]]}
{"text": "The first step was to issue queries \" distractor generation \" and \" multiple choice question generation \" to ACL Anthology and Google Scholar .", "entities": [[8, 10, "TaskName", "distractor generation"], [15, 17, "TaskName", "question generation"], [22, 23, "DatasetName", "Google"]]}
{"text": "The result was 20 articles from ACL Anthology and 4 additional ones from Google Scholar .", "entities": [[13, 14, "DatasetName", "Google"]]}
{"text": "Out of found 39 articles , 11 were filtered out ( 8 focused only on generating questions , 1 relied mostly on expert knowledge , 1 on the auxiliary relation extraction task and 1 was a demo paper ) , leaving 28 articles in total .", "entities": [[29, 31, "TaskName", "relation extraction"]]}
{"text": "Each test contains a text , a reading comprehension question based on the text , the explicitly marked correct answer to this question and a number of suggestions for wrong , but plausible alternatives ( distractors ) .", "entities": [[7, 9, "TaskName", "reading comprehension"]]}
{"text": "Suppose you would like to use the given question for testing reading comprehension of the given text .", "entities": [[11, 13, "TaskName", "reading comprehension"]]}
{"text": "The MCQ in sample 1 has an entropy of 0 , meaning all students have selected the same option , which in this case was the key .", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "Existing models for language identification in code - switched data are all supervised , requiring annotated training data which is only available for a limited number of language pairs .", "entities": [[3, 5, "TaskName", "language identification"]]}
{"text": "We experiment with word uni - grams , word n - grams , character ngrams , Viterbi Decoding , Latent Dirichlet Allocation , Support Vector Machine and Logistic Regression .", "entities": [[23, 26, "MethodName", "Support Vector Machine"], [27, 29, "MethodName", "Logistic Regression"]]}
{"text": "Classifying the language labels on the word level ( i.e. code - switch detection ) has shown to be beneficial to improve performance on downstream NLP tasks , like dependency parsing ( Bhat et al , 2018 ) or lexical normalization ( Barik et al , 2019 ) .", "entities": [[29, 31, "TaskName", "dependency parsing"], [39, 41, "TaskName", "lexical normalization"]]}
{"text": "To overcome this limitation , we focus on exploiting only mono - lingual datasets for performing word - level language identification in code - switched data .", "entities": [[19, 21, "TaskName", "language identification"]]}
{"text": "This enables the possibility to easily train models for new language pairs , and leads to the research question : How do semi - supervised models compare and perform in the task of language identification in English - Spanish code - switched data ?", "entities": [[33, 35, "TaskName", "language identification"]]}
{"text": "To evaluate and compare our models , we use the Spanish - English ( SPA - EN ) part of the LinCE benchmark ( a total of 32 , 651 posts equivalent to 390 , 953 tokens )", "entities": [[21, 22, "DatasetName", "LinCE"]]}
{"text": "In the original data , 8 labels are used , from which we only focus on the 3 labels necessary for the language identification task : lang1 , lang2 and other , for English , Spanish and punctuation , numbers , symbols and emoticons , respectively .", "entities": [[22, 24, "TaskName", "language identification"]]}
{"text": "In case this is 0 ( i.e. the word does not occur in our monolingual data ) , the emission probability is calculated by a relative character bi - gram probability .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "Generally , Latent Dirichlet Allocation ( LDA ) aims to find the topics a document belongs to using the words in the document as features .", "entities": [[6, 7, "MethodName", "LDA"]]}
{"text": "The LDA algorithm does not output labels for the resulting clusters , so we select the top 10 words based on weight that represent best each cluster , and assign them a language using the word uni - gram method ( Section 3.1 ) .", "entities": [[1, 2, "MethodName", "LDA"]]}
{"text": "We use the Scikit Learn 6 implementation of LDA with the TfidfVectorizer and use only the first 100 , 000 words from each monolingual dataset , in order to reduce training time .", "entities": [[8, 9, "MethodName", "LDA"]]}
{"text": "For our Support Vector Machine ( SVM ) model , we consider the monolingual data ( Section 2.2 ) to be the gold training data , without tokens from the other class .", "entities": [[2, 5, "MethodName", "Support Vector Machine"], [6, 7, "MethodName", "SVM"]]}
{"text": "We use Logistic Regression in a weakly - supervised manner , the same as with SVM , where we consider the first 100 , 000 words from each Wikipedia dataset to be the gold training data .", "entities": [[2, 4, "MethodName", "Logistic Regression"], [15, 16, "MethodName", "SVM"]]}
{"text": "As found in We use multilingual BERT and all default settings .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "Regarding the LDA model , its low performance can be explained by the results of ( Zhang et al , 2014 ) , which show that for the task of language filtering , the performance of LDA decreases when the dominating language decreases under 70 % of the whole text .", "entities": [[2, 3, "MethodName", "LDA"], [36, 37, "MethodName", "LDA"]]}
{"text": "Furthermore , the amount of evidence per sample is rather low compared to the normal use of LDA ( it is commonly used on the document level ) .", "entities": [[17, 18, "MethodName", "LDA"]]}
{"text": "For LDA , SVM and Logistic Regression models we tried to vectorize data with CountVectorizer from Scikit Learn , which gives the termfrequency for each n - gram in a word .", "entities": [[1, 2, "MethodName", "LDA"], [3, 4, "MethodName", "SVM"], [5, 7, "MethodName", "Logistic Regression"]]}
{"text": "However , TfidfVectorizer performed approximately 1 % better in LDA and Logistic Regression and 4 % for SVM in validation data .", "entities": [[9, 10, "MethodName", "LDA"], [11, 13, "MethodName", "Logistic Regression"], [17, 18, "MethodName", "SVM"]]}
{"text": "The training efficiency of the Viterbi model and the supervised model were measured in a Windows Sub - system for Linux environment on an i7 - 7700 K processor with 16 GB ram .", "entities": [[20, 21, "DatasetName", "Linux"]]}
{"text": "In this study we evaluated different types of models , namely word uni - grams , word n - grams , character n - grams , Viterbi Decoding , Latent Dirichlet Allocation , Support Vector Machine and Logistic Regression , for the task of semi - supervised language identification in English - Spanish codeswitched data .", "entities": [[33, 36, "MethodName", "Support Vector Machine"], [37, 39, "MethodName", "Logistic Regression"], [47, 49, "TaskName", "language identification"]]}
{"text": "Using this model , one can potentially train CS - detection for many more language pairs as previously possible .", "entities": [[8, 9, "DatasetName", "CS"]]}
{"text": "Cross - Sentence N - ary Relation Extraction using Lower - Arity Universal Schemas", "entities": [[6, 8, "TaskName", "Relation Extraction"]]}
{"text": "Most existing relation extraction approaches exclusively target binary relations , and n - ary relation extraction is relatively unexplored .", "entities": [[2, 4, "TaskName", "relation extraction"], [14, 16, "TaskName", "relation extraction"]]}
{"text": "Current state - of - the - art n - ary relation extraction method is based on a supervised learning approach and , therefore , may suffer from the lack of sufficient relation labels .", "entities": [[11, 13, "TaskName", "relation extraction"]]}
{"text": "In this paper , we propose a novel approach to cross - sentence n - ary relation extraction based on universal schemas .", "entities": [[16, 18, "TaskName", "relation extraction"]]}
{"text": "We conduct experiments with datasets for ternary relation extraction and empirically show that our method improves the n - ary relation extraction performance compared to previous methods .", "entities": [[7, 9, "TaskName", "relation extraction"], [20, 22, "TaskName", "relation extraction"]]}
{"text": "Relation extraction is a core natural language processing task which is concerned with the extraction of relations between entities from text .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}
{"text": "It has numerous applications ranging from question answering ( Xu et al , 2016 ) to automated knowledge base construction ( Dong et al , 2014 ) .", "entities": [[6, 8, "TaskName", "question answering"]]}
{"text": "In n - ary relation extraction , relation mentions tend to span multiple sentences more frequently as n increases .", "entities": [[4, 6, "TaskName", "relation extraction"]]}
{"text": "Thus , Peng et al ( 2017 ) recently extended the problem to cross - sentence n - ary relation extraction in which n - ary relations are extracted from multiple sentences .", "entities": [[19, 21, "TaskName", "relation extraction"]]}
{"text": "In Week 1 , he helped limit Houston Texans Pro - bowler Andre Johnson to four receptions for 35 yards . \"", "entities": [[7, 8, "DatasetName", "Houston"]]}
{"text": "With this graphical representation , they applied graph neural networks to predict ternary relations in the medical domain .", "entities": [[16, 18, "DatasetName", "medical domain"]]}
{"text": "On the other hand , for binary relation extraction , the problem of insufficient positive labels can be mitigated with universal schemas .", "entities": [[6, 9, "TaskName", "binary relation extraction"]]}
{"text": "n > 2 ) relation extraction is , however , not straight - forward due to the sparsity of higher - order relation mentions among a specific set of n > 2 entities .", "entities": [[4, 6, "TaskName", "relation extraction"]]}
{"text": "To evaluate the proposed method , we create new cross - sentence n - ary relation extraction datasets with multiple ternary relations .", "entities": [[15, 17, "TaskName", "relation extraction"]]}
{"text": "We show empirically that by jointly training lower - arity models and an nary score aggregation model , the proposed method improves the performance of n - ary relation extraction .", "entities": [[28, 30, "TaskName", "relation extraction"]]}
{"text": "To the best of our knowledge , this is the first attempt to apply universal schemas to n - ary relation extraction , taking advantage of the compositionality of higher - arity facts .", "entities": [[20, 22, "TaskName", "relation extraction"]]}
{"text": "The cross - sentence n - ary relation extraction task ( Peng et al , 2017 ) is defined as follows .", "entities": [[7, 9, "TaskName", "relation extraction"]]}
{"text": "In the cross - sentence n - ary relation extraction task , text section T can contain multiple sentences .", "entities": [[8, 10, "TaskName", "relation extraction"]]}
{"text": "In this paper , following ( Peng et al , 2017 ) , we define M consecutive sentences ( M \u2265 1 ) which contain n target entities as a text section in the crosssentence n - ary relation extraction task .", "entities": [[38, 40, "TaskName", "relation extraction"]]}
{"text": "The goal of the cross - sentence n - ary relation extraction task is to predict new facts r , ( e 1 , ... , e n ) /", "entities": [[10, 12, "TaskName", "relation extraction"]]}
{"text": "Unary encoder : For an unary textual relation r ( k ) = ( T , pos ( e k ) ) , we represent each section T by a sequence of word vectors and use a bidirectional LSTM ( Bi - LSTM )", "entities": [[37, 39, "MethodName", "bidirectional LSTM"], [42, 43, "MethodName", "LSTM"]]}
{"text": "We use a Bi - LSTM to compute a hidden representation h l R dr at each token position l , and max - pool along the path to compute the relation representation : v ( T ( k , l ) )", "entities": [[5, 6, "MethodName", "LSTM"]]}
{"text": "The loss functions contrast a score of an original fact r , p", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "i [ \u2212 log ( exp ( \u03b8 r , p + ) exp ( \u03b8 r , p + ) + k exp ( \u03b8 r , p \u2212 k ) )", "entities": [[7, 8, "HyperparameterName", "\u03b8"], [15, 16, "HyperparameterName", "\u03b8"], [25, 26, "HyperparameterName", "\u03b8"]]}
{"text": "The cross - sentence n - ary relation extraction dataset from Peng et al ( 2017 ) contains only 59 distinct ternary KB facts including the train and test set .", "entities": [[7, 9, "TaskName", "relation extraction"]]}
{"text": "Thus , we created two new n - ary cross - sentence relation extraction datasets ( dubbed with Wiki - 90k and WF - 20k ) that contain more known facts retrieved from public knowledge bases .", "entities": [[12, 14, "TaskName", "relation extraction"]]}
{"text": "Entity mentions are detected using DBpedia Spotlight ( Daiber et al , 2013 ) .", "entities": [[5, 6, "DatasetName", "DBpedia"]]}
{"text": "The state - of - the - art crosssentence n - ary relation extraction method proposed by Song et al ( 2018 ) represents each surface pattern by the concatenation of entity vectors from the last layer of a Graph State LSTM , a variant of a graph neural network .", "entities": [[12, 14, "TaskName", "relation extraction"], [41, 42, "MethodName", "LSTM"]]}
{"text": "Wiki - 90k WF - 20k average weighted average weighted Proposed 0.584 0.634 0.821 0.842 ( Song et al , 2018 ) 0.471 0.536 0.639 0.680 ( Toutanova et al , 2015 ) with Graph State LSTM ( Song et al , 2018 )", "entities": [[36, 37, "MethodName", "LSTM"]]}
{"text": "( Verga et al , 2017 ) with Graph State LSTM ( Song et al , 2018 ) multiple - test adjustment using Holm 's method ( Holm , 1979 ) .", "entities": [[10, 11, "MethodName", "LSTM"]]}
{"text": "Compared to the baseline methods , our proposed method achieves higher weighted MAP for both datasets .", "entities": [[12, 13, "DatasetName", "MAP"]]}
{"text": "Note that we used all positive labels in this experiment , that is , sufficient amount of positive labels are used for calculating the loss N. Data efficiency :", "entities": [[24, 25, "MetricName", "loss"]]}
{"text": "Japanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning", "entities": [[11, 13, "TaskName", "Transfer Learning"]]}
{"text": "However , rule - based cross - lingual transfer is hampered by error propagation in an NLP pipeline and the frequent lack of transparency in translation correspondences .", "entities": [[5, 9, "TaskName", "cross - lingual transfer"]]}
{"text": "In this paper , we propose implicit transfer by injecting machine translation ( MT ) as an intermediate task between pretraining and ZAR .", "entities": [[10, 12, "TaskName", "machine translation"]]}
{"text": "We employ a pretrained BERT model to initialize the encoder part of the encoder - decoder model for MT , and eject the encoder part for finetuning on ZAR .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "The proposed framework empirically demonstrates that ZAR performance can be improved by transfer learning from MT .", "entities": [[12, 14, "TaskName", "transfer learning"]]}
{"text": "Figuring out who did what to whom is an essential part of natural language understanding .", "entities": [[12, 15, "TaskName", "natural language understanding"]]}
{"text": "Although Japanese ZAR saw a performance boost with the introduction of BERT ( Ueda et al , 2020 ; Konno et al , 2020 ) , there is still a good amount of room for improvement .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "It is prone to error propagation due to its dependence on word alignment , parsing , and English coreference resolution .", "entities": [[11, 13, "TaskName", "word alignment"], [18, 20, "TaskName", "coreference resolution"]]}
{"text": "In this paper , we propose neural transfer learning from machine translation ( MT ) .", "entities": [[7, 9, "TaskName", "transfer learning"], [10, 12, "TaskName", "machine translation"]]}
{"text": "Given that state - of - the - art ZAR models are based on BERT ( Ueda et al , 2020 ; Konno et al , 2020Konno et", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "al , , 2021 , it is a natural choice to explore intermediate task transfer learning ( Phang et al , 2018 ; Wang et al , 2019a ; Pruksachatkun et al , 2020 ;", "entities": [[14, 16, "TaskName", "transfer learning"]]}
{"text": "A pretrained BERT model is first trained on MT and the resultant model is then fine - tuned on ZAR .", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "While BERT is an encoder , the dominant paradigm of neural MT is the encoder - decoder .", "entities": [[1, 2, "MethodName", "BERT"]]}
{"text": "Although both share Transformer ( Vaswani et al , 2017 )", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "We use a pretrained BERT model to initialize the encoder part of the encoder - decoder model for MT .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "We also demonstrate further improvements can be brought by incorporating encoder - side masked language model ( MLM ) training into the intermediate training on MT .", "entities": [[17, 18, "DatasetName", "MLM"]]}
{"text": "These efforts are , however , overshadowed by the surprising effectiveness of BERT 's pretraining ( Ueda et al , 2020 ;", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "Adopting BERT , recent studies seek gains through multi - task learning ( Ueda et al , 2020 ) , data augmentation ( Konno et al , 2020 ) , and an intermediate task tailored to ZAR ( Konno et al , 2021 ) .", "entities": [[1, 2, "MethodName", "BERT"], [8, 12, "TaskName", "multi - task learning"], [20, 22, "TaskName", "data augmentation"]]}
{"text": "The multi - task learning approach of Ueda et al ( 2020 ) covers verbal predicate analysis ( which subsumes ZAR ) , and nominal predicate analysis , coreference resolution , and bridging anaphora resolution .", "entities": [[1, 5, "TaskName", "multi - task learning"], [28, 30, "TaskName", "coreference resolution"], [32, 35, "TaskName", "bridging anaphora resolution"]]}
{"text": "Konno et al ( 2020 ) perform data augmentation by simply masking some tokens .", "entities": [[7, 9, "TaskName", "data augmentation"]]}
{"text": "Inspired by the great success of the pretraining / finetuning paradigm on a broad range of tasks ( Peters et al , 2018 ; Devlin et al , 2019 ) , a line of research inserts an intermediate task between pretraining and fine - tuning on a target task ( Phang et al , 2018 ; Wang et al , 2019a ; Pruksachatkun et al , 2020 ) .", "entities": [[0, 1, "DatasetName", "Inspired"]]}
{"text": "However , Wang et al ( 2019a ) found that MT used as an intermediate task led to performance degeneration in various target tasks , such as natural language inference and sentiment classification .", "entities": [[27, 30, "TaskName", "natural language inference"]]}
{"text": "They argue that the considerable difference between MLM pretraining and MT causes catastrophic forgetting ( CF ) .", "entities": [[7, 8, "DatasetName", "MLM"]]}
{"text": "Pruksachatkun et al ( 2020 ) suggest injecting the MLM objective during intermediate training as a possible way to mitigate CF , which we empirically test in this paper .", "entities": [[9, 10, "DatasetName", "MLM"]]}
{"text": "Motivated by BERT 's success in a wide range of applications , some studies incorporate BERT into MT models .", "entities": [[2, 3, "MethodName", "BERT"], [15, 16, "MethodName", "BERT"]]}
{"text": "A straightforward way to do this is to initialize the encoder part of the encoder - decoder with pretrained BERT , but it has had mixed success at best ( Clinchant et al , 2019 ; Zhu et al , 2020 ) .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "Abandoning this approach , Zhang et al ( 2020 ) simply use BERT as a supplier of context - aware embeddings to their own encoder - decoder model .", "entities": [[12, 13, "MethodName", "BERT"]]}
{"text": "Similarly , Guo et al ( 2020 ) stack adapter layers on top of two frozen BERT models to use them as the encoder and decoder of a non - autoregressive MT 2", "entities": [[16, 17, "MethodName", "BERT"]]}
{"text": "We suspect that the poor performance resulted in part from their excessively simple decoder , a single - layer LSTM .", "entities": [[19, 20, "MethodName", "LSTM"]]}
{"text": "However , these methods can not be adopted for our purpose because we want BERT itself to learn from MT .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "Although they offer some insights , it remains unclear how best to exploit BERT when MT is an intermediate task , not the target task .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "We adopt a ZAR model of Ueda et al ( 2020 ) , which adds a thin layer on top of BERT during fine - tuning to solve ZAR and related tasks ( Section 3.1 ) .", "entities": [[21, 22, "MethodName", "BERT"]]}
{"text": "Instead of directly moving from MLM pretraining to fine - tuning on ZAR , we inject MT as an intermediate task ( Section 3.2 ) .", "entities": [[5, 6, "DatasetName", "MLM"]]}
{"text": "In addition , we introduce the MLM training objective during the intermediate training ( Section 3.3 ) .", "entities": [[6, 7, "DatasetName", "MLM"]]}
{"text": "ZAR as argument selection As illustrated in Figure 3 , the basic idea behind BERT - based ZAR is that given the powerful neural encoder , the joint task of omission detection and antecedent identification can be formalized as argument selection ( Shibata and Kurohashi , 2018 ; Kurita et al , 2018 ; Ueda et al , 2020 ) .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "In practice , the input length limitation of BERT forces us to implement a sliding window approach .", "entities": [[8, 9, "MethodName", "BERT"]]}
{"text": "i provided by BERT , W c and U c are case - specific weight matrices , and v is a weight vector shared among cases .", "entities": [[3, 4, "MethodName", "BERT"]]}
{"text": "For each predicate , we repeat this for the nominative ( NOM ) , accusative ( ACC ) , and dative ( DAT ) cases , and another nominative case for the double nominative construction ( NOM2 ) .", "entities": [[16, 17, "MetricName", "ACC"]]}
{"text": "Following Imamura and Sumita ( 2019 ) and Clinchant et al ( 2019 ) , we use a pretrained BERT to initialize the encoder part of the Transformer - based encoderdecoder model while the decoder is randomly initialized .", "entities": [[19, 20, "MethodName", "BERT"], [27, 28, "MethodName", "Transformer"]]}
{"text": "As discussed in Section 2.2 , MT as an intermediate task reportedly harms target - task performance , probably because MT forces the model to forget what it has learned from MLM pretraining ( catastrophic forgetting ) .", "entities": [[31, 32, "DatasetName", "MLM"]]}
{"text": "To overcome this problem , we incorporate the MLM training objective into MT , as suggested by Pruksachatkun et al ( 2020 ) .", "entities": [[8, 9, "DatasetName", "MLM"]]}
{"text": "Our masking strategy is the same as BERT 's ( Devlin et al , 2019 ) :", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "The corresponding losses are added to the MT loss function .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "They contained manual annotation for predicate - argument structures ( including zero anaphora ) as well as word segmentation , part - of - speech tags , dependency relations , and coreference chains .", "entities": [[20, 23, "DatasetName", "part - of"]]}
{"text": "We discarded pairs with scores of 0 .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "We used Subword - NMT ( Sennrich et al , 2016 ) for Japanese and SentencePiece ( Kudo and Richardson , 2018 ) for English .", "entities": [[15, 16, "MethodName", "SentencePiece"]]}
{"text": "BERT We employed a Japanese BERT model with BPE segmentation distributed by NICT .", "entities": [[0, 1, "MethodName", "BERT"], [5, 6, "MethodName", "BERT"], [8, 9, "MethodName", "BPE"]]}
{"text": "It had the same architecture as Google 's BERT - Base ( Devlin et al , 2019 ) : 12 layers , 768 hidden units , and 12 attention heads .", "entities": [[6, 7, "DatasetName", "Google"], [8, 9, "MethodName", "BERT"]]}
{"text": "MT We used the Transformer encoder - decoder architecture ( Vaswani et al , 2017 ) .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "The encoder was initialized with BERT while the decoder was a randomly initialized six - layer Transformer .", "entities": [[5, 6, "MethodName", "BERT"], [16, 17, "MethodName", "Transformer"]]}
{"text": "The numbers of hidden units and heads were set to be the same as BERT 's ( i.e. , 768 units and 12 attention heads ) .", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "We ran the model with 3 seeds on MT and with 3 seeds on ZAR , which resulted in 9 seed combinations .", "entities": [[6, 7, "DatasetName", "seeds"], [12, 13, "DatasetName", "seeds"]]}
{"text": "Our baseline is Ueda et al ( 2020 ) , who drastically outperformed previous models , thanks to BERT .", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "+ MT refers to the model with intermediate training on MT while + MT w/ MLM corresponds to the model that incorporated the MLM objective into MT .", "entities": [[15, 16, "DatasetName", "MLM"], [23, 24, "DatasetName", "MLM"]]}
{"text": "We can see that MT combined with MLM performed the best and that the gains reached 1.6 points for both the Web and News .", "entities": [[7, 8, "DatasetName", "MLM"]]}
{"text": "For comparison , we performed additional pretraining with ordinary MLM on the Japanese part of the parallel corpus ( denoted as + MLM ) , because the possibility remained that the model simply took advantage of additional data .", "entities": [[9, 10, "DatasetName", "MLM"], [22, 23, "DatasetName", "MLM"]]}
{"text": "Although + MLM somehow beat the baseline , it was outperformed by most models trained on MT , ruling out the possibility that the gains were solely attributed to extra data .", "entities": [[2, 3, "DatasetName", "MLM"]]}
{"text": "We can conclude that Japanese ZAR benefits from parallel texts through neural transfer learning .", "entities": [[12, 14, "TaskName", "transfer learning"]]}
{"text": "What is worse , its combination with MLM led to performance degeneration on both datasets .", "entities": [[7, 8, "DatasetName", "MLM"]]}
{"text": "MLM achieved superior performance as it worked well in all settings .", "entities": [[0, 1, "DatasetName", "MLM"]]}
{"text": "Unfortunately , conventional evaluation metrics for MT ( e.g. , BLEU ) reveal little about the model 's ability to handle zero anaphora .", "entities": [[10, 11, "MetricName", "BLEU"]]}
{"text": "The number of instances with d = 0 was 218 while the number of remaining instances was 506 .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "We chose the model with the best performance ( i.e. , one - stage optimization with MLM ) .", "entities": [[16, 17, "DatasetName", "MLM"]]}
{"text": "The MLM objective during intermediate training on MT is shown to be very effective , but why ?", "entities": [[1, 2, "DatasetName", "MLM"]]}
{"text": "Not surprisingly , + MT w/ masking was beaten by + MT w/ MLM with large margins .", "entities": [[13, 14, "DatasetName", "MLM"]]}
{"text": "The fact that the contribution of the loss function was larger than that of token masking indicates that the improvements were mainly attributed to CF mitigation , but the contribution of token masking alone should not be overlooked .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "Due to space limitation , we have limited our focus to BERT , but for the sake of future practitioners , we would like to briefly note that we extensively tested BART and its variants before switching to BERT .", "entities": [[11, 12, "MethodName", "BERT"], [31, 32, "MethodName", "BART"], [38, 39, "MethodName", "BERT"]]}
{"text": "Unlike BERT , BART is an encoder - decoder model pretrained on a monolingual corpus ( original ) or a non - parallel multilingual corpus ( mBART ) .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "MethodName", "BART"], [26, 27, "MethodName", "mBART"]]}
{"text": "We specifically tested ( 1 ) the officially distributed mBART model , ( 2 ) a BART model we pretrained on Japanese Wikipedia , and ( 3 ) an mBART model we pretrained on Japanese and English texts .", "entities": [[9, 10, "MethodName", "mBART"], [16, 17, "MethodName", "BART"], [29, 30, "MethodName", "mBART"]]}
{"text": "A more serious problem was that they came close to but rarely outperformed the strong BERT baseline .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "In this paper , we proposed to exploit parallel texts for Japanese zero anaphora resolution ( ZAR ) by inserting machine translation ( MT ) as an intermediate task between masked language model ( MLM ) pretraining and fine - tuning on ZAR .", "entities": [[20, 22, "TaskName", "machine translation"], [34, 35, "DatasetName", "MLM"]]}
{"text": "We bridged the gap between BERT - based ZAR and the encoder - decoder architecture for MT by initializing the encoder part of the MT model with a pretrained BERT .", "entities": [[5, 6, "MethodName", "BERT"], [29, 30, "MethodName", "BERT"]]}
{"text": "We found that incorporating the MLM objective into the intermediate training was particularly effective .", "entities": [[5, 6, "DatasetName", "MLM"]]}
{"text": "Our experimental results were consistent with the speculation that MLM mitigated catastrophic forgetting during intermediate training .", "entities": [[9, 10, "DatasetName", "MLM"]]}
{"text": "With neural transfer learning , we successfully revived the old idea that Japanese ZAR can benefit from parallel texts ( Nakaiwa , 1999 ) .", "entities": [[2, 4, "TaskName", "transfer learning"]]}
{"text": "Thanks to the astonishing flexibility of neural networks , we would probably be able to connect ZAR to other tasks through transfer learning .", "entities": [[21, 23, "TaskName", "transfer learning"]]}
{"text": "Exploratory Analysis for Ontology Learning from Social Events on Social Media Streaming in Spanish", "entities": [[3, 4, "MethodName", "Ontology"]]}
{"text": "The problem of event analysis in Spanish social media streaming is that of difficulty on automatically processing the data as well as obtaining the most relevant information , such as mentioned by Derczynski et al ( 2015 ) .", "entities": [[32, 33, "DatasetName", "Derczynski"]]}
{"text": "The ontology structure used for the events representation as well as the algorithms employed in order to obtain entities and relationships between these are further explained on Section 4 .", "entities": [[1, 2, "MethodName", "ontology"]]}
{"text": "In Al - Smadi and Qawasmeh ( 2016 ) an unsupervised approach for event extraction from Arabic tweets is discussed .", "entities": [[13, 15, "TaskName", "event extraction"]]}
{"text": "Entities appearing in the data are linked to corresponding entities found on Wikipedia and DBpedia through an ontology based knowledge base .", "entities": [[14, 15, "DatasetName", "DBpedia"], [17, 18, "MethodName", "ontology"]]}
{"text": "In Derczynski et al ( 2015 ) a comparative evaluation of different NER is done based on three different datasets .", "entities": [[1, 2, "DatasetName", "Derczynski"], [12, 13, "TaskName", "NER"]]}
{"text": "Also , some common challenges or errors when handling data from Twitter are presented as well as methods for reducing microblog noise through pre - processing such as language identification , POStagging and normalization .", "entities": [[28, 30, "TaskName", "language identification"]]}
{"text": "In Raimond and Abdallah ( 2007 ) an event ontology is described .", "entities": [[9, 10, "MethodName", "ontology"]]}
{"text": "Finally , an ontology model for events is proposed in which entities are extracted using the CMU tweet analyzer and relationships are inferred from Wikipedia , DBpedia and Web data .", "entities": [[3, 4, "MethodName", "ontology"], [26, 27, "DatasetName", "DBpedia"]]}
{"text": "Ontology learning is defined by Cimiano ( 2006 ) as the automatic acquisition of a domain model from some dataset .", "entities": [[0, 1, "MethodName", "Ontology"]]}
{"text": "In this paper we focus on applying ontology learning techniques for data represented as text .", "entities": [[7, 8, "MethodName", "ontology"]]}
{"text": "Cimiano points towards two main approaches for ontology learning : 1 .", "entities": [[7, 8, "MethodName", "ontology"]]}
{"text": "Before we start using different techniques in order to populate an ontology or to learn entities and relationships from the data that was retrieved previously , an ontology structure had to be defined .", "entities": [[11, 12, "MethodName", "ontology"], [27, 28, "MethodName", "ontology"]]}
{"text": "The ontology structure that we define will point us towards different techniques depending on the information that must be retrieved to populate this particular structure .", "entities": [[1, 2, "MethodName", "ontology"]]}
{"text": "Therefore , the proposed ontology structure in this paper is defined on Figure 1 .", "entities": [[4, 5, "MethodName", "ontology"]]}
{"text": "The ontology will be populated by such triples composed of ( Entity , Temporal entity , object ) .", "entities": [[1, 2, "MethodName", "ontology"]]}
{"text": "In order to achieve this , two initial tools for entity retrieval were tested : 1 .", "entities": [[10, 12, "TaskName", "entity retrieval"]]}
{"text": "Stanford NER : The Stanford NER used with a trained Spanish model from late 2016 was used in order to retrieve persons , entities and organizations and group them all together as entities .", "entities": [[1, 2, "TaskName", "NER"], [5, 6, "TaskName", "NER"]]}
{"text": "The results showed that , while the Stanford NER worked really well in the case where the tweets were news related or had a more formal undertone , such as in the case of the Australian Open , it failed to find a lot of basic entities in the other two datasets where the data was more unstructured as one would very likely find when working on social streaming .", "entities": [[8, 9, "TaskName", "NER"]]}
{"text": "Also , the Stanford NER has heavily influenced by correct capitalization and punctuation , whereas UDPipe was n't influenced by these factors as much .", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "This approach is based on obtaining the formal context for a specific domain or dataset and then proceed to use it to create a hierarchy ontology .", "entities": [[25, 26, "MethodName", "ontology"]]}
{"text": "A desktop application was developed in order to allow for easier visualization of both the ontology and the resulting activities that each entity participated in , as well as the activities that create a relationship between two particular entities .", "entities": [[15, 16, "MethodName", "ontology"]]}
{"text": "In order to verify the approach applied for ontology extraction , we manually created ontologies for each test case where the most relevant entities and relationships are specified based on investigation related to these cases , these ontologies can be seen on Figures 5 , 6 and 7 .", "entities": [[8, 9, "MethodName", "ontology"]]}
{"text": "This paper 's aim was to give a foundation and a initial stage of exploratory analysis on social media streaming in Spanish by using ontologies , after which future work could be based upon in order to expand the knowledge in the ontologies or use this analysis together with an event detection system in order to be able to both detect and analyze events in real time .", "entities": [[50, 52, "TaskName", "event detection"]]}
{"text": "Multilingual Paraphrase Generation For Bootstrapping New Features in Task - Oriented Dialog Systems", "entities": [[1, 3, "TaskName", "Paraphrase Generation"]]}
{"text": "As a solution , we propose a multilingual paraphrase generation model that can be used to generate novel utterances for a target feature and target language .", "entities": [[8, 10, "TaskName", "paraphrase generation"]]}
{"text": "The generated utterances can be used to augment existing training data to improve intent classification and slot labeling models .", "entities": [[13, 15, "TaskName", "intent classification"]]}
{"text": "Spoken language understanding is a core problem in task oriented dialog systems with the goal of understanding and formalizing the intent expressed by an utterance ( Tur and De Mori , 2011 ) .", "entities": [[0, 3, "TaskName", "Spoken language understanding"]]}
{"text": "It is often modeled as intent classification ( IC ) , an utterance - level multi - class classification problem , and slot labeling ( SL ) , a sequence labeling problem over the utterance 's tokens .", "entities": [[5, 7, "TaskName", "intent classification"], [15, 19, "TaskName", "multi - class classification"]]}
{"text": "Many large - scale real - world dialog systems , e.g. Apple 's Siri , Amazon 's Alexa and Google 's Assistant , support interactions in multiple languages .", "entities": [[19, 20, "DatasetName", "Google"]]}
{"text": "With multilingual paraphrase generation , we can benefit from this setup and improve data augmentation for data - scarce languages via cross - lingual transfer from data - rich languages .", "entities": [[2, 4, "TaskName", "paraphrase generation"], [13, 15, "TaskName", "data augmentation"], [21, 25, "TaskName", "cross - lingual transfer"]]}
{"text": "As a result , the data augmentation can not only be applied with seed data , i.e. in a few - shot setting , but even under zero - shot conditions with no seeds at all for the target language .", "entities": [[5, 7, "TaskName", "data augmentation"], [33, 34, "DatasetName", "seeds"]]}
{"text": "With that , we are able to generate labeled data for a new feature even in the zero - shot setting where no seeds are available at all .", "entities": [[23, 24, "DatasetName", "seeds"]]}
{"text": "The experiments compare against several alternative methods , including previous work for mono - lingual paraphrase generation and machine translation .", "entities": [[15, 17, "TaskName", "paraphrase generation"], [18, 20, "TaskName", "machine translation"]]}
{"text": "Various studies have explored paraphrase generation for dialog systems .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}
{"text": "Bowman et al ( 2016 ) showed that generating sentences from a continuous latent space is possible using a variational autoencoder model and provided guidelines on how to train such a generation model .", "entities": [[19, 21, "MethodName", "variational autoencoder"]]}
{"text": "Malandrakis et al ( 2019 ) explored a variety of controlled paraphrase generation approaches for data augmentation and proposed to use conditional variational autoencoders which they showed obtained the best results .", "entities": [[11, 13, "TaskName", "paraphrase generation"], [15, 17, "TaskName", "data augmentation"], [23, 24, "MethodName", "autoencoders"]]}
{"text": "Our method is different as it uses a conditional seq2seq model that can generate text from any sequence of slots and does not require an utterance as an input .", "entities": [[9, 10, "MethodName", "seq2seq"]]}
{"text": "Xia et al ( 2020 ) propose a transformer - based conditional variational autoencoder for few shot utterance generation where the latent space represents the intent as two independent parts ( domain and action ) .", "entities": [[12, 14, "MethodName", "variational autoencoder"]]}
{"text": "Cho et al ( 2019 ) generate paraphrases for seed examples with a transformer seq2seq model and self - label them with a baseline intent and slot model .", "entities": [[14, 15, "MethodName", "seq2seq"]]}
{"text": "Several other studies follow a text - to - text ap - proach and assume training data in the form of paraphrase pairs for training paraphrase generation models in a single language Li et al , 2018Li et al , , 2019 .", "entities": [[25, 27, "TaskName", "paraphrase generation"]]}
{"text": "Using large pre - trained models has also been shown to be effective for paraphrase generation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}
{"text": "Chen et al ( 2020 ) for instance show the effectiveness of using GPT - 2 ( Radford et al , 2019 ) for generating text from tabular data ( a set of attributevalue pairs ) .", "entities": [[13, 14, "MethodName", "GPT"]]}
{"text": "Our model , however , does not rely on pre - trained weights from another model such as GPT - 2 , is scalable , and can be applied to training data from any domain , for instance , dialog domain .", "entities": [[18, 19, "MethodName", "GPT"]]}
{"text": "Beyond paraphrase generation , several other techniques have been proposed for feature bootstrapping .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}
{"text": "Machine translation can be used from data - rich to data - scarce languages ( Gaspers et al , 2018 ;", "entities": [[0, 2, "TaskName", "Machine translation"]]}
{"text": "Cross - lingual transfer learning can also leverage use existing data in other languages ( Do and Gaspers , 2019 ) .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"]]}
{"text": "In order to generate paraphrases , we train a multilingual paraphrase generation model that generates a paraphrase given a language , an intent and a set of slot types .", "entities": [[10, 12, "TaskName", "paraphrase generation"]]}
{"text": "In order to generate paraphrases which can be used for data augmentation , we would need the slot annotations and the intents of the generations .", "entities": [[10, 12, "TaskName", "data augmentation"]]}
{"text": "Generating the output sequence token - by - token can be done by using greedy decoding where given learned model parameters \u03b8 , the most likely token is picked at each decoding step as x t = argmax p \u03b8 ( x t | x < t ) .", "entities": [[21, 22, "HyperparameterName", "\u03b8"], [39, 40, "HyperparameterName", "\u03b8"]]}
{"text": "In top - k random sampling , we first scale the logits z w by using a temperature parameter \u03c4 before applying softmax .", "entities": [[22, 23, "MethodName", "softmax"]]}
{"text": "Since the training data we use is highly imbalanced , data augmentation might lead to disturbance in the original intent distribution .", "entities": [[10, 12, "TaskName", "data augmentation"]]}
{"text": "To ensure that the data augmentation process does not disturb the original intent distribution , we compute the number of samples to augment using the following constraint : the ratio of target intent to other intents for the target language should be the same as the ratio of target intent to other intents in the source language .", "entities": [[4, 6, "TaskName", "data augmentation"], [18, 21, "HyperparameterName", "number of samples"]]}
{"text": "Sometimes , using the above constraint results in a negligible number of samples for augmentation , in which cases we use a minimal number of samples ( see experiments ) .", "entities": [[10, 13, "HyperparameterName", "number of samples"], [23, 26, "HyperparameterName", "number of samples"]]}
{"text": "To cope with that problem , we use the baseline downstream intent classification and slot labeling model , which is trained only on the existing data , to compute the likelihood of the generated paraphrases to belong to the target intent .", "entities": [[11, 13, "TaskName", "intent classification"]]}
{"text": "We train a single model for paraphrase generation on all the languages as well as a single multi - lingual downstream IC / SL model .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}
{"text": "Paraphrase generation training Since the training data is imbalanced , we balanced the training data by oversampling the intents to match the frequency of the most frequent intent .", "entities": [[0, 2, "TaskName", "Paraphrase generation"]]}
{"text": "For both the encoder and the decoder , the multi - head attention layers ' hidden dimension was set to 128 and the position - wise feed forward layers ' hidden dimension was set to 256 .", "entities": [[9, 13, "MethodName", "multi - head attention"]]}
{"text": "Dropout of 0.1 was used in both the encoder and the decoder .", "entities": [[0, 1, "MethodName", "Dropout"]]}
{"text": "The model parameters were initialized with Xavier initialization ( Glorot and Bengio , 2010 ) .", "entities": [[6, 8, "MethodName", "Xavier initialization"]]}
{"text": "For a given input , we generated using the top - k random sampling three times with different random seeds .", "entities": [[19, 20, "DatasetName", "seeds"]]}
{"text": "Methods for comparison We compare our method against four alternatives : ( a ) Baseline : No data augmentation at all .", "entities": [[17, 19, "TaskName", "data augmentation"]]}
{"text": "( c ) CVAE seq2seq model : We generate paraphrases using the CVAE seq2seq model by Malandrakis et al ( 2019 ) .", "entities": [[3, 4, "MethodName", "CVAE"], [4, 5, "MethodName", "seq2seq"], [12, 13, "MethodName", "CVAE"], [13, 14, "MethodName", "seq2seq"]]}
{"text": "The original CVAE seq2seq model as proposed by Malandrakis et al ( 2019 ) defines the set { domain , intent , slots } as the signature of an utterance and denotes the carrier phrases for a given signature to be paraphrases .", "entities": [[2, 3, "MethodName", "CVAE"], [3, 4, "MethodName", "seq2seq"]]}
{"text": "These carrier phrases are then used to create input - output pairs for the CVAE seq2seq model training .", "entities": [[14, 15, "MethodName", "CVAE"], [15, 16, "MethodName", "seq2seq"]]}
{"text": "The CVAE seq2seq model was only applicable to the few shot setup since in the zero shot setup there are no existing carrier phrases in the target language in the target intent that can be used to sample from .", "entities": [[1, 2, "MethodName", "CVAE"], [2, 3, "MethodName", "seq2seq"]]}
{"text": "Machine translation : We augmented the translations generated from English using the MT+fastalign approach from the MultiATIS++ paper ( Xu et al , 2020 ) .", "entities": [[0, 2, "TaskName", "Machine translation"]]}
{"text": "| S | 0", "entities": [[3, 4, "DatasetName", "0"]]}
{"text": "r = 1 if S = G 0", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "| > 0 0", "entities": [[2, 3, "DatasetName", "0"], [3, 4, "DatasetName", "0"]]}
{"text": "For the top - k sampling based generation , we generate for each input three times with different random seeds and compute novelty and diversity scores .", "entities": [[19, 20, "DatasetName", "seeds"]]}
{"text": "For the few shot setup , the all retrieval , exact match , partial match , F1 slot and Jaccard index scores decrease upon increasing top - k and temperature .", "entities": [[10, 12, "MetricName", "exact match"], [16, 17, "MetricName", "F1"]]}
{"text": "In Table 3 , we show that the intrinsic evaluation results using the proposed approach are consistently better than the CVAE seq2seq paraphrase generation model ( Malandrakis et al , 2019 ) .", "entities": [[20, 21, "MethodName", "CVAE"], [21, 22, "MethodName", "seq2seq"], [22, 24, "TaskName", "paraphrase generation"]]}
{"text": "We evaluate the downstream intent classification using accuracy and the slot labeling using F1 score .", "entities": [[4, 6, "TaskName", "intent classification"], [7, 8, "MetricName", "accuracy"], [13, 15, "MetricName", "F1 score"]]}
{"text": "We found that the effect on the scores ( both intent classification and slot labeling ) for the other intents is negligible using paraphrasing and other methods .", "entities": [[10, 12, "TaskName", "intent classification"]]}
{"text": "Our paraphrasing approach results in the best intent classification scores overall ( 78 % ) .", "entities": [[7, 9, "TaskName", "intent classification"]]}
{"text": "In the zero shot setup , the MT approach outperforms our paraphrasing approach by a large margin in intent classification ( 62.5 % ) .", "entities": [[18, 20, "TaskName", "intent classification"]]}
{"text": "The lower slot F1 scores using the MT approach in few and zero shot setups indicate that the fast align method to align slots in source and translation might result in noisy training data affecting the SL model .", "entities": [[3, 4, "MetricName", "F1"]]}
{"text": "In this paper , we proposed a multilingual paraphrase generation model that can be used for feature bootstrapping with or without seed data in the target language .", "entities": [[8, 10, "TaskName", "paraphrase generation"]]}
{"text": "In addition to generating a paraphrase , the model also generates the associated slot labels , enabling the generation to be used directly for data augmentation to existing training data .", "entities": [[24, 26, "TaskName", "data augmentation"]]}
{"text": "Intrinsic evaluation shows that paraphrases generated using our approach have higher novelty and diversity in comparison to CVAE seq2seq based paraphrase generation .", "entities": [[17, 18, "MethodName", "CVAE"], [18, 19, "MethodName", "seq2seq"], [20, 22, "TaskName", "paraphrase generation"]]}
{"text": "Additionally , downstream evaluation shows that using the generated paraphrases for data augmentation results in improvements over baseline and related techniques in a wide range of languages and setups .", "entities": [[11, 13, "TaskName", "data augmentation"]]}
{"text": "In the future , we would like to explore strategies to exploit monolingual data in the target languages to further refine the paraphrase generation .", "entities": [[22, 24, "TaskName", "paraphrase generation"]]}
{"text": "We would also like to leverage pre - trained multilingual text - to - text models such as mT5 ( Xue et al , 2020 ) for multilingual paraphrase generation in the dialog system domain .", "entities": [[18, 19, "MethodName", "mT5"], [28, 30, "TaskName", "paraphrase generation"]]}
{"text": "We look into the task of generalizing word embeddings : given a set of pre - trained word vectors over a finite vocabulary , the goal is to predict embedding vectors for out - of - vocabulary words , without extra contextual information .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword - level models in the quality of generated word embeddings across languages .", "entities": [[0, 2, "TaskName", "Word similarity"], [22, 24, "TaskName", "word embeddings"]]}
{"text": "Word embeddings pre - trained over large texts have demonstrated benefits for many NLP tasks , especially when the task is label - deprived .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "However , many popular pre - trained sets of word embeddings assume fixed finite - size vocabularies 1 , 2 , which hinders their ability to provide useful word representations for out - of - vocabulary ( OOV ) words .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "We look into the task of generalizing word embeddings : extrapolating a set of pre - trained word embeddings to words out of its fixed vocabulary , without extra access to contextual information ( e.g. example sentences or text corpus ) .", "entities": [[7, 9, "TaskName", "word embeddings"], [17, 19, "TaskName", "word embeddings"]]}
{"text": "In contrast , the more common task of learning word embeddings , or often just word embedding , is to obtain distributed representations of words directly from large unlabeled text .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": "There have been works showing that contextual information can also help generalize word embeddings ( for example , Khodak et al , 2018 ; Schick and Sch\u00fctze , 2019a , b ) .", "entities": [[12, 14, "TaskName", "word embeddings"]]}
{"text": "Different models have been proposed for that task of generalizing word embeddings using word compositions , usually under the name of subword ( level ) models .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "Bag - of - subwords ( BoS ) is a simple yet effective model for learning and generalizing ( Zhao et al , 2018 ) word embeddings .", "entities": [[25, 27, "TaskName", "word embeddings"]]}
{"text": "Affix prediction experiment quantitatively shows that the subword weights given by PBoS are able to recover most eminent affixes of words with good accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "To assess the quality of generated word embeddings , we evaluate with the intrinsic task of word similarity which relates to the semantics ; as well as the extrinsic task of part - of - speech ( POS ) tagging which requires rich information to determine each word 's role in a sentence .", "entities": [[6, 8, "TaskName", "word embeddings"], [16, 18, "TaskName", "word similarity"], [31, 34, "DatasetName", "part - of"]]}
{"text": "English word similarity experiment shows that PBoS improves the correlation scores over previous best models under vari - ous settings and is the only model that consistently improves over the target pre - trained embeddings .", "entities": [[1, 3, "TaskName", "word similarity"]]}
{"text": "The model can be trained by fitting a set of pre - trained word embeddings .", "entities": [[13, 15, "TaskName", "word embeddings"]]}
{"text": "For a given language , let \u0393 be its alphabet .", "entities": [[6, 7, "HyperparameterName", "\u0393"]]}
{"text": "c l \u0393 l where w [ i ]", "entities": [[2, 3, "HyperparameterName", "\u0393"]]}
{"text": "Let p w [ 0 , 1 ] be the probability that w appears in the language .", "entities": [[4, 5, "DatasetName", "0"]]}
{"text": "We say a string s \u0393 + is a subword of word w , denoted as s \u2286 w , if s =", "entities": [[5, 6, "HyperparameterName", "\u0393"]]}
{"text": "The probability that subword s appears in the language can then be defined as p s \u221d w \u0393 + p w 1\u2264i\u2264j\u2264", "entities": [[18, 19, "HyperparameterName", "\u0393"]]}
{"text": "( 1 ) where 1 ( pred ) gives 1 and otherwise 0 only if pred holds .", "entities": [[12, 13, "DatasetName", "0"]]}
{"text": "The vertices N w = { 0 , . . .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "[ l \u2212 1 ] , as well as to the beginning ( vertiex 0 ) and the end ( vertex l ) of w. Each edge ( i , j ) E", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "w = { ( i , j ) : 0 \u2264", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "Proposition 1 . Paths from 0 to | w | in G w are in one - to - one correspondence to segmentations of w. Proposition 2 .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "We model the likelihood of segmentation g being a segmentation of w as being proportional to the product of all its subword likelihood - the 0 1 2 3 4 5 6 h p \" h \" i", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "Bold edges constituent a path from node 0 to 6 , corresponding to the segmentation of the word into \" high \" and \" er \" .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "transition along a path from 0 to | w | in G w : p g | w \u221d s g", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "Bold edges ( 0 , 4 ) and ( 4 , 6 ) form a path from 0 to 6 , which corresponds to the segmentation ( \" high \" , \" er \" ) .", "entities": [[3, 4, "DatasetName", "0"], [17, 18, "DatasetName", "0"]]}
{"text": "Based on the above modeling of subword segmentations , we propose the Probabilistic Bag - of - Subword ( PBoS ) model for composing word embeddings .", "entities": [[24, 26, "TaskName", "word embeddings"]]}
{"text": "We maintain a look - up table S : \u0393 + R d for all subword vectors ( i.e. s = S ( s ) ) as trainable parameters of the model , where d is the embedding dimension .", "entities": [[9, 10, "HyperparameterName", "\u0393"], [37, 39, "HyperparameterName", "embedding dimension"]]}
{"text": ", l with b 1 , 0 = 1 .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "2 : b 1 , 0 1 ; b l+1 , l 1 ; 3 : for i 1 . . .", "entities": [[5, 6, "DatasetName", "0"]]}
{"text": "k = l\u2212i+1 p", "entities": [[0, 2, "HyperparameterName", "k ="]]}
{"text": "w [ l\u2212i+1 : k ] b k+1 , l6 : end for 7 : \u00e3 s | w 0 for all s \u2286 w 8 : for i 1 . . .", "entities": [[19, 20, "DatasetName", "0"]]}
{"text": "For the latter , we evaluate the word embeddings composed by PBoS at word similarity task ( Section 4.3 ) and part - of - speech ( POS ) tagging task ( Section 4.4 ) .", "entities": [[7, 9, "TaskName", "word embeddings"], [13, 15, "TaskName", "word similarity"], [21, 24, "DatasetName", "part - of"]]}
{"text": "The calculation is based on the word frequency derived from the Google Web Trillion Word Corpus 6 .", "entities": [[11, 12, "DatasetName", "Google"]]}
{"text": "This allows the model to closely mimic the word embeddings for frequent words that are probably part of the target vectors .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "Given that PBoS is able to produce sensible segmentation likelihood and subword weights , we now turn our focus onto the quality of the generated 8 http://marcobaroni.org/PublicData/ affix_complete_set.txt.gz word embeddings .", "entities": [[28, 30, "TaskName", "word embeddings"]]}
{"text": "In this section , we evaluate the word vectors ' ability to capture word senses using the intrinsic task of word similarity .", "entities": [[20, 22, "TaskName", "word similarity"]]}
{"text": "Word similarity aims to test how well word embeddings capture words ' semantic similarity .", "entities": [[0, 2, "TaskName", "Word similarity"], [7, 9, "TaskName", "word embeddings"], [12, 14, "TaskName", "semantic similarity"]]}
{"text": "Given a set of word embeddings , we compute the similarity scores induced by the cosine distance between the embedding vectors of each pair of words .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "To better access models ' ability to generalize word embeddings towards OOV words , we include the rare word datasets RareWord ( RW ) from Luong et al ( 2013 ) and the newer Card - 660 ( Card ) from Pilehvar et al ( 2018 ) .", "entities": [[8, 10, "TaskName", "word embeddings"]]}
{"text": "Model Setup PBoS composes word embeddings out of subword vectors exactly as described in Section 3 .", "entities": [[4, 6, "TaskName", "word embeddings"]]}
{"text": "One may argue that PBoS has an advantage for using the most number of parameters .", "entities": [[12, 15, "HyperparameterName", "number of parameters"]]}
{"text": "In fact , restricting subword length and using hashing helped them for the word similarity task .", "entities": [[13, 15, "TaskName", "word similarity"]]}
{"text": "We train fastText 12 over the same English corpus on which the Polyglot target vectors are trained , in order to understand the quantitative impact of contextual information .", "entities": [[2, 3, "MethodName", "fastText"]]}
{"text": "The word similarity scores we get for the trained fastText model are 65/40/14 for WS / RW / Card .", "entities": [[1, 3, "TaskName", "word similarity"], [9, 10, "MethodName", "fastText"]]}
{"text": "We note the great gain for WS and RW , suggesting the helpfulness of contextual information in learning and generalizing word embeddings in the setting of small to moderate OOV rates .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "Surprisingly , we find that for the case of extremely high OOV rate ( Card ) , PBoS slightly surpasses fastText , suggesting PBoS ' effectiveness in generalizing embeddings to OOV words even without any help from contexts .", "entities": [[20, 21, "MethodName", "fastText"]]}
{"text": "We follow the evaluation protocol for sequential labeling used by Kiros et al ( 2015 ) and Li et al ( 2017 ) , and use logistic regression classifier 13 as the model for POS tagging .", "entities": [[26, 28, "MethodName", "logistic regression"]]}
{"text": "14 Dataset We train and evaluate the performance of generated word embeddings over 23 languages at the intersection of the Polyglot ( Al - Rfou ' et al , 2013 ) pre - trained embedding vectors 15 and the Universal Dependency ( UD , v1.4 16 ) dataset .", "entities": [[10, 12, "TaskName", "word embeddings"], [42, 43, "DatasetName", "UD"]]}
{"text": "LogisticRegression.html 14 As a side note , in our early trials , we tried to evaluate using an LSTM model following Pinter et al ( 2017 ) and", "entities": [[18, 19, "MethodName", "LSTM"]]}
{"text": "UD is used as the POS tagging dataset to train and test the POS tagging model .", "entities": [[0, 1, "DatasetName", "UD"]]}
{"text": "Following Sasaki et al ( 2019 ) , we tune the regularization term of the logistic regression model when evaluating KVQ - FH .", "entities": [[15, 17, "MethodName", "logistic regression"]]}
{"text": "Overall , Table 5 shows that the word embeddings composed by our PBoS is effective at predicting POS tags for a wide range of languages .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "Popular word embedding methods , such as word2vec ( Mikolov et al , 2013 ) , GloVe ( Pennington et al , 2014 ) , often assume finite - size vocabularies , giving rise to the problem of OOV words .", "entities": [[16, 17, "MethodName", "GloVe"]]}
{"text": "FastText attempted to alleviate the problem using subword - level model , and was followed by interests of using subword information to improve word embedding ( Wieting et al , 2016 ; Cao and Lu , 2017 ; Li et al , 2017 ; Athiwaratkun et al , 2018 ; Li et al , 2018 ; Salle and Villavicencio , 2018 ; Xu et", "entities": [[0, 1, "MethodName", "FastText"]]}
{"text": "The only exception is the character LSTM model by Cao and Rei ( 2016 ) , which has shown some ability to recover the morphological boundary as a byproduct of learning word embedding .", "entities": [[6, 7, "MethodName", "LSTM"]]}
{"text": "The most related works in generalizing pretrained word embeddings have been discussed in Section 1 and compared throughout the paper .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "We propose PBoS model for generalizing pretrained word embeddings without contextual information .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "PBoS simultaneously considers all possible subword segmentations of a word and derives meaningful subword weights that lead to better composed word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}
{"text": "Experiments on segmentation results , affix prediction , word similarity , and POS tagging over 23 languages support the claim .", "entities": [[8, 10, "TaskName", "word similarity"]]}
{"text": "lr : Learning rate .", "entities": [[2, 4, "HyperparameterName", "Learning rate"]]}
{"text": "lr decay : Whether to set learning rate to be inversely proportional to the square root of the epoch number .", "entities": [[6, 8, "HyperparameterName", "learning rate"], [18, 20, "HyperparameterName", "epoch number"]]}
{"text": "normalize semb : Whether to normalize subword embeddings before composing word embeddings .", "entities": [[10, 12, "TaskName", "word embeddings"]]}
{"text": "prob eps : Default likelihood for unknown characters .", "entities": [[1, 2, "HyperparameterName", "eps"]]}
{"text": "C : The inverse regularization term used by the logistic regression classifier .", "entities": [[9, 11, "MethodName", "logistic regression"]]}
{"text": "Table 6 and Table 8 show the hyperparameter values used in the word similarity experiment ( Section 4.3 ) .", "entities": [[12, 14, "TaskName", "word similarity"]]}
{"text": "We transform all words in the benchmarks into lowercase , following the convention in FastText , BoS ( Zhao et al , 2018 ) , and KVQ - FH ( Sasaki et al , 2019 ) .", "entities": [[14, 15, "MethodName", "FastText"]]}
{"text": "During the evaluation , we use 0 as the similarity score for a pair of words if we can not get word vector for one of the words , or the magnitude of the word vector is too small .", "entities": [[6, 7, "DatasetName", "0"]]}
{"text": "Table 9 lists experimental result for word similarity in greater detail .", "entities": [[6, 8, "TaskName", "word similarity"]]}
{"text": "The OOV rates and word similarity scores can be found in Table 10 .", "entities": [[4, 6, "TaskName", "word similarity"]]}
{"text": "We do not clean or filter words as we did for the English word similarity , because we found it difficult to have a consistent way of pre - processing words across languages .", "entities": [[13, 15, "TaskName", "word similarity"]]}
{"text": "We list the results for multilingual word similarity in Table 11 .", "entities": [[6, 8, "TaskName", "word similarity"]]}
{"text": "For the prediction model , we use the logistic regression classifier from scikit - learn 0.19.1 with the default settings .", "entities": [[8, 10, "MethodName", "logistic regression"]]}
{"text": ", 9 and b = \u22121 , 0 , . .", "entities": [[7, 8, "DatasetName", "0"]]}
{"text": "PBoS is able to achieve better accuracy over BoS and KVQ - FH in all languages regardless of their morphological type , OOV rate and number of training instances for POS tagging .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Knowledge Base Question Answering via Encoding of Complex Query Graphs", "entities": [[0, 4, "TaskName", "Knowledge Base Question Answering"]]}
{"text": "The knowledge - based question answering ( KBQA ) is a task which takes a natural language question as input and returns a factual answer using structured knowledge bases such as Freebase ( Bollacker et al , 2008 ) , YAGO ( Suchanek et al , 2007 ) and DBpedia ( Auer et al , 2007 ) .", "entities": [[4, 6, "TaskName", "question answering"], [40, 41, "DatasetName", "YAGO"], [49, 50, "DatasetName", "DBpedia"]]}
{"text": "As a classic branch of KBQA solutions , semantic parsing ( SP ) technique ( Berant et al , 2013 ; Yih et al , 2015 ; Hu et al , 2018 ) aims at learning semantic parse trees or equivalent query graphs 1 for representing semantic structures of the questions .", "entities": [[8, 10, "TaskName", "semantic parsing"]]}
{"text": "The common step of SP - based approaches is to first collect candidate query graphs using bottom up parsing ( Berant et al , 2013 ; Cai and Yates , 2013 ) or staged query generation methods ( Yih et al , 2015 ; Bao et al , 2016 ) , then predict the best graph mainly based on the semantic similarity with the given question .", "entities": [[60, 62, "TaskName", "semantic similarity"]]}
{"text": "Existing NN - based methods follow an encode - andcompare framework for answering simple questions , where both the question and the predicate sequence are encoded as semantic vectors in a common embedding space , and the semantic similarity is calculated by the cosine score between vectors .", "entities": [[37, 39, "TaskName", "semantic similarity"]]}
{"text": "In order to attack the above limitations , we propose a neural network based approach to improve the performance of semantic similarity measurement in complex question answering .", "entities": [[20, 22, "TaskName", "semantic similarity"], [25, 27, "TaskName", "question answering"]]}
{"text": "In addition , to cope with different semantic components of a query graph , we leverage dependency parsing information as a complementary of sentential information for question encoding , which makes the model better align each component to the question .", "entities": [[16, 18, "TaskName", "dependency parsing"]]}
{"text": "We leverage dependency parsing to enrich question representation in the NN model , and conduct thorough investigations to verify its effectiveness ( Section 2.2.2 ) ; We propose an ensemble method to enrich entity linking from a state - of - the - art linking tool , which further improves the performance of the overall task ( Section 2.3 ) ;", "entities": [[2, 4, "TaskName", "dependency parsing"], [33, 35, "TaskName", "entity linking"]]}
{"text": "Then we introduce an ensemble approach for entity linking enrichment ( Section 2.3 ) , Finally , we discuss the prediction and parameter learning step of this task ( Section 2.4 ) .", "entities": [[7, 9, "TaskName", "entity linking"]]}
{"text": "For entity linking , we generate ( mention , entity ) pairs using the state - of - the - art entity linking tool S - MART ( Yang and Chang , 2015 ) .", "entities": [[1, 3, "TaskName", "entity linking"], [21, 23, "TaskName", "entity linking"]]}
{"text": "Our improvement in this step is to filter type constraints using implicit types 2~2 0 superlative words , such as largest , highest , latest .", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "We introduce in detail the encoding methods for questions and predicate sequences , and how to calculate the semantic similarity score .", "entities": [[18, 20, "TaskName", "semantic similarity"]]}
{"text": "Vw | \u00d7d to convert the original sequence into word embeddings { p ( w ) 1 , . .", "entities": [[9, 11, "TaskName", "word embeddings"]]}
{"text": ", p ( w ) n } , where | V w | denotes the vocabulary size of natural language words , and d denotes the embedding dimension .", "entities": [[26, 28, "HyperparameterName", "embedding dimension"]]}
{"text": "Then we encode the token sequence by applying bidirectional GRU network ( Cho et al , 2014 ) .", "entities": [[8, 10, "MethodName", "bidirectional GRU"]]}
{"text": "The representation of the token sequence is the concatenation of the last forward and backward hidden states through the Bi - GRU layer , q ( tok ) =", "entities": [[21, 22, "MethodName", "GRU"]]}
{"text": "To encode the question at local level , we leverage dependency parsing to represent long - range dependencies between the answer and the focus node in p.", "entities": [[10, 12, "TaskName", "dependency parsing"]]}
{"text": "We apply another bidirectional GRU layer to produce the vector representation at dependency level q ( dep ) p , capturing both syntactic features and local semantic features .", "entities": [[3, 5, "MethodName", "bidirectional GRU"]]}
{"text": "We apply max pooling over the hidden vectors of semantic components , and get the compositional semantic representation of the entire query graph .", "entities": [[2, 4, "MethodName", "max pooling"]]}
{"text": "Similarly , we perform max pooling for the question vectors with respect to each semantic component .", "entities": [[4, 6, "MethodName", "max pooling"]]}
{"text": "Finally , we compute the semantic similarity score between the graph and question : Based on this framework , our proposed method ensures the vector spaces of the question and the entire query graph are comparable , and captures complementary semantic features from different parts of the query graph .", "entities": [[5, 7, "TaskName", "semantic similarity"]]}
{"text": "It 's worth mentioning that the semantic matching model is agnostic to the candidate generation method of the query graphs , hence it can be applied to the other existing semantic parsing frameworks .", "entities": [[30, 32, "TaskName", "semantic parsing"]]}
{"text": "To seek a better balance at entity linking , we propose an ensemble approach to enrich linking results .", "entities": [[6, 8, "TaskName", "entity linking"]]}
{"text": "For the pairs found in S - MART results , we take the above features as the input to a 2 - layer linear regression model fitting their linking scores .", "entities": [[23, 25, "MethodName", "linear regression"]]}
{"text": "To predict the best query graph from candidates , we calculate the overall association score S ( q , G ) between the question q and each candidate G , which is the weighted sum of features over entity linking , semantic matching and structural level .", "entities": [[38, 40, "TaskName", "entity linking"]]}
{"text": "QA datasets : We conduct our experiments on ComplexQuestions ( Bao et al , 2016 ) , We - bQuestions ( Berant et al , 2013 ) and SimpleQuestions ( Bordes et al , 2015 ) .", "entities": [[28, 29, "DatasetName", "SimpleQuestions"]]}
{"text": "WebQ contains 5 , 810 questions collected from Google Suggest API , and is split into 3 , 778 training and 2 , 032 testing QA pairs .", "entities": [[8, 9, "DatasetName", "Google"]]}
{"text": "Implementation detail : For all experiments in this section , we initialize word embeddings using GloVe ( Pennington et al , 2014 ) word vectors with dimensions set to 300 , and the size of Bi - GRU hidden layer is also set to 300 .", "entities": [[12, 14, "TaskName", "word embeddings"], [15, 16, "MethodName", "GloVe"], [37, 38, "MethodName", "GRU"]]}
{"text": "Jain ( 2016 ) achieves highest F 1 score on WebQ using memory networks , which is not semantic parsing based , and thus less interpretable .", "entities": [[18, 20, "TaskName", "semantic parsing"]]}
{"text": "The best result is from Qu et al ( 2018 ) , which learns the semantic similarity through both attentive RNN and similarity matrix based CNN .", "entities": [[15, 17, "TaskName", "semantic similarity"]]}
{"text": "For encoding predicate word sequence , we use BiGRU ( the same setting as encoding question word sequence ) as the alternative of average word embedding .", "entities": [[8, 9, "MethodName", "BiGRU"]]}
{"text": "For the encoding of word sequences , the average word embedding method outperforms BiGRU on CompQ , and the gap becomes smaller when running on WebQ. This is mainly because the training set of WebQ is about 3 times larger than that of CompQ , making it easier for training a more complex model .", "entities": [[13, 14, "MethodName", "BiGRU"]]}
{"text": "Semantic composition and question representation : To demonstrate the effectiveness of semantic composition , we construct a straightforward baseline , where we remove the max pooling operation in Eq .", "entities": [[0, 2, "TaskName", "Semantic composition"], [11, 13, "TaskName", "semantic composition"], [24, 26, "MethodName", "max pooling"]]}
{"text": "( 2 ) , and instead calculate the semantic similarity score as the summation of individual cosine similarities : S sem ( q , G )", "entities": [[8, 10, "TaskName", "semantic similarity"]]}
{"text": "In terms of semantic composition , Our max pooling based method consistently outperforms the baseline method .", "entities": [[3, 5, "TaskName", "semantic composition"], [7, 9, "MethodName", "max pooling"]]}
{"text": "To further explain the advantage of semantic composition , we take the following question as an example : \" who is gimli 's father in the hobbit \" .", "entities": [[6, 8, "TaskName", "semantic composition"]]}
{"text": "Entity linking error ( 16 % ) :", "entities": [[0, 2, "TaskName", "Entity linking"]]}
{"text": "For example , the question \" What character did Robert Pattinson play in Harry Potter \" expects the film \" Harry Potter and the Goblet of Fire \" as the focus , while there are 7 movies in Harry Potter series ; Miscellaneous ( 32 % ) :", "entities": [[42, 43, "TaskName", "Miscellaneous"]]}
{"text": "Knowledge Base Question Answering ( KBQA ) has been a hot research top in recent years .", "entities": [[0, 4, "TaskName", "Knowledge Base Question Answering"]]}
{"text": "Generally speaking , the most popular methods for KBQA can be mainly divided into two classes : information retrieval and semantic parsing .", "entities": [[17, 19, "TaskName", "information retrieval"], [20, 22, "TaskName", "semantic parsing"]]}
{"text": "Information retrieval based system tries to obtain target answer directly from question information and KB knowledge without explicit considering interior query structure .", "entities": [[0, 2, "TaskName", "Information retrieval"]]}
{"text": "Semantic parsing based approach focuses on constructing a semantic parsing tree or equivalent query structure that represents the semantic meaning of the question .", "entities": [[0, 2, "TaskName", "Semantic parsing"], [8, 10, "TaskName", "semantic parsing"]]}
{"text": "Belonging to NN - based semantic parsing category , our approach employs a novel encoding structure method to solve complex questions .", "entities": [[5, 7, "TaskName", "semantic parsing"]]}
{"text": "Unlike their approaches , our method encodes multiple relations ( paths ) into a uniform query structure representation ( semantic composition ) , which allows more flexible query structures .", "entities": [[19, 21, "TaskName", "semantic composition"]]}
{"text": "Jain ( 2016 ) introduces Factual Memory Network , which tries to encode KB and questions in same word vector space , extract a subset of initial candidate facts , then try to employ multi - hop reasoning and refinement to find a path to answer entity .", "entities": [[6, 8, "MethodName", "Memory Network"]]}
{"text": "This paper describes the system of the team Orange - Deski\u00f1 , used for the CoNLL 2017 UD Shared Task .", "entities": [[17, 18, "DatasetName", "UD"]]}
{"text": "In most cases we also employed word embeddings .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Thus during the last year we tried several freely available open source tools available ( e.g. MaltParser 2 , Google 's SyntaxNet 3 , Standford Dependency Tools 4 , Bist - 1 Since we are interested in semantic relations a good CLAS score ( Nivre and Fang , 2017 ) is even more relevant .", "entities": [[19, 20, "DatasetName", "Google"]]}
{"text": "2 http://www.maltparser.org/ 3 https://www.tensorflow.org/versions/ r0.11 / tutorials / syntaxnet/ 4 https://nlp.stanford.edu/software/ stanford - dependencies.shtml Parser 5 and HTParser 6 ) , trained on different Treebanks ( notably French Sequoia ( Candito et al , 2014 ) and Universal Dependencies ( McDonald et al , 2013 ) ) .", "entities": [[37, 39, "DatasetName", "Universal Dependencies"]]}
{"text": "In a next step we enriched the French treebanks with additional information like lemmas , morphological features and more fine - graded XPOS in addition to the about 20 UPOS categories of the treebanks ( UD - French v1.2 does not contain neither lemmas nor morphological features ) and conducted a new training / test / evaluation cycle .", "entities": [[35, 36, "DatasetName", "UD"]]}
{"text": "Since the initial results for French were encouraging we tried the same approaches with other languages , such as the languages proposed for CoNLL 2017 UD Shared Task ( Zeman et al , 2017 ) .", "entities": [[25, 26, "DatasetName", "UD"]]}
{"text": "However , for participation at the shared task , we relied exclusively on the data provided by Universal Dependencies ( Nivre et al , 2016 ( Nivre et al , , 2017b , also for French in spite of our previous work .", "entities": [[17, 19, "DatasetName", "Universal Dependencies"]]}
{"text": "For the shared task , we used an ( older ) version of BistParser for all treebanks ( ud - treebanks - conll2017 ) .", "entities": [[18, 19, "DatasetName", "ud"]]}
{"text": "BistParser ( Kiperwasser and Goldberg , 2016 ) is a transition based parser ( Nivre ( 2008 ) , and which uses the arc - hybrid transition system ( Kuhlmann et al , 2011 ) ) with the three \" basic \" transitions LEFT ARC , RIGHT ARC and SHIFT .", "entities": [[44, 45, "DatasetName", "ARC"], [47, 48, "DatasetName", "ARC"]]}
{"text": "BistParser uses a bidirectional LSTM neural network .", "entities": [[3, 5, "MethodName", "bidirectional LSTM"]]}
{"text": "We have started implementing the use of feature column as well , but this has not been used for the CoNLL 2017 UD Shared Task .", "entities": [[22, 23, "DatasetName", "UD"]]}
{"text": "In order to reduce memory usage during training and prediction , we modified BistParser and the underlying CNN library 7 to load word embeddings only for the words present in the training or test data .", "entities": [[22, 24, "TaskName", "word embeddings"]]}
{"text": "We trained our models using all treebanks provided by the CoNLL 2017 UD Shared Task .", "entities": [[12, 13, "DatasetName", "UD"]]}
{"text": "In most cases , adding word embeddings improved the LAS considerably .", "entities": [[5, 7, "TaskName", "word embeddings"]]}
{"text": "We downloaded the language specific corpora provided 9 by the task organisers and calculated our own word embeddings with Mikolov 's word2vec ( Mikolov et al , 2013 ) 10 , which gave better results than the 100dimensional word embeddings provided .", "entities": [[16, 18, "TaskName", "word embeddings"], [38, 40, "TaskName", "word embeddings"]]}
{"text": "Finally we trained word embeddings with 300 and 500 dimensional vectors respectively .", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "The word embeddings were calculated on a server with a 32 core CPU running Ubuntu 14.04 11 .", "entities": [[1, 3, "TaskName", "word embeddings"]]}
{"text": "A similar approach to word2vec is fastText The fundamental difference is the adoption of the \" subword model \" described in .", "entities": [[6, 7, "MethodName", "fastText"]]}
{"text": "As a matter of fact , word2vec can been seen as the minimum configuration of fastText where only the words are considered .", "entities": [[15, 16, "MethodName", "fastText"]]}
{"text": "FastText has been demonstrated to perform rather well in two different tasks i.e. sentiment analysis and tag prediction .", "entities": [[0, 1, "MethodName", "FastText"], [13, 15, "TaskName", "sentiment analysis"]]}
{"text": "For the CoNLL 2017 UD Shared Task we finally used word2vec , since the results were similar , but fastText was taking significantly more time to train .", "entities": [[4, 5, "DatasetName", "UD"], [19, 20, "MethodName", "fastText"]]}
{"text": "We trained all treebanks without any word embeddings , with 300 and with 500 dimensional word embeddings .", "entities": [[6, 8, "TaskName", "word embeddings"], [15, 17, "TaskName", "word embeddings"]]}
{"text": "We did all training on two Ubuntu 16.04 servers 13 with 64 GB RAM .", "entities": [[13, 14, "MethodName", "RAM"]]}
{"text": "The training processes used up to 15 GB RAM , and took between 1 minute ( Kazakh ) and 53 hours ( Czech ) .", "entities": [[8, 9, "MethodName", "RAM"]]}
{"text": "Training was on the gold values ( form , lemma , XPOS , UPOS , deprel , head ) of the training treebanks 14 , however , both , the development set ( on the Tira - platform ) and the final test set use the UD - Pipe output e.g. lemma , XPOS or UPOS ( Straka et al , 2016 ) which may be erroneous .", "entities": [[9, 10, "DatasetName", "lemma"], [46, 47, "DatasetName", "UD"], [51, 52, "DatasetName", "lemma"]]}
{"text": "With 16 GB RAM on the virtual machine provided by Tira ( Potthast et al , 2014 ) 15 the 56 development corpora ( on the Tira platform ) were processed in about 130 minutes .", "entities": [[3, 4, "MethodName", "RAM"]]}
{"text": "In all three cases we replaced the forms of all closed word classes ( i.e. all but nouns , adjectives and verbs ) with the corresponding UPOS in the training and in the test corpus ( for the CoNLL 2017 UD Shared Task we inserted the original forms again after predicting the dependency relations .", "entities": [[40, 41, "DatasetName", "UD"]]}
{"text": "We initially tested these models using the test corpus for the Tamil treebank ( UD v2.0 ) .", "entities": [[14, 15, "DatasetName", "UD"]]}
{"text": "With Urdu , which is very similar to Hindi apart from the fact that it uses the Arabic alphabet instead of Devanagari , the LAS was less good .", "entities": [[1, 2, "DatasetName", "Urdu"]]}
{"text": "Possibly the fact that the raw text corpus used to calculate word embeddings for Kazakh and Uyghur are much bigger than those of the surprise languages allowed to produce usable word embeddings .", "entities": [[11, 13, "TaskName", "word embeddings"], [30, 32, "TaskName", "word embeddings"]]}
{"text": "If so , this would mean that word embeddings play a very prominent role in data driven dependency parsing .", "entities": [[7, 9, "TaskName", "word embeddings"], [17, 19, "TaskName", "dependency parsing"]]}
{"text": "Further we made en error uploading the models for the gl TreeGal , fr parTut and sl sst treebanks .", "entities": [[17, 18, "DatasetName", "sst"]]}
{"text": "Apart from the result for gl TreeGal and sl sst , which went up to 66.13 % ( from 22.46 % ) and to 47.68 ( from 40.25 ) respectively once the correct model was used , the results for the other corpora changed only slightly , the global results could have been 69.38 % .", "entities": [[9, 10, "DatasetName", "sst"]]}
{"text": "For gl TreeGal , fr parTut and sl sst , this is due to errors when installing our system on the Tira - platform .", "entities": [[8, 9, "DatasetName", "sst"]]}
{"text": "ASSIST : Towards Label Noise - Robust Dialogue State Tracking", "entities": [[7, 10, "TaskName", "Dialogue State Tracking"]]}
{"text": "The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state tracking ( DST ) .", "entities": [[1, 3, "DatasetName", "MultiWOZ 2.0"], [10, 13, "TaskName", "dialogue state tracking"]]}
{"text": "Although several refined versions , including MultiWOZ 2.1 - 2.4 , have been published recently , there are still lots of noisy labels , especially in the training set .", "entities": [[6, 8, "DatasetName", "MultiWOZ 2.1"]]}
{"text": "In this paper , instead of improving the annotation quality further , we propose a general framework , named ASSIST ( lAbel noiSe - robuSt dIalogue State Tracking ) , to train DST models robustly from noisy labels .", "entities": [[25, 28, "TaskName", "dIalogue State Tracking"]]}
{"text": "Task - oriented dialogue systems play an important role in helping users accomplish a variety of tasks through verbal interactions ( Young et al , 2013 ; Gao et al , 2019 ) .", "entities": [[0, 5, "TaskName", "Task - oriented dialogue systems"]]}
{"text": "Dialogue state tracking ( DST ) is an essential component of the dialogue manager in pipeline - based task - oriented dialogue systems .", "entities": [[0, 3, "TaskName", "Dialogue state tracking"], [18, 23, "TaskName", "task - oriented dialogue systems"]]}
{"text": "al , 2017 ) , MultiWOZ 2.0 ( Budzianowski et al , 2018 ) , Cross - WOZ , and SGD .", "entities": [[5, 7, "DatasetName", "MultiWOZ 2.0"], [20, 21, "MethodName", "SGD"]]}
{"text": "Among these datasets , MultiWOZ 2.0 is the most popular one .", "entities": [[4, 6, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "However , it has been found out that there is substantial noise in the state annotations of MultiWOZ 2.0 ( Eric et al , 2020 ) .", "entities": [[17, 19, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "To remedy this issue , massive efforts have been devoted to rectifying the annotations , and four refined versions , including MultiWOZ 2.1 ( Eric et al , 2020 ) , MultiWOZ 2.2 , MultiWOZ 2.3 ( Han et al , 2020b ) , and MultiWOZ 2.4 ( Ye et al , 2021a ) , have been released .", "entities": [[21, 23, "DatasetName", "MultiWOZ 2.1"], [31, 33, "DatasetName", "MultiWOZ 2.2"], [34, 36, "DatasetName", "MultiWOZ 2.3"], [45, 47, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "For example , in the latest version MultiWOZ 2.4 , the validation set and test set have been manually re - annotated and tend to be noise - free .", "entities": [[7, 9, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "Although loads of noisy label learning algorithms ( Natarajan et al , 2013 ; Han et al , 2020a ) have been proposed in the machine learning community , most of them target only multi - class classification ( Song et al , 2020 ) .", "entities": [[34, 38, "TaskName", "multi - class classification"]]}
{"text": "In this paper we propose a general framework , named ASSIST ( lAbel noiSe - robuSt dIalogue State Tracking ) , to train DST models robustly from noisy labels .", "entities": [[16, 19, "TaskName", "dIalogue State Tracking"]]}
{"text": "Figure 2 shows the architecture , which consists of a dialogue context semantic encoder , a slot attention module , and a slot - value matching module .", "entities": [[16, 18, "MethodName", "slot attention"]]}
{"text": "al , 2021b ) , we utilize the pre - trained language model BERT ( Devlin et al , 2019 ) to encode the dialogue context X t into contextual semantic representations .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "In view of this , we treat the previous turn dialogue state B t\u22121 as part of the input as well , which can be beneficial when X t exceeds the maximum input length of BERT .", "entities": [[35, 36, "MethodName", "BERT"]]}
{"text": "Slot Attention Let H t R |", "entities": [[0, 2, "MethodName", "Slot Attention"]]}
{"text": "Here , | I t | and d denote the sequence length of I t and the BERT output dimension , respectively .", "entities": [[17, 18, "MethodName", "BERT"]]}
{"text": "Then , we have : H t = BERT f inetune ( I t ) , where BERT f inetune means that the BERT model will be fine - tuned during the training process .", "entities": [[8, 9, "MethodName", "BERT"], [17, 18, "MethodName", "BERT"], [23, 24, "MethodName", "BERT"]]}
{"text": "For each slot s and its candidate value v V s , we employ another BERT to encode them into semantic vectors h s R d and h v R d .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "Unlike the dialogue context , we leverage the pre - trained BERT without fine - tuning to embed s and v .", "entities": [[11, 12, "MethodName", "BERT"]]}
{"text": "BERT", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "[ CLS ] f ixed ( [ CLS ] s [ SEP ] ) , h v = BERT [ CLS ] f ixed ( [ CLS ]", "entities": [[18, 19, "MethodName", "BERT"]]}
{"text": "The slot attention module is exploited to retrieve slot - relevant information for all the slots from the same dialogue context .", "entities": [[1, 3, "MethodName", "slot attention"]]}
{"text": "The slot attention is a multihead attention ( Vaswani et al , 2017 ) .", "entities": [[1, 3, "MethodName", "slot attention"]]}
{"text": "The slot attention matches h s to the semantic vector of each word in the dialogue context and calculates the attention score , based on which the slot - specific information can be extracted .", "entities": [[1, 3, "MethodName", "slot attention"]]}
{"text": "s. Considering that the output of BERT is normalized by layer normalization ( Ba et al , 2016 ) , we also feed a s t to a layer normalization layer , which is preceded by a linear transformation layer .", "entities": [[6, 7, "MethodName", "BERT"], [10, 12, "MethodName", "layer normalization"], [28, 30, "MethodName", "layer normalization"]]}
{"text": "Without loss of generality , the pseudo label generation process is denoted as follows : B t = A ( X t , S ) , where X t belongs to the noisy training set .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "\u03b1 ( s , vt )", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "We adopt the mean squared loss to define the approximation error of any corrupted labelsv t associated with the noisy training set D n as :", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "The optimal approximation error with respect to the combined labels v c t is smaller than that of the vanilla labels\u1e7d t and pseudo labelsv t , i.e. , min \u03b1 Y v c < min { Y\u1e7d , Yv } .", "entities": [[30, 31, "HyperparameterName", "\u03b1"]]}
{"text": "By setting \u03b1 = Y\u1e7d Y\u1e7d+Yv , Y v c reaches its minimum : min \u03b1 Y", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [15, 16, "HyperparameterName", "\u03b1"]]}
{"text": "Note that we can not calculate the optimal value of \u03b1 directly .", "entities": [[10, 11, "HyperparameterName", "\u03b1"]]}
{"text": "We adopt MultiWOZ 2.0 ( Budzianowski et al , 2018 ) and MultiWOZ 2.4 ( Ye et al , 2021a ) as the datasets in our experiments .", "entities": [[2, 4, "DatasetName", "MultiWOZ 2.0"], [12, 14, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "MultiWOZ 2.0 is one of the largest publicly available multi - domain taskoriented dialogue datasets , including about 10 , 000 dialogues spanning seven domains .", "entities": [[0, 2, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "MultiWOZ 2.4 is the latest refined version of MultiWOZ 2.0 .", "entities": [[0, 2, "DatasetName", "MultiWOZ 2.4"], [8, 10, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "While its training set remains intact and is the same as that of MultiWOZ 2.1 ( Eric et al , 2020 ) , in which 41.34 % of the state values are changed , compared to MultiWOZ 2.0 .", "entities": [[13, 15, "DatasetName", "MultiWOZ 2.1"], [36, 38, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "Considering that the validation set and test set of MultiWOZ 2.0 are noisy , we replace them with the counterparts of MultiWOZ 2.4 2 .", "entities": [[9, 11, "DatasetName", "MultiWOZ 2.0"], [21, 23, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "We exploit joint goal accuracy and slot accuracy as the evaluation metrics .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 8, "MetricName", "accuracy"]]}
{"text": "The slot accuracy is defined as the average of all individual slot accuracies .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "The accuracy of an individual slot is calculated as the ratio of dialogue turns in which its value is correctly predicted .", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "We also propose a new evaluation metric , termed as joint turn accuracy .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "We define joint turn accuracy as the proportion of dialogue turns in which the values of all active slots are correctly predicted .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "The advantage of joint turn accuracy is that it can tell us in how many turns the turn - level information is fully captured by the model .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "SOM - DST : SOM - DST ) is an open vocabulary - based method .", "entities": [[0, 1, "MethodName", "SOM"], [4, 5, "MethodName", "SOM"]]}
{"text": "STAR :", "entities": [[0, 1, "DatasetName", "STAR"]]}
{"text": "STAR ( Ye et al , 2021b ) is a predefined ontology - based method .", "entities": [[0, 1, "DatasetName", "STAR"], [11, 12, "MethodName", "ontology"]]}
{"text": "For the auxiliary model , the pre - trained BERT - baseuncased model is utilized as the dialogue context encoder .", "entities": [[9, 10, "MethodName", "BERT"]]}
{"text": "Another pre - trained BERT - base - uncased model with fixed weights is employed to encode the slots and their candidate values .", "entities": [[4, 5, "MethodName", "BERT"]]}
{"text": "The maximum input length of the BERT model is set to 512 .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "The number of heads in the slot attention module is set to 4 .", "entities": [[6, 8, "MethodName", "slot attention"]]}
{"text": "The output dimension of the linear transformation layer is set to 768 , which is the same as the dimension of the BERT outputs .", "entities": [[22, 23, "MethodName", "BERT"]]}
{"text": "Recall that the previous turn dialogue state is treated as part of the input .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "Table 1 presents the performance scores of the three different primary DST models on the test sets of MultiWOZ 2.0 & 2.4 when they are trained using our proposed framework ASSIST .", "entities": [[18, 20, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "From Table 1 , we further observe that the performance improvements on MultiWOZ 2.4 are lower than on MultiWOZ 2.0 .", "entities": [[12, 14, "DatasetName", "MultiWOZ 2.4"], [18, 20, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "This is because the training set of MultiWOZ 2.4 is the same as that of MultiWOZ 2.1 ( Eric et al , 2020 ) , in which lots of annotation errors have been fixed .", "entities": [[7, 9, "DatasetName", "MultiWOZ 2.4"], [15, 17, "DatasetName", "MultiWOZ 2.1"]]}
{"text": "This is because SOM - DST focuses on turn - active slots and copies the values for other slots from previous turns , while both STAR and AUX - DST predict the values of all slots from scratch at each turn .", "entities": [[3, 4, "MethodName", "SOM"], [25, 26, "DatasetName", "STAR"]]}
{"text": "These results show that the joint turn accuracy can help us understand in more depth how different models behave .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "In order to verify the superiority of the proposed model , we also apply STAR as the auxiliary model and compare their performance in Figure 3 .", "entities": [[14, 15, "DatasetName", "STAR"]]}
{"text": "We chose STAR due to its good performance , as shown in Table 1 .", "entities": [[2, 3, "DatasetName", "STAR"]]}
{"text": "From Figure 3 , we observe that all three primary 0 0.1 0 . 2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 models demonstrate higher performance on both datasets when using the proposed auxiliary model than using STAR as the auxiliary model .", "entities": [[10, 11, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [38, 39, "DatasetName", "STAR"]]}
{"text": "The parameter \u03b1 adjusts the weights of the pseudo labels and vanilla labels in the training phase .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "As can be seen , \u03b1 plays an important role in balancing the pseudo labels and vanilla labels .", "entities": [[5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "Since the training set of MultiWOZ 2.0 has more noisy labels than that of MultiWOZ 2.4 , more emphasis should be put on its pseudo labels to obtain the best performance .", "entities": [[5, 7, "DatasetName", "MultiWOZ 2.0"], [14, 16, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "This is because the vanilla labels will contribute less to the training of the primary model when \u03b1 is set to be larger .", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "To achieve this goal , we can compute the joint goal accuracy and joint turn accuracy of the auxiliary model on the training set .", "entities": [[11, 12, "MetricName", "accuracy"], [15, 16, "MetricName", "accuracy"]]}
{"text": "( train - leaveat , 12:30 ) [ usr ] : I am staying in the west part of Cambridge and would like to know about some places to go .", "entities": [[19, 20, "DatasetName", "Cambridge"]]}
{"text": "These dialogue snippets are chosen from the training set of MultiWOZ 2.4 .", "entities": [[10, 12, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "However , the performance improvements are lower , compared to using pseudo labels ( especially on MultiWOZ 2.0 due to its noisier training set ) .", "entities": [[16, 18, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "The results on the test set of MultiWOZ 2.4 are illustrated in Figure 7 , from which we can observe that the slot \" hotel - type \" has the highest error rate .", "entities": [[7, 9, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "This is probably due to the fact that we have used the same parameter \u03b1 to combine the pseudo labels and vanilla labels of all slots .", "entities": [[14, 15, "HyperparameterName", "\u03b1"]]}
{"text": "Recently , DST has got an enormous amount of attention , thanks to the availability of multiple largescale multi - domain dialogue datasets such as Multi - WOZ 2.0 ( Budzianowski et al , 2018 ) , MultiWOZ 2.1 ( Eric et al , 2020 ) , RiSAWOZ ( Quan et al , 2020 ) , and SGD .", "entities": [[37, 39, "DatasetName", "MultiWOZ 2.1"], [47, 48, "DatasetName", "RiSAWOZ"], [57, 58, "MethodName", "SGD"]]}
{"text": "The most popular datasets are MultiWOZ 2.0 and MultiWOZ 2.1 , and lots of DST models have been built on top of them Wu et al , 2019 ;", "entities": [[5, 7, "DatasetName", "MultiWOZ 2.0"], [8, 10, "DatasetName", "MultiWOZ 2.1"]]}
{"text": "These recent DST models can be grouped into two categories : predefined ontology - based models and open vocabulary - based models .", "entities": [[12, 13, "MethodName", "ontology"]]}
{"text": "The predefined ontology - based models treat DST as a multi - label classification problem and tend to demonstrate better performance Shan et al , 2020 ; Ye et al , 2021b ) .", "entities": [[2, 3, "MethodName", "ontology"], [10, 14, "TaskName", "multi - label classification"]]}
{"text": "Even so , these methods mainly focus on multi - class classification ( Song et al , 2020 ) , which makes it not straightforward to apply them to the DST task .", "entities": [[8, 12, "TaskName", "multi - class classification"]]}
{"text": "= [ E Dc [ \u1e7d t \u2212 v t ] ] T 0 = 0 .", "entities": [[13, 14, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}
{"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}
{"text": "Y v c reaches its minimum when \u03b1 = Y\u1e7d Y\u1e7d+Yv , and", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "min \u03b1 Y v c = Y\u1e7dYv Y\u1e7d + Yv , which concludes the proof .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "We apply left truncation when the input exceeds the maximum input length of BERT .", "entities": [[13, 14, "MethodName", "BERT"]]}
{"text": "For SOM - DST and STAR , the default hyperparameters are adopted when they are applied as the primary model ( except setting num_workers = 0 ) .", "entities": [[1, 2, "MethodName", "SOM"], [5, 6, "DatasetName", "STAR"], [25, 26, "DatasetName", "0"]]}
{"text": "The results of AUX - DST on MultiWOZ 2.4 are shown in Figure 8 .", "entities": [[7, 9, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "This project was funded by the EPSRC Fellowship titled \" Task Based Information Retrieval \" and grant reference number EP / P024289/1 .", "entities": [[12, 14, "TaskName", "Information Retrieval"]]}
{"text": "Transfer Learning for Related Languages : Submissions to the WMT20 Similar Language Translation Task", "entities": [[0, 2, "TaskName", "Transfer Learning"], [12, 13, "TaskName", "Translation"]]}
{"text": "In this paper , we describe IIT Delhi 's submissions to the WMT 2020 task on Similar Language Translation for four language directions : Hindi \u2194 Marathi and Spanish \u2194 Portuguese .", "entities": [[12, 14, "DatasetName", "WMT 2020"], [18, 19, "TaskName", "Translation"]]}
{"text": "For our best submissions , we fine - tune the mBART model ( Liu et al , 2020 ) on the parallel data provided for the task .", "entities": [[10, 11, "MethodName", "mBART"]]}
{"text": "Machine Translation ( MT ) is currently tackled using rule - based methods ( RBMT )", "entities": [[0, 2, "TaskName", "Machine Translation"]]}
{"text": "Recently , pre - training methods for sequence - to - sequence ( seq2seq ) models have been introduced like MASS ( Song et al , 2019a ) , XLM ( Conneau and Lample , 2019 ) , BART ( Lewis et al , 2019 ) , and mBART ( Liu et al , 2020 ) .", "entities": [[13, 14, "MethodName", "seq2seq"], [29, 30, "MethodName", "XLM"], [38, 39, "MethodName", "BART"], [48, 49, "MethodName", "mBART"]]}
{"text": "These methods show significant gains in downstream tasks like NMT , summarization , natural language inference ( NLI ) , etc .", "entities": [[11, 12, "TaskName", "summarization"], [13, 16, "TaskName", "natural language inference"]]}
{"text": "In this paper , we focus on the transfer learning capabilities in NMT for the task of translation between related languages where parallel data is scarce .", "entities": [[8, 10, "TaskName", "transfer learning"]]}
{"text": "IIT Delhi participated in the WMT 2020", "entities": [[5, 7, "DatasetName", "WMT 2020"]]}
{"text": "Shared task on Similar Language Translation for four language directions : Hindi ( hi ) \u2194 Marathi ( mr ) and Spanish ( es ) \u2194 Portuguese ( pt ) .", "entities": [[5, 6, "TaskName", "Translation"]]}
{"text": "We fine - tuned the pre - trained mBART model ( Liu et al , 2020 ) on the parallel data provided for the task .", "entities": [[8, 9, "MethodName", "mBART"]]}
{"text": "mBART gives better performance than SMT models even when the parallel data is very limited .", "entities": [[0, 1, "MethodName", "mBART"]]}
{"text": "mBART is pre - trained on 25 languages , which contain Hindi and Spanish , but not Marathi and Portuguese .", "entities": [[0, 1, "MethodName", "mBART"]]}
{"text": "mBART is able to leverage transfer learning capabilities even for those languages that are originally not present during the pre - training phase .", "entities": [[0, 1, "MethodName", "mBART"], [5, 7, "TaskName", "transfer learning"]]}
{"text": "The fine - tuned mBART architecture forms our best submissions for both language pairs : hi \u2194 mr and es \u2194 pt .", "entities": [[4, 5, "MethodName", "mBART"]]}
{"text": "NMT is modeled using Encoder - Decoder models ( Cho et al , 2014 ; Sutskever et al , 2014 ; Bahdanau et al , 2015 ) , with the Transformer model ( Vaswani et al , 2017 ) achieving state - of - the - art on many MT problems .", "entities": [[30, 31, "MethodName", "Transformer"]]}
{"text": "Back - Translation Hoang et al , 2018 ) increases the amount of training data by using monolingual corpus along with partially - trained NMT models on the limited parallel data .", "entities": [[2, 3, "TaskName", "Translation"]]}
{"text": "Edunov et al , 2019 ) , or pre - training the complete seq2seq model ( Ramachandran et al , 2017 ; Song et al , 2019b ; Liu et al , 2020 ) .", "entities": [[13, 14, "MethodName", "seq2seq"]]}
{"text": "These pre - training methods leverage different kinds of masking techniques and the pretraining objective is to predict these masked tokens , similar to BERT", "entities": [[24, 25, "MethodName", "BERT"]]}
{"text": "Denoising auto - encoding can also be used where a sentence is corrupted by various noising techniques and the pre - training objective is to generate the original uncorrupted sentence as in BART ( Lewis et al , 2019 ) and mBART ( Liu et al , 2020 ) .", "entities": [[0, 1, "TaskName", "Denoising"], [32, 33, "MethodName", "BART"], [41, 42, "MethodName", "mBART"]]}
{"text": "There also have been works to improve low / medium resource NMT by adding linguistic information either using data augmentation ( Currey and Heafield , 2019 ) , subword embedding augmentation , or architectural changes ( Eriguchi et al , 2017 ) .", "entities": [[18, 20, "TaskName", "data augmentation"]]}
{"text": "NMT ( Transformer )", "entities": [[2, 3, "MethodName", "Transformer"]]}
{"text": "For this , we used the standard Transformer large architecture from Vaswani et", "entities": [[7, 8, "MethodName", "Transformer"]]}
{"text": "al ( 2017 ) for training on the parallel data provided for the task . NMT ( mBART ) mBART ( Liu et al , 2020 ) is a large Transformer pre - trained on monolingual data for 25 languages .", "entities": [[17, 18, "MethodName", "mBART"], [19, 20, "MethodName", "mBART"], [30, 31, "MethodName", "Transformer"]]}
{"text": "The pre - training objective for mBART is seq2seq de - noising for natural text as in BART ( Lewis et al , 2019 ) .", "entities": [[6, 7, "MethodName", "mBART"], [8, 9, "MethodName", "seq2seq"], [17, 18, "MethodName", "BART"]]}
{"text": "mBART provides a general - purpose pre - trained Transformer for any downstream task .", "entities": [[0, 1, "MethodName", "mBART"], [9, 10, "MethodName", "Transformer"]]}
{"text": "Implementation Details mBART uses a shared subword vocabulary of 250 K tokens for all the 25 languages present in the pre - training .", "entities": [[2, 3, "MethodName", "mBART"]]}
{"text": "Marathi shares its subword vocabulary with languages like Hindi and Nepali in mBART , and Portuguese shares with Spanish , Italian and other European languages present in mBART .", "entities": [[12, 13, "MethodName", "mBART"], [27, 28, "MethodName", "mBART"]]}
{"text": "The percentage of unknown tokens [ UNK ] in Marathi and Portuguese parallel datasets is less than 0.003 % when using the shared mBART vocabulary .", "entities": [[23, 24, "MethodName", "mBART"]]}
{"text": "Additionally , the mBART architecture requires language specific token at the end of each input sequence to provide the language specific context for the decoder .", "entities": [[3, 4, "MethodName", "mBART"]]}
{"text": "Since Marathi and Portuguese were not present during the pre - training phase , we use the token corresponding to the second most related language present in mBART pre - training for specifying the context at the time of decoding in each case .", "entities": [[27, 28, "MethodName", "mBART"]]}
{"text": "For the standard Transformer , we train a sentence piece model using 40 K subword tokens for hi \u2194 mr .", "entities": [[3, 4, "MethodName", "Transformer"]]}
{"text": "For mBART , we use Liu et al ( 2020 ) 's pre - trained 1 sentence piece model comprising of 250 K subword tokens as the vocabulary .", "entities": [[1, 2, "MethodName", "mBART"]]}
{"text": "We use the large Transformer from Vaswani et al ( 2017 ) with 8 encoder and decoder layers and replicate all the parameters from .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "NMT ( mBART )", "entities": [[2, 3, "MethodName", "mBART"]]}
{"text": "For this , we use 12 Transformer encoder and decoder layers , with total number of model parameters \u223c611 Million .", "entities": [[6, 7, "MethodName", "Transformer"]]}
{"text": "We use the pretrained mBART for initializing the model weights .", "entities": [[4, 5, "MethodName", "mBART"]]}
{"text": "We observed the performance of our three model settings on the validation set , and we selected the mBART model as our primary submission and SMT model as the contrastive submission for hi \u2194 mr .", "entities": [[18, 19, "MethodName", "mBART"]]}
{"text": "Similarly , the mBART model forms our primary submission for es \u2194 pt .", "entities": [[3, 4, "MethodName", "mBART"]]}
{"text": "We also list the BLEU scores for the submission that got first rank in each of the language directions .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "Since the test sets were hidden at the time of submission , we do not report our numbers on the standard Transformer architecture .", "entities": [[21, 22, "MethodName", "Transformer"]]}
{"text": "Analysis Even though Marathi and Portuguese are not present during the pre - training phase of mBART , fine - tuning on these languages provides significant boosts over SMT and standard Transformer .", "entities": [[16, 17, "MethodName", "mBART"], [31, 32, "MethodName", "Transformer"]]}
{"text": "We have participated in the Similar Language Translation task on four language directions .", "entities": [[7, 8, "TaskName", "Translation"]]}
{"text": "Our best system uses the pre - trained mBART model ( Liu et al , 2020 ) and fine - tunes on the parallel data provided for the specific translation task .", "entities": [[8, 9, "MethodName", "mBART"]]}
{"text": "by the DARPA Explainable Artificial Intelligence ( XAI ) Program with number N66001 - 17 - 2 - 4032 , Visvesvaraya Young Faculty Fellowships by Govt .", "entities": [[2, 3, "DatasetName", "DARPA"], [3, 6, "TaskName", "Explainable Artificial Intelligence"]]}
{"text": "Comparative Analysis of Neural QA models on SQuAD", "entities": [[7, 8, "DatasetName", "SQuAD"]]}
{"text": "The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language .", "entities": [[3, 5, "TaskName", "Question Answering"]]}
{"text": "Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks .", "entities": [[21, 23, "TaskName", "information retrieval"]]}
{"text": "As a first step towards achieving generalization across multiple domains , we attempt to understand and compare the peculiarities of existing end - to - end neural models on the Stanford Question Answering Dataset ( SQuAD ) by performing quantitative as well as qualitative analysis of the results attained by each of them .", "entities": [[29, 34, "DatasetName", "the Stanford Question Answering Dataset"], [35, 36, "DatasetName", "SQuAD"]]}
{"text": "Machine Reading is a task in which a model reads a piece of text and attempts to formally represent it or performs a downstream task like Question Answering ( QA ) .", "entities": [[26, 28, "TaskName", "Question Answering"]]}
{"text": "These datasets are created from different underlying sources such as web resources in MS MARCO ( Nguyen et al , 2016 ) ; trivia and web in QUASAR - S and QUASAR - T ( Dhingra et al , 2017 ) , SearchQA ( Dunn et al , 2017 ) , TriviaQA", "entities": [[13, 15, "DatasetName", "MS MARCO"], [27, 30, "DatasetName", "QUASAR - S"], [31, 34, "DatasetName", "QUASAR - T"], [42, 43, "DatasetName", "SearchQA"], [51, 52, "DatasetName", "TriviaQA"]]}
{"text": "( Joshi et al , 2017 ) ; news articles in CNN / Daily Mail ( Chen et al ) , NewsQA ( Trischler et al , 2016 ) and stories in NarrativeQA ( Ko\u010disk\u1ef3 et al , 2017 ) .", "entities": [[11, 15, "DatasetName", "CNN / Daily Mail"], [21, 22, "DatasetName", "NewsQA"], [32, 33, "DatasetName", "NarrativeQA"]]}
{"text": "Another common source is large unstructured text documents from Wikipedia such as in SQuAD ( Rajpurkar et al , 2016 ) , WikiReading ( Hewlett et al , 2016 ) and WikiHop ( Welbl et al , 2017 ) .", "entities": [[13, 14, "DatasetName", "SQuAD"], [22, 23, "DatasetName", "WikiReading"], [31, 32, "DatasetName", "WikiHop"]]}
{"text": "Evaluating and analyzing systems on QA tasks can lead to insights for advancements in machine reading and natural language understanding , and Pe\u00f1as et al ( 2011 ) have also previously worked on this .", "entities": [[17, 20, "TaskName", "natural language understanding"]]}
{"text": "One of the first large MRC datasets ( over 100k QA pairs ) is the Stanford Question Answering Dataset ( SQuAD ) ( Rajpurkar et al , 2016 ) .", "entities": [[14, 19, "DatasetName", "the Stanford Question Answering Dataset"], [20, 21, "DatasetName", "SQuAD"]]}
{"text": "Our main focus in this work is to perform comparative subjective and empirical analysis of errors in answer predictions by four top performing models on the SQuAD leaderboard 1 .", "entities": [[26, 27, "DatasetName", "SQuAD"]]}
{"text": "( Chen et al , 2017 ) , Multi - Paragraph Reading Comprehension ( DocQA ) ( Clark and Gardner , 2017 ) , and the Logistic Regression baseline model ( Rajpurkar et al , 2016 )", "entities": [[11, 13, "TaskName", "Reading Comprehension"], [26, 28, "MethodName", "Logistic Regression"]]}
{"text": "While we limit ourselves to in - domain analysis of the performance of these models on SQuAD in this paper , similar principles can be used to extend this work to study biases of combinations of different models on different datasets and thereby understand the generalization capabilities of these neural architectures .", "entities": [[16, 17, "DatasetName", "SQuAD"]]}
{"text": "Many versions of this model ( with different types of input features ) exist on the SQuAD leaderboard , but the basic architecture 2 ( which we use for our experiments in this paper ) contains character , word and phrase embedding layers , followed by an attention flow layer , a modeling layer and an output layer .", "entities": [[16, 17, "DatasetName", "SQuAD"]]}
{"text": "Chen et al ( 2017 ) , focuses on answering open - domain factoid questions using Wikipedia , but also performs well on SQuAD ( skipping the document retrieval stage ) .", "entities": [[23, 24, "DatasetName", "SQuAD"]]}
{"text": "The paragraph encoding is computed by representing each context as a sequence of feature vectors derived from tokens : word embedding , exact match with question word , POS / NER / TF and aligned question embedding , and passing these as inputs to a recurrent neural network .", "entities": [[22, 24, "MetricName", "exact match"], [30, 31, "TaskName", "NER"]]}
{"text": "The question encoding is obtained by using word embeddings as inputs to a recurrent neural network .", "entities": [[7, 9, "TaskName", "word embeddings"]]}
{"text": "Multi - Paragraph Reading Comprehension ( DocQA ) :", "entities": [[3, 5, "TaskName", "Reading Comprehension"]]}
{"text": "This model , proposed by Clark and Gardner ( 2017 ) , aims to answer questions based on entire documents ( multiple paras ) rather than specific paragraphs , but also gives good results for SQuAD ( considering the given paragraph as the document ) .", "entities": [[35, 36, "DatasetName", "SQuAD"]]}
{"text": "The implementation 5 contains input , embedding ( character and word - level ) , pre - processing ( shared bidirectional GRU between question and passage ) , attention ( similar to BiDAF ) , self - attention ( residual ) and output ( bidirectional GRU and linear scoring ) layers .", "entities": [[20, 22, "MethodName", "bidirectional GRU"], [44, 46, "MethodName", "bidirectional GRU"]]}
{"text": "This model was proposed as a baseline in the SQuAD dataset paper ( Rajpurkar et al , 2016 ) and uses features based on n - gram frequencies , lengths , part - of - speech tags , constituency and dependency parse trees of questions and passages as inputs to a logistic regression classifier 6 to predict whether each constituent span is an answer or not .", "entities": [[9, 10, "DatasetName", "SQuAD"], [31, 34, "DatasetName", "part - of"], [51, 53, "MethodName", "logistic regression"]]}
{"text": "We trained the aforementioned end - to - end neural models and compare their performance on the SQuAD development set which contains 10 , 570 question - answer pairs based on Wikipedia articles .", "entities": [[17, 18, "DatasetName", "SQuAD"]]}
{"text": "The span - level performance is measured typically by Exact Match ( EM ) and F1 metrics which are reported with respect to the ground truth answer spans .", "entities": [[9, 11, "MetricName", "Exact Match"], [12, 13, "MetricName", "EM"], [15, 16, "MetricName", "F1"]]}
{"text": "Since the default sentence tokenizer for English in NLTK is pre - trained on Penn Treebank data which contains formal language ( news articles ) , we expect it to perform reasonably well on Wikipedia articles too .", "entities": [[14, 16, "DatasetName", "Penn Treebank"]]}
{"text": "We observe that all the models have high sentencelevel accuracy , with DocQA outperforming the other models with respect to this metric as well .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "Interestingly , DrQA performs better on sentence retrieval accuracy than both BiDAF and R - Net , but has a worse span - level exact match score , which is probably because of the rich feature vector representation of the passage due to the model 's focus on open domain QA ( and hence retrieval ) .", "entities": [[8, 9, "MetricName", "accuracy"], [24, 26, "MetricName", "exact match"]]}
{"text": "But , none of these neural models have near - perfect ability to identify the correct sentence , and \u223c90 % accuracy indicates that even if we have a perfect answer selection method , this is the best EM score we can achieve .", "entities": [[21, 22, "MetricName", "accuracy"], [30, 32, "TaskName", "answer selection"], [38, 39, "MetricName", "EM"]]}
{"text": "However , incorrect span identification contributes more to errors in prediction for all the models , as seen from the disparity between the sentence - level accuracies and the final spanlevel exact match score values .", "entities": [[31, 33, "MetricName", "exact match"]]}
{"text": "After determining an approximately optimal set of models which such an ensemble should be composed of , each of these models can be trained independently followed by multi - label classification ( to select one of the generated answers ) using techniques like logistic regression , a feed - forward neural network or a recurrent or convolutional neural network with input features based on the question , the passage and their token overlap .", "entities": [[27, 31, "TaskName", "multi - label classification"], [43, 45, "MethodName", "logistic regression"]]}
{"text": "Paraphrasing is the most frequent error category observed for DrQA , which makes sense if we con - sider the features used to represent each passage , such as exact match with a question word , which depend on lexical overlap between the question and passage .", "entities": [[29, 31, "MetricName", "exact match"]]}
{"text": "Also , the high number of Soft Correct outputs across all models points to some deficiencies in the SQuAD annotations , which might limit the reliability of the performance evaluation metrics .", "entities": [[18, 19, "DatasetName", "SQuAD"]]}
{"text": "In this work , we analyze - both quantitatively and qualitatively - results generated by 4 end - to - end neural models on the Stanford Question Answering Dataset .", "entities": [[24, 29, "DatasetName", "the Stanford Question Answering Dataset"]]}
{"text": "Even though the scope of this paper is restricted to SQuAD , similar analysis can be done for any datasets / models / features , to gain a better understanding and enable a better assessment of stateof - the - art in neural machine reading .", "entities": [[10, 11, "DatasetName", "SQuAD"]]}
{"text": "To this end , we also performed some preliminary experiments on TriviaQA so as to analyze the difference between the properties of the two datasets , but were unable to replicate the published results owing to pre - processing / hyperparameters .", "entities": [[11, 12, "DatasetName", "TriviaQA"]]}
{"text": "Minimally - Augmented Grammatical Error Correction", "entities": [[3, 6, "TaskName", "Grammatical Error Correction"]]}
{"text": "There has been an increased interest in lowresource approaches to automatic grammatical error correction .", "entities": [[11, 14, "TaskName", "grammatical error correction"]]}
{"text": "We introduce Minimally - Augmented Grammatical Error Correction ( MAGEC ) that does not require any errorlabelled data .", "entities": [[5, 8, "TaskName", "Grammatical Error Correction"]]}
{"text": "Most neural approaches to automatic grammatical error correction ( GEC ) require error - labelled training data to achieve their best performance .", "entities": [[5, 8, "TaskName", "grammatical error correction"]]}
{"text": "We present Minimally - Augmented Grammatical Error Correction ( MAGEC ) , a simple but effective approach to unsupervised and low - resource GEC which does not require any authentic error - labelled training data .", "entities": [[5, 8, "TaskName", "Grammatical Error Correction"]]}
{"text": "In comparison to pretraining with BERT ( Devlin et al , 2019 ) , synthetic errors provide more task - specific training examples than masking .", "entities": [[5, 6, "MethodName", "BERT"]]}
{"text": "Word embeddings Confusion sets contain the most similar words to the confused word based on the cosine similarity of their word embedding vectors ( Mikolov et al , 2013 ) .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "A large amount of training examples increases the chance that synthetic errors resemble real error patterns and results in better language modelling properties .", "entities": [[20, 22, "TaskName", "language modelling"]]}
{"text": "During training , we limit the vocabulary size to 32 , 000 subwords computed with SentencePiece using the unigram method ( Kudo and Richardson , 2018 ) .", "entities": [[15, 16, "MethodName", "SentencePiece"]]}
{"text": "Word embeddings are computed with word2vec 7 from monolingual data .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}
{"text": "Training settings We adapt the recent state - ofthe - art GEC system by Junczys - Dowmunt et al ( 2018b ) , an ensemble of sequence - to - sequence Transformer models ( Vaswani et al , 2017 ) and a neural language model .", "entities": [[31, 32, "MethodName", "Transformer"]]}
{"text": "We increase the size of language model used for ensembling to match the Transformer - big configuration ( Vaswani et al , 2017 ) with 16 - head self - attention , embeddings size of 1024 and feed - forward filter size size of 4096 .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "Confusion sets On English data , all proposed confusion set generation methods perform better than random word substitution ( Table 3 ) .Confusion sets based on word embeddings are the least effective , while spell - broken sets perform best at 26.66 F 0.5 .", "entities": [[26, 28, "TaskName", "word embeddings"]]}
{"text": "Our best unsupervised ensemble systems that combine three Transformer models and a LM 12 outperform the state - of - the - art results for these languages by", "entities": [[8, 9, "MethodName", "Transformer"]]}
{"text": "MAGEC systems are generally on par with the results achieved by a recent unsupervised contribution based on finite state transducers by Stahlberg et al ( 2019 ) on the CoNLL - 2014 ( Dahlmeier et al , 2013 ) and JFLEG test sets ( Napoles et al , 2017 All unsupervised systems benefit from domainadaptation via fine - tuning on authentic labelled data ( Miceli Barone et al , 2017 ) .", "entities": [[40, 41, "DatasetName", "JFLEG"]]}
{"text": "To counter the argument that - mostly due to the introduced character - level noise and strong language modelling - MAGEC can only correct these \" simple \" errors , we evaluate it against test sets that contain either spelling and punctuation errors or all other error types ; with the complement errors corrected ( Table 6 ) .", "entities": [[17, 19, "TaskName", "language modelling"]]}
{"text": "We have presented Minimally - Augmented Grammatical Error Correction ( MAGEC ) , which can be effectively used in both unsupervised and lowresource scenarios .", "entities": [[6, 9, "TaskName", "Grammatical Error Correction"]]}
{"text": "Incorporating Priors with Feature Attribution on Text Classification", "entities": [[6, 8, "TaskName", "Text Classification"]]}
{"text": "Fairness in ML models rightfully came under heavy scrutiny in recent years ( Zhang et al , 2018a ; Dixon et al , 2018 ; Angwin et al , 2016 ) .", "entities": [[0, 1, "TaskName", "Fairness"]]}
{"text": "Some examples include sentiment analysis models weighing negatively for inputs containing identity terms such as \" jew \" and \" black \" , and hate speech classifiers leaning to predict any sentence containing \" islam \" as toxic ( Waseem and Hovy , 2016 ) .", "entities": [[3, 5, "TaskName", "sentiment analysis"], [24, 26, "DatasetName", "hate speech"]]}
{"text": "We demonstrate our approach on two problems in text classification settings : ( 1 ) model biases towards protected identity groups ; ( 2 ) low classification performance due to lack of data .", "entities": [[8, 10, "TaskName", "text classification"]]}
{"text": "Our fairness experiments show that the classifiers trained with our method achieve the same performance , if not better , on the original task , while improving AUC and fairness metrics on a synthetic , unbiased dataset .", "entities": [[27, 28, "MetricName", "AUC"]]}
{"text": "Given a function f : R n [ 0 , 1 ] that represents a model , and an input x = ( x 1 , ... , x n ) R n .", "entities": [[8, 9, "DatasetName", "0"]]}
{"text": "Despite the complexity of these models , prior work has been able to extract attributions with gradient based methods ( Smilkov et al , 2017 ) , Shapley values from game theory ( SHAP ) ( Lundberg and Lee , 2017 ) , or other similar methods ( Bach et al , 2015 ; Shrikumar et al , 2017 ) .", "entities": [[33, 34, "MethodName", "SHAP"]]}
{"text": "Some of these attributions methods , for example Path Intergrated Gradients and SHAP , not only follow Definition 2.1 , but also satisfy axioms or properties that resemble linear models .", "entities": [[12, 13, "MethodName", "SHAP"]]}
{"text": "Traditionally , model deficiencies were addressed by providing priors through extensive feature engineering and collecting more data .", "entities": [[11, 13, "TaskName", "feature engineering"]]}
{"text": "The prior loss for a scalar output is defined as : L prior ( a , t )", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "The proposed objective is not dataset - dependent and is applicable to different problem settings such as sentiment classification , abuse detection , etc .", "entities": [[20, 22, "TaskName", "abuse detection"]]}
{"text": "We illustrate the effectiveness of our method by applying it to a toxic comment classification problem .", "entities": [[12, 15, "TaskName", "toxic comment classification"]]}
{"text": "First , we tackle the problem of unintended bias in toxic comment classification ( Dixon et al , 2018 ) with our proposed method .", "entities": [[10, 13, "TaskName", "toxic comment classification"]]}
{"text": "The network contains a convolution layer with 128 2 - , 3 - , 4 - gram filters for a sequence length of 100 followed by a max - pooling layer and softmax function .", "entities": [[4, 5, "MethodName", "convolution"], [32, 33, "MethodName", "softmax"]]}
{"text": "For fairness experiments , we set k to be 0", "entities": [[9, 10, "DatasetName", "0"]]}
{"text": "Lastly , training with joint loss reaches its best performance in later epochs than training with crossentropy loss .", "entities": [[5, 6, "MetricName", "loss"], [17, 18, "MetricName", "loss"]]}
{"text": "When taking the derivative with respect to the loss , we treat the interpolated embeddings as constants .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "Thus , the prior loss does not back - propagate to the embedding parameters .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "The implementation decision does not imply that prior loss has no effect on the word embeddings , though .", "entities": [[8, 9, "MetricName", "loss"], [14, 16, "TaskName", "word embeddings"]]}
{"text": "Therefore , the word embeddings had to adjust accordingly to the new model parameters by updating the embedding parameters with cross - entropy loss .", "entities": [[3, 5, "TaskName", "word embeddings"], [23, 24, "MetricName", "loss"]]}
{"text": "We also explore a different training schedule for cases where a model has been trained to optimize for a classification loss : Finetuned : An already - trained classifier is finetuned with joint loss for several epochs .", "entities": [[20, 21, "MetricName", "loss"], [33, 34, "MetricName", "loss"]]}
{"text": "Our method provides substantial improvement on AUC and almost completely eliminates false positive and false negative inequality across identities .", "entities": [[6, 7, "MetricName", "AUC"]]}
{"text": "Although they are not as high quality as one would expect general - purpose word embeddings to be possibly due to data size and the model having a different objective , the results show that our method yields inherently unbiased embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}
{"text": "It removes the necessity to initialize word embeddings with pre - debiased embeddings as proposed in Bolukbasi et al ( 2016 ) .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Park et al ( 2018 ) also try to address gender bias for abusive language detection models by debiasing word vectors , augmenting more data and changing model architecture .", "entities": [[13, 15, "TaskName", "abusive language"]]}
{"text": "Also , both methods trade original task accuracy for fairness , while our method does not .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "We apply this technique to model fairness in toxic comment classification .", "entities": [[8, 11, "TaskName", "toxic comment classification"]]}
{"text": "Our experiments indicate that the models trained jointly with cross - entropy and prior loss do not suffer a performance drop on the original task , while achieving a better performance in fairness metrics on the template - based dataset .", "entities": [[14, 15, "MetricName", "loss"]]}
