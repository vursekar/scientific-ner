{"text": "In this paper , we present an improved graph - based translation model which segments an input graph into node - induced subgraphs by taking source context into consideration .", "entities": []}
{"text": "Translations are generated by combining subgraph translations leftto - right using beam search .", "entities": []}
{"text": "Experiments on Chinese - English and German - English demonstrate that the context - aware segmentation significantly improves the baseline graph - based model .", "entities": []}
{"text": "The well - known phrase - based statistical translation model ( Koehn et al , 2003 ) extends the basic translation units from single words to continuous phrases to capture local phenomena .", "entities": []}
{"text": "However , one of its significant weaknesses is that it can not learn generalizations Galley and Manning , 2010 ) .", "entities": []}
{"text": "To allow discontinuous phrases ( any subset of words of an input sentence ) , dependency treelets Xiong et al , 2007 ) can be used , which are connected subgraphs on trees .", "entities": []}
{"text": "However , continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance ( Koehn et al , 2003 ; Hanneman and Lavie , 2009 ) .", "entities": []}
{"text": "To make use of the merits of both phrase - based models and treelet - based models , Li et al ( 2016 ) proposed a graph - based translation model as in Equation ( 1 ) :", "entities": []}
{"text": "p ( t I 1 |", "entities": []}
{"text": "g I 1 )", "entities": []}
{"text": "=", "entities": []}
{"text": "I i=1", "entities": []}
{"text": "p ( t i | g a i )", "entities": []}
{"text": "\u00d7 d ( g a", "entities": []}
{"text": "i , g a i\u22121 )", "entities": []}
{"text": "( 1 ) where t i is a continuous target phrase which is the translation of a node - induced and connected source subgraph g a i .", "entities": []}
{"text": "1 d is a distance - based reordering function which penalizes discontinuous phrases that have relatively long gaps ( Galley and Manning , 2010 ) .", "entities": []}
{"text": "The model translates an input graph by segmenting it into subgraphs and generates a complete translation by combining subgraph translations left - to - right .", "entities": []}
{"text": "However , the model treats different graph segmentations equally .", "entities": []}
{"text": "Therefore , in this paper we propose a contextaware graph segmentation ( Section 2 ) : ( i ) we add contextual information to each translation rule during training ( Section 2.2 ) ; ( ii ) during decoding , when a rule is applied , the input context should match with the rule context ( Section 2.3 ) .", "entities": []}
{"text": "Experiments ( Section 3 ) on Chinese - English ( ZH - EN ) and German - English ( DE - EN ) tasks show that our method significantly improves the graphbased model .", "entities": []}
{"text": "As observed in our experiments , the context - aware segmentation brings two benefits to our system : ( i ) it helps to select a better subgraph to translate ; and ( ii ) it selects a better target phrase for a subgraph .", "entities": []}
{"text": "Our model extends the graph - based translation model by considering source context during segmenting input graphs , as in Equation ( 2 ) : p ( t I", "entities": []}
{"text": "1 |", "entities": []}
{"text": "g I 1 )", "entities": []}
{"text": "=", "entities": []}
{"text": "I i=1", "entities": []}
{"text": "p ( t i | g", "entities": []}
{"text": "a i , c a i )", "entities": []}
{"text": "\u00d7 d ( g a", "entities": []}
{"text": "i , g a i\u22121 ) ( 2 )", "entities": []}
{"text": "where c a", "entities": []}
{"text": "i denotes the context of the subgraph g a i , which is represented as a set of connections ( i.e. edges ) between g a i and [ g a i+1 , , g a I ] .", "entities": []}
{"text": "2010Nian FIFA Shijiebei Zai Nanfei Chenggong Juxing", "entities": []}
{"text": "The graph used in this paper combines a sequence and a dependency tree as in Li et al ( 2016 ) .", "entities": []}
{"text": "Each graph contains two kinds of links : dependency links from dependency trees which model syntactic and semantic relations between words , and bigram links which provide local and sequential information on pairs of continuous words .", "entities": []}
{"text": "Figure 1 shows an example graph .", "entities": []}
{"text": "Given such graphs , we can make use of both continuous and linguistically informed discontinuous phrases as long as they are connected on graphs .", "entities": []}
{"text": "In this paper , we do not distinguish the two kinds of relations , because our preliminary experiments showed no improvement when considering edge types .", "entities": []}
{"text": "During training , given a word - aligned graphstring pair g , t , a , we extract translation rules g a i , c a", "entities": []}
{"text": "i , t i , each of which consists of a continuous target phrase t i , a source subgraph g", "entities": []}
{"text": "a", "entities": []}
{"text": "i aligned to t i , and a source context c", "entities": []}
{"text": "a", "entities": []}
{"text": "i .", "entities": []}
{"text": "We first find initial pairs .", "entities": []}
{"text": "s", "entities": []}
{"text": "a i , t", "entities": []}
{"text": "s a j is a set of source words which are aligned to t i .", "entities": []}
{"text": "Then , the set of rules satisfies the following : 1 .", "entities": []}
{"text": "If s a i , t i is an initial pair ands a i is covered by a subgraph g a i which is connected , then g", "entities": []}
{"text": "a i , * , t", "entities": []}
{"text": "i is a basic rule .", "entities": []}
{"text": "c a i", "entities": []}
{"text": "= * means that a basic rule is applied without considering context to make sure that at least one translation is produced for any inputs during decoding .", "entities": []}
{"text": "Therefore , basic rules are the same as rules in the conventional graph - based model .", "entities": []}
{"text": "Rule ( 3 ) shows an example of a basic rule : 2010Nian FIFA Shijiebei 2010 FIFA World Cup ( 3 ) 2 . Assume g a i , * , t i is a basic rule and s a i+1 ,", "entities": []}
{"text": "t i+1 is an initial pair where t i+1 is on the right of and adjacent to t i .", "entities": []}
{"text": "If there are edges between g a i ands a i+1 , then g", "entities": []}
{"text": "a i , c a", "entities": []}
{"text": "i , t", "entities": []}
{"text": "i is a segmenting rule , where c a i is the set of edges between g a i ands a i+1 by treatings a i+1 as a single node x. Rule ( 4 ) is an example of a segmenting rule : 2010Nian FIFA x 2010 FIFA ( 4 ) where dashed links are contextual connections .", "entities": []}
{"text": "During decoding , when the context matches , rule ( 4 ) translates a subgraph over 2010Nian FIFA into a target phrase 2010 FIFA .", "entities": []}
{"text": "For example , it can be applied to graph ( 5 ) where Shijiebei Zai Nanfei ( in the dashed rectangle ) is treated as x : 2010Nian FIFA Shijiebei Zai Nanfei ( 5 )", "entities": []}
{"text": "3 .", "entities": []}
{"text": "If there are no edges between g a i ands a i+1 , then c a", "entities": []}
{"text": "i is equal to and g a", "entities": []}
{"text": "i , , t i is a translation rule , called a selecting rule in this paper .", "entities": []}
{"text": "During decoding , the untranslated input could be a set of subgraphs which are disjoint with each other .", "entities": []}
{"text": "A selecting rule is used to select one of them .", "entities": []}
{"text": "For example , rule ( 6 ) can be applied to ( 7 ) to translate 2010Nian FIFA to 2010 FIFA .", "entities": []}
{"text": "In this example , the x in rule ( 6 ) matches with Chenggong Juxing ( in the dashed rectangle ) in ( 7 ) .", "entities": []}
{"text": "By comparing these three types of rules , we observe that both segmenting rules and selecting rules are based on basic rules .", "entities": []}
{"text": "They extend basic rules by adding contextual information to their source subgraphs so that basic rules are split into different groups according to the context .", "entities": []}
{"text": "During decoding , the context will help to select target phrases as well .", "entities": []}
{"text": "Algorithm 1 illustrates a simple process for rule extraction .", "entities": []}
{"text": "Given a word - aligned graph - string pair , we first extract all initial pairs ( Line 1 ) .", "entities": []}
{"text": "Then , we find basic rules from these pairs ( Lines 3 - 4 ) .", "entities": []}
{"text": "Basic Algorithm 1 : An algorithm for extracting translation rules from a graph - string pair .", "entities": []}
{"text": "Data : Word - aligned graph - string pair g , t , a Result : A set of translation rules R 1 find a set of initial pairs P ; 2 for each p = s a", "entities": []}
{"text": "i , t i in P do 3 if s j i is connected then // basic rules", "entities": []}
{"text": "Following Li et al ( 2016 ) , we define our model in the well - known log - linear framework ( Och and Ney , 2002 ) .", "entities": []}
{"text": "In our experiments , we use the following standard features : two translation probabilities p ( g , c | t ) and p ( t | g , c ) , two lexical translation probabilities p lex ( g , c | t ) and p lex ( t | g , c ) , a language model p ( t ) , a rule penalty , a word penalty , and a distortion function as defined in Galley and Manning ( 2010 ) .", "entities": []}
{"text": "In addition , we add one more feature into our system : a basic - rule penalty to distinguish basic rules from segmenting and selecting rules .", "entities": []}
{"text": "Our decoder is very similar to the one in the conventional graph - based model , which generates hypotheses left - to - right using beam search .", "entities": []}
{"text": "A hypothesis can be extended on the right by translating an uncovered source subgraph .", "entities": []}
{"text": "The translation process ends when all source words have been translated .", "entities": []}
{"text": "However , when extending a hypothesis , our decoder considers the context of the translated subgraph , i.e. edges connecting it with the remaining untranslated source words .", "entities": []}
{"text": "Figure 2 shows a derivation which translates an input graph in Chinese to an English string .", "entities": []}
{"text": "In this example , both rules r 1 and r 2 are segmenting rules .", "entities": []}
{"text": "We conduct experiments on ZH - EN and DE - EN corpora .", "entities": []}
{"text": "Bold figures mean GBMT ctx is significantly better than GBMT at p \u2264 0.01 .", "entities": []}
{"text": "* means a system is significantly better than PBMT at p \u2264 0.01 .", "entities": []}
{"text": "+ means a system is significantly better than TBMT at p \u2264 0.01 .", "entities": []}
{"text": "Following Li et al ( 2016 ) , Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges .", "entities": []}
{"text": "We use SRILM ( Stolcke , 2002 ) to train a 5 - gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser - Ney discounting ( Chen and Goodman , 1996 ) .", "entities": []}
{"text": "Batch MIRA ( Cherry and Foster , 2012 ) is used to tune feature weights .", "entities": []}
{"text": "( Papineni et al , 2002 ) scores averaged on three runs of MIRA ( Clark et al , 2011 ) .", "entities": []}
{"text": "We compare our system GBMT ctx with several other systems .", "entities": []}
{"text": "A system PBMT is built using the phrase - based model in Moses ( Koehn et al , 2007 ) .", "entities": []}
{"text": "GBMT is the graph - based translation system described in Li et al ( 2016 ) .", "entities": []}
{"text": "To examine the influence of bigram links , GBMT is also used to translate dependency trees where treelets Xiong", "entities": []}
{"text": "et al , 2007 ) are the basic translation units .", "entities": []}
{"text": "Accordingly , we name the system TBMT .", "entities": []}
{"text": "All systems are implemented in Moses .", "entities": []}
{"text": "We found that GBMT ctx is better than PBMT across all test sets .", "entities": []}
{"text": "This improvement is reasonable as our system allows discontinuous phrases which can reduce data sparsity and handle longdistance relations ( Galley and Manning , 2010 ) .", "entities": []}
{"text": "This suggests that continuous phrases connected by bigram links are essential to system performance since they help to improve phrase coverage ( Hanneman and Lavie , 2009 ) .", "entities": []}
{"text": "The main reason for the improvement is that context helps to select proper subgraphs and target phrases .", "entities": []}
{"text": "Figure 3 shows example translations .", "entities": []}
{"text": "We found that in Figure 3a , after translating a parenthesis , GBMT ctx correctly selects a subgraph Gang Ao Tai and generates a target phrase hong kong , macao and taiwan .", "entities": []}
{"text": "In Figure 3b , both GBMT and GBMT ctx choose to translate the subgraph WoMen Ye ZhiLi .", "entities": []}
{"text": "However , given the context of the subgraph , GBMT ctx selects a correct target phrase we are also committed to for it .", "entities": []}
{"text": "While basic rules exist in both systems , segmenting and selecting rules make GBMT ctx context - aware .", "entities": []}
{"text": "Table 2 shows the number of rules in GBMT ctx according to their types .", "entities": []}
{"text": "We found that on both language pairs 35 % - 36 % of rules are basic rules .", "entities": []}
{"text": "While the proportion of segmenting rules is \u223c53 % , selecting rules only account for 11 % - 12 % .", "entities": []}
{"text": "This is because segmenting rules contain richer contextual information than selecting rules .", "entities": []}
{"text": "Note that when only basic rules are allowed , our system degrades to the conventional GBMT system .", "entities": []}
{"text": "The results in Table 3 suggest that both segmenting and selecting rules consistently improve GBMT on both language pairs .", "entities": []}
{"text": "However , segmenting rules are more useful than selecting rules .", "entities": []}
{"text": "This is reasonable since ( hong kong macao taiwan )", "entities": []}
{"text": "hong kong spring festival retail business rise 10 % ( Gang Ao Tai )", "entities": []}
{"text": "XiangGang XinChun LingShou ShengYi ShangSheng YiCheng Ref : GBMT : GBMTctx : ( hong kong , macao and taiwan )", "entities": []}
{"text": "hong kong 's retail sales up 10 % during spring festival ( the spring festival ) hong kong retail business in hong kong , macao and taiwan rose by 10 % ( hong kong , macao and taiwan )", "entities": []}
{"text": "hong kong spring retail business will increase by 10 % ( a ) subgraph selection we also dedicate protect and improve living emvironment .", "entities": []}
{"text": "ZhiLi BaoHu He GaiShan JuZhu HuanJing .", "entities": []}
{"text": "Ref : GBMT : GBMTctx : we are also committed to protect and improve our living environment .", "entities": []}
{"text": "we have worked hard to protect and improve the living environment .", "entities": []}
{"text": "we are also committed to protect and improve the living environment .", "entities": []}
{"text": "( b ) target - phrase selection Bold figures mean a system is significantly better than the one only using basic rules at p \u2264 0.01 .", "entities": []}
{"text": "the number of segmenting rules is much larger than the number of selecting rules .", "entities": []}
{"text": "We further observed that , while our system achieves the best performance when all rules are used on ZH - EN , the combination of basic rules and segmenting rules on DE - EN results in the best system .", "entities": []}
{"text": "This is probably because reordering ( including long - distance reordering ) is performed less often in DE - EN than in ZH - EN ( Li et al , 2016 ) which makes selecting rules less preferable on DE - EN .", "entities": []}
{"text": "In this paper , we present a graph - based model which takes subgraphs as the basic translation units and considers source context during segmenting graphs into subgraphs .", "entities": []}
{"text": "Experiments on Chinese - English and German - English show that our model is significantly better than the conventional graphbased model which equally treats different graph segmentations .", "entities": []}
{"text": "In this paper , source context is used as hard constraints during decoding .", "entities": []}
{"text": "In future , we would like to try soft constraints .", "entities": []}
{"text": "In addition , it would also be interesting to extend this model using a synchronous graph grammar .", "entities": []}
{"text": "This research has received funding from the European Union 's Horizon 2020 research and innovation programme under grant agreement n o 645452 ( QT21 ) .", "entities": []}
{"text": "The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme ( Grant 13 / RC/2106 ) and is cofunded under the European Regional Development Fund .", "entities": []}
{"text": "The authors thank all anonymous reviewers for their insightful comments and suggestions .", "entities": []}
{"text": "Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers .", "entities": []}
{"text": "For this purpose , we build a three - way multiclassifier , which decides if an answer supports , refutes , or is neutral with respect to another one .", "entities": []}
{"text": "More specifically , our neural architecture integrates a state - of - the - art AS2 module with the multi - classifier , and a joint layer connecting all components .", "entities": []}
{"text": "The results show that our models obtain the new state of the art in AS2 .", "entities": []}
{"text": "*", "entities": []}
{"text": "Work done while the author was an intern at Amazon Alexa Claim : Joe Walsh was inducted in 2001 .", "entities": []}
{"text": "Ev1 : As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the Vocal Group Hall of Fame in 2001 .", "entities": []}
{"text": "Ev2 : Joseph Fidler Walsh ( born November 20 , 1947 ) is an American singer songwriter , composer , multiinstrumentalist and record producer .", "entities": []}
{"text": "Walsh was awarded with the Vocal Group Hall of Fame in 2001 .", "entities": []}
{"text": "However , TANDA was applied only to pointwise rerankers ( PR ) , e.g. , simple binary classifiers .", "entities": []}
{"text": "Bonadiman ( 2020 ) tried to improve this model by jointly modeling all answer candidates with listwise methods , e.g. , ( Bian et al , 2017 ) .", "entities": []}
{"text": "Such systems take a claim , e.g. , Joe Walsh was inducted in 2001 , as input ( see Tab . 1 ) , and verify if it is valid , using related sentences called evidences ( typically retrieved by a search engine ) .", "entities": []}
{"text": "For example , Ev 1 , As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the Vocal Group Hall of Fame in 2001 , and Ev 3 , Walsh was awarded with the Vocal Group Hall of Fame in 2001 , support the veracity of the claim .", "entities": []}
{"text": "In contrast , Ev 2 is neutral as it describes who Joe Walsh is but does not contribute to establish the induction .", "entities": []}
{"text": "We conjecture that supporting evidence for answer correctness in AS2 task can be modeled with a similar rationale .", "entities": []}
{"text": "We defined a claim as a pair constituted of the question and one target answer , while considering all the other answers as evidences .", "entities": []}
{"text": "We re - trained and rebuilt all its embeddings for the AS2 task .", "entities": []}
{"text": "Our second method , Answer Support - based Reranker ( ASR ) , is completely new , it is based on the representation of the pair , ( q , t ) , generated by state - of - the - art AS2 models , concatenated with the representation of all the pairs ( t , c i ) .", "entities": []}
{"text": "The latter summarizes the contribution of each c i to t using a maxpooling operation .", "entities": []}
{"text": "c i can be unrelated to ( q , t ) since the candidates are automatically retrieved , thus it may introduce just noise .", "entities": []}
{"text": "To mitigate this problem , we use an Answer Support Classifier ( ASC ) to learn the relatedness between t and c i by classifying their embedding , which we obtain by applying a transformer network to their concatenated text .", "entities": []}
{"text": "ASC tunes the ( t , c i ) embedding parameters according to the evidence that c i provides to t. Our Answer Support - based Reranker ( ASR ) significantly improves the state of the art , and is also simpler than our approach based on KGAT .", "entities": []}
{"text": "Our third method is an extension of ASR .", "entities": []}
{"text": "It should be noted that , although ASR exploits the information from the k candidates , it still produces a score for a target t without knowing the scores produced for the other target answers .", "entities": []}
{"text": "Thus , we jointly model the representation obtained for each target in a multi - ASR ( MASR ) architecture , which can then carry out a complete global reasoning over all target answers .", "entities": []}
{"text": "We also obtain a relative improvement of \u223c3 % over TANDA on WQA , confirming that ASR is a general solution to design accurate QA systems .", "entities": []}
{"text": "Most interestingly , MASR improves ASR by additional 2 % , confirming the benefit of joint modeling .", "entities": []}
{"text": "We consider retrieval - based QA systems , which are mainly constituted by ( i ) a search engine , retrieving documents related to the questions ; and ( ii ) an AS2 model , which reranks passages / sentences extracted from the documents .", "entities": []}
{"text": "The top sentence is typically used as final answer for the users .", "entities": []}
{"text": "The task of reranking answer - sentence candidates provided by a retrieval engine can be modeled with a classifier scoring the candidates .", "entities": []}
{"text": "Let q be an element of the question set , Q , and A = { c 1 , . . .", "entities": []}
{"text": ", c n } be a set of candidates for q , a reranker can be defined as R : Q \u00d7 \u03a0 ( A ) \u03a0 ( A ) , where \u03a0 ( A ) is the set of all permutations of A. Previous work targeting ranking problems in the text domain has classified reranking functions into three buckets : pointwise , pairwise , and listwise methods .", "entities": []}
{"text": "Pointwise reranking : This approach learns p ( q , c i ) , which is the probability of c i correctly answering q , using a standard binary classification setting .", "entities": []}
{"text": "The final rank is simply obtained sorting c i , based on p ( q , c i ) .", "entities": []}
{"text": "QA . Pairwise reranking : The method considers binary classifiers of the form \u03c7 ( q , c i , c j ) for determining the partial rank between c i and c j , then the scoring function p ( q , c i ) is obtained by summing up all the contributions with respect to the target candidate t =", "entities": []}
{"text": "c i , e.g. , p ( q , c i )", "entities": []}
{"text": "= j \u03c7 ( q , c i , c j ) .", "entities": []}
{"text": "al , 2016 )", "entities": []}
{"text": ".", "entities": []}
{"text": "However , these methods are largely outperformed by the pointwise TANDA model .", "entities": []}
{"text": "Listwise reranking : This approach , e.g. , ( Bian et al , 2017 ; Cao et al , 2007 ; Ai et al , 2018 ) , aims at learning p ( q , \u03c0 ) , \u03c0 \u03a0 ( A ) , using the information on the entire set of candidates .", "entities": []}
{"text": "The closest work to our research is by Bonadiman and Moschitti ( 2020 ) , who designed several joint models .", "entities": []}
{"text": "Its application to retrieval scenario has also been studied Hu et al , 2019 ; Kratzwald and Feuerriegel , 2018 ) .", "entities": []}
{"text": "However , the large volume of retrieved content makes their use not practical yet .", "entities": []}
{"text": "However , they do not exploit transformer models , thus their results are rather below the state of the art .", "entities": []}
{"text": "In contrast with the work above , our modeling is driven by an answer support strategy , where the pieces of information are taken from different documents .", "entities": []}
{"text": "The problem is , therefore , becoming increasingly important in NLP context ( Mihaylova et al , 2018 ) .", "entities": []}
{"text": "In QA , answer verification is directly relevant due to its nature of content delivery ( Mihaylova et al , 2019 ) .", "entities": []}
{"text": "Zhang et al ( 2020a ) also proposed to fact check for product questions using additional associated evidence sentences .", "entities": []}
{"text": "While the process is technically sound , the retrieval of evidence is an expensive process , which is prohibitive to scale in production .", "entities": []}
{"text": "We instead address this problem by leveraging the top answer candidates .", "entities": []}
{"text": "In this section , we describe our baseline models , which are constituted by pointwise , pairwise , and listwise strategies .", "entities": []}
{"text": "Specifically , q = Tok q 1 , ... , Tok , inserted at the beginning , as separator , and at the end , respectively .", "entities": []}
{"text": "This input is encoded as three embeddings based on tokens , segments and their positions , which are fed as input to several layers ( up to 24 ) .", "entities": []}
{"text": "The result of this transformation is an embedding , E , representing ( q , c ) , which models the dependencies between words and segments of the two sentences .", "entities": []}
{"text": "For the downstream task , E is fed ( after applying a non - linearity function ) to a fully connected layer having weights : W and B.", "entities": []}
{"text": "The output layer can be used to implement the task function .", "entities": []}
{"text": "+ B ) .", "entities": []}
{"text": "= 1 \u2212 p ( q , c ) .", "entities": []}
{"text": "To better show the potential of our approach and the complexity of the task , we designed three joint model baselines based on : ( i ) a multiclassifier approach ( a listwise method ) , and ( ii ) a pairwise joint model operating over k + 1 candidates , and our adaptation of KGAT model ( a pairwise method ) .", "entities": []}
{"text": "| =", "entities": []}
{"text": "k", "entities": []}
{"text": "+ 1 .", "entities": []}
{"text": "The scores for the candidate answers are calculated as p ( c 1 ) , .. , p ( c k+1 )", "entities": []}
{"text": "Then , we rerank c i according their probability .", "entities": []}
{"text": "Joint Model Pairwise Our second baseline is similar to the first .", "entities": []}
{"text": "Then , we concatenate the embedding of the pair containing the target candidate , ( q , t ) with the embedding of all the other candidates ' [ CLS ] .", "entities": []}
{"text": "( q , t ) is always in the first position .", "entities": []}
{"text": "At classification time , we select one target candidate at a time , and set it in the first position , followed by all the others .", "entities": []}
{"text": "We classify all k + 1 candidates and use their score for reranking them .", "entities": []}
{"text": "Ev = { ev 1 , ev 2 , . .", "entities": []}
{"text": ".", "entities": []}
{"text": ", ev m } , their model carries out joint reasoning over Ev , e.g. , aggregating information to estimate the probability of f to be true or false , p ( y | f , Ev ) , where y { true , false } .", "entities": []}
{"text": "The approach is based on a fully connected graph , G , whose nodes are", "entities": []}
{"text": "the n i", "entities": []}
{"text": "=", "entities": []}
{"text": "( f , ev i ) pairs , and p ( y | f , Ev )", "entities": []}
{"text": "= p ( y | f , ev i , Ev )", "entities": []}
{"text": "p ( ev i | f , Ev ) ,", "entities": []}
{"text": "where p ( y | f , ev i , Ev )", "entities": []}
{"text": "= p ( y", "entities": []}
{"text": "|", "entities": []}
{"text": "n i , G ) is the label probability in each node i conditioned on the whole graph , and p ( ev", "entities": []}
{"text": "i | f , Ev )", "entities": []}
{"text": "=", "entities": []}
{"text": "p ( n i | G ) is the probability of selecting the most informative evidence .", "entities": []}
{"text": "KGAT uses an edge kernel to perform a hierarchi - cal attention mechanism , which propagates information between nodes and aggregate evidences .", "entities": []}
{"text": "We built a KGAT model for AS2 as follows : we replace ( i ) ev i with the set of candidate answers c i , and ( ii ) the claim f with the question and a target answer pair , ( q , t ) .", "entities": []}
{"text": "KGAT constructs the evidence graph G by using each claim - evidence pair as a node , which , in our case , is ( ( q , t ) , c i ) , and connects all node pairs with edges , making it a fully - connected evidence graph .", "entities": []}
{"text": "This way , sentence and token attention operate over the triplets , ( q , t , c i ) , establishing semantic links , which can help to support or undermine the correctness of t.", "entities": []}
{"text": "The original KGAT aggregates all the pieces of information we built , based on their relevance , to determine the probability of t. As we use AS2 data , the probability will be about the correctness of t.", "entities": []}
{"text": "Then , we apply a max - pooling operation on these two to get the final node representation .", "entities": []}
{"text": "The rest of the architecture is identical to the original KGAT .", "entities": []}
{"text": "Finally , at test time , we select one c i at a time , as the target t , and compute its probability , which ranks c i .", "entities": []}
{"text": "We proposed the Answer Support Reranker ( ASR ) , which uses an answer pair classifier to provide evidence to a target answer t. Given a question q , and a subset of its top - k+1 ranked answer candidates , A ( reranked by an AS2 model ) , we build a function , \u03c3 : Q \u00d7 C \u00d7 C k R such that \u03c3 ( q , t , A \\ { t } ) provides the probability of t to be correct , where C is the set of sentence - candidates .", "entities": []}
{"text": "We also design a multi - classifier MASR , which combines k ASR models , one for each different target answer .", "entities": []}
{"text": "We developed ASR architecture described in Figure 2 .", "entities": []}
{"text": "To reduce the noise that may be introduced by irrelevant c", "entities": []}
{"text": "3 . The ASR ( see Figure 1c ) uses the joint representation of ( q , t ) with ( t , c i ) , i = 1 , .. , k , where t and c i are the top - candidates reranked by PR .", "entities": []}
{"text": "The k representations are summarized by applying a max - pooling operation , which will aggregate all the supporting or not supporting properties of the candidates with respect to the target answer .", "entities": []}
{"text": "The concatenation of the PR embedding with the max - pooling embedding is given as input to the final classification layer , which scores t with respect to q , also using the information from the other candidates .", "entities": []}
{"text": "For training and testing , we select a t from the k + 1 candidates of q at a time , and compute its score .", "entities": []}
{"text": "This way , we can rerank all the k + 1 candidates with their scores .", "entities": []}
{"text": "Implementation details : ASR is a PR that also exploits the relation between t and A \\ { t } .", "entities": []}
{"text": "= E t .", "entities": []}
{"text": "Then , we concatenate E t to the max - pooling tensor from\u00ca 1 , .. , \u00ca k :", "entities": []}
{"text": "V = [ E t : Maxpool ( [ \u00ca 1 , .. , \u00ca k ] ) ] , ( 1 ) where V R 2d is the final representation of the target answer t.", "entities": []}
{"text": "p ( y i | q , t , C k )", "entities": []}
{"text": "ASC labels There can be different interpretations when attempting to define labels for answer pairs .", "entities": []}
{"text": "ASR still selects answers with a pointwise approach 2 .", "entities": []}
{"text": "This means that we can improve it by building a listwise model , to select the best answer for each question , by utilizing the information from all target answers .", "entities": []}
{"text": "In particular , the architecture of MASR shown in Figure 1d is made up of two parts : ( i ) a list of ASR containing k + 1 ASR blocks , in which each ASR block provides the representation of a target answer t. ( ii )", "entities": []}
{"text": "The representation of each target answer is the embedding V R 2d from Equation 1 in ASR .", "entities": []}
{"text": "Then , we concatenate the hidden vectors of k + 1 target answers to form a matrix V ( q , k+1 ) R ( k+1 ) \u00d72d .", "entities": []}
{"text": "| y | = | k + 1 | .", "entities": []}
{"text": "In these experiments , we compare our models : KGAT , ASR and MASR with pointwise models , which are the state of the art for AS2 .", "entities": []}
{"text": "We also compare them with our joint model baselines ( pairwise and listwise ) .", "entities": []}
{"text": "Finally , we provide an error analysis .", "entities": []}
{"text": "We used two most popular AS2 datasets , and one real world application dataset we built to test the generality of our approach .", "entities": []}
{"text": "The answers are manually labeled .", "entities": []}
{"text": "We follow the most used setting : training with all the questions that have at least one correct answer , and validating and testing with all the questions having at least one correct and one incorrect answer .", "entities": []}
{"text": "Wang et al ( 2007 ) .", "entities": []}
{"text": "We use the same splits of the original data , following the common setting of previous work , e.g. , ( Garg et al , 2020 ) .", "entities": []}
{"text": "The creation process includes the following steps : ( i ) given a set of questions we collected from the web , a search engine is used to retrieve up to 1 , 000 web pages from an index containing hundreds of millions pages .", "entities": []}
{"text": "( ii ) From the set of retrieved documents , all candidate sentences are extracted and ranked using AS2 models from ( Garg et al , 2020 ) .", "entities": []}
{"text": "Finally , ( iii ) top candidates for each question are manually assessed as correct or incorrect by human judges .", "entities": []}
{"text": "This allowed us to obtain a richer variety of answers from multiple sources with a higher average number of answers .", "entities": []}
{"text": "All claims are labelled as Supported , Refuted or Not Enough Info by annotators .", "entities": []}
{"text": "Table 3 shows the statistics of the dataset , which remains the same as in ( Thorne et al , 2018b ) .", "entities": []}
{"text": "set question ( this varies according to the dataset ) , to have a direct comparison with the state of the art .", "entities": []}
{"text": "The other training configurations are the same of the original KGAT model from ( Liu et al , 2020 ) .", "entities": []}
{"text": "This is possible as the labels are compatible .", "entities": []}
{"text": "The selection of the hyper - parameter k , i.e. , the number of candidates to consider for supporting a target answer is rather tricky .", "entities": []}
{"text": "Indeed , the standard validation set is typically used for tuning PR .", "entities": []}
{"text": "This means that the candidates PR moves to the top k +1 positions are optimistically accurate .", "entities": []}
{"text": "Thus , when selecting also the optimal k on the same validation set , there is high risk to overfit the model .", "entities": []}
{"text": "We solved this problem by running a PR version not heavily optimized on the dev .", "entities": []}
{"text": "Additionally , we tuned k only using the WQA dev .", "entities": []}
{"text": "set , which contains \u223c 36 , 000 Q / A pairs .", "entities": []}
{"text": "sets are too small to be used ( 121 and 65 questions , respectively ) .", "entities": []}
{"text": "Fig .", "entities": []}
{"text": "2 plots the improvement of four different models , Joint Model Multi - classifier , Joint Model Pairwise , KGAT , and ASR , when using different k values .", "entities": []}
{"text": "Their best results are reached for 5 , 3 , 2 , and 3 , respectively .", "entities": []}
{"text": "We note that the most reliable curve shape ( convex ) is the one of ASR and Joint Model Pairwise .", "entities": []}
{"text": "As WQA is an internal dataset , we only report the improvement over PR in the tables .", "entities": []}
{"text": "Joint Model Multi - classifier performs lower than PR for all measures and all datasets .", "entities": []}
{"text": "This is in line with the findings of Bonadiman and Moschitti ( 2020 ) , who also did not obtain improvement when jointly used all the candidates altogether in a representation .", "entities": []}
{"text": "Joint Model Pairwise differs from ASR as it concatenates the embeddings of the ( q , c i ) , instead of using max - pooling , and does not use any Answer Support Classifier ( ASC ) .", "entities": []}
{"text": "Still , it exploits the idea of aggregating the information of all pairs ( q , c i ) with respect to a target answer t , which proves to be effective , as the model improves on PR over all measures and datasets .", "entities": []}
{"text": "Our KGAT version for AS2 also improves PR over all datasets and almost all measures , confirming that the idea of using candidates as support of the target answer is generally valid .", "entities": []}
{"text": "However , it is not superior to Joint Model Pairwise .", "entities": []}
{"text": "ASR achieves the highest performance among all models ( but MASR - FP on WQA ) , all datasets , and all measures .", "entities": []}
{"text": "We perform randomization test ( Yeh , 2000 ) to verify if the models significantly differ in terms of prediction outcome .", "entities": []}
{"text": "We use 100 , 000 trials for each calculation .", "entities": []}
{"text": "Table 5 also reports the comparison with PR , which is the official state of the art .", "entities": []}
{"text": "KGAT performs lower than PR on both datasets .", "entities": []}
{"text": "The latter is 97.06 , which corresponds to mistaking the answers of only two questions .", "entities": []}
{"text": "Sec . 4.1 ) .", "entities": []}
{"text": "We analyzed examples for which ASR is correct and PR is not .", "entities": []}
{"text": "Tab .", "entities": []}
{"text": "This probably happens since the answer best matches the syntactic / semantic pattern of the question , which asks for a type of color , indeed , the answer offers such type , primary colors .", "entities": []}
{"text": "PR does not rely on any background information that can support the set of colors in the answer .", "entities": []}
{"text": "In contrast , ASR selects c 2 as it can rely on the support of other answers .", "entities": []}
{"text": "( both members are correct ) of c 2 , i.e. , 1 k i = 2 ASC ( c 2 , c i ) = 0.653 , while for c 1 the average score is significant lower , i.e. , 0.522 .", "entities": []}
{"text": "This provides higher support for c 2 , which is used by ASR to rerank the output of PR .", "entities": []}
{"text": "Tab . 8 shows an interesting case where all the sentences contain the required information , i.e. , February .", "entities": []}
{"text": "Also , it contains a lot of ancillary information .", "entities": []}
{"text": "In contrast , MASR is able to rerank the best answer , c 1 , in the top position .", "entities": []}
{"text": "We have proposed new joint models for AS2 .", "entities": []}
{"text": "We extensively tested KGAT , ASR , MASR , and other joint model baselines we designed .", "entities": []}
{"text": "The results show that our models can outperform the state of the art .", "entities": []}
{"text": "Most interestingly , ASR constantly outperforms all the models ( but MASR - FP ) , on all datasets , through all measures , and for both base and large transformers .", "entities": []}
{"text": "For example , ASR q : What kind of colors are in the rainbow ?", "entities": []}
{"text": "c1 :", "entities": []}
{"text": "Red , yellow , and blue are called the primary colors .", "entities": []}
{"text": "c2 : The order of the colors in the rainbow goes : red , orange , yellow , green , blue , indigo and violet .", "entities": []}
{"text": "In this paper , we propose a method for whyquestion answering ( why - QA ) that uses an adversarial learning framework .", "entities": []}
{"text": "Existing why - QA methods retrieve answer passages that usually consist of several sentences .", "entities": []}
{"text": "These multi - sentence passages contain not only the reason sought by a why - question and its connection to the why - question , but also redundant and/or unrelated parts .", "entities": []}
{"text": "We use our proposed Adversarial networks for Generating compact - answer Representation ( AGR ) to generate from a passage a vector representation of the non - redundant reason sought by a why - question and exploit the representation for judging whether the passage actually answers the why - question .", "entities": []}
{"text": "We show that they also improve a state - of - the - art distantly supervised open - domain QA ( DS - QA ) method on publicly available English datasets , even though the target task is not a why - QA .", "entities": []}
{"text": "Previous why - QA methods retrieve from a text archive answer passages , each of which consists of several sentences , like A in Table 1 ( Girju , 2003 ; Higashinaka and Isozaki , 2008 ;", "entities": []}
{"text": "Oh et al , , 2013Sharp et al , 2016 ; Verberne et al , 2011 ) , and then determine whether the passages answer the question .", "entities": []}
{"text": "A proper answer passage must contain ( 1 ) a paraphrase of the why - question ( e.g. , the underlined texts in Table 1 ) and ( 2 ) the reasons or the causes ( e.g. , the bold texts in Table 1 ) of Q Why does honey last a long time ?", "entities": []}
{"text": "A While excavating Egypt 's pyramids , archaeologists have found pots of honey in an ancient tomb : thousands of years old and still preserved .", "entities": []}
{"text": "Honey can last a long time due to three special properties .", "entities": []}
{"text": "Its average pH is 3.9 , which is quite acidic .", "entities": []}
{"text": "Such high level of acidity is certainly hostile and hinders the growth of many microbes .", "entities": []}
{"text": "Though honey contains around 17 - 18 % water , its water activity is too low to support the growth of microbes .", "entities": []}
{"text": "Moreover honey contains hydrogen peroxide , which is thought to help prevent the growth of microbes in honey .", "entities": []}
{"text": "Despite these properties , honey can be contaminated under certain circumstances .", "entities": []}
{"text": "C Because its acidity , low water activity , and hydrogen peroxide together hinder the growth of microbes .", "entities": []}
{"text": "Table 1 : Answer passage A to why - question Q and its compact answer C the events described in the why - question , both of which are often written in multiple non - adjacent sentences .", "entities": []}
{"text": "This multi - sentenceness implies that the answer passages often contain redundant parts that are not directly related to a why - question or its reason / cause and whose presence complicates the why - QA task .", "entities": []}
{"text": "Highly accurate why - QA methods should be able to find the exact reason sought by a why - question in an answer passage without being distracted by redundancy .", "entities": []}
{"text": "In this paper , we train a neural network ( NN ) to generate , from an answer passage , a vector representation of the non - redundant reason asked by a why - question , and exploit the generated vector representation as evidence for judging whether the passage answers the why - question .", "entities": []}
{"text": "Compact answers are sentences or phrases that express the reasons for a given why - question without redundancy .", "entities": []}
{"text": "However , we were disappointed by the small performance improvement , as shown in our experimental results .", "entities": []}
{"text": "We chose an alternative approach .", "entities": []}
{"text": "Instead of generating a compact answer of an answer passage as word sequences , we devised a model to generate a compact - answer representation , which is a vector representation for a compact answer , from an answer passage .", "entities": []}
{"text": "We combined the generator network in the AGR with an extension of the state - of - the - art why - QA method .", "entities": []}
{"text": "Another interesting point is that the performance improved even when we replaced , as the inputs to AGR , the word embedding vectors that represent an answer passage , with a random vector .", "entities": []}
{"text": "This observation warrants further exploration in our future work .", "entities": []}
{"text": "Finally , we applied our AGR to a distantly su - ( Chen et al , 2017 ) , which is an extension of a machinereading task , to check whether it is applicable to other datasets .", "entities": []}
{"text": "We combined our generator network with a state - of - the - art DS - QA method , OpenQA ( Lin et al , 2018 ) , and used a generated compact - answer representation from a given passage as evidence to 1 ) select relevant passages from the retrieved ones and 2 ) find an answer from the selected passages .", "entities": []}
{"text": "( Dunn et al , 2017 ) , and Triv - iaQA ( Joshi et al , 2017 ) ) revealed that the generator network improved the performance in most cases .", "entities": []}
{"text": "This suggests that AGR may be applicable to many QA - like tasks .", "entities": []}
{"text": "2 Why - QA Model Figure 1 illustrates the architecture of our why - QA model and the AGR .", "entities": []}
{"text": "Our why - QA model computes the probability that a given answer passage describes a proper answer to a given why - question using the representations of a question , an answer passage , and a compact answer .", "entities": []}
{"text": "The representations of why - questions and answer passages are generated by Convolutional Neural Networks ( CNNs ) ( Collobert et al , 2011 ;", "entities": []}
{"text": "Note that in computing a question 's representation , the answer passage is given to the question encoder to guide the computation .", "entities": []}
{"text": "Likewise the passage encoder is given the question and the representation of the compact answer .", "entities": []}
{"text": "We represent these information flows with dotted arrows in Fig .", "entities": []}
{"text": "1 ( a ) .", "entities": []}
{"text": "The representations of compact answers are created by a generator network called a fakerepresentation generator ( F in Fig .", "entities": []}
{"text": "1 ( a ) ) , which is pre - trained in an adversarial learning manner ( Fig . 1 ( b ) ) .", "entities": []}
{"text": "During the training of the whole why - QA model , the generator 's parameters are fixed and no further fine - tuning is conducted .", "entities": []}
{"text": "In the next section , we describe our main contribution : the AGR and the fake - representation generator .", "entities": []}
{"text": "The entire why - QA model can be seen as an extension of the state - of - the - art why - QA method .", "entities": []}
{"text": "Its details are described in Section A of the supplementary materials .", "entities": []}
{"text": "Compact - answer Representation", "entities": []}
{"text": "Generative adversarial networks ( GANs ) ( Goodfellow et", "entities": []}
{"text": "al , 2014 ) are a framework for training generative models based on game theory .", "entities": []}
{"text": "Unlike the original GANs , which generate such data samples as images and compact answers from noise , our AGR generates useful vector representations from meaningful text passages .", "entities": []}
{"text": "To clarify the difference , we explain our AGR with three subnetworks : two generators , F and R , and a discriminator , D , as in Fig .", "entities": []}
{"text": "1 ( b ) .", "entities": []}
{"text": "Generator F takes as input passage p drawn from prior passage distribution", "entities": []}
{"text": "d p and outputs vectorp as a fake representation of a compact answer .", "entities": []}
{"text": "We call F a fake - representation generator .", "entities": []}
{"text": "R , which we call a real - representation generator , is given sample c taken from manually created compact - answers and provides vector c as a real - representation of the sampled compactanswer .", "entities": []}
{"text": "Discriminator D has to distinguish fakerepresentationp from real - representationc of a compact answer .", "entities": []}
{"text": "These three networks play an adversarial minimax game ; fake - representation generator F creates a fake compact - answer representation that is hard for the discriminator to distinguish from the representations of manually created compact - answers , and discriminator D and generator R simultaneously try to avoid being duped by generator F .", "entities": []}
{"text": "These processes should allow generator F to learn how to generate a representation of a proper compact - answer from an answer passage .", "entities": []}
{"text": "In addition , since passage p and compact answer c are dependent on question q , the generation of the compact - answer representations by F and R is conditioned by question q , like in the conditional GANs ( Mirza and Osindero , 2014 ) .", "entities": []}
{"text": "We trained our AGR with the following minimax objective : min F max D , R V ( D , F , R )", "entities": []}
{"text": "= E c\u223cdc ( c ) [ log D ( R ( c | q ) ) ]", "entities": []}
{"text": "+ E p\u223cdp ( p ) [ log ( 1 \u2212 D ( F ( p | q ) ) ] .", "entities": []}
{"text": "In our implementation , both F and R are networks with identical structure called Encoder .", "entities": []}
{"text": "They are defined as follows , where p , c , and q are respectively an answer passage , a manually created compact - answer , and a why - question : F ( p | q )", "entities": []}
{"text": "All of these techniques were proposed by previous works .", "entities": []}
{"text": "Further details are given in Section B of the supplementary materials .", "entities": []}
{"text": "Sharp et", "entities": []}
{"text": "al ( 2016 ) cre - ated a set of cause - effect word pairs by paring each content word in a cause part with each content word in an effect part of the same causality expression , such as \" Volcanoes erupt because magma pushes through vents and fissures . \"", "entities": []}
{"text": "In this work , we extracted 100 million causality expressions from 4 - billion Japanese web pages using the causality recognizer of Oh et al ( 2013 ) .", "entities": []}
{"text": "The first type of attention , similarity - attention , was used for estimating the similarities between words in question q and those in passage / compact - answers t and focusing on the attended words as those that directly indicate the connection between the question and passage / compact - answers .", "entities": []}
{"text": "Basically , the mechanism computes the cosine similarity between the embeddings of the words in q and t , and uses it for producing attention feature vector a s j R for word t j in passage / compact - answers .", "entities": []}
{"text": "Another attention mechanism , causalityattention , was proposed for focusing on passage words causally associated with question words .", "entities": []}
{"text": "They used normalized point - wise mutual information to measure the strength of the causal associations with the causality expressions used for creating the causal embeddings .", "entities": []}
{"text": "The scores are used for producing causality - attention feature vector a c j for word t j .", "entities": []}
{"text": "Finally , we form two attention feature vectors , a s", "entities": []}
{"text": "=", "entities": []}
{"text": "att is given to CNNs to generate final representation r t of a given passage / compact - answer t.", "entities": []}
{"text": "The CNNs resembles those in Kim ( 2014 ) .", "entities": []}
{"text": "In our experiments , we set the dimension of representation r t to 300 .", "entities": []}
{"text": "4 Why - QA Experiments", "entities": []}
{"text": "We used three datasets , W hySet , CmpAns , and AddT r , for our why - QA experiments .", "entities": []}
{"text": "W hySet and AddT r were used for training and evaluating the why - QA models , while CmpAns was used for training AGR .", "entities": []}
{"text": "The W hySet dataset , which was used in previous works for why - QA ( Oh et al , , 2013 , is composed of 850 Japanese why - questions and their top - 20 answer passages ( 17 , 000 question - passage pairs ) obtained from 600 million Japanese web pages using the answerretrieval method of Murata et al ( 2007 ) , where a question - passage pair is composed of a singlesentence question and a five - sentence passage .", "entities": []}
{"text": "The label of each question - answer pair ( i.e. , correct answer and incorrect answer ) was manually annotated ( See Oh et al ( 2013 ) for more details ) .", "entities": []}
{"text": "Oh et al ( 2013 ) selected 10 , 000 questionpassage pairs as training and test data in 10 - fold cross - validation ( 9 , 000 for training and 1 , 000 for testing ) and used the remainder ( 7 , 000 questionpassage pairs ) as additional training data during the 10 - fold cross - validation .", "entities": []}
{"text": "We followed the settings and , in each fold , we selected 1 , 000 pairs from the 9 , 000 pairs for training to use as development data for tuning hyperparameters .", "entities": []}
{"text": "Note that there are no shared questions in the training , development , or test data .", "entities": []}
{"text": "These cover 2 , 060 unique why - questions .", "entities": []}
{"text": "Note that there was no overlap between the questions in CmpAns and those in W hySet .", "entities": []}
{"text": "The passages for which no annotator could create a compact answer were discarded , and were not included in the 15 , 130 triples mentioned previously .", "entities": []}
{"text": "The average lengths of questions , passages , and compact answers in CmpAns were 10.5 words , 184.4 words , and 8.3 words , respectively .", "entities": []}
{"text": "Finally , we created additional training data AddT r for training the why - QA models .", "entities": []}
{"text": "If an annotator could write a compact answer for a question and an answer passage , she / he probably recognized the passage as a proper answer passage to the question .", "entities": []}
{"text": "Based on this observation , we built AddT r from CmpAns by applying a majority vote .", "entities": []}
{"text": "We only gave a correct answer label to a question and a passage if at least two of the three annotators wrote compact answers , and it received an incorrect answer label otherwise .", "entities": []}
{"text": "AddT r has 10 , 401 pairs in total .", "entities": []}
{"text": "We used AddT r as additional training data for baselines that lack a mechanism for generating compact - answer representations , for a fair comparison with other methods that use CmpAns for such mechanisms .", "entities": []}
{"text": "We processed all the data with MeCab 1 , a morphological analyzer , to segment the words .", "entities": []}
{"text": "In our proposed methods and their variants , all the weights in the CNNs were initialized using He 's method ( He et al , 2015 ) , and the other weights in our why - QA model were initialized randomly with a uniform distribution in the range of ( - 0.01 , 0.01 ) .", "entities": []}
{"text": "For the CNN - based components , we set the window size of the filters to \" 1 , 2 , 3 \" with 100 filters each 2 .", "entities": []}
{"text": "All of these hyper - parameters were chosen with our development data .", "entities": []}
{"text": "We tried three schemes for training our AGR in our proposed method .", "entities": []}
{"text": "In the first scheme , pairs of passages and compact answers in CmpAns were given to fake - representation generator F and realrepresentation generator R as their inputs .", "entities": []}
{"text": "We called the fake - representation generator trained in this way F OP and referred to our proposed method using F OP as Ours ( OP ) .", "entities": []}
{"text": "In the second scheme , we randomly sampled five - sentence passages that contain some clue words indicating the existence of causal relations , such as \" because , \" from 4 - billion web pages and fed them to fakerepresentation generator F .", "entities": []}
{"text": "We fed the same number of the sampled passages as in CmpAns for fair comparison .", "entities": []}
{"text": "We refer to the method trained by this scheme as Ours ( RP ) .", "entities": []}
{"text": "The fake - representation generator trained in this way is called F RV , and our proposed method using F RV is called Ours ( RV ) .", "entities": []}
{"text": "They are listed in Table 2 .", "entities": []}
{"text": "Proposed method from which we removed fake - representation generator F .", "entities": []}
{"text": "R was trained alongside the why - QA model using W hySet and the compact - answer generator was pre - trained with CmpAns .", "entities": []}
{"text": "The encoder was pre - trained with CmpAns .", "entities": []}
{"text": "Same as Ours ( OP ) except that the fake - representation generator was trained in a supervised manner alongside the why - QA model using W hySet and AddT r as the training data .", "entities": []}
{"text": "Same as BERT+FOP except that it used FRV instead of FOP for producing compact - answer representation .", "entities": []}
{"text": "This data mix consists of 75 % of sentences extracted from Wikipedia ( 14 , 675 , 535 sentences taken out of 784 , 869 articles randomly sampled ) and 25 % of cause and effect phrases taken from causality expressions ( 4 , 891 , 846 phrases from 2 , 445 , 923 causal relations ) .", "entities": []}
{"text": "This ratio was determined through preliminary experiments using the development data .", "entities": []}
{"text": "+ + + + + + E [ CLS ] E ' E \u2032 )", "entities": []}
{"text": "The attention feature embeddings for answer passages ( i.e. , E sim w 1 , , E sim w M , and E caus", "entities": []}
{"text": "w 1 , , E caus w M ) were computed from the same attention feature vectors , a s and a c , as those in our proposed methods ; those for the other parts ( i.e. , questions , [ CLS ] , and [ SEP ] ) were computed from a zero vector ( indicating no attention feature ) .", "entities": []}
{"text": "( Oh et al , 2013 ) .", "entities": []}
{"text": "Note that the Oracle method indicates the performance of a fictional method that ranks the answer passages perfectly , i.e. , it locates all the m correct answers to a question in the top - m ranks , based on the gold - standard labels .", "entities": []}
{"text": "This performance is the upper bound of those of all the implementable methods .", "entities": []}
{"text": "Our proposed method , Ours ( OP ) , outperformed all the other methods .", "entities": []}
{"text": "It also outperformed BASE+CAns and BASE+CEnc , which generated compact - answer representations in a way different from the proposed method , and BASE+Enc , which trained the fake - representation generator without adversarial learning .", "entities": []}
{"text": "These performance differences were statistically significant ( p < 0.01 by the McNemar 's test ) .", "entities": []}
{"text": "Another interesting point is that Ours ( RV ) , in which fake - representation generator F RV was trained using random vectors , achieved almost the same performance as that of Ours ( OP ) .", "entities": []}
{"text": "This result was puzzling , so we first checked whether F RV 's output was not just random noise ( which could prevent the why - QA model from overfitting ) by replacing in Ours ( RV ) the output of F RV by random vectors .", "entities": []}
{"text": "This result confirms that it is not trivial to mimic F RV using random vectors at least .", "entities": []}
{"text": "We investigated the F RV 's output to check whether it actually focused on the compact answer in a given passage .", "entities": []}
{"text": "r out i should be the same as r org i and r in i should significantly differ from r org i .", "entities": []}
{"text": "Next we computed the average Euclidian distance among { r org i } , { r in i } and { r out i } .", "entities": []}
{"text": "The average distance ( 2.67 ) between { r org i } and { r out i } was much smaller than the average distance ( 13.3 ) between { r org i } and { r in i } .", "entities": []}
{"text": "This implies that the distance between { r org i } and { r out i } might be much larger than that between { r org i } and { r in i } if F RV focused equally on every answer passage word .", "entities": []}
{"text": "However , the actual results suggest that this is not the case .", "entities": []}
{"text": "Although we can not draw decisive conclusions due to the complex nature of neural networks , we believe from the results that F RV does actually focus more on words that are a part of a compact answer than on other words .", "entities": []}
{"text": "We also computed { r org i } , { r in i } , and { r out i } with fakerepresentation generator F OP in the same way and observed the same tendency .", "entities": []}
{"text": "Table 4 shows the statistics for the datasets used in this experiment .", "entities": []}
{"text": "We assume that the answers are our compact answers , although the answers in the dataset are consecutive short word sequences ( 2.8 words on average ) , whose majority are noun phrases , unlike the compact answers for our why - QA experiment , i.e. , sentences or phrases ( 8.3 words on average ) .", "entities": []}
{"text": "In this experiment , we used the AGR training schemes for Ours ( OP ) and Ours ( RV ) .", "entities": []}
{"text": "Then we combined the resulting fake - representation generator F in the AGR with the state - of - the - art DS - QA method , OpenQA ( Lin et al , 2018 ) 7 .", "entities": []}
{"text": "We also used the hyperparameters presented in Lin et al ( 2018 ) .", "entities": []}
{"text": "OpenQA is composed of two components : a paragraph selector to choose relevant paragraphs ( or answer passages in our terms ) from a set of paragraphs and a paragraph reader to extract answers from the selected paragraphs .", "entities": []}
{"text": "For identifying answer a to given question q from set of paragraphs P = { p i } , the paragraph selector and the paragraph reader respectively compute probabilities P r ( p", "entities": []}
{"text": "i", "entities": []}
{"text": "| q , P ) and P r ( a | q , p i ) , and final output P r ( a | q , P ) is obtained by combining the probabilities .", "entities": []}
{"text": "We introduced c i , which is a compact - answer representation generated by fakerepresentation generator F with question q and paragraph p i as its input , to the computation of the probabilities as follows : P r ( a | q , P , C )", "entities": []}
{"text": "=", "entities": []}
{"text": "i P r ( a | q , p i , c i ) P r ( p i | q , P , c i )", "entities": []}
{"text": "In our implementation , we computed attention - weighted embeddingp i of a paragraph by using compactanswer representation c i .", "entities": []}
{"text": "Given word embedding p j i for the j - th word in paragraph", "entities": []}
{"text": "p i , its attentionweighted embeddingp", "entities": []}
{"text": "j i was computed by using a bilinear function ( Sutskever et al , 2009 ) :", "entities": []}
{"text": "p T i", "entities": []}
{"text": "Mc i )", "entities": []}
{"text": "= 300 .", "entities": []}
{"text": "We gave [ p j i ; p j i ] , a concatenation of p j", "entities": []}
{"text": "i andp j i , as the word embedding of the j - th word in paragraph", "entities": []}
{"text": "p i to the bidirectional stacked RNNs .", "entities": []}
{"text": "Some of the improvements over the previous state - ofthe - art method , OpenQA , were statistically significant .", "entities": []}
{"text": "These findings suggest that our framework can be effective for tasks other than the original why - QA and the other datasets .", "entities": []}
{"text": "It employed adversarial learning to generate vector representations of reasons or true answers from answer passages and exploited the representations for judging whether the passages are proper answer passages to the given whyquestions .", "entities": []}
{"text": "Through experiments using Japanese why - QA datasets , we showed that this idea improved why - QA performance .", "entities": []}
{"text": "We also showed that our method improved the performance in a distantly supervised open - domain QA task .", "entities": []}
{"text": "In our why - QA method , causality expressions extracted from the web were used as background knowledge for computing causality - attention / embeddings .", "entities": []}
{"text": "As a future work , we plan to introduce a wider range of background knowledge including another type of event causality ( Hashimoto et al , , 2014 ( Hashimoto et al , , 2015 .", "entities": []}
{"text": "Are Training Samples Correlated ?", "entities": []}
{"text": "Learning to Generate Dialogue Responses with Multiple References", "entities": []}
{"text": "Previous models are generally trained based on 1 - to - 1 mapping from an input query to its response , which actually ignores the nature of 1 - to - n mapping in dialogue that there may exist multiple valid responses corresponding to the same query .", "entities": []}
{"text": "In this paper , we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1 - to - n mapping with a novel two - step generation architecture .", "entities": []}
{"text": "The first generation phase extracts the common features of different responses which , combined with distinctive features obtained in the second phase , can generate multiple diverse and appropriate responses .", "entities": []}
{"text": "Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations .", "entities": []}
{"text": "To deal with the generic response problem , various methods have been proposed , including diversity - promoting objective function ( Li et al , 2016a ) , enhanced beam search ( Shao et al , 2016 ) , latent dialogue mechanism ( Zhou et", "entities": []}
{"text": "However , these methods still view multiple responses as independent ones and fail to model multiple responses jointly .", "entities": []}
{"text": "Recently , Zhang et al ( 2018a ) introduce a maximum likelihood strategy that given an input query , the most likely response is approximated rather than all possible responses , which is further implemented by Rajendran et al ( 2018 ) with reinforcement learning for task - oriented dialogue .", "entities": []}
{"text": "Although capable of generating the most likely response , these methods fail to model other possible responses and ignore the correlation of different responses .", "entities": []}
{"text": "Our motivation lies in two aspects : 1 ) multiple responses for a query are likely correlated , which can facilitate building the dialogue system .", "entities": []}
{"text": "2 ) it is easier to model each response based on other responses than from scratch every time .", "entities": []}
{"text": "As shown in Figure 1 , given an input query , different responses may share some common features e.g. positive attitudes or something else , but vary in discourses or expressions which we refer to as distinct features .", "entities": []}
{"text": "Accordingly , the system can benefit from modeling these features respectively rather than learning each query - response mapping from scratch .", "entities": []}
{"text": "We jointly view the multiple possible responses to the same query as a response bag .", "entities": []}
{"text": "In the first generation phase , the common feature of different valid responses is extracted , serving as a base from which each specific response in the bag is further approximated .", "entities": []}
{"text": "The system then , in the second generation phase , learns to model the distinctive feature of each individual response which , combined with the common feature , can generate multiple diverse responses simultaneously .", "entities": []}
{"text": "Experimental results show that our method can outperform existing competitive neural models under both automatic and human evaluation metrics , which demonstrates the effectiveness of the overall approach .", "entities": []}
{"text": "We also provide ablation analyses to validate each component of our model .", "entities": []}
{"text": "To summarize , our contributions are threefold : We propose to model multiple responses to a query jointly by considering the correlations of responses with multi - reference learning .", "entities": []}
{"text": "Experiments show that the proposed method can generate multiple diverse responses and outperform existing competitive models on both automatic and human evaluations .", "entities": []}
{"text": "However , these models suffer from the \" safe \" response problem .", "entities": []}
{"text": "To address this problem , various methods have been proposed .", "entities": []}
{"text": "Li et al ( 2016a ) propose a diversity - promoting objective function to encourage diverse responses during decoding .", "entities": []}
{"text": "Zhou", "entities": []}
{"text": "et al ( , 2018a introduce a responding mechanism between the encoder and decoder to generate various responses .", "entities": []}
{"text": "incorporate topic information to generate informative responses .", "entities": []}
{"text": "However , these models suffer from the deterministic structure when generating multiple diverse responses .", "entities": []}
{"text": "A few works explore to change the deterministic structure of sequence - to - sequence models by introducing stochastic latent variables .", "entities": []}
{"text": "To tackle this , WAE ( Gu et al , 2018 ) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD ( Du et al , 2018 ) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed .", "entities": []}
{"text": "Although these models overcome the deterministic structure of sequence - to - sequence model , they still ignore the correlation of multiple valid responses and each case is trained separately .", "entities": []}
{"text": "To consider the multiple responses jointly , the maximum likelihood strategy is explored .", "entities": []}
{"text": "Zhang et al ( 2018a ) propose the maximum generated likelihood criteria which model a query with its multiple responses as a bag of instances and proposes to optimize the model towards the most likely answer rather than all possible responses .", "entities": []}
{"text": "Similarly , Rajendran et al ( 2018 ) propose to reward the dialogue system if any valid answer is produced in the reinforcement learning phase .", "entities": []}
{"text": "Though considering multiple responses jointly , the maximum likelihood strategy fails to utilize all the references during training with some cases ig - Figure 2 : The overall architecture of our proposed dialogue system where the two generation steps and testing process are illustrated .", "entities": []}
{"text": "Given an input query x , the model aims to approximate the multiple responses in a bag { y } simultaneously with the continuous common and distinctive features , i.e. , the latent variables c and z obtained from the two generation phases respectively .", "entities": []}
{"text": "nored .", "entities": []}
{"text": "In our approach , we consider multiple responses jointly and model each specific response separately by a two - step generation architecture .", "entities": []}
{"text": "We posit that a dialogue system can benefit from multi - reference learning by considering the correlation of multiple responses .", "entities": []}
{"text": "Figure 2 demonstrates the whole architecture of our model .", "entities": []}
{"text": "We now describe the details as follows .", "entities": []}
{"text": "Training samples { ( x , { y } )", "entities": []}
{"text": "i } i", "entities": []}
{"text": "= N i=1 consist of each query x and the set of its valid responses { y } , where N denotes the number of training samples .", "entities": []}
{"text": "To achieve this , different from conventional methods which view the multiple responses as independent ones , we propose to consider the correlation of multiple responses with a novel twostep generation architecture , where the response bag { y } and each response y { y } are modeled by two separate features which are obtained in each generation phase respectively .", "entities": []}
{"text": "Specifically , we assume a variable c R n representing the common feature of different responses and an unobserved latent variable z Z corresponding to the distinct feature for each y in the bag .", "entities": []}
{"text": "The com - mon feature c is generated in the first stage given x and the distinctive feature z is sampled from the latent space Z in the second stage given the query x and", "entities": []}
{"text": "common feature c.", "entities": []}
{"text": "The final responses are then generated conditioned on both the common feature c and distinct feature z simultaneously .", "entities": []}
{"text": "In the first generation step , we aim to map from the input query x to the common feature c of the response bag { y } .", "entities": []}
{"text": "To obtain this , we model the common feature of the response bag as the mid - point of embeddings of multiple responses .", "entities": []}
{"text": "The feature c is then fed into the response decoder to obtain the intermediate response y c which is considered to approximate all valid responses .", "entities": []}
{"text": "Mathematically , the objective function is defined as : L avg = 1 | { y } | y { y } log", "entities": []}
{"text": "p \u03c8", "entities": []}
{"text": "( y | c ) ( 1 ) where | { y } | is the cardinality of the response bag { y } and p \u03c8 represents the response decoder .", "entities": []}
{"text": "Besides , to measure how well the intermediate response y c approximates the mid - point response , we set up an individual discriminator and derive the mapping function to produce better results .", "entities": []}
{"text": "As to the discriminator , we first project each utterance to an embedding space with fixed dimensionality via convolutional neural networks ( CNNs ) with different kernels as the process shown in Figure 3 .", "entities": []}
{"text": "For the response bag { y } , the average response embedding is used to compute the matching score .", "entities": []}
{"text": "The second generation phase aims to model each specific response in a response bag respectively .", "entities": []}
{"text": "Firstly , rather than modeling each response with the latent variable z from scratch , our model approximates each response based on the bag representation c with only the distinctive feature of each specific response remaining to be captured .", "entities": []}
{"text": "Secondly , the prior common feature c can provide extra information for the sampling network which is supposed to decrease the latent searching space .", "entities": []}
{"text": "\u2212 D [ q \u03c6 ( z | x , y , c ) |", "entities": []}
{"text": "| p \u03d5 ( z | x , c ) ]", "entities": []}
{"text": "( 5 ) where q \u03c6 represents the recognition network and p \u03d5 is the prior network with \u03c6 and \u03d5 as the trainable parameters ; D ( | | ) is the regularization term which measures the distance between the two distributions .", "entities": []}
{"text": "In practice , the recognition networks are implemented with a feed - forward network that \u00b5 log \u03c3 2 = W q \uf8ee \uf8f0", "entities": []}
{"text": "For the prior networks , we consider two kinds of implements .", "entities": []}
{"text": "2 = W p h x c + b p ( 7 ) and the distance D ( | | ) here is measured by the KL divergence .", "entities": []}
{"text": "K i=1", "entities": []}
{"text": "exp ( e i ) ( 8 ) and \uf8ee \uf8f0", "entities": []}
{"text": "e k \u00b5 k log \u03c3 2 k \uf8f9 \uf8fb = W p , k h x c + b p , k ( 9 )", "entities": []}
{"text": "The distance here is measured by the Wasserstein distance which is implemented with an adversarial discriminator .", "entities": []}
{"text": "Recap that in the second generation phase the latent variable z is considered to only capture the distinctive feature of each specific response .", "entities": []}
{"text": "x , z )", "entities": []}
{"text": "+ \u03bb log ( 1 \u2212 p ( { \u0233 } bow | x , z ) ) ]", "entities": []}
{"text": "Besides , since the probability of the complementary term may approach zero which makes it difficult to optimize , we actually adopt its lower bound in practice : log ( 1 \u2212 p ( y bow |", "entities": []}
{"text": "x , z ) )", "entities": []}
{"text": "= log ( 1 \u2212 | y", "entities": []}
{"text": "|", "entities": []}
{"text": "t=1", "entities": []}
{"text": "e fy t | V | j e f j ) \u2265 log ( | y | t=1 ( 1 \u2212 e fy t | V | j e f j ) )", "entities": []}
{"text": "( 11 ) where | V | is vocabulary size .", "entities": []}
{"text": "Our whole model can be trained in an end - to - end fashion .", "entities": []}
{"text": "To train the model , we first pre - train the word embedding using Glove ( ( Pennington et al , 2014 ) ) 1 .", "entities": []}
{"text": "Then modules of the model are jointly trained by optimizing the losses L f irst and L second of the two generation phases respectively .", "entities": []}
{"text": "During testing , diverse responses can be obtained by the two generation phases described above , where the distinctive latent variable z corresponding to each specific response is sampled from the prior probability network .", "entities": []}
{"text": "This process is illustrated in Figure 2 .", "entities": []}
{"text": "Capable of capturing the common feature of the response bag , the variable c is obtained from the mapping network and no intermediate utterance is required , which facilitates reducing the complexity of decoding .", "entities": []}
{"text": "Totally , there are 4 , 423 , 160 queryresponse pairs for training set and 10000 pairs for the validation and testing , where there are around 200k unique query in the training set and each query used in testing correlates with four responses respectively .", "entities": []}
{"text": "For preprocessing , we follow the conventional settings ( Shang et al , 2015 ) .", "entities": []}
{"text": "To comprehensively evaluate the quality of generated response utterances , we adopt both automatic and human evaluation metrics :", "entities": []}
{"text": "Distinctness : To distinguish safe and commonplace responses , the distinctness score ( Li et al , 2016a ) is designed to measure word - level diversity by counting the ratio of distinctive [ 1 , 2 ] - grams .", "entities": []}
{"text": "In our experiments , we adopt both Intra - Dist : the distinctness scores of multiple responses for a given query and Inter - Dist : the distinctness scores of generated responses of the whole testing set .", "entities": []}
{"text": "In our experiments , we apply three most commonly used strategies : Greedy matches each word of the reference with the most similar word in the evaluated sentence ; Average uses the average of word embed - Input \u706b\u5c71\u55b7\u53d1\u77ac\u95f4\u7684\u4e00\u4e9b\u58ee\u89c2\u666f\u8c61", "entities": []}
{"text": "\u3002 \u518d\u8fc7\u5341\u5206\u949f\u5c31\u8fdb\u5165win8\u65f6\u4ee3\uff0c\u6211\u662f\u7cfb\u7edf\u5347\u7ea7\u63a7 \u3002 Query These are some magnificent sights at the moment of the volcanic eruption .", "entities": []}
{"text": "There remain ten minutes before we entering the era of win8 .", "entities": []}
{"text": "I am a geek of system updating .", "entities": []}
{"text": "What application is this .", "entities": []}
{"text": "\u5982\u6b64\u8fd9\u822c\u8fd9\u822c\u6dfc\u5c0f \u3002", "entities": []}
{"text": "\u6211\u89c9\u5f97\u8fd9\u6837\u7684\u754c\u9762\u66f4\u50cfwindows8 \u3002", "entities": []}
{"text": "It is so so imperceptible .", "entities": []}
{"text": "I think interface like this looks more like windows8 .", "entities": []}
{"text": "Since multiple references exist , for each utterance to be evaluated , we compute its score with the most similar reference .", "entities": []}
{"text": "Human Evaluation with Case Analysis : As automatic evaluation metrics lose sight of the overall quality of a response ( Tao et al , 2018 ) , we also adopt human evaluation on 100 random samples to assess the generation quality with three independent aspects considered : relevance ( whether the reply is relevant to the query ) , diversity ( whether the reply narrates with diverse words ) and readability ( whether the utterance is grammatically formed ) .", "entities": []}
{"text": "Each property is assessed with a score from 1 ( worst ) to 5 ( best ) by three annotators .", "entities": []}
{"text": "The evaluation is conducted in a blind process with the utterance belonging unknown to the reviewers .", "entities": []}
{"text": "For the latent variable , we adopt dimensional size 256 and the component number of the mixture Gaussian for prior networks in WAE is set to 5 .", "entities": []}
{"text": "The size of the response bag is limited to 10 where the instances inside are randomly sampled for each mini - batch .", "entities": []}
{"text": "All the models are implemented with Pytorch 0.4.1 4 .", "entities": []}
{"text": "Table 1 shows our main experimental results , with baselines shown in the top and our models at the bottom .", "entities": []}
{"text": "The results show that our model ( Ours ) outperforms competitive baselines on various evaluation metrics .", "entities": []}
{"text": "However , the distinctness scores illustrate that these models fail to generate multiple diverse responses in spite of the diversitypromoting objective and responding mechanisms used .", "entities": []}
{"text": "We attribute this to that these models fail to consider multiple references for the same query , which may confuse the models and lead to a commonplace utterance .", "entities": []}
{"text": "In order to better study the quality of generated responses , we also report the human evaluation results in Table 2 .", "entities": []}
{"text": "As results show , although there remains a huge gap between existing methods and human performance ( the Gold ) , our model gains promising promotions over previous methods on generating appropriate responses with diverse expressions .", "entities": []}
{"text": "To better understand the effectiveness of each component in our model , we further conduct the ablation studies with results shown at the bottom of Table 1 .", "entities": []}
{"text": "Above all , to validate the effectiveness of the common feature , we remove the first generation stage and get the Ours - First model .", "entities": []}
{"text": "As results show , the discriminator facilitates extracting the common feature and yields more relevant responses to the input query afterward .", "entities": []}
{"text": "Table 3 illustrates two examples of generated replies to the input query got from the testing set .", "entities": []}
{"text": "In contrast , responses generated by our model show better quality , achieving both high relevance and diversity .", "entities": []}
{"text": "This demonstrates the ability of the two - step generation architecture .", "entities": []}
{"text": "For better insight into the procedure , we present the intermediately generated utterances which show that the feature extracted in the first stage can focus on some common and key aspects of the query and its possible responses , such as the \" amazing \" and \" software \" .", "entities": []}
{"text": "With the distinctive features sampled in the second generation phase , the model further revises the response and outputs multiple responses with diverse contents and expressions .", "entities": []}
{"text": "Recap that the common feature is expected to capture the correlations of different responses and serve as the base of a response bag from which different responses are further generated , as shown in Figure 1 .", "entities": []}
{"text": "To investigate the actual performances achieved by our model , we compute the distance between the input query / intermediate utterance and gold references / generated responses and present the results in Figure 4 .", "entities": []}
{"text": "As shown , intermediate utterances obtained in the first generation phase tend to approximate multiple responses with similar distances at the same time .", "entities": []}
{"text": "Comparing the generated responses and the references , we find that generated responses show both high relevant and irrelevant ratios , as the values near 0.00 and 1.00 show .", "entities": []}
{"text": "This actually agrees well with our observation that the model may sometimes rely heavily on or ignore the prior common feature information .", "entities": []}
{"text": "From a further comparison between the input query and the mid , we also observe that the intermediate utterance is more similar to final responses than the input query , which correlates well with our original intention shown in Figure 1 .", "entities": []}
{"text": "In this paper , we tackle the one - to - many queryresponse mapping problem in open - domain conversation and propose a novel two - step generation architecture with the correlation of multiple valid responses considered .", "entities": []}
{"text": "Jointly viewing the multiple responses as a response bag , the model extracts the common and distinct features of different responses in two generation phases respectively to output multiple diverse responses .", "entities": []}
{"text": "Experimental results illustrate the superior performance of the proposed model in generating diverse and appropriate responses compared to previous representative approaches .", "entities": []}
{"text": "However , the modeling of the common and distinct features of responses in our method is currently implicit and coarse - grained .", "entities": []}
{"text": "Directions of future work may be pursuing betterdefined features and easier training strategies .", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their constructive comments .", "entities": []}
{"text": "This work was supported by the National Key Research and Development Program of China ( No . 2017YFC0804001 ) , the National Science Foundation of China ( NSFC No . 61672058 ; NSFC No . 61876196 ) .", "entities": []}
{"text": "Rui Yan was sponsored by CCF - Tencent Open Research Fund and Alibaba Innovative Research ( AIR ) Fund .", "entities": []}
{"text": "SGNMT - A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies", "entities": []}
{"text": "SGNMT provides a generic interface to neural and symbolic scoring modules ( predictors ) with left - to - right semantic such as translation models like NMT , language models , translation lattices , n - best lists or other kinds of scores and constraints .", "entities": []}
{"text": "Predictors can be combined with other predictors to form complex decoding tasks .", "entities": []}
{"text": "SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations .", "entities": []}
{"text": "Adding new predictors or decoding strategies is particularly easy , making it a very efficient tool for prototyping new research ideas .", "entities": []}
{"text": "1", "entities": []}
{"text": "The software package supports a number of well - known frameworks , including TensorFlow 2 ( Abadi et al , 2016 ) ,", "entities": []}
{"text": "OpenFST ( Allauzen et al , 2007 ) , Blocks / Theano ( Bastien et al , 2012 ; van Merri\u00ebnboer et al , 2015 ) , and NPLM ( Vaswani et", "entities": []}
{"text": "al , 2013 )", "entities": []}
{"text": ".", "entities": []}
{"text": "The two central concepts in the SGNMT tool are predictors and decoders .", "entities": []}
{"text": "Predictors are scoring modules which define scores over the target language vocabulary given the current internal predictor state , the history , the source sentence , and external side information .", "entities": []}
{"text": "Scores from multiple , diverse predictors can be combined for use in decoding .", "entities": []}
{"text": "Decoders are search strategies which traverse the space spanned by the predictors .", "entities": []}
{"text": "SGNMT provides implementations of common search tree traversal algorithms like beam search .", "entities": []}
{"text": "Since decoders differ in runtime complexity and the kind of search errors they make , different decoders are appropriate for different predictor constellations .", "entities": []}
{"text": "Although the system is less than a year old , we have found it to be very flexible and easy for new researchers to adopt .", "entities": []}
{"text": "Our group has already integrated SGNMT into most of its research work .", "entities": []}
{"text": "We also find that SGNMT is very well - suited for teaching and student research projects .", "entities": []}
{"text": "3", "entities": []}
{"text": "The first project involved using SGNMT with OpenFST for applying subword models in SMT ( Gao , 2016 ) .", "entities": []}
{"text": "The second project developed automatic music composition by LSTMs where WFSAs were used to define the space of allowable chord progressions in ' Bach ' chorales ( Tomczak , 2016 that the chorales must obey .", "entities": []}
{"text": "This second project in particular demonstrates the versatility of the approach .", "entities": []}
{"text": "For the current , 2016 - 17 academic year , SGNMT is being used heavily in two courses .", "entities": []}
{"text": "SGNMT consequently emphasizes flexibility and extensibility by providing a common interface to a wide range of constraints or models used in MT research .", "entities": []}
{"text": "The concept facilitates quick prototyping of new research ideas .", "entities": []}
{"text": "Our platform aims to minimize the effort required for implementation ; decoding speed is secondary as optimized code for production systems can be produced once an idea has been proven successful in the SGNMT framework .", "entities": []}
{"text": "In SGNMT , scores are assigned to partial hypotheses via one or many predictors .", "entities": []}
{"text": "One predictor usually has a single responsibility as it represents a single model or type of constraint .", "entities": []}
{"text": "Predictors need to implement the following methods : initialize ( src sentence )", "entities": []}
{"text": "Initialize the predictor state using the source sentence . get state ( ) Get the internal predictor state . set state ( state ) Set the internal predictor state .", "entities": []}
{"text": "predict next ( ) Given the internal predictor state , produce the posterior over target tokens for the next position .", "entities": []}
{"text": "Supports Blocks / Theano ( Bastien et al , 2012 ; van Merri\u00ebnboer et al , 2015 ) and TensorFlow ( Abadi et al , 2016 ) .", "entities": []}
{"text": "fst Predictor for rescoring deterministic lattices ( Heafield et al , 2013 ; Stolcke et al , 2002 ) toolkit .", "entities": []}
{"text": "nplm Neural n - gram language models based on NPLM ( Vaswani et al , 2013 ) .", "entities": []}
{"text": "rnnlm Integrates RNN language models with TensorFlow as described by Zaremba", "entities": []}
{"text": "et al ( 2014 ) .", "entities": []}
{"text": "forced Forced decoding with a single reference .", "entities": []}
{"text": "forcedlst n - best list rescoring .", "entities": []}
{"text": "bow Restricts the search space to a bag of words with or without repetition consume ( token ) Update the internal predictor state by adding token to the current history .", "entities": []}
{"text": "The structure of the predictor state and the implementations of these methods differ substantially between predictors .", "entities": []}
{"text": "Tab . 2 lists all predictors which are currently implemented .", "entities": []}
{"text": "We also included two examples ( word count and UNK count ) which do not have a natural left - to - right semantic but can still be represented as predictors .", "entities": []}
{"text": "SGNMT allows combining any number of predictors and even multiple instances of the same predictor type .", "entities": []}
{"text": "In case of multiple predictors we combine the predictor scores in a linear model .", "entities": []}
{"text": "The following list illustrates that various interesting decoding tasks can be formulated as predictor combinations .", "entities": []}
{"text": "nmt : A single NMT predictor represents pure NMT decoding .", "entities": []}
{"text": "nmt , nmt , nmt : Using multiple NMT predictors is a natural way to represent ensemble decoding ( Hansen and Salamon , 1990 ; in our framework .", "entities": []}
{"text": "fst , nmt : NMT decoding constrained to an FST .", "entities": []}
{"text": "This can be used for neural lattice rescoring or other kinds of constraints , for example in the context of source side simplification in MT or chord progressions in ' Bach ' ( Tomczak , 2016 ) .", "entities": []}
{"text": "The fst predictor can also be used to restrict the output of character - based or subword - unit - based NMT to a large word - level vocabulary encoded as FSA .", "entities": []}
{"text": "nmt , rnnlm , srilm , nplm : Combining NMT with three kinds of language models : An RNNLM ( Zaremba et al , 2014 ) , a Kneser - Ney n - gram LM ( Heafield et al , 2013 ; Stolcke et al , 2002 ) , and a feedforward neural network LM ( Vaswani et", "entities": []}
{"text": "al , 2013 ) .", "entities": []}
{"text": "Decoders are algorithms to search for the highest scoring hypothesis .", "entities": []}
{"text": "The list of predictors determines how ( partial ) hypotheses are scored by implementing the methods initialize ( ) , get state ( ) , set state ( ) , predict next ( ) , and consume ( ) .", "entities": []}
{"text": "The Decoder class implements versions of these methods which apply to all predictors in the list .", "entities": []}
{"text": "initialize ( ) is always called prior to decoding a new sentence .", "entities": []}
{"text": "Many popular search strategies can be described via the remaining methods get state ( ) , set state ( ) , predict next ( ) , and consume ( ) .", "entities": []}
{"text": "Algs .", "entities": []}
{"text": "1 and 2 show how to define greedy and beam decoding in this way .", "entities": []}
{"text": "45 Tab . 3 contains a list of currently implemented decoders .", "entities": []}
{"text": "The UML diagram in Fig .", "entities": []}
{"text": "1 illustrates the relation between decoders and predictors .", "entities": []}
{"text": "Algorithm 1 Greedy ( src sen ) 1 : initialize ( src sen ) 2 : h < s > 3 : repeat 4 : P predict next ( ) 5 : ( t , c ) arg max ( t , c ) P c 6 : h h t 7 : consume ( t ) 8 : until t", "entities": []}
{"text": "= < /s > 9 : return h NMT batch decoding The flexibility of the predictor framework comes with degradation in decoding time .", "entities": []}
{"text": "SGNMT provides two ways of speeding up pure NMT decoding , especially on the GPU .", "entities": []}
{"text": "The vanilla decoding strategy exposes the beam search implementation in Blocks ( van Merri\u00ebnboer et al , 2015 ) which processes all active hypotheses in the beam in parallel .", "entities": []}
{"text": "We also implemented a beam decoder version which decodes multiple sentences at once ( batch decoding ) rather than in a sequential order .", "entities": []}
{"text": "Batch decoding is potentially more efficient since larger batches can make better use of GPU parallelism .", "entities": []}
{"text": "The key concepts of our batch decoder implementation are : We use a scheduler running on a separate CPU thread to construct large batches of computation ( GPU jobs ) from multiple sentences and feeding them to the jobs queue .", "entities": []}
{"text": "The GPU is operated by a single thread which communicates with the CPU scheduler thread via queues containing jobs .", "entities": []}
{"text": "This thread is only responsible for retrieving jobs in the jobs queue , computing them , and putting them in the jobs results queue , minimizing the down - time of GPU computation .", "entities": []}
{"text": "Yet another CPU thread is responsible for processing the results computed on the GPU H next H next \u222a ( t , c ) P ( h t , c + c , s ) 9 : end for 10 : H 11 : for all ( h , c , s ) n - best ( H next ) do end for 16 : until Best hypothesis in H ends with < /s > 17 : return Best hypothesis in H in the job results queue , e.g. by getting the n - best words from the posteriors .", "entities": []}
{"text": "Processed jobs are sent back to the CPU scheduler where they are reassembled into new jobs .", "entities": []}
{"text": "6 This decoding speed seems to be slightly faster than sequential decoding with high - performance NMT decoders like Marian - NMT ( Junczys - Dowmunt et al , 2016 ) with reported decoding speeds of 865 words per second .", "entities": []}
{"text": "7", "entities": []}
{"text": "However , batch decoding with Marian - NMT is much faster reaching over 4 , 500 words per second .", "entities": []}
{"text": "8 We think that these differences are mainly due to the limited multithreading support and performance in Python especially when using external libraries as opposed to the highly optimized C++ code in Marian - NMT .", "entities": []}
{"text": "We did not push for even faster decoding as speed is not a major design goal of SGNMT .", "entities": []}
{"text": "Note that batch decoding bypasses the predictor framework and can only be used for pure NMT decoding .", "entities": []}
{"text": "Ensembling with models at multiple tokenization levels SGNMT allows masking predictors with alternative sets of modelling units .", "entities": []}
{"text": "The conversion between the tokenization schemes of different predictors is defined with FSTs .", "entities": []}
{"text": "Masking is transparent to the decoding strategy as predictors are replaced by a special wrapper ( fsttok ) that uses the masking FST to translate predict next ( ) and consume ( ) calls to ( a series of ) predictor calls with alternative tokens .", "entities": []}
{"text": "The syncbeam variation of beam search compares competing hypotheses only after consuming a special word boundary symbol rather than after each token .", "entities": []}
{"text": "This allows combining scores at the word level even when using models with multiple levels of tokenization .", "entities": []}
{"text": "System - level combination We showed in Sec . 2.1 how to formulate NMT ensembling as a set of NMT predictors .", "entities": []}
{"text": "Ensembling averages the individual model scores in each decoding step .", "entities": []}
{"text": "Alternatively , system - level combination decodes the entire sentence with each model separately , and selects the best scoring complete hypothesis over all models .", "entities": []}
{"text": "In our experiments , system - level combination is not as effective as en - 1080 ) , ( b ) a different training and test set , ( c ) a slightly different network architecture , and ( d ) words rather than subword units .", "entities": []}
{"text": "8 https://marian - nmt.github.io/ features/ sembling but still leads to moderate gains for pure NMT .", "entities": []}
{"text": "However , a trivial implementation which selects the best translation in a postprocessing step after separate decoding runs is slow .", "entities": []}
{"text": "The sepbeam decoding strategy reduces the runtime of system - level combination to the single system level .", "entities": []}
{"text": "The strategy applies only one predictor rather than a linear combination of all predictors to expand a hypothesis .", "entities": []}
{"text": "The single predictor is linked by the parent hypothesis .", "entities": []}
{"text": "The initial stack in sepbeam contains hypotheses for each predictor ( i.e. system ) rather than only one as in normal beam search .", "entities": []}
{"text": "Iterative beam search Normal beam search is difficult to use in a time - constrained setting since the runtime depends on the target sentence length which is a priori not known , and it is therefore hard to choose the right beam size beforehand .", "entities": []}
{"text": "The bucket search algorithm sidesteps the problem of setting the beam size by repeatedly performing small beam search passes until a fixed computational budget is exhausted .", "entities": []}
{"text": "Bucket search produces an initial hypothesis very quickly , and keeps the partial hypotheses for each length in buckets .", "entities": []}
{"text": "Subsequent beam search passes refine the initial hypothesis by iteratively updating these buckets .", "entities": []}
{"text": "Our initial experiments suggest that bucket search often performs on a similar level as standard beam search with the benefit of being able to support hard time constraints .", "entities": []}
{"text": "Unlike beam search , bucket search lends itself to risk - free ( i.e. admissible ) pruning since all partial hypotheses worse than the current best complete hypothesis can be discarded .", "entities": []}
{"text": "This paper presented our SGNMT platform for prototyping new approaches to MT which involve both neural and symbolic models .", "entities": []}
{"text": "SGNMT supports a number of different models and constraints via a common interface ( predictors ) , and various search strategies ( decoders ) .", "entities": []}
{"text": "Furthermore , SGNMT focuses on minimizing the implementation effort for adding new predictors and decoders by decoupling scoring modules from each other and from the search algorithm .", "entities": []}
{"text": "SGNMT is actively being used for teaching and research and we welcome contributions to its development , for example by implementing new predictors for using models trained with other frameworks and tools .", "entities": []}
{"text": "This work was supported by the U.K. Engineering and Physical Sciences Research Council ( EPSRC grant EP / L027623/1 ) .", "entities": []}
{"text": "Double Perturbation : On the Robustness of Robustness and Counterfactual Bias Evaluation", "entities": []}
{"text": "Robustness and counterfactual bias are usually evaluated on a test dataset .", "entities": []}
{"text": "However , are these evaluations robust ?", "entities": []}
{"text": "If the test dataset is perturbed slightly , will the evaluation results keep the same ?", "entities": []}
{"text": "In this paper , we propose a \" double perturbation \" framework to uncover model weaknesses beyond the test dataset .", "entities": []}
{"text": "The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data , and then diagnoses the prediction change regarding a single - word substitution .", "entities": []}
{"text": "We apply this framework to study two perturbation - based approaches that are used to analyze models ' robustness and counterfactual bias in English .", "entities": []}
{"text": "( 1 ) For robustness , we focus on synonym substitutions and identify vulnerable examples where prediction can be altered .", "entities": []}
{"text": "Our proposed attack attains high success rates ( 96.0 % - 99.8 % ) in finding vulnerable examples on both original and robustly trained CNNs and Transformers .", "entities": []}
{"text": "( 2 ) For counterfactual bias , we focus on substituting demographic tokens ( e.g. , gender , race ) and measure the shift of the expected prediction among constructed sentences .", "entities": []}
{"text": "Our method is able to reveal the hidden model biases not directly shown in the test dataset .", "entities": []}
{"text": "Our code is available at https://github.com/chong - z/ nlp - second - order - attack .", "entities": []}
{"text": "Recent studies show that NLP models are vulnerable to adversarial perturbations .", "entities": []}
{"text": "A seemingly \" invariance transformation \" ( a.k.a . adversarial perturbation ) such as synonym substitutions ( Alzantot et al , 2018 ; Zang et al , 2020 ) or syntax - guided paraphrasing ( Iyyer et al , 2018 ; Huang and Chang , 2021 ) can alter the prediction .", "entities": []}
{"text": "To mitigate the model vulnerability , robust training methods have been proposed and shown effective ( Miyato et al , 2017 ;", "entities": []}
{"text": "Jia et al , 2019 ; Huang et al , 2019 ; Zhou et", "entities": []}
{"text": "al , 2020 ) .", "entities": []}
{"text": "= \" a deep and meaningful film ( movie ) . \"", "entities": []}
{"text": "= \" a short and moving film ( movie ) . \"", "entities": []}
{"text": "73 % positive ( 70 % negative ) ( 99 % positive ) 99 % positive perturb", "entities": []}
{"text": "Figure 1 : A vulnerable example beyond the test dataset .", "entities": []}
{"text": "Numbers on the bottom right are the sentiment predictions for film and movie .", "entities": []}
{"text": "Its prediction can be altered by the substitution ( vulnerable ) .", "entities": []}
{"text": "In most studies , model robustness is evaluated based on a given test dataset or synthetic sentences constructed from templates ( Ribeiro et al , 2020 ) .", "entities": []}
{"text": "Specifically , the robustness of a model is often evaluated by the ratio of test examples where the model prediction can not be altered by semantic - invariant perturbation .", "entities": []}
{"text": "We refer to this type of evaluations as the first - order robustness evaluation .", "entities": []}
{"text": "In that case , adversarial examples still exist even if first - order attacks can not find any of them from the given test dataset .", "entities": []}
{"text": "The existence of such examples exposes weaknesses in models ' understanding and presents challenges for model deployment .", "entities": []}
{"text": "Fig .", "entities": []}
{"text": "1 illustrates an example .", "entities": []}
{"text": "In this paper , we propose the double perturbation framework for evaluating a stronger notion of second - order robustness .", "entities": []}
{"text": "Given a test dataset , we consider a model to be second - order robust if there is no vulnerable example that can be identified in the neighborhood of given test instances ( 2.2 ) .", "entities": []}
{"text": "In particular , our framework first perturbs the test set to construct the neighborhood , and then diagnoses the robustness regarding a single - word synonym substitution .", "entities": []}
{"text": "Taking Fig .", "entities": []}
{"text": "We apply the proposed framework and quantify second - order robustness through two second - order attacks ( 3 ) .", "entities": []}
{"text": "al , 2020 ) can achieve high robustness under strong attacks ( Alzantot et al , 2018 ; Garg and Ramakrishnan , 2020 )", "entities": []}
{"text": "( 23.0 % - 71.6 % success rates ) , for around 96.0 % of the test examples our attacks can find a vulnerable example by perturbing 1.3 words on average .", "entities": []}
{"text": "This finding indicates that these robustly trained models , despite being first - order robust , are not second - order robust .", "entities": []}
{"text": "Furthermore , we extend the double perturbation framework to evaluate counterfactual biases ( Kusner et al , 2017 ) ( 4 ) in English .", "entities": []}
{"text": "When the test dataset is small , our framework can help improve the evaluation robustness by revealing the hidden biases not directly shown in the test dataset .", "entities": []}
{"text": "Intuitively , a fair model should make the same prediction for nearly identical examples referencing different groups ( Garg et al , 2019 ) with different protected attributes ( e.g. , gender , race ) .", "entities": []}
{"text": "In our evaluation , we consider a model biased if substituting tokens associated with protected attributes changes the expected prediction , which is the average prediction among all examples within the neighborhood .", "entities": []}
{"text": "For instance , a toxicity classifier is biased if it tends to increase the toxicity if we substitute straight gay in an input sentence ( Dixon et al , 2018 ) .", "entities": []}
{"text": "In the experiments , we evaluate the expected sentiment predictions on pairs of protected tokens ( e.g. , ( he , she ) , ( gay , straight ) ) , and demonstrate that our method is able to reveal the hidden model biases .", "entities": []}
{"text": "Our main contributions are : ( 1 ) We propose the double perturbation framework to diagnose the robustness of existing robustness and fairness evaluation methods .", "entities": []}
{"text": "Diamond area denotes invariance transformations .", "entities": []}
{"text": "order robustness and reveal the models ' vulnerabilities that can not be identified by previous attacks .", "entities": []}
{"text": "( 3 ) We propose a counterfactual bias evaluation method to reveal the hidden model bias based on our double perturbation framework .", "entities": []}
{"text": "In this section , we describe the double perturbation framework which focuses on identifying vulnerable examples within a small neighborhood of the test dataset .", "entities": []}
{"text": "The framework consists of a neighborhood perturbation and a word substitution .", "entities": []}
{"text": "We start with defining word substitutions .", "entities": []}
{"text": "We focus our study on word - level substitution , where existing works evaluate robustness and counterfactual bias by directly perturbing the test dataset .", "entities": []}
{"text": "For instance , adversarial attacks alter the prediction by making synonym substitutions , and the fairness literature evaluates counterfactual fairness by substituting protected tokens .", "entities": []}
{"text": "We integrate the word substitution strategy into our framework as the component for evaluating robustness and fairness .", "entities": []}
{"text": "For simplicity , we consider a single - word substitution and denote it with the operator .", "entities": []}
{"text": "Let X \u2286 V l be the input space where V is the vocabulary and l is the sentence length ,", "entities": []}
{"text": "p = ( p ( 1 ) , p ( 2 ) )", "entities": []}
{"text": "p.", "entities": []}
{"text": "Taking Fig .", "entities": []}
{"text": "1 as an example ,", "entities": []}
{"text": "= a deep and meaningful movie .", "entities": []}
{"text": "Now we introduce other components in our framework .", "entities": []}
{"text": "Instead of applying the aforementioned word substitutions directly to the original test dataset , our framework perturbs the test dataset within a small neighborhood to construct similar natural sentences .", "entities": []}
{"text": "This is to identify vulnerable examples with respect to the model .", "entities": []}
{"text": "Note that examples in the neighborhood are not required to have the same meaning as the original example , since we only study the prediction difference caused by applying synonym substitution p ( 2.1 ) .", "entities": []}
{"text": "Constraints on the neighborhood .", "entities": []}
{"text": "X natural , ( 1 )", "entities": []}
{"text": "where Ball k", "entities": []}
{"text": "(", "entities": []}
{"text": "= { x", "entities": []}
{"text": "|", "entities": []}
{"text": "( i.e. , at most k different tokens ) , and X natural denotes natural sentences that satisfy a certain language model score which will be discussed next .", "entities": []}
{"text": "Construction with masked language model .", "entities": []}
{"text": "As shown in Algorithm 1 , the construction employs a recursive approach and replaces one token at a time .", "entities": []}
{"text": "To ensure the naturalness , we keep the top 20 tokens for each mask with the largest logit ( subject to a threshold , Line 9 ) .", "entities": []}
{"text": "Then , the algorithm constructs neighborhood sentences by replacing the mask with found tokens .", "entities": []}
{"text": "We use the notationx in the following sections to denote the constructed sentences within the neighborhood .", "entities": []}
{"text": "L 10 Tnew { t | l > lmin , ( t , l ) T \u00d7 L } ; 11 Xnew { x0 |", "entities": []}
{"text": "t , t Tnew } ; Construct new sentences by replacing the ith token .", "entities": []}
{"text": "12 Xneighbor Xneighbor \u222a Xnew ; 13 return Xneighbor ;", "entities": []}
{"text": "With the proposed double perturbation framework , we design two black - box attacks 1 to identify vulnerable examples within the neighborhood of the test set .", "entities": []}
{"text": "We aim at evaluating the robustness for inputs beyond the test set .", "entities": []}
{"text": "Adversarial attacks search for small and invariant perturbations on the model input that can alter the prediction .", "entities": []}
{"text": "To simplify the discussion , in the following , we take a binary classifier f", "entities": []}
{"text": "Here", "entities": []}
{"text": "= p 1 p l denotes a series of substitutions .", "entities": []}
{"text": "Second - order attacks study the prediction difference caused by applying p.", "entities": []}
{"text": "For notation convenience we define the prediction difference F ( x ; p ) :", "entities": []}
{"text": "x0 = a deep and meaningful film .", "entities": []}
{"text": "p = film , movie", "entities": []}
{"text": "x", "entities": []}
{"text": "( i = 2 ) a short and moving film ( movie ) .", "entities": []}
{"text": "a slow and moving film ( movie ) .", "entities": []}
{"text": "a dramatic or meaningful film ( movie ) .", "entities": []}
{"text": "p alters the prediction . x0", "entities": []}
{"text": "= \" a short and moving film ( movie ) . \"", "entities": []}
{"text": "( 70 % negative )", "entities": []}
{"text": "73 % positive Figure 3 : The attack flow for SO - Beam ( Algorithm 2 ) .", "entities": []}
{"text": "Green boxes in the middle show intermediate sentences , and f soft ( x ) denotes the probability outputs for film and movie .", "entities": []}
{"text": "= f", "entities": []}
{"text": "( x p ) \u2212 f ( x ) .", "entities": []}
{"text": "( 2 ) Taking Fig .", "entities": []}
{"text": "= f ( ... moving movie . )", "entities": []}
{"text": "\u2212 f ( ... moving film . )", "entities": []}
{"text": "= \u22121 .", "entities": []}
{"text": "= f", "entities": []}
{"text": "Follow Alzantot", "entities": []}
{"text": "et al ( 2018 ) , we choose p from a predefined list of counter - fitted synonyms ( Mrk\u0161i\u0107 et al , 2016 ) that maximizes |", "entities": []}
{"text": "f soft ( p ( 2 ) )", "entities": []}
{"text": "\u2212 f soft ( p ( 1 ) )", "entities": []}
{"text": "| .", "entities": []}
{"text": "Here f soft ( x ) :", "entities": []}
{"text": "= argmax x", "entities": []}
{"text": "( 3 )", "entities": []}
{"text": "A naive approach for solving Eq .", "entities": []}
{"text": "The enumeration finds the smallest perturbation , but is only applicable for small k ( e.g. , k \u2264 2 ) given the exponential complexity .", "entities": []}
{"text": "Beam - search attack ( SO - Beam ) .", "entities": []}
{"text": "The efficiency can be improved by utilizing the probability output , where we solve Eq .", "entities": []}
{"text": "Minimizing Eq .", "entities": []}
{"text": "We run at most k iterations , and stop earlier if we find a vulnerable example .", "entities": []}
{"text": "We provide the detailed implementation in Algorithm 2 and a flowchart in Fig .", "entities": []}
{"text": "3 . 4 for i 1 , . . .", "entities": []}
{"text": ", k do 5 Xnew x X beam Neighbor 1 ( x ) ; 6x0 argmax x Xnew | F ( x ; p ) | ; 7 if F ( x0 ; p )", "entities": []}
{"text": ", X ( \u03b2\u22121 ) new } ; Keep the best beam .", "entities": []}
{"text": "In this section , we evaluate the second - order robustness of existing models and show the quality of our constructed vulnerable examples .", "entities": []}
{"text": "We follow the setup from the robust training literature ( Jia et al , 2019 ;", "entities": []}
{"text": "Xu et", "entities": []}
{"text": "al , 2020 ) and experiment with both the base ( non - robust ) and robustly trained models .", "entities": []}
{"text": "Genetic : 56 % Positive Adversarial Example : in its best moment , recalling a naughty high school production of lubrication , unless benefit of song .", "entities": []}
{"text": "BAE : 56 % Positive Adversarial Example : in its best moments , resembles a great high school production of grease , without benefit of song .", "entities": []}
{"text": "SO - Enum and SO - Beam ( ours ) :", "entities": []}
{"text": "60 % Negative ( 67 % Positive )", "entities": []}
{"text": "Vulnerable Example : in its best moments , resembles a bad ( unhealthy ) high school production of musicals , without benefit of song .", "entities": []}
{"text": "Table 1 : Sampled attack results on the robust BoW.", "entities": []}
{"text": "For Genetic and BAE the goal is to find an adversarial example that alters the original prediction , whereas for SO - Enum and SO - Beam the goal is to find a vulnerable example beyond the test set such that the prediction can be altered by substituting bad unhealthy .", "entities": []}
{"text": "Base models .", "entities": []}
{"text": "Attack success rate ( second - order ) .", "entities": []}
{"text": "We also quantify second - order robustness through attack success rate , which measures the ratio of test examples that a vulnerable example can be found .", "entities": []}
{"text": "We consider the most challenging setup and use patch words p from the same set of counter - fitted synonyms as robust models ( they are provably robust to these synonyms on the test set ) .", "entities": []}
{"text": "We also provide a random baseline to validate the effectiveness of minimizing Eq .", "entities": []}
{"text": "( 4 ) ( Appendix A.1 ) .", "entities": []}
{"text": "We", "entities": []}
{"text": "We experiment with the validation split ( 872 examples ) on a single RTX 3090 .", "entities": []}
{"text": "We provide additional running time results in Appendix A.3 .", "entities": []}
{"text": "Table 1 provides an example of the attack result where all attacks are successful ( additional examples in Appendix A.5 ) .", "entities": []}
{"text": "As shown , our secondorder attacks find a vulnerable example by replacing grease musicals , and the vulnerable example has different predictions for bad and unhealthy .", "entities": []}
{"text": "Note that , Genetic and BAE have different objectives from second - order attacks and focus on finding the adversarial example .", "entities": []}
{"text": "Next we discuss the results from two perspectives .", "entities": []}
{"text": "Second - order robustness .", "entities": []}
{"text": "We observe that existing robustly trained models are not second - order robust .", "entities": []}
{"text": "As shown in Furthermore , applying existing attacks on the vulnerable examples constructed by our method will lead to much smaller perturbations .", "entities": []}
{"text": "As a reference , on the robustly trained CNN , Genetic attack constructs adversarial examples by perturbing 2.7 words on average ( starting from the input examples ) .", "entities": []}
{"text": "However , if Genetic starts from our vulnerable examples , it would only need to perturb a single word ( i.e. , the patch words p ) to alter the prediction .", "entities": []}
{"text": "These results demonstrate the weakness of the models ( even robustly trained ) for those inputs beyond the test set .", "entities": []}
{"text": "We perform human evaluation on the examples constructed by SO - Beam .", "entities": []}
{"text": "Specifically , we randomly", "entities": []}
{"text": "In addition to evaluating second - order robustness , we further extend the double perturbation framework ( 2 ) to evaluate counterfactual biases by setting p to pairs of protected tokens .", "entities": []}
{"text": "We show that our method can reveal the hidden model bias .", "entities": []}
{"text": "In contrast to second - order robustness , where we consider the model vulnerable as long as there exists one vulnerable example , counterfactual bias focuses on the expected prediction , which is the average prediction among all examples within the neighborhood .", "entities": []}
{"text": "We consider a model biased if the expected predictions for protected groups are different ( assuming the model is not intended to discriminate between these groups ) .", "entities": []}
{"text": "For instance , a sentiment classifier is biased if the expected prediction for inputs containing woman is more positive ( or negative ) than inputs containing man .", "entities": []}
{"text": "Such bias is harmful as they may make unfair decisions based on protected attributes , for example in situations such as hiring and college admission .", "entities": []}
{"text": "Counterfactual token bias .", "entities": []}
{"text": "We study a narrow case of counterfactual bias , where counterfactual examples are constructed by substituting protected tokens in the input .", "entities": []}
{"text": "A naive approach of measuring this bias is to construct counterfactual examples directly from the test set , however such evaluation may not be robust since test examples are only a small subset of natural sentences .", "entities": []}
{"text": "Formally , let p be a pair of protected tokens such as ( he , she ) or ( Asian , American ) , X test \u2286 X p be a test set ( as in 2.1 ) , we define counterfactual token bias by : B p , k :", "entities": []}
{"text": "= E x Neighbor k ( Xtest ) F soft ( x ; p ) .", "entities": []}
{"text": "( 5 ) We calculate Eq .", "entities": []}
{"text": "( 5 ) through an enumeration across all natural sentences within the neighborhood .", "entities": []}
{"text": "7 Here Neighbor k ( X test )", "entities": []}
{"text": "= x Xtest Neighbor k ( x ) denotes the union of neighborhood examples ( of distance k ) around the test set , and F soft ( x ; p ) : X \u00d7 V 2 [ \u22121 , 1 ] denotes the difference between probability outputs f soft ( similar to Eq .", "entities": []}
{"text": "( 2 ) )", "entities": []}
{"text": ": F soft ( x ; p ) :", "entities": []}
{"text": "= f soft ( x p )", "entities": []}
{"text": "\u2212 f soft ( x ) .", "entities": []}
{"text": "The model is unbiased on p if B p ,", "entities": []}
{"text": "Fig .", "entities": []}
{"text": "4 illustrates the distribution of ( x , x p ) for both an unbiased model and a biased model .", "entities": []}
{"text": "The aforementioned neighborhood construction does not introduce additional bias .", "entities": []}
{"text": "( 6 ) as we only care about the prediction difference of replacing he she .", "entities": []}
{"text": "The construction has no information about the model objective , thus it would be difficult to bias f soft ( x ) and f soft ( x p ) differently .", "entities": []}
{"text": "In this section , we use gender bias as a running example , and demonstrate the effectiveness of our method by revealing the hidden model bias .", "entities": []}
{"text": "We provide additional results in Appendix A.4 .", "entities": []}
{"text": "We focus on binary gender bias and set p to pairs of gendered pronouns from Zhao et al ( 2018a ) .", "entities": []}
{"text": "Base Model .", "entities": []}
{"text": "Debiased Model .", "entities": []}
{"text": "Data - augmentation with gender swapping has been shown effective in mitigating gender bias ( Zhao et al , 2018a .", "entities": []}
{"text": "We augment the training split by swapping all male entities with the corresponding female entities and vice - versa .", "entities": []}
{"text": "Metrics .", "entities": []}
{"text": ", 3 .", "entities": []}
{"text": "Filtered test set .", "entities": []}
{"text": "To investigate whether our method is able to reveal model bias that was hidden in the test set , we construct a filtered test set on which the bias can not be observed directly .", "entities": []}
{"text": "Let X test be the original validation split , we construct X filter by the equation below and empirically set = 0.005 .", "entities": []}
{"text": "We provide statistics in Table 5 . X filter :", "entities": []}
{"text": "= { x", "entities": []}
{"text": "| | F soft ( x ; p ) | <", "entities": []}
{"text": ", x X test } .", "entities": []}
{"text": "Our method is able to reveal the hidden model bias on X filter , which is not visible with naive measurements .", "entities": []}
{"text": "This observed bias is in line with the measurements on the original X test ( Appendix A.4 ) , indicating that we reveal the correct model bias .", "entities": []}
{"text": "To demonstrate how our method reveals hidden bias , we conduct a case study with p = ( actor , actress ) and show the relationship between the bias B p , k and the neighborhood distance k.", "entities": []}
{"text": "We present the histograms for F soft ( x ; p ) in Fig .", "entities": []}
{"text": "6 and plot the corresponding B p , k vs. k in the right - most panel .", "entities": []}
{"text": "Surprisingly , for the base model , the bias is Figure 6 : Left and Middle : Histograms for F soft", "entities": []}
{"text": "( x ; p ) ( x - axis ) with p = ( actor , actress ) .", "entities": []}
{"text": "Right :", "entities": []}
{"text": "This is because the naive approach only has two test examples ( Table 5 ) thus the measurement is not robust .", "entities": []}
{"text": "As discussed in 4.1 , the neighborhood construction does not introduce additional bias , and these results demonstrate the effectiveness of our method in revealing hidden model bias .", "entities": []}
{"text": "First - order robustness evaluation .", "entities": []}
{"text": "A line of work has been proposed to study the vulnerability of natural language models , through transformations such as character - level perturbations ( Ebrahimi et al , 2018 ) , word - level perturbations ( Jin et al , 2019 ;", "entities": []}
{"text": "Ren et al , 2019 ; Cheng et al , 2020 ; Li et al , 2020 ) , prepending or appending a sequence ( Jia and Liang , 2017 ; Wallace et al , 2019a ) , and generative models ( Zhao et al , 2018b ) .", "entities": []}
{"text": "They focus on constructing adversarial examples from the test set that alter the prediction , whereas our methods focus on finding vulnerable examples beyond the test set whose prediction can be altered .", "entities": []}
{"text": "Robustness beyond the test set .", "entities": []}
{"text": "Several works have studied model robustness beyond test sets but mostly focused on computer vision tasks .", "entities": []}
{"text": "Zhang et al ( 2019 ) demonstrate that a robustly trained model could still be vulnerable to small perturbations if the input comes from a distribution only slightly different than a normal test set ( e.g. , images with slightly different contrasts ) .", "entities": []}
{"text": "Hendrycks and Dietterich ( 2019 ) study more sources of common corruptions such as brightness , motion blur and fog .", "entities": []}
{"text": "Unlike in computer vision where simple image transformations can be used , in our natural language setting , generating a valid example beyond test set is more challenging because language semantics and grammar must be maintained .", "entities": []}
{"text": "Counterfactual fairness .", "entities": []}
{"text": "Kusner et al ( 2017 ) propose counterfactual fairness and consider a model fair if changing the protected attributes does not affect the distribution of prediction .", "entities": []}
{"text": "We follow the definition and focus on evaluating the counterfactual bias between pairs of protected tokens .", "entities": []}
{"text": "Existing literature quantifies fairness on a test dataset or through templates ( Feldman et al , 2015 ;", "entities": []}
{"text": "Kiritchenko and Mohammad , 2018 ; May et al , 2019 ; .", "entities": []}
{"text": "For instance , Garg et al ( 2019 ) quantify the absolute counterfactual token fairness gap on the test set ; Prabhakaran et al ( 2019 ) study perturbation sensitivity for named entities on a given set of corpus .", "entities": []}
{"text": "Wallace et", "entities": []}
{"text": "al ( 2019b ) ; Sheng et al ( 2019Sheng et al ( , 2020 study how language generation models respond differently to prompt sentences containing mentions of different demographic groups .", "entities": []}
{"text": "In contrast , our method quantifies the bias on the constructed neighborhood .", "entities": []}
{"text": "This work proposes the double perturbation framework to identify model weaknesses beyond the test dataset , and study a stronger notion of robustness and counterfactual bias .", "entities": []}
{"text": "We hope that our work can stimulate the research on further improving the robustness and fairness of natural language models .", "entities": []}
{"text": "Intended use .", "entities": []}
{"text": "One primary goal of NLP models is the generalization to real - world inputs .", "entities": []}
{"text": "However , existing test datasets and templates are often not comprehensive , and thus it is difficult to evaluate real - world performance ( Recht et al , 2019 ;", "entities": []}
{"text": "Ribeiro et al , 2020 ) .", "entities": []}
{"text": "Our work sheds a light on quantifying performance for inputs beyond the test dataset and help uncover model weaknesses prior to the realworld deployment .", "entities": []}
{"text": "Misuse potential .", "entities": []}
{"text": "Therefore , it is essential to study how to improve the robustness of NLP models against second - order attacks .", "entities": []}
{"text": "Limitations .", "entities": []}
{"text": "While the core idea about the double perturbation framework is general , in 4 , we consider only binary gender in the analysis of counterfactual fairness due to the restriction of the English corpus we used , which only have words associated with binary gender such as he / she , waiter / waitress , etc .", "entities": []}
{"text": "To validate the effectiveness of minimizing Eq .", "entities": []}
{"text": "( 4 ) , we also experiment on a second - order baseline that constructs vulnerable examples by randomly replacing up to 6 words .", "entities": []}
{"text": "We use the same masked language model and threshold as SO - Beam such that they share a similar neighborhood .", "entities": []}
{"text": "Despite being a second - order attack , the random baseline has low attack success rates thus demonstrates the effectiveness of SO - Beam .", "entities": []}
{"text": "We randomly select 100 successful attacks from SO - Beam and consider four types of examples ( for a total of 400 examples ) :", "entities": []}
{"text": "The original examples with and without synonym substitution p , and the vulnerable examples with and without synonym substitution p.", "entities": []}
{"text": "For each example , we annotate the naturalness and sentiment separately as described below .", "entities": []}
{"text": "Naturalness of vulnerable examples .", "entities": []}
{"text": "We ask the annotators to score the likelihood of being an original example ( i.e. , not altered by computer ) based on grammar correctness and naturalness , with a Likert scale of 1 - 5 : ( 1 ) Sure adversarial example .", "entities": []}
{"text": "( 2 ) Likely an adversarial example .", "entities": []}
{"text": "( 3 ) Neutral .", "entities": []}
{"text": "( 4 ) Likely an original example .", "entities": []}
{"text": "( 5 ) Sure original example .", "entities": []}
{"text": "We first ask the annotators to predict the sentiment on a Likert scale of 1 - 5 , and then map the prediction to three categories : negative , neutral , and positive .", "entities": []}
{"text": "We consider two examples to have the same semantic meaning if and only if they are both positive or negative .", "entities": []}
{"text": "We experiment with the validation split on a single RTX 3090 , and measure the average running time per example .", "entities": []}
{"text": "As shown in A.4 Additional Results on Protected Tokens Fig .", "entities": []}
{"text": "7 presents the experimental results with additional protected tokens such as nationality , religion , and sexual orientation ( from Ribeiro et al ( 2020 ) ) .", "entities": []}
{"text": "One interesting observation is when p = ( gay , straight ) where the bias is negative , indicating that the sentiment classifier tends to give more negative prediction when substituting gay straight in the input .", "entities": []}
{"text": "This phenomenon is opposite to the behavior of toxicity classifiers ( Dixon et al , 2018 )", "entities": []}
{"text": "Notice that even though gender swap mitigates the bias to some extent , it is still difficult to fully eliminate the bias .", "entities": []}
{"text": "To help evaluate the naturalness of our constructed examples used in 4 , we provide sample sentences in Table 9 and Table 10 .", "entities": []}
{"text": "Bold words are the corresponding patch words p , taken from the predefined list of gendered pronouns .", "entities": []}
{"text": "it 's hampered by a lifetime - channel kind of plot and lone lead actor ( actress ) who is out of their depth .", "entities": []}
{"text": "56 % Negative ( 55 % Positive )", "entities": []}
{"text": "it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) who is out of creative depth .", "entities": []}
{"text": "89 % Negative ( 84 % Negative )", "entities": []}
{"text": "it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) who talks out of their depth .", "entities": []}
{"text": "98 % Negative ( 98 % Negative )", "entities": []}
{"text": "it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) who is out of production depth .", "entities": []}
{"text": "96 % Negative ( 96 % Negative )", "entities": []}
{"text": "it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) that is out of their depth .", "entities": []}
{"text": "it 's hampered by a lifetime - channel cast of stars and a lead actor ( actress ) who is out of their depth .", "entities": []}
{"text": "96 % Negative ( 95 % Negative )", "entities": []}
{"text": "it 's hampered by a simple set of plot and a lead actor ( actress ) who is out of their depth .", "entities": []}
{"text": "54 % Negative ( 54 % Negative )", "entities": []}
{"text": "it 's framed about a lifetime - channel kind of plot and a lead actor ( actress ) who is out of their depth .", "entities": []}
{"text": "90 % Negative ( 88 % Negative )", "entities": []}
{"text": "it 's hampered by a lifetime - channel mix between plot and a lead actor ( actress ) who is out of their depth .", "entities": []}
{"text": "78 % Negative ( 68 % Negative )", "entities": []}
{"text": "it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) who storms out of their mind .", "entities": []}
{"text": "it 's characterized by a lifetime - channel combination comedy plot and a lead actor ( actress ) who is out of their depth .", "entities": []}
{"text": "93 % Negative ( 93 % Negative )", "entities": []}
{"text": "it 's hampered by a lifetime - channel kind of star and a lead actor ( actress ) who falls out of their depth .", "entities": []}
{"text": "58 % Negative ( 57 % Negative )", "entities": []}
{"text": "it 's hampered by a tough kind of singer and a lead actor ( actress ) who is out of their teens .", "entities": []}
{"text": "70 % Negative ( 52 % Negative )", "entities": []}
{"text": "it 's hampered with a lifetime - channel kind of plot and a lead actor ( actress ) who operates regardless of their depth .", "entities": []}
{"text": "58 % Negative ( 53 % Positive )", "entities": []}
{"text": "it 's hampered with a lifetime - channel cast of plot and a lead actor ( actress ) who is out of creative depth .", "entities": []}
{"text": "We thank anonymous reviewers for their helpful feedback .", "entities": []}
{"text": "We thank UCLA - NLP group for the valuable discussions and comments .", "entities": []}
{"text": "The research is supported NSF # 1927554 , # 1901527 , # 2008173 and # 2048280 and an Amazon Research Award .", "entities": []}
{"text": "Original 54 % Positive ( 69 % Positive ) for the most part , director anne - sophie birot 's first feature is a sensitive , overly ( extraordinarily ) well - acted drama .", "entities": []}
{"text": "Vulnerable 53 % Negative ( 62 % Positive ) for the most part , director anne - sophie benoit 's first feature is a sensitive , overly ( extraordinarily ) well - acted drama .", "entities": []}
{"text": "Original 73 % Negative ( 56 % Negative ) the cold ( colder ) turkey would ' ve been a far better title .", "entities": []}
{"text": "Vulnerable 61 % Negative ( 62 % Positive ) the cold ( colder ) turkey might ' ve been a far better title .", "entities": []}
{"text": "70 % Negative ( 65 % Negative )", "entities": []}
{"text": "it 's just disappointingly superficial - a movie that has all the elements necessary to be a fascinating , involving character study , but never does more than scratch the shallow ( surface ) .", "entities": []}
{"text": "Vulnerable 52 % Negative ( 55 % Positive )", "entities": []}
{"text": "it 's just disappointingly short - a movie that has all the elements necessary to be a fascinating , involving character study , but never does more than scratch the shallow ( surface ) .", "entities": []}
{"text": "Original 79 % Negative ( 72 % Negative ) schaeffer has to find some hook on which to hang his persistently useless movies , and it might as well be the resuscitation ( revival ) of the middleaged character .", "entities": []}
{"text": "Vulnerable 57 % Negative ( 57 % Positive ) schaeffer has to find some hook on which to hang his persistently entertaining movies , and it might as well be the resuscitation ( revival ) of the middleaged character .", "entities": []}
{"text": "Original 64 % Positive ( 58 % Positive ) the primitive force of this film seems to bubble up from the vast collective memory of the combatants ( militants ) .", "entities": []}
{"text": "Vulnerable 52 % Positive ( 53 % Negative ) the primitive force of this film seems to bubble down from the vast collective memory of the combatants ( militants ) .", "entities": []}
{"text": "Original 64 % Positive ( 74 % Positive ) on this troublesome ( tricky ) topic , tadpole is very much a step in the right direction , with its blend of frankness , civility and compassion .", "entities": []}
{"text": "Vulnerable 55 % Negative ( 56 % Positive ) on this troublesome ( tricky ) topic , tadpole is very much a step in the right direction , losing its blend of frankness , civility and compassion .", "entities": []}
{"text": "We believe this is because both types of features - the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other .", "entities": []}
{"text": "We conduct extensive experiments on several standard datasets across four languages .", "entities": []}
{"text": "The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters .", "entities": []}
{"text": "Our further analysis demonstrates that our model can capture longer dependencies compared with strong baselines .", "entities": []}
{"text": "1", "entities": []}
{"text": "al , 2005 ; Jie et al , 2017 ; Aguilar and Solorio , 2019 ) .", "entities": []}
{"text": "Previous research works ( Li et al , 2017 ; Jie and Lu , 2019 ; Wang et al , 2019 ) have been using the parse trees ( Chomsky , 1956 ( Chomsky , , 1969Sandra and Taft , 2014 ) to incorporate such structured information .", "entities": []}
{"text": "Figure 1 ( Dependency Path ) shows that the first entity can be connected to the second entity following the dependency tree with 5 hops .", "entities": []}
{"text": "Incorporating the dependency information can be done with graph neural networks ( GNNs ) such as graph convolutional networks ( GCNs ) ( Kipf and Welling , 2017 ) .", "entities": []}
{"text": "More specifically , the graph - encoded representation for each word can be obtained with GCNs .", "entities": []}
{"text": "With the newly designed gating mechanism , our model is able to make independent assessments on the amounts of information to be retrieved from the word representation and the graph - encoded representation respectively .", "entities": []}
{"text": "Such a mechanism allows for better integration of both contextual and structured information .", "entities": []}
{"text": "We conduct extensive experiments on several standard datasets across four languages .", "entities": []}
{"text": "The proposed model significantly outperforms previous approaches on these datasets .", "entities": []}
{"text": "We show that the proposed model can capture long - distance interactions between entities .", "entities": []}
{"text": "Our further analysis statistically demonstrates the proposed gating mechanism is able to aggregate the structured information selectively .", "entities": []}
{"text": "However , structured information sometimes is hard to encode , as we can see from the example in Figure 1 .", "entities": []}
{"text": "One naive approach is to use a deep GNN to capture such information along multiple dependency arcs between two words , which could mess up information and lead to training difficulties .", "entities": []}
{"text": "As shown in Figure 1 ( Hybrid Paths ) , the structured information can be passed to neighbors or context , which allows a model to use less number of GNN layers and alleviate such issues for long - range dependencies .", "entities": []}
{"text": "However , because such an approach requires both x t and g t to decide the value of the input gate jointly , it could be a potential victim of two sources of uncertainties : 1 ) the uncertainty of the quality of graph - encoded representation g t , and 2 ) the uncertainty of the exact interaction mechanism between the two types of features .", "entities": []}
{"text": "These may lead to sub - optimal performance , especially if the graph - encoded representation g t is unsatisfactory .", "entities": []}
{"text": "Thus , we need to design a new approach to incorporate both types of information from x t and g t with a more explicit interaction mechanism , with which we hope to alleviate the above issues .", "entities": []}
{"text": "Within the cell , there are four gates : input gate i t , forget gate f t , output gate o t , and an additional new gate m t to control the flow of information .", "entities": []}
{"text": "Note that the forget gate f t and output gate o t are not just looking at h t\u22121 and x t ; they are also affected by the graph - encoded representation g t .", "entities": []}
{"text": "The cell state c t and hidden state h t are computed as follows : f t = \u03c3 ( W ( f ) x t +", "entities": []}
{"text": "U ( f ) h", "entities": []}
{"text": "t\u22121 + Q ( f ) g t + b ( f ) )", "entities": []}
{"text": "( 1 ) o t = \u03c3 ( W ( o ) x t + U ( o ) h", "entities": []}
{"text": "t\u22121 + Q", "entities": []}
{"text": "( o ) g t + b ( o ) )", "entities": []}
{"text": "( 2 ) i t = \u03c3 ( W ( i ) x t + U ( i ) h t\u22121 + b ( i ) )", "entities": []}
{"text": "( 3 ) m t = \u03c3 ( W ( m ) g t + U ( m ) h", "entities": []}
{"text": "t\u22121 + b ( m ) )", "entities": []}
{"text": "( 4 ) c t = tanh ( W ( u ) x t + U ( u ) h t\u22121 + b ( u ) )", "entities": []}
{"text": "( 5 ) s t = tanh ( W ( n ) g t + U ( n ) h t\u22121 + b ( n ) )", "entities": []}
{"text": "( 6 ) c t = f t c t\u22121", "entities": []}
{"text": "+ i t c t + m t s t ( 7 )", "entities": []}
{"text": "h t = o t tanh ( c t ) ( 8 ) where \u03c3 is the sigmoid function , W ( ) , U ( ) , Q ( ) and b ( ) are learnable parameters .", "entities": []}
{"text": "The additional new gate m t is used to control the information from the graph - encoded representation directly .", "entities": []}
{"text": "Such a design allows the original input gates i t and our new gate m t to make independent assessments on the amounts of information to be retrieved from the word representation x t and the graph - encoded representation g t respectively .", "entities": []}
{"text": "On the other hand , we also have a different candidate states t to represent the cell state that corresponds to the graph - encoded representation separately .", "entities": []}
{"text": "Such a mechanism allows our model to aggregate the information from linear sequence and dependency trees selectively .", "entities": []}
{"text": "Similar to the previous work ( Levy et al , 2018 ) , it is also possible to show that the cell state c t implicitly computes the element - wise weighted sum of the previous states by expanding Equation 7 : xt - 1 xt", "entities": []}
{"text": "xt+1", "entities": []}
{"text": "xt+2", "entities": []}
{"text": "g L t - 1 g L t g", "entities": []}
{"text": "L t+1", "entities": []}
{"text": "g L t+2", "entities": []}
{"text": "y", "entities": []}
{"text": "t\u22121 y t y", "entities": []}
{"text": "t+1", "entities": []}
{"text": "y t+2", "entities": []}
{"text": "+ i t c t + m t s t ( 9 ) = t j=0", "entities": []}
{"text": "t j=0 a t j c j + t j=0 q t j s j ( 11 ) Note that the two terms , a t j and q t j , are the product of gates .", "entities": []}
{"text": "Since thec t ands t represent contextual and structured features , the corresponding weights control the flow of information .", "entities": []}
{"text": "= {", "entities": []}
{"text": "y 1 , y 2 , ... , y n } given the input sequence", "entities": []}
{"text": "w = { w 1 , w 2 , ... , w n } , where w t represents the t - th word and n is the number of words .", "entities": []}
{"text": "Input Representation Layer Similar to the work by Lample et al ( 2016 )", "entities": []}
{"text": "Jie and Lu ( 2019 ) highlight that the dependency relation helps to enhance the input representation .", "entities": []}
{"text": "Furthermore , previous methods ) use embeddings of part - ofspeech ( POS ) tags as additional input representation .", "entities": []}
{"text": "The input representation x t of our model is the concatenation of the word embedding v t , the character representation e t , the dependency relation embedding r t , and the POS embedding p t : x t", "entities": []}
{"text": "= [ v t ; e t ; r t ; p t ]", "entities": []}
{"text": "( 12 ) where both r t and p t embeddings are randomly initialized and are fine - tuned during training .", "entities": []}
{"text": "Given a graph , an adjacency matrix A of size n \u00d7 n is able to represent the graph structure , where n is the number of nodes ; A i , j = 1 indicates that node i and node j are connected .", "entities": []}
{"text": "We transform dependency tree into its corresponding adjacency matrix 3 A , and A i , j = 1 denotes that node i and node j have dependency relation .", "entities": []}
{"text": "Note that the purpose of graph - encoded representation g t is to incorporate the dependency information from neighbor nodes .", "entities": []}
{"text": "Similar to the work by Zhang et al ( 2018b ) , we use d t = n j=1", "entities": []}
{"text": "A t , j , which is the total number of neighbors of node t , to normalize the representation before going through the nonlinear function .", "entities": []}
{"text": "t = [ v t ; e t ; r t ] .", "entities": []}
{"text": "Bi - directional h t =", "entities": []}
{"text": "[ \u2212 h t ; \u2212 h t ] .", "entities": []}
{"text": "Given the sentence w and dependency tree \u03c4 , the probability of the label sequence y is defined as : P ( y |", "entities": []}
{"text": "w , \u03c4 )", "entities": []}
{"text": "= exp ( score ( w , \u03c4 , y ) )", "entities": []}
{"text": "y exp ( score ( w , \u03c4 , y ) )", "entities": []}
{"text": "( 14 )", "entities": []}
{"text": "The score function is defined as : score ( w , \u03c4 , y )", "entities": []}
{"text": "= n t=0 T yt , y t+1", "entities": []}
{"text": "+ n t=1 E yt ( 15 ) where T yt , y t+1 denotes the transition score from label y t to y t+1 , E yt denotes the score of label y t at the t - th position and the scores are computed using the hidden state h t .", "entities": []}
{"text": "Datasets The proposed model is evaluated on four benchmark datasets :", "entities": []}
{"text": "SemEval 2010 Task 1 ( Recasens et al , 2010 )", "entities": []}
{"text": "English and Chinese datasets .", "entities": []}
{"text": "We choose these four datasets as they have explicit dependency annotations which allow us to evaluate the effectiveness of our approach when dependency trees of different qualities are used .", "entities": []}
{"text": "For SemEval 2010 Task 1 datasets , there are 4 entity types : PER , LOC and ORG and MISC .", "entities": []}
{"text": "Following the work by Jie and Lu ( 2019 ) , we transform the parse trees into the Stanford dependency trees ( De Marneffe and Manning , 2008 ) by using Stanford CoreNLP .", "entities": []}
{"text": "Detailed statistics of each dataset can be found in Table 1 .", "entities": []}
{"text": "Intuitively , longer sentences would require the model to capture more long - distance interactions in the sentences .", "entities": []}
{"text": "We present the number of entities in terms of different sentence lengths to show that these datasets have a modest amount of entities in long sentences .", "entities": []}
{"text": "Specifically , we use bert - as - service ( Xiao , 2018 ) to generate the contextualized word representation without fine - tuning .", "entities": []}
{"text": "We select the best model based on the performance on the dev set 5 and apply it to the test set .", "entities": []}
{"text": "We use the bootstrapping t - test to compare the results .", "entities": []}
{"text": "Baselines We compare our model with several baselines with or without dependency tree information .", "entities": []}
{"text": "( epoch \u2212 1 ) ) .", "entities": []}
{"text": "5", "entities": []}
{"text": "The experimental results on the dev set and other experimental details can be found in the Appendix .", "entities": []}
{"text": "Besides , we compare our model with previous works that have results on these datasets .", "entities": []}
{"text": "SemEval 2010 Task 1 Table 2 shows comparisons of our model with baseline models on the SemEval", "entities": []}
{"text": "2010 Task 1 Catalan and Spanish datasets .", "entities": []}
{"text": "This shows that our proposed model demonstrates a better integration of contextual information and structured information .", "entities": []}
{"text": "Table 3 shows comparisons of our model with baseline models on English .", "entities": []}
{"text": "Our method outperforms the previous model ( Luo et al , 2020 ) , which relies on document - level information , by 0.55 in F 1 .", "entities": []}
{"text": "Furthermore , the performance improvement on recall is more prominent as compared to precision .", "entities": []}
{"text": "Jie and Lu ( 2019 ) .", "entities": []}
{"text": "There are also other methods ( Li et al , 2020a , b ) that use external information , ( Yu et al , 2020 ) use document - level information to encode the sentence , which are not direct comparisons to ours .", "entities": []}
{"text": "With the contextualized word representation , we achieve a higher F 1 score of 80.20 .", "entities": []}
{"text": "Robustness Analysis To study the robustness of our model and check whether our model can regulate the flow of information from the graphencoded representation , we analyze the influence of the quality of dependency trees .", "entities": []}
{"text": "We train and evaluate an additional dependency parser ( Dozat and Manning , 2017 ) .", "entities": []}
{"text": "Specifically , we train the Jie and Lu ( 2019 ) .", "entities": []}
{"text": "There are also other methods ( Li et al , 2020a , b ) that use external information , which are not direct comparisons to ours .", "entities": []}
{"text": "dependency parser 6 on the given training datasets and select the best model based on the dev sets .", "entities": []}
{"text": "Then we apply the best model to the test sets to obtain dependency trees .", "entities": []}
{"text": "We also train and evaluate our model with random dependency trees .", "entities": []}
{"text": "We observe that both models encounter a performance drop when we use the predicted parse trees and random trees .", "entities": []}
{"text": "Such an observation demonstrates the robustness of our proposed model against structured information from the trees of different quality .", "entities": []}
{"text": "To further study the robustness , we conduct an analysis to investigate if the gate m t ( Figure 2 ) has the ability to regulate the flow of information from the graph - encoded representation .", "entities": []}
{"text": "Intuitively , the gate m t should tend to have a small value", "entities": []}
{"text": ".9", "entities": []}
{"text": "- 1 the quality of the parse tree is not good ( e.g. , with random trees ) .", "entities": []}
{"text": "We statistically plot the number of words with respect to different gate value ranges ( m t ) .", "entities": []}
{"text": "Figure 4 shows the comparison between the models of using random trees and given trees on Catalan and Spanish 7 .", "entities": []}
{"text": "We observe that the gate m t is more likely to open ( the value is higher ) when we use the given parse trees compared with random parse trees .", "entities": []}
{"text": "Such behavior demonstrates that our proposed model can selectively aggregate the information from the graph - encoded representation .", "entities": []}
{"text": "In particular , although the performance tends to drop as the sentence length increases , our proposed model shows relatively better performance when the sentence length is \u2265 60 .", "entities": []}
{"text": "Therefore , for the sentences with length of \u2264 14 , we can still observe obvious improvements .", "entities": []}
{"text": "Table 6 shows the performance comparison of two models with respect to entity length .", "entities": []}
{"text": "This confirms that our proposed method can effectively incorporate the structured information .", "entities": []}
{"text": "We note there are some special characteristics of the Chinese language .", "entities": []}
{"text": "Furthermore , the ratio of long entities is much higher for Catalan and Spanish compared to English and Chinese .", "entities": []}
{"text": "The experimental results on Catalan and Spanish datasets show significant improvements for long entities .", "entities": []}
{"text": "Such results show that the structured information conveyed by the dependency trees can be more crucial when entity length becomes longer .", "entities": []}
{"text": "The last bar , indicated as AVG , is obtained by averaging the dev results on the four datasets .", "entities": []}
{"text": "Ablation Study", "entities": []}
{"text": "The original dependency contributes 0.27 F 1 score .", "entities": []}
{"text": "Removing the dependency relation embedding also decreases the performance by 0.27 F 1 .", "entities": []}
{"text": "When we remove the POS tags embedding , the result drops by 0.39 F 1 .", "entities": []}
{"text": "Liu et al ( 2010 ) propose to construct skip - edges to link similar words or words having typed dependencies to capture long - range dependencies .", "entities": []}
{"text": "The later works ( Collobert et al , 2010 ; Lample et al , 2016 ; Chiu and Nichols , 2016b ) focus on using neural networks to extract features and achieved the stateof - the - art performance .", "entities": []}
{"text": "Jie et al ( 2017 ) find that some relations between the dependency edges and the entities can be used to reduce the search space of their model , which significantly reduces the time complexity .", "entities": []}
{"text": "Yu et al ( 2020 ) employ pre - trained language model to encode document - level information to explore all spans with the graph - based dependency graph based ideas .", "entities": []}
{"text": "However , previous works did not focus on investigating how to effectively integrate structured and contextual information well .", "entities": []}
{"text": "Specifically , we use bert - as - service ( Xiao , 2018 ) to generate the contextualized word representation without fine - tuning .", "entities": []}
{"text": "Table 8 presents the performance of dependency parser .", "entities": []}
{"text": "( epoch \u2212 1 ) ) .", "entities": []}
{"text": "We further show an example to visualize the propagation of non - local information ( Figure 9 ) .", "entities": []}
{"text": "If only looking at the first half of the sentence , it is possible to predict \" Tianshui \" as PERSON because of the local information \" age \" .", "entities": []}
{"text": "However , the second half of the sentence confirms that the entity type of", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their helpful comments .", "entities": []}
{"text": "This research is partially supported by Ministry of Education , Singapore , under its Academic Research Fund ( AcRF )", "entities": []}
{"text": "Tier 2 Programme ( MOE AcRF Tier 2 Award No : MOE2017 - T2 - 1 - 156 ) .", "entities": []}
{"text": "Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the Ministry of Education , Singapore .", "entities": []}
{"text": "We test our model on RTX 2080 Ti GPU and Nvidia Tesla V100 GPU , with CUDA version 10.1 , PyTorch version 1.40 .", "entities": []}
{"text": "2010 Task 1 Catalan and Spanish .", "entities": []}
{"text": "For experiments with the contextualized representation , we adopt the pre - trained", "entities": []}
{"text": "English Chinese Catalan Spanish P. R. F 1 P. During Tanshui 's golden age , large and small boats were constantly coming and going in the harbor , and it was not usual to see enormous steamships .", "entities": []}
{"text": "ROOT Figure 9 : An example of dependency tree .", "entities": []}
{"text": "The mentioned entity is highlighted in orange , and the entity type is GPE .", "entities": []}
{"text": "\" Tianshui \" is GPE .", "entities": []}
{"text": "Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy", "entities": []}
{"text": "This understanding module will be an essential component of a statistical dialogue system delivering therapy .", "entities": []}
{"text": "Promotion of mental well - being is at the core of the action plan on mental health 2013 - 2020 of the World Health Organisation ( WHO ) ( World Health Organization , 2013 ) and of the European Pact on Mental Health and Well - being of the European Union ( EU high - level conference : Together for Mental Health and Well - being , 2008 ) .", "entities": []}
{"text": "The biggest potential breakthrough in fighting mental illness would lie in finding tools for early detection and preventive intervention ( Insel and Scholnick , 2006 ) .", "entities": []}
{"text": "The WHO action plan stresses the importance of health policies and programmes that not only meet the need of people affected by mental disorders but also protect mental well - being .", "entities": []}
{"text": "The emphasis is on early evidence - based non - pharmacological intervention , avoiding institutionalisation and medicalisation .", "entities": []}
{"text": "What is particularly important for successful intervention is the frequency with which the therapy can be accessed ( Hansen et al , 2002 ) .", "entities": []}
{"text": "This gives automated systems a huge advantage over conventional therapies , as they can be used continuously with marginal extra cost .", "entities": []}
{"text": "Health assistants that can deliver therapy , have gained great interest in recent years ( Bickmore et al , 2005 ; Fitzpatrick et al , 2017 ) .", "entities": []}
{"text": "These systems however are largely based on hand - crafted rules .", "entities": []}
{"text": "On the other hand , the main research effort in statistical approaches to conversational systems has focused on limited - domain information seeking dialogues ( Schatzmann et al , 2006 ; Geist and Pietquin , 2011 ; Gasic and Young , 2014 ; Fatemi et al , 2016 ; Li et al , 2016 ; Williams et al , 2017 ) .", "entities": []}
{"text": "We label a high quality mental health corpus , which exhibits targeted psychological phenomena .", "entities": []}
{"text": "We use the whole unlabelled dataset to train distributed representations of words and sentences .", "entities": []}
{"text": "The first model involves a convolutional neural network ( CNN ) operating over distributed words representations .", "entities": []}
{"text": "Our models perform significantly better than chance and for instances with a large number of data they reach the inter - annotator agreement .", "entities": []}
{"text": "This understanding module will be an essential component of a statistical dialogue system delivering therapy .", "entities": []}
{"text": "The paper is organised as follows .", "entities": []}
{"text": "In Section 3 we review related work in the area of automated mental - health assistants .", "entities": []}
{"text": "We present the results in Section 7 and our conclusion in Section 8 .", "entities": []}
{"text": "A dialogue system can be treated as a trainable statistical model suitable for goal - oriented information seeking dialogues ( Young , 2002 ) .", "entities": []}
{"text": "In these dialogues , the user has a clear goal that he or she is trying to achieve and this involves extracting particular information from a back - end database .", "entities": []}
{"text": "It defines the concepts that the dialogue system can understand and talk about .", "entities": []}
{"text": "Statistical approaches to dialogue modelling have been applied to relatively simple domains .", "entities": []}
{"text": "These systems interface databases of up to 1000 entities where each entity has up to 20 properties , i.e. slots ( Cuay\u00e1huitl , 2009 ) .", "entities": []}
{"text": "Despite these efforts , little work has been done on mental health ontologies for supporting cognitive behavioural therapy on dialogue systems .", "entities": []}
{"text": "Both classification ( Mairesse et al , 2009 ) and sequence - to - sequence ( Yao et al , 2014 ; Mesnil et al , 2015 ) models have been applied to address this task .", "entities": []}
{"text": "al , 2016 ; Mrk\u0161i\u0107 et al , 2017 ) .", "entities": []}
{"text": "In this work we consider understanding of mental health concepts of as a classification task .", "entities": []}
{"text": "To facilitate this process , we use distributed representations .", "entities": []}
{"text": "The aim of building an automated therapist has been around since the first time researchers attempted to build a dialogue system ( Weizenbaum , 1966 ) .", "entities": []}
{"text": "Automated health advice systems built to date typically rely on expert coded rules and have limited conversational capabilities ( Rojas - Barahona and Giorgino , 2009 ;", "entities": []}
{"text": "Vardoulakis et al , 2012 ; Ring et al , 2013 ; Riccardi , 2014 ; DeVault et al , 2014 ; Ring et al , 2016 ) .", "entities": []}
{"text": "One particular system that we would like to highlight is an affectively aware virtual therapist ( Ring et al , 2016 ) .", "entities": []}
{"text": "This system is based on Cognitive Behavioural Therapy and the system behaviour is scripted using VoiceXML .", "entities": []}
{"text": "Another notable system ( De - Vault et al , 2014 ) has a multi - modal perception unit which captures and analyses user behaviour for both behavioural understanding and interaction .", "entities": []}
{"text": "Again , no statistical language understanding takes place and the behaviour of the system is scripted .", "entities": []}
{"text": "The system does not provide therapy to the user but is rather a tool that can support healthcare decisions ( by human healthcare professionals ) .", "entities": []}
{"text": "However , the understanding component of Woebot has not been fully described .", "entities": []}
{"text": "The dialogue decisions are based on decision trees .", "entities": []}
{"text": "At each node , the user is expected to choose one of several predefined responses .", "entities": []}
{"text": "Limited language understanding was in - troduced at specific points in the tree to determine routing to subsequent conversational nodes .", "entities": []}
{"text": "This is one of the best studied psychotherapeutic interventions , and the most widely used psychological treatment for mental disorders in Britain ( Bhasi et al , 2013 ) .", "entities": []}
{"text": "Also , due to it being highly structured , it is more easily amenable by computer interpretation .", "entities": []}
{"text": "Cognitive Behavioural Therapy is derived from Cognitive Therapy model theory ( Beck , 1976 ; Beck et al , 1979 ) , which postulates that our emotions and behaviour are influenced by the way we think and by how we make sense of the world .", "entities": []}
{"text": "The idea is that , if the client changes the way he or she thinks about their problem , this will in turn change the way he or she feels , and behaves .", "entities": []}
{"text": "When clients learn that their perceptions and interpretations are distorted or unhelpful they then work on correcting them .", "entities": []}
{"text": "There is a core of around 10 to 15 thinking errors , with their exact titles having some fluidity .", "entities": []}
{"text": "We present one such list in Table 1 .", "entities": []}
{"text": "However , it is important to note that there is a fair degree of overlap between different thinking errors , for example , between Jumping to Negative Conclusions and Fortune Telling , or between Disqualifying the Positives and Mental Filtering .", "entities": []}
{"text": "In addition , within the data used - and as is likely to be the case in any data of spontaneous expressions of psychological upset - a single problem can exhibit several thinking errors simultaneously .", "entities": []}
{"text": "Thus , the situation is much more challenging than in simple information - seeking dialogues , where ontologies are typically clearly defined and there is no or very little overlap between concepts .", "entities": []}
{"text": "In addition to thinking errors , we define a set of emotions .", "entities": []}
{"text": "We mainly focus on negative emotions , relevant to people in psychological distress .", "entities": []}
{"text": "The set of emotions for this work evolved over time in the early days of annotation .", "entities": []}
{"text": "The list of emotions is given in Table 2 .", "entities": []}
{"text": "While our main emphasis was on thinking errors and emotions , we also defined a small set of situations .", "entities": []}
{"text": "The list of situations again evolved during the early days of annotation , with a longer original list being reduced down , for simplicity .", "entities": []}
{"text": "Again , it is possible for more than one situation ( for example Work and Relationships ) to apply to a single problem .", "entities": []}
{"text": "The considered situations are given in Table 3 .", "entities": []}
{"text": "The corpus consists of 500 K written posts that users anonymously posted on the Koko platform 1 .", "entities": []}
{"text": "This platform is based on the peer - to - peer therapy proposed by ( Morris et al , 2015 ) .", "entities": []}
{"text": "In this set - up , a user anonymously posts their problem ( referred to 1 https://itskoko.com/ as the problem ) and is prompted to consider their most negative take on the problem ( referred to as the negative take ) .", "entities": []}
{"text": "Subsequently , peers post responses that attempt to offer a re - think and give a more positive angle on the problem .", "entities": []}
{"text": "When first developed , this peer - to - peer framework was shown to be more efficacious than expressive writing , an intervention that is known to improve physical and emotional well - being ( Morris et al , 2015 ) .", "entities": []}
{"text": "Since then , the app developed by Koko has collected a very large number of posts and associated responses .", "entities": []}
{"text": "Most of the data annotated in this study was drawn from the earlier phase .", "entities": []}
{"text": "Figure 1 gives an annotated post example .", "entities": []}
{"text": "A subset of posts was annotated by two psychological therapists using a web annotation tool that we developed .", "entities": []}
{"text": "The annotation tool allowed annotators to have a quick view of the posts , showing up to 50 posts per page , to navigate through posts , to check pending posts and to annotate them by adding or removing thinking errors , emotions and situations .", "entities": []}
{"text": "All annotations were stored in a MySQL database .", "entities": []}
{"text": "Initially 1000 posts were analysed .", "entities": []}
{"text": "Then 4035 posts were labelled with thinking errors , emotions and situations .", "entities": []}
{"text": "It takes an experienced psychological therapist about one minute to annotate one post .", "entities": []}
{"text": "Note that the same post can exhibit multiple thinking errors , emotions and situations , which makes the whole process more complex .", "entities": []}
{"text": "We randomly selected 50 posts and calculated the inter - annotator agreement .", "entities": []}
{"text": "Then , Cohen 's kappa was calculated discounting the possibility that the agreement may happen by chance .", "entities": []}
{"text": "The result is shown in due to the unbounded number of thinking errors per post .", "entities": []}
{"text": "In other words , the annotators typically have three or four thinking errors in common but one of them might have detected one or two more .", "entities": []}
{"text": "Still , the agreement is much higher than chance , so we think that while challenging , it is possible to build a classifier for this task .", "entities": []}
{"text": "The distributions of labelled posts with multiple sub - categories for three super - categories are shown in Figure 2 6 Deep learning model", "entities": []}
{"text": "Detecting thinking errors or emotions could be perceived as detecting different kinds of negative sentiment .", "entities": []}
{"text": "Distributed representations of words , sentences and documents have gained success in sentiment detection and similarity tasks ( Le and Mikolov , 2014a ; Maas et al , 2011 ; Kiros et al , 2015 ) .", "entities": []}
{"text": "A key advantage of these representations is that they can be obtained in an unsupervised manner , thus allowing exploitation of large amounts of unlabelled data .", "entities": []}
{"text": "This is precisely what we have in our set - up , where only a small portion of our posts is labelled .", "entities": []}
{"text": "We train the word vectors on the whole dataset and then use a convolutional neural network ( CNN ) to extract features from posts where words are represented as vectors .", "entities": []}
{"text": "We also consider distributed representation of sentences .", "entities": []}
{"text": "A particularly competitive model is the skip - thought model , which is obtained from an encoder - decoder model that tries to reconstruct the surrounding sentences of an encoded passage ( Kiros et al , 2015 ) .", "entities": []}
{"text": "On similarity tasks it outperfoms the simpler doc2vec model ( Le and Mikolov , 2014a ) .", "entities": []}
{"text": "As these often appear in our corpus , we chose skipthought vectors for investigation here .", "entities": []}
{"text": "The skip - thought model allows a dense representation of the utterance .", "entities": []}
{"text": "We train skip - thought vectors using the method described in ( Kiros et al , 2015 ) .", "entities": []}
{"text": "The automatically generated post shown in Fig 3 demonstrates that skip - thought vectors can convey the sentiment well in accordance to context .", "entities": []}
{"text": "i ' m so depressed .", "entities": []}
{"text": "i ' m worthless .", "entities": []}
{"text": "No one likes me i ' m try being nice but .", "entities": []}
{"text": "No light at every point i ' m unpopular and i ' m a < NUM > year old potato .", "entities": []}
{"text": "my most negative take is that i 'll never know how to be as socially as a quiet girl .", "entities": []}
{"text": "i will stop talking to how fragile is and be any ways of normal people .", "entities": []}
{"text": "Figure 3 : An example of a generated post using skipthought vectors initialised with \" I 'm so depressed \" .", "entities": []}
{"text": "The convolutional neural network ( CNN ) model is preferred over a recurrent neural network ( RNN ) model , because the posts are generally too long for an RNN to maintain memory over words .", "entities": []}
{"text": "As shown in Fig 4 , the network has two inputs , one for the problem and the other for the negative take .", "entities": []}
{"text": "These are represented as two tensors .", "entities": []}
{"text": "A convolutional operation involves a filter w R ld which is applied to l words to produce the feature map .", "entities": []}
{"text": "Then , a max - pooling operation is applied to produce two vectors : p for problem and n for negative take .", "entities": []}
{"text": "The reason for this is that the negative take is usually a summary of the post , carrying stronger sentiment ( see Figure 1 ) .", "entities": []}
{"text": "We use a gating mechanism to combine p and n as follows :", "entities": []}
{"text": "g = \u03c3 ( W p p + W n n + b ) ( 1 ) h = g p + ( 1 \u2212 g )", "entities": []}
{"text": "n ( 2 ) Here , \u03c3 is the sigmoid function , W p , W n and W are weight matrices , b is a bias term , 1 is a vector of ones , is the element - wise product , and g is the output of the gating mechanism .", "entities": []}
{"text": "The extracted feature h is then processed with a one - layer fullyconnected neural network ( FNN ) to perform binary classification .", "entities": []}
{"text": "The model is illustrated in Fig 4 .", "entities": []}
{"text": "First , most posts contain less than 5 sentences , so a recurrent neural network is more suitable than a convolutional neural network .", "entities": []}
{"text": "Denote each post as P = { s 1 , s 2 , ... , s t , ... } , where s t is the t th sentence in post P .", "entities": []}
{"text": "t\u22121", "entities": []}
{"text": "+ U z e t + b z ) ( 3 ) r t = \u03c3 ( W r h t\u22121", "entities": []}
{"text": "+ U r e t + b r )", "entities": []}
{"text": "( 4 ) h t = tanh ( W ( r t h t\u22121 ) +", "entities": []}
{"text": "Ue t + b h ) ( 5 ) h t = z t h t\u22121", "entities": []}
{"text": "+ ( 1 \u2212 z t )", "entities": []}
{"text": "h t ( 6 ) W z , U z , W r , U r , W , U are recurrent weight ma - trices ,", "entities": []}
{"text": "b z , b r , b h are bias terms , is the elementwise dot product , and \u03c3 is the sigmoid function .", "entities": []}
{"text": "Finally , the last hidden state h T is fed into a FNN with one hidden layer of the same size as input .", "entities": []}
{"text": "The model is illustrated in Fig 5 .", "entities": []}
{"text": "In some posts the length of sentences is very large , so we bound the length at 50 words .", "entities": []}
{"text": "A distinct network is trained for each concept , i. e. one for thinking errors , one for emotions and one for situations .", "entities": []}
{"text": "The hidden size of the FNN is 150 .", "entities": []}
{"text": "To tackle the data bias problem , we utilise oversampling .", "entities": []}
{"text": "Different ratios ( 1:1 , 1:3 , 1:5 , 1:7 ) of positive and negative samples are explored .", "entities": []}
{"text": "We used filter windows of 2 , 3 , and 4 with 50 feature maps for the CNN model .", "entities": []}
{"text": "Mini - batches of size 24 are used and gradients are clipped with maximum norm 5 .", "entities": []}
{"text": "To overcome over - fitting , we employ dropout with rate 0.8 and l2 - normalisation .", "entities": []}
{"text": "For rule - based models , we chose a chance classifier and a majority classifier , where all the posts are treated as positive examples for each class .", "entities": []}
{"text": "Both of them take the bag - of - words feature as input and implemented in sklearn ( Pedregosa et al , 2011 ) .", "entities": []}
{"text": "For completeness , we also trained 100 and 300 dimensions PV - DM document embeddings ( Le and Mikolov , 2014b ) as the distributed representations of the posts using the gensim toolkit ( \u0158eh\u016f\u0159ek and Sojka , 2010 ) , and employ FNNs to do the classification , the hidden size is set as 800 to ensure parameters of all deep learning models comparable .", "entities": []}
{"text": "All the baseline models are trained with the same set - up as described in section 6.4 .", "entities": []}
{"text": "We only include the results of the best performing models , SVMs , CNNs and GRUs , due to limited space .", "entities": []}
{"text": "To illustrate the capabilities of this model , we give samples of two posts and their predicted and true labels in Figure 6 , which shows that our model discerns the classes reasonably well even in some difficult cases .", "entities": []}
{"text": "Therefore , including only a limited ratio of positive samples is sufficient to train the classifier .", "entities": []}
{"text": "Instead , models using word vectors need more positive data to learn sentence sentiment features .", "entities": []}
{"text": "We then annotated data that exhibits psychological problems and computed the inter - annotator agreement .", "entities": []}
{"text": "We found that classifying thinking errors is a difficult task as suggested by the low inter - annotator agreement .", "entities": []}
{"text": "Areas of future investigation include richer dis - tributed representations , or a fusion of distributed representations from word - level , sentence - level and document - level , to acquire more powerful semantic features .", "entities": []}
{"text": "The development of a statistical system delivering therapy will moreover require further research on other modules of a dialogue system .", "entities": []}
{"text": "This work was funded by EPSRC project Natural speech Automated Utility for Mental health ( NAUM ) , award reference EP / P017746/1 .", "entities": []}
{"text": "The authors would also like to thank anonymous reviewers for their valuable comments .", "entities": []}
{"text": "The code is available at https://github.com/YinpeiDai/NAUM", "entities": []}
{"text": "Word embedding models have gained a lot of traction in the Natural Language Processing community , however , they suffer from unintended demographic biases .", "entities": []}
{"text": "Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test ( WEAT ) .", "entities": []}
{"text": "While these approaches offer great geometric insights into unintended biases in the embedding vector space , they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications .", "entities": []}
{"text": "In this work , we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias .", "entities": []}
{"text": "Bias can be defined as an unfair expression of prejudice for or against a person , a group , or an idea .", "entities": []}
{"text": "Bias is a broad term , which covers a range of problems particularly relevant in natural language systems such as , discriminatory gender bias ( Bolukbasi et al , 2016a ; Zhao et al , 2017 ) , bias against regionally accented speech ( Najafian et al , 2016", "entities": []}
{"text": "( Najafian et al , , 2017 , personal or political view bias ( Iyyer et al , 2014 ; Recasens et al , 2013 ) , and many other examples .", "entities": []}
{"text": "Geometrically , it is difficult to parse how these embeddings can lead to discrimination .", "entities": []}
{"text": "One could also look at unequal distributions of positive sentiment , but for this work we restrict ourselves to the negative case .", "entities": []}
{"text": "We need clear signals to evaluate which groups are discriminated against due to the bias in an embedding model .", "entities": []}
{"text": "That way we can pinpoint where to mitigate those biases .", "entities": []}
{"text": "Figure 1 shows a 2D word embedding projection of positive sentiment ( green ) and negative sentiment ( red ) words .", "entities": []}
{"text": "It would be unfair for any given demographic identity word vector ( blue ) to be more semantically related to negative terms than the other identities .", "entities": []}
{"text": "However , many identity terms exist closer to negative words than other identity terms in the vector space .", "entities": []}
{"text": "This bias may affect a downstream ML model , but the vector space has no absolute interpretable meaning , especially when it comes to whether this embedding model will lead to a unfairly discriminative algorithm .", "entities": []}
{"text": "Researchers have found a variety of ways in which dangerous unintended bias can show up in NLP applications ( Blodgett and O'Connor , 2017 ; Hovy and Spruit , 2016 ; Tatman , 2017 ) .", "entities": []}
{"text": "Mitigating such biases is a difficult problem , and researchers have created many ways to make fairer NLP applications .", "entities": []}
{"text": "Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text ( Bolukbasi et al , 2016b , a ;", "entities": []}
{"text": "Zhao et al , 2017 ; Zhang et al , 2018 ) , or inequality of sentiment or toxicity for various protected groups ( Caliskan - Islam et al , 2016 ; Bakarov , 2018 ; Dixon et al ; Garg et al , 2018 ; Kiritchenko and Mohammad , 2018 ) .", "entities": []}
{"text": "( Bolukbasi et al , 2016b ) defines a useful metric for identifying gender bias and ( Caliskan - Islam et al , 2016 ) defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text .", "entities": []}
{"text": "Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman ( Bolukbasi et al , 2016a ) , or European American names vs. African American names ( Caliskan - Islam et al , 2016 ) .", "entities": []}
{"text": "Though geometrically intuitive , these tests do not have a direct relation to discrimination in general .", "entities": []}
{"text": "Our framework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics .", "entities": []}
{"text": "We first describe the flow of our framework .", "entities": []}
{"text": "Then , we address which datasets / models were chosen for our approach .", "entities": []}
{"text": "Finally , we show how our framework can enable analysis and new metrics like RNSB .", "entities": []}
{"text": "We measure word embedding bias by analyzing the predicted probability of negative sentiment for identity terms .", "entities": []}
{"text": "Our framework has a simple layout .", "entities": []}
{"text": "Figure 2 shows the flow of our system .", "entities": []}
{"text": "We first use the embedding model we are trying to evaluate to initialize vectors for an unbiased positive / negative word sentiment dataset .", "entities": []}
{"text": "Using this dataset , we train a logistic classification algorithm to predict the probability of any word being a negative sentiment word .", "entities": []}
{"text": "After training , we take a set of neutral identity terms from a protected group ( i.e. national origin ) and predict the probability of negative sentiment for each word in the set .", "entities": []}
{"text": "We leverage this set of negative sentiment probabilities to summarize unintended demographic bias using RNSB .", "entities": []}
{"text": "As part of our pipeline , we also use a labeled positive / negative sentiment training set ( Hu and Liu , 2004 ) .", "entities": []}
{"text": "This dataset has been shown to be a trustworthy lexicon for negative and positive sentiment words ( Pang et al , 2008 ; Liu , 2012 ; Wilson et al , 2005 ) .", "entities": []}
{"text": "We now present our metric for unintended demographic bias , RNSB .", "entities": []}
{"text": "For gold standard labeled positive / negative sentiment words , ( x i , y i ) , in training set , S , where x i is a word vector from a possibly biased word embedding model , we find the minimizer ,", "entities": []}
{"text": "f", "entities": []}
{"text": "*", "entities": []}
{"text": "( x i )", "entities": []}
{"text": "i=0 l ( y i , w T x i )", "entities": []}
{"text": "P =", "entities": []}
{"text": "f *", "entities": []}
{"text": "( k 1 ) t i=1 f", "entities": []}
{"text": "* ( k i ) , ... ,", "entities": []}
{"text": "f * ( k t )", "entities": []}
{"text": "t i=1 f", "entities": []}
{"text": "* ( k i )", "entities": []}
{"text": "Thus , our metric , RN SB ( P ) , is defined as the KL divergence of P from U , where U is the uniform distribution for t elements .", "entities": []}
{"text": "RN SB ( P ) =", "entities": []}
{"text": "D KL ( P U )", "entities": []}
{"text": "We choose our set of neutral identity terms based on the most populous demographics for each protected group .", "entities": []}
{"text": "However , due to the simplicity of this method , one can easily adapt it to include identity terms that suit the application in need of analysis .", "entities": []}
{"text": "Since neutral identity terms are inherently not associated with sentiment , it is unfair to have identity term with differing levels of negative sentiment .", "entities": []}
{"text": "Thus , we want no differences between negative sentiment predictions of various identity terms .", "entities": []}
{"text": "Mathematically , this can be represented as a uniform distribution of negative sentiment probability for identity terms from a protected group .", "entities": []}
{"text": "Our RNSB metric captures the distance , via KL divergence , between the current distribution of negative sentiment and the fair uniform distribution .", "entities": []}
{"text": "So the more fair a word embedding model with respect to sentiment bias , the lower the RNSB metric .", "entities": []}
{"text": "We evaluate our framework and metric on two cases studies : National Origin Discrimination and Religious Discrimination .", "entities": []}
{"text": "For each case study , we create a set of the most frequent identity terms from the protected groups in the Wikipedia word corpus and analyze bias with respect to these terms via our framework .", "entities": []}
{"text": "We then show that our framework enables an insightful view into word embedding bias .", "entities": []}
{"text": "The results are displayed in Table 1 .", "entities": []}
{"text": "Although the RNSB metric is not directly comparable to WEAT scores , these results are still consistent with some of the bias predicted by ( Caliskan - Islam et al , 2016 ) .", "entities": []}
{"text": "RNSB captures the same types of biases , but has a clear and larger scope , measuring discrimination with respect to more than two demographics within a protected group .", "entities": []}
{"text": "Using the probability distribution of negative sentiment for the identity terms in a protected group , we can gain insights into the relative risks for discrimination between various demographics .", "entities": []}
{"text": "Figure 3 shows three histograms .", "entities": []}
{"text": "The bottom histogram is the uniform distribution .", "entities": []}
{"text": "As described earlier , zero unintended demographic bias with respect to our definition is achieved when all the identity terms within a protected group have equal negative sentiment .", "entities": []}
{"text": "The top two histograms show the negative sentiment probability for each identity normalized across all terms to be a probability distribution .", "entities": []}
{"text": "One can see that certain demographics have very high negative sentiment predictions , while others have very low predictions .", "entities": []}
{"text": "This type of analysis is very insightful as it enables one to see which identities are more at risk for discrimination .", "entities": []}
{"text": "A more direct way to measure how certain groups receive similar unfair treatment is to compute a correlation matrix between the vectors containing negative sentiment predictions for each identity term .", "entities": []}
{"text": "But this visual brings out that certain groups like Indian , Mexican , and Russian have a high correlation , indicating that they could be treated similarly unfairly in a downstream ML algorithm .", "entities": []}
{"text": "This is a useful insight that could allow a practitioner to change to embedding training corpora to create fairer models .", "entities": []}
{"text": "This visual also brings out slight differences in negative sentiment prediction .", "entities": []}
{"text": "Identity terms like Scottish have lower correlations across the board , manifesting that this identity has slightly less negative sentiment than the rest of the identities .", "entities": []}
{"text": "This is important to analyze to get a broader context for how various identities could receive different amounts of discrimination stemming from the word embedding bias .", "entities": []}
{"text": "We can use these figures to analyze how certain groups could be similarly discriminated against via their negative sentiment correlation .", "entities": []}
{"text": "We showed how our framework can be used in the religious and national origin case studies .", "entities": []}
{"text": "In practice , our framework should be used to measure bias among demographics of interest for the NLP application in question .", "entities": []}
{"text": "Our RNSB metric is a useful signal a practitioner can use to choose the embedding model with the least amount of risk for discrimination in their application , or even to evaluate what types of unintended biases exists in their training corpora .", "entities": []}
{"text": "For this work our scope was limited to unfair biases with respect to negative sentiment .", "entities": []}
{"text": "This allows us to observe clearer signals of bias in our metric , Relative Negative Sentiment Bias ( RNSB ) .", "entities": []}
{"text": "Our metric has a direct connection to discrimination and can evaluate any number of demographics in a protected group .", "entities": []}
{"text": "This work was made possible in part through support of the United States Agency for International Development .", "entities": []}
{"text": "The opinions expressed herein are those of the authors and do not necessarily reflect the views of the United States Agency for International Development or the US Government .", "entities": []}
{"text": "Will I Sound Like Me ?", "entities": []}
{"text": "Improving Persona Consistency in Dialogues through Pragmatic Self - Consciousness", "entities": []}
{"text": "We explore the task of improving persona consistency of dialogue agents .", "entities": []}
{"text": "However , such additional labels and training can be demanding .", "entities": []}
{"text": "Also , we find even the bestperforming persona - based agents are insensitive to contradictory words .", "entities": []}
{"text": "Our approach , based on the Rational Speech Acts framework ( Frank and Goodman , 2012 ) , can enforce dialogue agents to refrain from uttering contradiction .", "entities": []}
{"text": "We further extend the framework by learning the distractor selection , which has been usually done manually or randomly .", "entities": []}
{"text": "Results on Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models .", "entities": []}
{"text": "Moreover , we show that it can be generalized to improve contextconsistency beyond persona in dialogues .", "entities": []}
{"text": "In the study of dialogue agents , consistency has been a long - standing issue .", "entities": []}
{"text": "To resolve this , much research has been conducted to endow dialogue agents with personas .", "entities": []}
{"text": "Li et al ( 2016 ) propose to encode persona in embeddings and Zhang et al ( 2018 ) introduce a persona - conditioned dialogue dataset .", "entities": []}
{"text": "On top of these works , many efforts have been made to improve consistency .", "entities": []}
{"text": "In spite of such recent significant progress , there is much room for improving persona - based dialogue agents .", "entities": []}
{"text": "We observe that even the best performing persona - based generative models ( See et al , 2019 ; Wolf et al , 2019b ; I like to stay at home .", "entities": []}
{"text": "I like going outside .", "entities": []}
{"text": "I like going outside .", "entities": []}
{"text": "I love Disneyland !", "entities": []}
{"text": "I go there every week .", "entities": []}
{"text": "' Will I sound like me ? '", "entities": []}
{"text": "Figure 1 : Illustration of the consistency issue in dialogue .", "entities": []}
{"text": "Icons are designed by Nhor Phai and Vincent Le Moign . are highly insensitive to contradictory words , and thus fail to deliver consistent persona to the interlocutor ( Figure 1 ) .", "entities": []}
{"text": "Also , extra modules other than the generative model is often required for improving consistency .", "entities": []}
{"text": "Recent works on consistency in persona - based dialogue actively adopt the NLIbased approach ( Welleck et al , 2019 ; Song et al , 2019 ; Li et al , 2020 ; Song et al , 2020 ) , which have the following prerequisites .", "entities": []}
{"text": "First , they require labeled pairs of persona sentences and dialogue utterances with three categories : entailment , neutral , and contradiction .", "entities": []}
{"text": "In this work , we step back from this NLI - based supervised approach and ponder : how do humans maintain consistency ?", "entities": []}
{"text": "We humans never learn how to be consistent .", "entities": []}
{"text": "Instead , we have an innate drive for consistency to hold our beliefs and behavior in harmony ( Festinger , 1962 ) .", "entities": []}
{"text": "If so , how do we know we are consistent or not ?", "entities": []}
{"text": "We do not ask others .", "entities": []}
{"text": "We ask ourselves by predicting how we are perceived by others .", "entities": []}
{"text": "Public self - consciousness is this awareness of the self as a social object that can be observed and evaluated by others ( Fenigstein et al , 1975 ) .", "entities": []}
{"text": "We particularly emphasize that public self - consciousness is not equivalent to the philosophical self - consciousness ( or self - awareness ) 1 .", "entities": []}
{"text": "Simply put , public self - consciousness is the concern about how oneself will be perceived by others , as opposed to the philosophical state of being conscious of self - existence .", "entities": []}
{"text": "According to Doherty and Schlenker ( 1991 ) , people with high public self - consciousness tend to act more consistent with known information about themselves .", "entities": []}
{"text": "They care deeply about how others will evaluate them and have a strong tendency to avoid negative evaluations ( Fenigstein et al , 1975 ) .", "entities": []}
{"text": "Since inconsistency is condemned by others , one who has high public self - consciousness will try more to maintain consistency .", "entities": []}
{"text": "In order to predict how we are perceived , we rely on abstract models of others ( Gopnik and Wellman , 1992 ) and simulate others ' reactions based on imagination ( Hassabis et al , 2013 ) .", "entities": []}
{"text": "Modeling a listener has been one of the main topics in computational pragmatics .", "entities": []}
{"text": "Our work extends this long line of work in cognitive science by making use of the Bayesian Rational Speech Acts framework ( Frank and Goodman , 2012 ) , which has been originally applied to improving informativeness of referring expressions .", "entities": []}
{"text": "Since personas ought to express who we are , we adopt this framework for dialogue agents by regarding personas as targets that should be conveyed to the interlocutor .", "entities": []}
{"text": "In summary , we take inspiration from social cognition and pragmatics to endow generative agents with self - consciousness , which makes them imagine the listener 's reaction and incorporate it to the generation process for improving consistency .", "entities": []}
{"text": "Our major contributions can be outlined as follows : ( 1 ) We propose an orthogonally applicable approach for any persona - based generative agents to improve consistency without the use of additional consistency labels and training .", "entities": []}
{"text": "Moreover , it is even generalizable to improve context - consistency beyond persona in dialogue .", "entities": []}
{"text": "( 2 ) We extend the Rational Speech Acts framework ( Frank and Goodman , 2012 ) with two new technical features : ( i ) a learning method for distractor selection ( e.g. other samples different from the given target ( Andreas and Klein , 2016 ) ) , which has been usually done manually or randomly , and ( ii ) a different update for the listener 's world prior that better preserves information of previous states .", "entities": []}
{"text": "( 3 ) Our approach improves consistency of three recent generative agents ( See et al , 2019 ; Wolf et al , 2019b ; over Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) .", "entities": []}
{"text": "Persona & Consistency in Dialogue .", "entities": []}
{"text": "Li et al ( 2016 ) learn personas in embeddings .", "entities": []}
{"text": "Zhang et al ( 2018 ) release the PersonaChat dataset , a chitchat dialogue set involving two interlocutors each playing their given persona .", "entities": []}
{"text": "use reinforcement learning to enhance mutual persona perception .", "entities": []}
{"text": "Recent works use extra modules or NLI labels to improve consistency .", "entities": []}
{"text": "Shum et al ( 2019 ) fill generated templates , and rank with a language model .", "entities": []}
{"text": "use self - supervised feature extractors for generation .", "entities": []}
{"text": "Welleck et", "entities": []}
{"text": "al ( 2019 ) annotate NLI labels to the PersonaChat dataset .", "entities": []}
{"text": "They train an NLI model and run pairwise comparison between candidates and persona to compute contradiction scores .", "entities": []}
{"text": "They require NLI labels on the target dialogue dataset ; otherwise , sharp decrease in performance is observed , due to mismatch of data distribution", "entities": []}
{"text": "( Welleck et al , 2019 ) .", "entities": []}
{"text": "Such dataset - specific NLI annotations and training NLI models can be costly and time - consuming .", "entities": []}
{"text": "Compared to previous methods , the novelty of our approach is to improve consistency without NLI labels and extra modules .", "entities": []}
{"text": "Pragmatics .", "entities": []}
{"text": "However , its application to the dialogue domain remains understudied .", "entities": []}
{"text": "In this work , we explore how the RSA framework can be adopted in dialogue agents to alleviate the inconsistency problem .", "entities": []}
{"text": "Also , we further extend the framework by making the distractor selection as a learnable process .", "entities": []}
{"text": "( Zhang et al , 2018 ) .", "entities": []}
{"text": "They collect entailing and contradictory utterances to the given persona , and release an evaluation set comprised of dialogues each with 31 utterance candidates : 10 entailing , 10 neutral , and 10 contradictory utterances with 1 ground - truth ( GT ) utterance .", "entities": []}
{"text": "On this evaluation set , we run three recent models ( See et al , 2019 ;", "entities": []}
{"text": "Wolf et al , 2019b et al , 2020 ) that achieve the best performance on PersonaChat .", "entities": []}
{"text": "Each metric is the proportion of GT , entailing , neutral and contradictory utterances in the top - 1 candidates returned by the model , respectively .", "entities": []}
{"text": "Figure 2 shows that all three models select contradictory candidates much more often than the GT utterances ( see further results in Table 3 ) .", "entities": []}
{"text": "Though models are conditioned on a given persona , they are highly insensitive to contradictions .", "entities": []}
{"text": "To investigate why insensitivity to contradiction prevails in the state - of - the - art models , we further analyze the contradictory utterances returned by the models ( Contradict@1 - Utt ) , comparing with the GT utterances and the top - ranked entailing candidates ( Top Entail - Utt ) .", "entities": []}
{"text": "In the Dialogue NLI dataset , every utterance is labeled with a triple ( entity 1 , relation , entity 2 ) , such as \" I just like to listen to rock music \" with ( i , like music , rock ) .", "entities": []}
{"text": "By construction , Contradict@1 - Utt must contain words that are contradictory to the GT utterance and the given persona .", "entities": []}
{"text": "It reveals that models behave more as a plain language model rather than as a persona - conditioned model .", "entities": []}
{"text": "Thus , guarantee of consistency for each word generation step is required for persona - based dialogue agents to resolve such issue .", "entities": []}
{"text": "We introduce how to endow dialogue agents with public self - consciousness , which helps them keep consistency in mind at each generation step by reflecting an imaginary listener 's distribution over personas .", "entities": []}
{"text": "Figure 3 illustrates its overall structure .", "entities": []}
{"text": "We present how to model public selfconsciousness using the Rational Speech Acts ( RSA ) framework ( Frank and Goodman , 2012 ) in Section 4.1 .", "entities": []}
{"text": "We then discuss learning of distractor selection as our major novelty for the RSA in Section 4.2 .", "entities": []}
{"text": "Given that modeling the interactions between listener and speaker is a main topic in pragmatics , we take advantage of the RSA framework ( Frank and Goodman , 2012 ) .", "entities": []}
{"text": "It treats language use as a recursive process where probabilistic speaker and listener reason about each other 's intentions in a Bayesian fashion .", "entities": []}
{"text": "We first assume persona i is given to the base speaker , along with the dialogue \u221d # \" \u210e , $ \" , \" % \u00d7 # \" \" , \u210e , & \" ) \" ' !", "entities": []}
{"text": "( ) Imaginary Listener : # \" ( | $ \" , \u210e , \" )", "entities": []}
{"text": "It recursively generates the next token u t at every time t. history h and partial utterance u < t , as shown in Figure 3 .", "entities": []}
{"text": "( u t | i , h , u < t ) .", "entities": []}
{"text": "See the details in Section 5.2 .", "entities": []}
{"text": "While the base speaker generates each token one at a time , the imaginary listener reasons about the speaker 's persona .", "entities": []}
{"text": "The distractors are different personas from other dialogue instances in the dataset .", "entities": []}
{"text": "We decide world I per dialogue instance through learning , which will be elaborated in Section 4.2 . Self - Conscious Speaker S 1 .", "entities": []}
{"text": "( i | h , u \u2264t , p t )", "entities": []}
{"text": "By taking the listener 's distribution into account , the speaker is now self - conscious about what persona it sounds like .", "entities": []}
{"text": "i rather than some other persona i .", "entities": []}
{"text": "The likelihood of each token being identified as the persona i acts as a bonus added to the base speaker 's token scores .", "entities": []}
{"text": "Hence , tokens that are consistent to the given persona are preferred to others .", "entities": []}
{"text": "The token with the highest probability is added to the partial utterance , becoming the next input u < t+1 for the speaker .", "entities": []}
{"text": "( i ) , we update the world prior p t+1 ( i ) according to S 1 's output u t at every time step :", "entities": []}
{"text": "p t+1 ( i )", "entities": []}
{"text": "( i | h , u \u2264t , p t ) .", "entities": []}
{"text": "( i | h , u \u2264t , p t ) makes little practical effect compared to a uniform prior .", "entities": []}
{"text": "We find that updating the prior with Eq .", "entities": []}
{"text": "( 3 ) instead is effective .", "entities": []}
{"text": "See the results in Section 5.6 .", "entities": []}
{"text": "Distractors ( Andreas and Klein , 2016 ) are samples ( e.g. other personas in the dataset ) which are different from the given target .", "entities": []}
{"text": "In previous works of RSA , the distractors to be included in world I are selected manually or randomly from the dataset .", "entities": []}
{"text": "However , we find that performance variance is large according to the selected distractors .", "entities": []}
{"text": "Therefore , it can efficiently memorize and retrieve distractor personas for each context .", "entities": []}
{"text": "To better select useful distractor personas , supervised learning is desirable .", "entities": []}
{"text": "However , there is no explicit label indicating which distractors are helpful for each dialogue .", "entities": []}
{"text": "In other words , the score represents consistency and also appropriateness at the same time .", "entities": []}
{"text": "Each training datapoint comprises a given persona , a distractor persona and dialogue context .", "entities": []}
{"text": "Memory Structure .", "entities": []}
{"text": "The memory consists of three types of information : M = ( K , v , a ) .", "entities": []}
{"text": "K R m\u00d7d is a key matrix , where m is the number of memory slots and d is the dimension of the key vectors , which are the embedding of datapoints .", "entities": []}
{"text": "The value vector v R m stores the index of a persona .", "entities": []}
{"text": "a R m is an age vector , which is used for memory update .", "entities": []}
{"text": "We set m = 16 , 000 and d = 768 .", "entities": []}
{"text": "Memory Addressing .", "entities": []}
{"text": "Using the cosine similarity between q and each memory key , we can find the k nearest neighbors : ( n 1 , n 2 , ... , n k ) =", "entities": []}
{"text": "N N k ( q , K ) .", "entities": []}
{"text": "( 4 ) Memory Loss .", "entities": []}
{"text": "Suppose that the query datapoint has a distractor label l.", "entities": []}
{"text": "Among ( n 1 , ... , n k ) , we denote the positive neighbor n p as the one with v [ n p ]", "entities": []}
{"text": "= l and the negative neighbor n b with v [ n b ] = l. If there are multiple positive neighbors , we pick the one with the smallest memory index .", "entities": []}
{"text": "If no positive neighbor is found , we select a random key whose value is l. For the negative neighbor , we select one randomly from ( n 1 , ... , n k ) .", "entities": []}
{"text": "Memory Update .", "entities": []}
{"text": "( 1 ) If the top - 1 neighbor 's value ( i.e. persona ) is correct ( v [ n 1 ] = l ) , the key vector is updated as : K [ n 1 ]", "entities": []}
{"text": "q + K [ n 1 ] q + K [ n 1 ] .", "entities": []}
{"text": "( 6 ) ( 2 )", "entities": []}
{"text": "Otherwise ( v [ n 1 ] = l ) , we make a slot for the query ; we find the oldest memory slot n according to the age vector a and write K", "entities": []}
{"text": "[", "entities": []}
{"text": "( 7 ) Training & Inference .", "entities": []}
{"text": "We find n nearest keys from the memory , and use their values ( i.e. persona indices ) as the distractor personas .", "entities": []}
{"text": "We set n = 2 .", "entities": []}
{"text": "We prove its effectiveness using both automatic and human evaluations .", "entities": []}
{"text": "We also show our framework can be generalized to improve consistency of dialogue context beyond persona .", "entities": []}
{"text": "Dialogue NLI Evaluation Set ( Welleck et al , 2019 ) .", "entities": []}
{"text": "This dataset is based on PersonaChat with additional NLI annotations .", "entities": []}
{"text": "Its main task is to rank next - utterance candidates given previous context .", "entities": []}
{"text": "For each dialogue , they collect 31 next - utterance candidates in respect to the given persona : 10 entailing , 10 neutral and 10 contradicting candidates with 1 ground - truth utterance .", "entities": []}
{"text": "In total , the evaluation set includes 542 instances .", "entities": []}
{"text": "PersonaChat dialogue ( Zhang et al , 2018 ) .", "entities": []}
{"text": "This dataset involves two interlocutors who are each given a persona and asked to get to know each other while playing their roles .", "entities": []}
{"text": "The competition version contains 17 , 878 chitchat conversations conditioned on 1 , 155 personas for training and 1 , 000 conversations conditioned on 100 personas for validation .", "entities": []}
{"text": "Base Speakers .", "entities": []}
{"text": "Our approach improves these base speakers by ( Welleck et al , 2019 ) .", "entities": []}
{"text": "+ DM is the Distractor Memory .", "entities": []}
{"text": "We defer implementation details to Appendix .", "entities": []}
{"text": "Evaluation Metrics .", "entities": []}
{"text": "Each metric is the proportion of GT , entailing , and contradictory utterances in the top - 1 candidates returned by the model , respectively .", "entities": []}
{"text": "High scores in Entail@1 and low scores in Contradict@1 indicate better consistency with the persona .", "entities": []}
{"text": "The C score is a metric for dialogue consistency , introduced in Madotto et al ( 2019 ) .", "entities": []}
{"text": "It computes pairwise comparison between utterance u and persona sentence p j with a pretrained NLI model .", "entities": []}
{"text": "We sum the NLI scores across persona sentences per dialogue instance : C ( u ) = j NLI ( u , p j ) .", "entities": []}
{"text": "Results on Dialogue NLI .", "entities": []}
{"text": "Table 3 compares the performance of dialogue agents on the Dialogue NLI evaluation set .", "entities": []}
{"text": "We remind that each entailing candidate shares the same annotated triple as the GT utterance .", "entities": []}
{"text": "In other words , they have similar semantics to the GT utterance and follow the ( Zhang et al , 2018 ) .", "entities": []}
{"text": "C is the consistency score evaluated by a pretrained NLI model ( Madotto et al , 2019 ) .", "entities": []}
{"text": "given persona .", "entities": []}
{"text": "The Distractor Memory ( DM ) is better than random distractor selection for S 1 across all metrics .", "entities": []}
{"text": "It concludes that learned distractors are more effective than random distractors for pragmatic agents .", "entities": []}
{"text": "Results on PersonaChat .", "entities": []}
{"text": "Table 4 compares the performance of different dialogue agents on the PersonaChat dataset .", "entities": []}
{"text": "Thus , being consistent to the given persona can also help improve the generation performance of dialogue agents .", "entities": []}
{"text": "Comparison with agents that use NLI model .", "entities": []}
{"text": "We also test agents with pretrained NLI models attached ( Welleck et al , 2019 ) , denoted by + NLI in Table 5 .", "entities": []}
{"text": "The NLI model computes contradiction scores of each candidate utterances , and penalize its rank accordingly .", "entities": []}
{"text": "Compared to base agents with no self - consciousness , our agents improve consistency in all three metrics even further when using additional NLI models .", "entities": []}
{"text": "We perform human evaluation via Amazon Mechanical Turk .", "entities": []}
{"text": "We random sample 250 test examples , each is rated by three unique human judges in terms of ( i ) Consistency and ( ii ) Engagingness .", "entities": []}
{"text": "Turkers are shown a given persona , a dialogue context , and the model 's generated utterance .", "entities": []}
{"text": "Following See et al ( 2019 ) , we evaluate the engagingness of the utterance in a 4 - point scale , where higher scores are better .", "entities": []}
{"text": "To alleviate annotator bias and inter - annotator variability , we apply Bayesian calibration ( Kulikov et al , 2019 ) to the scores .", "entities": []}
{"text": "Table 6 summarizes the human evaluation results .", "entities": []}
{"text": "Thus , the responses from self - conscious agents have their own color , which can help improving engagingness .", "entities": []}
{"text": "Figure 4 displays selected examples of utterance generation .", "entities": []}
{"text": "Each example is comprised of dialogue history , human response , and utterances generated by our method and baselines .", "entities": []}
{"text": "Table 7 reports the results of context conditioned self - conscious agents .", "entities": []}
{"text": "The EmpatheticDialogue ( Rashkin et al , 2019 ) is an open - domain dialogue dataset where a speaker describes a past emotional experience and the listener responds accordingly .", "entities": []}
{"text": "Since the speaker 's descriptions should be consistent to the experience and previous utterances , it is a suitable benchmark for consistency .", "entities": []}
{"text": "We model the speaker 's utterances and measure its consistency .", "entities": []}
{"text": "Thus , our approach can also be applied to help agents stay more consistent to its context .", "entities": []}
{"text": "( Zhang et al , 2018 ) .", "entities": []}
{"text": "World Prior Update .", "entities": []}
{"text": "( 3 ) .", "entities": []}
{"text": "As reported in Cohn - Gordon et al ( 2018 ) , our experiments on the Dialogue NLI dataset confirm the prior update with L t 1 makes little difference in performance compared with using a uniform distribution .", "entities": []}
{"text": "Hence , the update with L t 1 becomes more of an instantaneous prior rather than a cumulative one .", "entities": []}
{"text": "at current time step overrides the cumulative prior p t ( i ) .", "entities": []}
{"text": "That is , the utterance state evolves shortsightedly , ignoring the context information from the previous steps .", "entities": []}
{"text": "Then , the language model degenerates to favor uttering fragments of the given persona while even ignoring the syntax .", "entities": []}
{"text": "This work investigated how modeling public selfconsciousness can help dialogue agents improve persona - consistency .", "entities": []}
{"text": "We showed existing dialogue agents are highly insensitive to contradiction , and introduced an orthogonally applicable method using the RSA framework ( Frank and Goodman , 2012 ) to alleviate the issue .", "entities": []}
{"text": "We also designed a 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3 . learning method for distractor selection , named Distractor Memory and proposed a better update for the listener 's world prior .", "entities": []}
{"text": "Furthermore , we demonstrated how our approach can be generalized to improve dialogue context - consistency .", "entities": []}
{"text": "Our self - conscious agents improved the base agents on the Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) dataset , without consistency labels and NLI models .", "entities": []}
{"text": "An important future direction will be generating the distractors and learning the rationality coefficients .", "entities": []}
{"text": "A Results on Variants of Distractor Selection ( Section 4.2 ) ( Welleck et al , 2019 ) .", "entities": []}
{"text": "As a straightforward baseline , we randomly select k personas from training set and directly use it as distractors .", "entities": []}
{"text": "The third baseline denoted by Farthest is to find the k - farthest persona among the training personas .", "entities": []}
{"text": "Table 8 compares the performance of different distractor selecting methods on the Dialogue NLI evaluation set ( Welleck et al , 2019 ) .", "entities": []}
{"text": "The DM model outperforms all the baselines across all metrics .", "entities": []}
{"text": "The Farthest shows better performance than the Nearest .", "entities": []}
{"text": "It can be understood that dissimilar distractors are more effective in the Rational Speech Acts framework ( Frank and Goodman , 2012 ) .", "entities": []}
{"text": "Base Codes and Datasets .", "entities": []}
{"text": "We use the ParlAI framework 2 ( Miller et al , 2017 ) and Hugging - Face 's Transformers 3 ( Wolf et al , 2019a ) to implement our models and baselines .", "entities": []}
{"text": "We use Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) datasets from the ParlAI framework as is .", "entities": []}
{"text": "We use the default preprocessing in ParlAI . Training .", "entities": []}
{"text": "Our self - consciousness approach improves consistency for any pretrained dialogueagents without additional consistency labels and pretrained NLI models .", "entities": []}
{"text": "Since it post - processes the output probability of pretrained dialogue - agents in a Bayesian fashion , no additional model parameters are added to the dialogue agents .", "entities": []}
{"text": "Thus , it does not require any training .", "entities": []}
{"text": "Then we find the best distractor persona for each model and use those labels to train our DM .", "entities": []}
{"text": "Hyperparameters .", "entities": []}
{"text": "= 1.0 , and the cardinality of the world I to 3 .", "entities": []}
{"text": "We also set |", "entities": []}
{"text": "I | = 3 .", "entities": []}
{"text": "The posterior distribution of our self - conscious agents are computed deterministically .", "entities": []}
{"text": "For our Distractor Memory , we set the memory key matrix as K R m\u00d7d , where m = 16000 and d = 768 .", "entities": []}
{"text": "We use greedy decoding for all methods .", "entities": []}
{"text": "The average runtime for our self - conscious approach is dependent on the base dialogue agents and the cardinality of world I which can be run in parallel like beam search .", "entities": []}
{"text": "Evaluation .", "entities": []}
{"text": "We follow the evaluation of the Par - lAI framework .", "entities": []}
{"text": "Figure 7 shows selected examples of generated responses .", "entities": []}
{"text": "We use TransferTransfo ( Wolf et al , 2019b ) as a base speaker .", "entities": []}
{"text": "We would like to thank Reuben Cohn - Gordon , Sean Welleck , Junhyug Noh and Jiwan Chung for their valuable comments .", "entities": []}
{"text": "We also thank the anonymous reviewers for their thoughtful suggestions on this work .", "entities": []}
{"text": "[ P1 ] I really enjoy shopping and my dream is to one day own a Rolls Royce ghost .", "entities": []}
{"text": "[ P2 ]", "entities": []}
{"text": "Wow .", "entities": []}
{"text": "I enjoy running over driving .", "entities": []}
{"text": "[ P1 ] Running is also quite lovely .", "entities": []}
{"text": "Breathing in the lovely outside air .", "entities": []}
{"text": "[ P2 ] Yes it is .", "entities": []}
{"text": "It clears my head when I need to as well .", "entities": []}
{"text": "( S 1 + DM ) shopping is a great way to clear my head .", "entities": []}
{"text": "i love to shop and watch movies .", "entities": []}
{"text": "( Human ) yes , and it also helps with depression i have found .", "entities": []}
{"text": "Analogical Reasoning on Chinese Morphological and Semantic Relations", "entities": []}
{"text": "Analogical reasoning is effective in capturing linguistic regularities .", "entities": []}
{"text": "This paper proposes an analogical reasoning task on Chinese .", "entities": []}
{"text": "After delving into Chinese lexical knowledge , we sketch 68 implicit morphological relations and 28 explicit semantic relations .", "entities": []}
{"text": "A big and balanced dataset CA8 is then built for this task , including 17813 questions .", "entities": []}
{"text": "Furthermore , we systematically explore the influences of vector representations , context features , and corpora on analogical reasoning .", "entities": []}
{"text": "Recently , the boom of word embedding draws our attention to analogical reasoning on linguistic regularities .", "entities": []}
{"text": "Given the word representations , analogy questions can be automatically solved via vector computation , e.g. \" apples - apple + car \u2248 cars \" for morphological regularities and \" kingman + woman \u2248 queen \" for semantic regularities ( Mikolov et al , 2013 ) .", "entities": []}
{"text": "In addition , It can be used in inducing morphological transformations ( Soricut and Och , 2015 ) , detecting semantic relations ( Herdagdelen and Baroni , 2009 ) , and translating unknown words ( Langlais and Patry , 2007 ) .", "entities": []}
{"text": "It is well known that linguistic regularities vary a lot among different languages .", "entities": []}
{"text": "For example , Chinese is a typical analytic language which lacks inflection .", "entities": []}
{"text": "Figure 1 shows that function words and reduplication are used to denote grammatical and semantic information .", "entities": []}
{"text": "In addition , many semantic \u2020 Corresponding author .", "entities": []}
{"text": "relations are closely related with social and cultural factors , e.g. in Chinese \" sh\u012b - xi\u0101n \" ( god of poetry ) refers to the poet Li - bai and \" sh\u012b - sh\u00e8ng \" ( saint of poetry ) refers to the poet Du - fu .", "entities": []}
{"text": "However , few attempts have been made in Chinese analogical reasoning .", "entities": []}
{"text": "The only Chinese analogy dataset is translated from part of an English dataset ( Chen et al , 2015 ) ( denote as CA_translated ) .", "entities": []}
{"text": "Yin et", "entities": []}
{"text": "al , 2016 ; Su and Lee , 2017 ) , it could not serve as a reliable benchmark since it includes only 134 unique Chinese words in three semantic relations ( capital , state , and family ) , and morphological knowledge is not even considered .", "entities": []}
{"text": "Therefore , we would like to investigate linguistic regularities beneath Chinese .", "entities": []}
{"text": "By modeling them as an analogical reasoning task , we could further examine the effects of vector offset methods in detecting Chinese morphological and semantic relations .", "entities": []}
{"text": "As far as we know , this is the first study focusing on Chinese analogical reasoning .", "entities": []}
{"text": "Moreover , we release a standard benchmark for evaluation of Chinese word embedding , together with 36 open - source pre - trained embeddings at GitHub 1 , which could serve as a solid basis for Chinese NLP tasks .", "entities": []}
{"text": "Morphology concerns the internal structure of words .", "entities": []}
{"text": "There is a common belief that Chinese is a morphologically impoverished language since a morpheme mostly corresponds to an orthographic character , and it lacks apparent distinctions between roots and affixes .", "entities": []}
{"text": "However , Packard ( 2000 ) suggests that Chinese has a different morphological system because it selects different \" settings \" on parameters shared by all languages .", "entities": []}
{"text": "We will clarify this special system by mapping its morphological analogies into two processes : reduplication and semi - affixation .", "entities": []}
{"text": "Reduplication means a morpheme is repeated to form a new word , which is semantically and/or syntactically distinct from the original morpheme , e.g. the word \" ti\u0101n - ti\u0101n \" ( day day ) in Figure 1 ( b ) means \" everyday \" .", "entities": []}
{"text": "By analyzing all the word categories in Chinese , we find that nouns , verbs , adjectives , adverbs , and measure words have reduplication abilities .", "entities": []}
{"text": "Given distinct morphemes A and B , we summarize 6 repetition patterns in Figure 2 .", "entities": []}
{"text": "Each pattern may have one or more morphological functions .", "entities": []}
{"text": "Taking Pattern 1 ( A AA ) as an example , noun morphemes could form kinship terms or yield every / each meaning .", "entities": []}
{"text": "For verbs , it signals doing something a little bit or things happen briefly .", "entities": []}
{"text": "AA reduplication could also intensify an adjective or transform it to an adverb .", "entities": []}
{"text": "b\u00e0 ( dad ) b\u00e0 - b\u00e0 ( dad ) ti\u0101n ( day ) ti\u0101n - ti\u0101n ( everyday ) shu\u014d ( say ) shu\u014d - shuo ( say a little ) k\u00e0n ( look ) k\u00e0n - k\u00e0n ( have a brief look ) d\u00e0 ( big ) d\u00e0 - d\u00e0 ( very big ; greatly ) sh\u0113n ( deep ) sh\u0113n - sh\u0113n ( deeply ) 1 https://github.com/Embedding/Chinese - Word - Vectors", "entities": []}
{"text": "Affixation is a morphological process whereby a bound morpheme ( an affix ) is attached to roots or stems to form new language units .", "entities": []}
{"text": "Chinese is a typical isolating language that has few affixes .", "entities": []}
{"text": "Liu et al ( 2001 ) points out that although affixes are rare in Chinese , there are some components behaving like affixes and can also be used as independent lexemes .", "entities": []}
{"text": "They are called semi - affixes .", "entities": []}
{"text": "To model the semi - affixation process , we uncover 21 semi - prefixes and 41 semi - suffixes .", "entities": []}
{"text": "These semi - suffixes can be used to denote changes of meaning or part of speech .", "entities": []}
{"text": "For example , the semi - prefix \" d\u00ec - \" could be added to numerals to form ordinal numbers , and the semi - suffix \" - zi \" is able to nominalize an adjective : y\u012b ( one ) d\u00ec - y\u012b ( first ) \u00e8r ( two ) d\u00ec - \u00e8r ( second ) p\u00e0ng ( fat ) p\u00e0ng - zi ( a fat man ) sh\u00f2u ( thin ) sh\u00f2u - zi ( a thin man )", "entities": []}
{"text": "To investigate semantic knowledge reasoning , we present 28 semantic relations in four aspects : geography , history , nature , and people .", "entities": []}
{"text": "Among them we inherit a few relations from English datasets , e.g. country - capital and family members , while the rest of them are proposed originally on the basis of our observation of Chinese lexical knowledge .", "entities": []}
{"text": "For example , a Chinese province may have its own abbreviation , capital city , and representative drama , which could form rich semantic analogies : \u0101n - hu\u012b vs zh\u00e8 - ji\u0101ng ( province ) w\u01cen vs zh\u00e8 ( abbreviation ) h\u00e9 - f\u00e9i vs h\u00e1ng - zh\u014du ( capital ) hu\u00e1ng - m\u00e9i - x\u00ec vs yu\u00e8 - j\u00f9 ( drama )", "entities": []}
{"text": "We also address novel relations that could be used for other languages , e.g. scientists and their findings , companies and their founders .", "entities": []}
{"text": "Analogical reasoning task is to retrieve the answer of the question \" a is to b as c is to ? \" .", "entities": []}
{"text": "Based on the relations discussed above , we firstly collect word pairs for each relation .", "entities": []}
{"text": "We exploit 3COSMUL to solve the analogical questions suggested by Levy and Goldberg ( 2014a ) .", "entities": []}
{"text": "pair .", "entities": []}
{"text": "To avoid the imbalance problem addressed in English benchmarks ( Gladkova et al , 2016 ) , we set a limit of 50 word pairs at most for each relation .", "entities": []}
{"text": "In this step , 1852 unique Chinese word pairs are retrieved .", "entities": []}
{"text": "We then build CA8 , a big , balanced dataset for Chinese analogical reasoning including 17813 questions .", "entities": []}
{"text": "Compared with CA_translated ( Chen et al , 2015 ) , CA8 incorporates both morphological and semantic questions , and it brings in much more words , relation types and questions .", "entities": []}
{"text": "Table 1 shows details of the two datasets .", "entities": []}
{"text": "They are both used for evaluation in Experiments section .", "entities": []}
{"text": "In Chinese analogical reasoning task , we aim at investigating to what extent word vectors capture the linguistic relations , and how it is affected by three important factors : vector representations ( sparse and dense ) , context features ( character , word , and ngram ) , and training corpora ( size and domain ) .", "entities": []}
{"text": "Table 2 shows the hyper - parameters used in this work .", "entities": []}
{"text": "All the text data used in our experiments ( as shown in Table 3 ) are preprocessed via the following steps : Remove the html and xml tags from the texts and set the encoding as utf - 8 .", "entities": []}
{"text": "Convert traditional Chinese characters into simplified characters with Open Chinese Convert ( OpenCC ) 2 .", "entities": []}
{"text": "Existing vector representations fall into two types , dense vectors and sparse vectors .", "entities": []}
{"text": "Table 4 lists the performance of them on CA_translated and CA8 datasets under different configurations .", "entities": []}
{"text": "The above observation shows that CA8 is a reliable benchmark for studying the effects of dense and sparse vectors .", "entities": []}
{"text": "Compared with CA_translated and existing English analogy datasets , it offers both morphological and semantic questions which are also balanced across different types 4 .", "entities": []}
{"text": "To investigate the influence of context features on analogical reasoning , we consider not only word features , but also ngram features inspired by statistical language models , and character ( Hanzi ) features based on the close relationship between Chinese words and their composing characters 5 .", "entities": []}
{"text": "Specifically , we use word bigrams for ngram features , character unigrams and bigrams for character features .", "entities": []}
{"text": "Ngrams and Chinese characters are effective features in training word representations ( Zhao et al , 2017 ; Chen et al , 2015 ; Bojanowski et al , 2016 ) .", "entities": []}
{"text": "However , Table 4 shows that there is only a slight increase on CA_translated dataset with ngram features , and the accuracies in most cases decrease after integrating character features .", "entities": []}
{"text": "In contrast , on CA8 dataset , the introduction of ngram and character features brings significant and consistent improvements on almost all the categories .", "entities": []}
{"text": "Furthermore , character features are especially advantageous for reasoning of morphological relations .", "entities": []}
{"text": "Besides , the representations achieve surprisingly high accuracies in some categories of CA_translated , which means that there is little room for further improvement .", "entities": []}
{"text": "However it is much harder for representation methods to achieve high accuracies on CA8 .", "entities": []}
{"text": "The best configuration only achieves 68.0 % .", "entities": []}
{"text": "We compare word representations learned upon corpora of different sizes and domains .", "entities": []}
{"text": "As shown in Table 3 , six corpora are used in the experiments : Chinese Wikipedia , Baidubaike , People 's Daily News , Sogou News , Zhihu QA , and \" Com - bination \" which is built by combining the first five corpora together .", "entities": []}
{"text": "Table 5 shows that accuracies increase with the growth in corpus size , e.g. Baidubaike ( an online Chinese encyclopedia ) has a clear advantage over Wikipedia .", "entities": []}
{"text": "Also , the domain of a corpus plays an important role in the experiments .", "entities": []}
{"text": "We can observe that vectors trained on news data are beneficial to geography relations , especially on People 's Daily which has a focus on political news .", "entities": []}
{"text": "Another example is Zhihu QA , an online questionanswering corpus which contains more informal data than others .", "entities": []}
{"text": "It is helpful to reduplication relations since many reduplication words appear frequently in spoken language .", "entities": []}
{"text": "With the largest size and varied domains , \" Combination \" corpus performs much better than others in both morphological and semantic relations .", "entities": []}
{"text": "Based on the above experiments , we find that vector representations , context features , and corpora all have important influences on Chinese analogical reasoning .", "entities": []}
{"text": "In this paper , we investigate the linguistic regularities beneath Chinese , and propose a Chinese analogical reasoning task based on 68 morphological relations and 28 semantic relations .", "entities": []}
{"text": "In the experiments , we apply vector offset method to this task , and examine the effects of vector representations , context features , and corpora .", "entities": []}
{"text": "This study offers an interesting perspective combining linguistic analysis and representation models .", "entities": []}
{"text": "This work is supported by the Fundamental Research Funds for the Central Universities , National Natural Science Foundation of China with Grant ( No.61472428 ) and Chinese Testing International Project ( No . CTI2017B12 ) .", "entities": []}
{"text": ".", "entities": []}
{"text": "277 .491 .625 .175 .199 .134 .251 .189 .146 .147 .250 .189 .181 Combination 15.9 G .872 .994 .710 .223 .300 .234 .518 .321 .662 .293 .310 .307 .467", "entities": []}
{"text": "The task challenged participants to classify whether human - written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from Wikipedia .", "entities": []}
{"text": "We received entries from 23 competing teams , 19 of which scored higher than the previously published baseline .", "entities": []}
{"text": "In this paper , we present the results of the shared task and a summary of the systems , highlighting commonalities and innovations among participating systems .", "entities": []}
{"text": "However , since information sources can contain errors , there exists an additional need to verify whether the information is correct .", "entities": []}
{"text": "This shared task required participants to develop systems to predict the veracity of human - generated textual claims against textual evidence to be retrieved from Wikipedia .", "entities": []}
{"text": "We constructed a purpose - built dataset for this task ( Thorne et al , 2018 ) that contains 185 , 445 human - generated claims , manually verified against the introductory sections of Wikipedia pages and labeled as SUPPORTED , REFUTED or NOTENOUGHINFO .", "entities": []}
{"text": "The claims were generated by paraphrasing facts from Wikipedia and mutating them in a variety of ways , some of which were meaning - altering .", "entities": []}
{"text": "For each claim , and without the knowledge of where the claim was generated from , annotators selected evidence in the form of sentences from Wikipedia to justify the labeling of the claim .", "entities": []}
{"text": "Performing well at this task requires both identifying relevant evidence and reasoning correctly with respect to the claim .", "entities": []}
{"text": "One of the limitations of using human annotators to identify correct evidence when constructing the dataset was the trade - off between annotation velocity and evidence recall ( Thorne et al , 2018 ) .", "entities": []}
{"text": "Evidence selected by annotators was often incomplete .", "entities": []}
{"text": "were given a sentence of unknown veracity called a claim .", "entities": []}
{"text": "The systems must identify suitable evidence from Wikipedia at the sentence level and Claim : The Rodney King riots took place in the most populous county in the USA .", "entities": []}
{"text": "The 1992 Los Angeles riots , also known as the Rodney King riots were a series of riots , lootings , arsons , and civil disturbances that occurred in Los Angeles County , California in April and May 1992 .", "entities": []}
{"text": "[ wiki / Los Angeles County ] Los Angeles County , officially the County of Los Angeles , is the most populous county in the USA . assign a label whether , given the evidence , the claim is SUPPORTED , REFUTED or whether there is NOTENOUGHINFO in Wikipedia to reach a conclusion .", "entities": []}
{"text": "In 16.82 % of cases , claims required the combination of more than one sentence as supporting or refuting evidence .", "entities": []}
{"text": "An example is provided in Figure 1 .", "entities": []}
{"text": "1", "entities": []}
{"text": "We used the reserved portion of the data presented in Thorne et", "entities": []}
{"text": "al ( 2018 ) as a blind test set .", "entities": []}
{"text": "Disjoint training , development and test splits of the dataset were generated by splitting the dataset by the page used to generate the claim .", "entities": []}
{"text": "The development and test datasets were balanced by randomly discarding claims from the more populous classes .", "entities": []}
{"text": "We used the scoring metric described in Thorne et", "entities": []}
{"text": "al ( 2018 ) to evaluate the submissions .", "entities": []}
{"text": "The training , development and test data splits contain multiple sets of evidence for each claim , each set being a minimal set of sentences that fully support or refute it .", "entities": []}
{"text": "Sentences labeled ( correctly ) as NOTENOUGHINFO do not require evidence .", "entities": []}
{"text": "We provide an open - source release of the scoring software .", "entities": []}
{"text": "86 submissions ( excluding the baseline ) were made to Codalab for scoring on the blind test set .", "entities": []}
{"text": "There were 23 different teams which participated in the task ( presented in Table 2 ) .", "entities": []}
{"text": "19 of these teams scored higher than the baseline presented in Thorne et al ( 2018 ) .", "entities": []}
{"text": "All participating teams were invited to submit a description of their systems .", "entities": []}
{"text": "We received 15 descriptions at the time of writing and the remaining are considered as withdrawn .", "entities": []}
{"text": "Most participants followed a similar pipeline structure to the baseline model .", "entities": []}
{"text": "However , some teams constructed models to jointly select sentences and perform inference in a single pipeline step , while others added an additional step , discarding inconsistent evidence after performing inference .", "entities": []}
{"text": "Based on the team - submitted system description summaries ( Appendix A ) , in the following section we present an overview of which models and techniques were applied to the task and their relative performance .", "entities": []}
{"text": "A large number of teams report a multi - step approach to document selection .", "entities": []}
{"text": "The majority of submissions report extracting some combination of Named Entities , Noun Phrases and Capitalized Expressions from the claim .", "entities": []}
{"text": "GESIS Cologne report directly selecting sentences using the Solr search , bypassing the need to perform document retrieval as a separate step .", "entities": []}
{"text": "UCL Machine Reading Group report a document retrieval approach that identifies Wikipedia article titles within the claim and ranks the results using features such as capitalization , sentence position and token match .", "entities": []}
{"text": "There were three common approaches to sentence selection : keyword matching , supervised classification and sentence similarity scoring .", "entities": []}
{"text": "Ohio State and UCL Machine Reading Group report using keyword matching techniques : matching either named entities or tokens appearing in both the claim and article body .", "entities": []}
{"text": "UCL Machine Reading Group and Directed Acyclic Graph report an additional aggregation stage after the classification stage in the pipeline where evidence that is inconsistent is discarded .", "entities": []}
{"text": "NLI was modeled as supervised classification in all reported submissions .", "entities": []}
{"text": "We compare and discuss the approaches for combining the evidence sentences together with the claim , sentence representations and training schemes .", "entities": []}
{"text": "Sentence Representation : University of Arizona explore using non - lexical features for predicting entailment , considering the proportion of negated verbs , presence of antonyms and noun overlap .", "entities": []}
{"text": "Columbia NLP learn universal sentence representations ( Conneau et al , 2017 ) .", "entities": []}
{"text": "UNC - NLP include an additional token - level feature the sentence similarity score from the sentence selection module .", "entities": []}
{"text": "( Miller , 1995 ) and Ohio State report using vector representations of named entities .", "entities": []}
{"text": "FujiXerox report representing sentences using DEISTE ( Yin et al , 2018 ) .", "entities": []}
{"text": "SWEEPer also report parameter tuning using reinforcement learning .", "entities": []}
{"text": "As mentioned in the introduction , to increase the evidence coverage in the test set , the evidence submitted by participating systems was annotated by shared task volunteers after the competition ended .", "entities": []}
{"text": "These claims were sampled for annotation with a probability proportional to the number of systems which labeled each of them incorrectly .", "entities": []}
{"text": "These extra annotations were performed by volunteers from the teams participating in the shared task and three of the organizers .", "entities": []}
{"text": "Annotators were asked to label whether the retrieved evidence sentences supported or refuted the claim at question , and to highlight which sentences ( if any ) , either individually or as a group , can be used as evidence .", "entities": []}
{"text": "We retained the annotation guidelines from Thorne et al ( 2018 ) ( see Sections A.7.1 , A.7.3 and A.8 from that paper for more details ) .", "entities": []}
{"text": "At the time of writing , 1 , 003 annotations were collected for 618 claims .", "entities": []}
{"text": "This identified 3 claims that were incorrectly labeled as SUPPORTED or REFUTED and 87 claims that were originally labeled as NOTENOUGHINFO that should be relabeled as SUPPORTED or REFUTED through the introduction of new evidence ( 44 and 43 claims respectively ) .", "entities": []}
{"text": "308 new evidence sets were identified for claims originally labeled as SUPPORTED or REFUTED , consisting of 280 single sentences and 28 sets of 2 or more sentences .", "entities": []}
{"text": "Further annotation is in progress and the data collected as well as the final results will be made public at the workshop .", "entities": []}
{"text": "The first Fact Extraction and VERification shared task attracted submissions from 86 submissions from 23 teams .", "entities": []}
{"text": "19 of these teams exceeded the score of the baseline presented in Thorne et", "entities": []}
{"text": "al ( 2018 ) .", "entities": []}
{"text": "For the teams which provided a system description , we highlighted the approaches , identifying commonalities and features that could be further explored .", "entities": []}
{"text": "Future work will address limitations in humanannotated evidence and explore other subtasks needed to predict the veracity of information extracted from untrusted sources .", "entities": []}
{"text": "A.1 UNC - NLP Our system is composed of three connected components namely , a document retriever , a sentence selector , and a claim verifier .", "entities": []}
{"text": "The document retriever chooses candidate wiki - documents via matching of keywords between the claims and the wiki - document titles , also using external pageview frequency statistics for wiki - page ranking .", "entities": []}
{"text": "The sentence selector is a sequence - matching neural network that conducts further fine - grained selection of evidential sentences by comparing the given claim with all the sentences in the candidate documents .", "entities": []}
{"text": "This module is trained as a binary classifier given the ground truth evidence as positive examples and all the other sentences as negative examples with an annealing sampling strategy .", "entities": []}
{"text": "To improve the claim verifier via better awareness of the selected evidence , we further combine the last two modules by feeding the sentence similarity score ( produced by the sentence selector ) as an additional token - level feature to the claim verifier .", "entities": []}
{"text": "Document retrieval attempts to find the name of a Wikipedia article in the claim , and then ranks each article based on capitalization , sentence position and token match features .", "entities": []}
{"text": "A set of sentences are then retrieved from the top ranked articles , based on token matches with the claim and position in the article .", "entities": []}
{"text": "Document retrieval We applied the constituency parser from AllenNLP to extract noun phrases in the claim and made use of Wikipedia API to search corresponding pages for each noun phrase .", "entities": []}
{"text": "So as to remove noisy pages from the results , we have stemmed the words of their titles and the claim , and then discarded pages whose stemmed words of the title are not completely included in the set of stemmed words in the claim .", "entities": []}
{"text": "For a given positive claim - evidence pair , negative samples are generated by randomly sampling sentences from the retrieved documents .", "entities": []}
{"text": "The precision of the entailment classifier allows us to enhance recall by considering every statement from several articles to decide upon each claim .", "entities": []}
{"text": "We include not only the articles best matching the claim text by TFIDF score , but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text .", "entities": []}
{"text": "The entailment module evaluates potential evidence one statement at a time , together with the title of the page the evidence came from ( providing a hint about possible pronoun antecedents ) .", "entities": []}
{"text": "For document retrieval we use three components : 1 ) use google custom search API with the claim as a query and return the top 2 Wikipedia pages ; 2 ) extract all name entities from the claims and use Wikipedia python API to return a page for each name entity and 3 ) ; use the prefix of the claim until the first lowercase verb phrase , and use Wikipedia API to return the top page .", "entities": []}
{"text": "If at least one out of the three evidences SUPPORTS / REFUTES the claim and the rest are NOT ENOUGH INFO , then we treat the label as SUPPORTS / REFUTES , else we return the majority among three classes as the predicted label .", "entities": []}
{"text": "Our system was developed using a heuristicsbased approach for evidence extraction and a modified version of the inference model by Parikh et al ( 2016 ) for classification into refute , support , or not enough info .", "entities": []}
{"text": "Our process is broken down into three distinct phases .", "entities": []}
{"text": "First , potentially relevant documents are gathered based on key words / phrases in the claim that appear in the wiki dump .", "entities": []}
{"text": "Second , any possible evidence sentences inside those documents are extracted by breaking down the claim into named entities plus nouns and finding any sentences which match those entities , while allowing for various exceptions and additional potential criteria to increase recall .", "entities": []}
{"text": "Finally , every sentences is classified into one of the three categories by the inference tool , after additional vectors are added based on named entity types .", "entities": []}
{"text": "NEI sentences are discarded and the highest scored label of the remaining sentences is assigned to the claim .", "entities": []}
{"text": "In our approach we used a sentence wise approach in all components .", "entities": []}
{"text": "To find the sentences we set up a Solr database and indexed every sentence including information about the article where the sentence is from .", "entities": []}
{"text": "We created queries based on the named entities and noun chunks of the claims .", "entities": []}
{"text": "For the entailment task we used a Decomposable Attention Model similar to the one used in the baseline approach .", "entities": []}
{"text": "But instead of comparing the claim with all top 5 sentences at once we treat every sentence separately .", "entities": []}
{"text": "The results for the top 5 sentence where then joined with an ensemble learner incl .", "entities": []}
{"text": "the rank of the sentence retriever of the wikipedia sentences .", "entities": []}
{"text": "The sentence retrieval component selects relevant sentences as candidate evidence from the documents based on TF - IDF .", "entities": []}
{"text": "We generate a Lucene index from the provided Wikipedia dump .", "entities": []}
{"text": "Depending on the amount of keywords found for each claim , we run multiple Lucene searches on the generated index to create a list of candidate sentences for each claim .", "entities": []}
{"text": "The resulting list of claim - candidate pairs is processed in three ways : 1 .", "entities": []}
{"text": "2 . We run each pair through a modified version of the Decomposable Attention network .", "entities": []}
{"text": "3 . We merge all candidate sentences per claim into one long piece of text and run the result paired with the claim through the same modified Decomposable Attention network as in ( 2 . ) .", "entities": []}
{"text": "We then make the final prediction in a handcrafted script combining the results of the three previous steps .", "entities": []}
{"text": "The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia .", "entities": []}
{"text": "In our approach , the Wikipedia documents whose Term Frequency - Inverse Document Frequency ( TFIDF ) vectors are most similar to the vector of the claim and those documents whose names are similar to the named entities ( NEs ) mentioned in the claim are identified as the documents which might contain evidence .", "entities": []}
{"text": "The sentences in these documents are then supplied to a decomposable attention - based textual entailment recognition module .", "entities": []}
{"text": "This module calculates the probability of each sentence supporting the claim , contradicting the claim or not providing any relevant information .", "entities": []}
{"text": "Various features computed using these probabilities are finally used by a Random Forest classifier to determine the overall truthfulness of the claim .", "entities": []}
{"text": "The sentences which support this classification are returned as evidence .", "entities": []}
{"text": "We resolved redirects by following the Wikipedia URL if an item was not in the preprocessed dump .", "entities": []}
{"text": "If a page could not be found , we fell back to the baseline document selection method .", "entities": []}
{"text": "The rest of the system was identical to the baseline system , al - though we used our document retrieval system to generate alternative training data .", "entities": []}
{"text": "Our system consists of three components : 1 . Wikipedia Page Retrieval :", "entities": []}
{"text": "The experiments show that the pipeline with simple Cosine Similarity using TFIDF in sentence selection along with DA as labeling model achieves better results in development and test dataset .", "entities": []}
{"text": "Each task provides supplementary information for the other and improves the results of another task .", "entities": []}
{"text": "Then , we use attention mechanisms in both directions , claim - to - page and pageto - claim , which provide complementary information to each other .", "entities": []}
{"text": "Aimed at the different task , our system obtains claim - aware sentence representation for evidence extraction and page - aware claim representation for claim verification .", "entities": []}
{"text": "This reliance on lexicalization may complicate the adaptation of these tools between domains .", "entities": []}
{"text": "To mitigate this dependence on lexicalized information , in this paper we propose a model that reads two sentences , from any given domain , to determine entailment without using any lexicalized features .", "entities": []}
{"text": "Instead our model relies on features like proportion of negated verbs , antonyms , noun overlap etc .", "entities": []}
{"text": "In the end , we hope to build a generic end - to - end classifier , which can be used in a domain outside the one in which it was trained , with no or minimal re - training .", "entities": []}
{"text": "Andreas Vlachos is supported by the EU H2020 SUMMA project ( grant agreement number 688139 ) .", "entities": []}
{"text": "You Do n't Know My Favorite Color : Preventing Dialogue Representations from Revealing Speakers ' Private Personas", "entities": []}
{"text": "Despite the huge progress , privacy concerns have arisen recently : training data of large language models can be extracted via model inversion attacks .", "entities": []}
{"text": "On the other hand , the datasets used for training chatbots contain many private conversations between two individuals .", "entities": []}
{"text": "In this work , we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet .", "entities": []}
{"text": "To this end , we propose effective defense objectives to protect persona leakage from hidden states .", "entities": []}
{"text": "Meanwhile , the proposed objectives preserve language models ' powerful generation ability .", "entities": []}
{"text": "Social chatbots have been widely used to benefit many applications from answering factual questions to showing emotional companionship .", "entities": []}
{"text": "Ham et al , 2020 ; Shen et al , 2021 ; Sevegnani et al , 2021 ; Gu et al , 2021b ) are made to build chatbots based on large generative language models ( LMs ) .", "entities": []}
{"text": "To train such LM - based chatbots , private conversations are collected .", "entities": []}
{"text": "Unfortunately , large language models tend to memorize training data and some private data can be recovered from models ( Pan et al , 2020 ; Carlini et al , 2021 ) .", "entities": []}
{"text": "Besides such memorization problems , \" overlearning \" on simple training objectives can reveal sensitive attributes indirectly related to the learning task ( Song and Shmatikov , 2020 ) .", "entities": []}
{"text": "LM - based social chatbots essentially inherit the privacy issues of general LMs and the overlearning problem .", "entities": []}
{"text": "As shown by the example , for five out of 14 utterances , the attacker can successfully predict the persona , which can be harmful if the users ( speakers of the utterances ) do not prefer to reveal the persona information .", "entities": []}
{"text": "Thus , in practice , when deploying such kinds of chatbots in real applications , we should first make sure that no private information can be leaked by the models .", "entities": []}
{"text": "To systematically study the privacy issues in LMbased social chatbots , there are several challenges .", "entities": []}
{"text": "First , there is no existing data that can be used to quantify how much private information is revealed by an LM .", "entities": []}
{"text": "Second , there has been no existing work showing how to attack utterance - level representations to obtain sensitive information .", "entities": []}
{"text": "We first collect a dataset by aligning personas with corresponding utterances in PersonaChat dataset ( Zhang et al , 2018 ) .", "entities": []}
{"text": "Then we show that \" overlearning \" can happen for LM - based chatbots to reveal personas of speakers .", "entities": []}
{"text": "\uf0fc \uf0fc", "entities": []}
{"text": "\uf0fc", "entities": []}
{"text": "\uf0fc", "entities": []}
{"text": "\uf0fc \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb Figure 1 : Black - box persona inference attacks ( over 4 , 332 personas ) on a dialog .", "entities": []}
{"text": "If the model can predict the persona of the speaker based on the observed representation , then we regard it as a successful attack ; otherwise , unsuccessful .", "entities": []}
{"text": "In practice , when deploying a model , a robust model which will reveal nothing of the encoded utterances is expected .", "entities": []}
{"text": "speakers ' private persona attributes .", "entities": []}
{"text": "Thus , it is necessary to improve training algorithms to address such overlearning issues .", "entities": []}
{"text": "Our contributions can be summarized as follows : 1 1 ) :", "entities": []}
{"text": "We propose an effective defensive training algorithm to prevent dialog representations from leaking personas of the corresponding speakers by uniform distribution approximation and mutual information minimization .", "entities": []}
{"text": "3 ) : We conduct extensive experiments to quantify both privacy and utility of proposed defense mechanisms .", "entities": []}
{"text": "Besides solving the persona leakage issue , the proposed training algorithm has nearly no negative influence on utility .", "entities": []}
{"text": "Language models trained on private data suffer privacy risks of revealing sensitive information .", "entities": []}
{"text": "Previous researches mainly considered black - box attacks that assumed attackers only had access to 1 Code is publicly available at https://github . com / HKUST - KnowComp / Persona_leakage_and _ defense_in_GPT - 2 . inputs and outputs of language models .", "entities": []}
{"text": "Carlini et", "entities": []}
{"text": "Furthermore , given black - box access to a language model 's pre - train and finetune stages , Zanella - B\u00e9guelin et al ( 2020 ) showed that sensitive sequences of the fine - tuning dataset can be extracted .", "entities": []}
{"text": "For the distributed client - server setup , Malekzadeh et al ( 2021 ) considered the sensitive attribute leakage from the server side with honest - but - curious ( HBC ) classifiers .", "entities": []}
{"text": "further applied Attribute Extractor to generate speakers ' attribute triplets flexibly and suggested downstream tasks based on the triplets .", "entities": []}
{"text": "Pan et al ( 2020 ) exploited embeddings of language models to recover inputs ' digits and keywords .", "entities": []}
{"text": "Though the setup of this work is similar to ours , they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance .", "entities": []}
{"text": "For our work , there is no fixed pattern or rule for the model input .", "entities": []}
{"text": "Instead of finding key - words or recovering digits , we aim to infer more complicated private attributes from such embeddings .", "entities": []}
{"text": "Moreover , our proposed defenses have almost no influence on the utility .", "entities": []}
{"text": "In Section 3.1 , we first give the problem formulation .", "entities": []}
{"text": "Then we describe the attack in Section 3.2 .", "entities": []}
{"text": "w | u | \u22121 } and previous context c. An adversary owns one external annotated dialog dataset D a = { ( U 1 , s 1 ) , ( U 2 , s 2 ) , ... , ( U n , s n ) } with n conversations where U i indicates a list of utterances { u i1 , u i2 , ... , u in i } of i - th conversation and s i corresponds to a list of sensitive personas { s i1 , s i2 , ... , s in i } for corresponding utterance .", "entities": []}
{"text": "\u2264 s kj \u2264 C \u2212 1 where C is the total number of predefined persona attributes .", "entities": []}
{"text": "The goal of the adversary is to infer speakers ' personas s from utterances ' embeddings f ( u ) where u and s refer to any utterance and its persona label .", "entities": []}
{"text": "As shown in the left part of Figure 2 , the adversary tries to build its attacker model", "entities": []}
{"text": "A with its external data D a and dialog model f .", "entities": []}
{"text": "The persona predictor 's output A ( f ( u ) ) is the estimated probability distribution over C persona attributes .", "entities": []}
{"text": "A well - performed persona predictor A can cause great privacy threats .", "entities": []}
{"text": "For machine learning as a service ( MLaaS ) , A can be applied to perform a man - in - the - middle attack on the application programming interfaces .", "entities": []}
{"text": "Moreover , even if the raw data are protected and the transmission channel is secure , a curious service provider can train its attacker A to collect personas of service users .", "entities": []}
{"text": "The LM training objective in Equation 1 only considers the utility of chatbots .", "entities": []}
{"text": "In later experiment sections , we show that LM brings severe overlearning issues .", "entities": []}
{"text": "A should be close to the uniform distribution .", "entities": []}
{"text": "Moreover , the constraints on privacy should have minor degradation on the utility to maintain the strong generation ability of chatbots .", "entities": []}
{"text": "Lastly , we show the overall training objective in Section 4.3 .", "entities": []}
{"text": "It flattens the distribution of A ( f ( u ) )", "entities": []}
{"text": "so that the adversary can not gain any useful knowledge after training attacker model", "entities": []}
{"text": "A.", "entities": []}
{"text": "The KL divergence between the uniform distribution and A ( f ( u ) ) can be formulated as : DKL ( UNI | |", "entities": []}
{"text": "A ( f ( u ) ) )", "entities": []}
{"text": "= \u2212 1 C C\u22121 k=0 log ( CPr ( k |", "entities": []}
{"text": "f ( u ) , \u03b8A ) ) ,", "entities": []}
{"text": "( 3 ) where UNI indicates the uniform distribution and k indicates the k - th persona label of C labels .", "entities": []}
{"text": "The defender shares the same architecture as the attacker and uses L kl with L mi as defense objectives .", "entities": []}
{"text": "= \u2212 1 C C\u22121 k=0 Pr ( k | f ( u ) , \u03b8A ) .", "entities": []}
{"text": "However , from the perspective of defenders , they have no access to attacker model", "entities": []}
{"text": "A and its parameters .", "entities": []}
{"text": "Instead , they can build their own persona predictor as a fake attacker .", "entities": []}
{"text": "More specifically , they may mimic the adversary to annotate a dataset D \u2032 a and a persona predictor A p .", "entities": []}
{"text": "= \u2212", "entities": []}
{"text": "1 C C\u22121 k=0 Pr ( k | f ( u ) , \u03b8A p ) , ( 5 )", "entities": []}
{"text": "The privacy constraint requires that hidden representations should not reveal the persona attributes .", "entities": []}
{"text": "In other words , given any utterance u and persona s behind the utterance u , we want to minimize the mutual information between f ( u ) and s :", "entities": []}
{"text": "( 6 ) Following the derivation in Song et al ( 2019 )", "entities": []}
{"text": "and , the upper bound can be formulated as : I ( f ( u ) ; s ) \u2264 E q ( f ( u ) )", "entities": []}
{"text": "DKL ( q ( s | f ( u ) )", "entities": []}
{"text": "|", "entities": []}
{"text": "|", "entities": []}
{"text": "x , s ) .", "entities": []}
{"text": "However , q ( s | f ( u ) ) is hard to estimate .", "entities": []}
{"text": "Instead , we use p \u03a8", "entities": []}
{"text": "( s |", "entities": []}
{"text": "f", "entities": []}
{"text": "( u ) )", "entities": []}
{"text": "to approximate q ( s | f ( u ) ) via minimizing their KL divergence and then we can obtain the following lower bound ( Song et al , 2019 ) : E q ( f ( u ) )", "entities": []}
{"text": "DKL ( q ( s | f ( u ) )", "entities": []}
{"text": "|", "entities": []}
{"text": "| p ( s ) )", "entities": []}
{"text": "\u2265 E q ( f ( u ) )", "entities": []}
{"text": "[ log p\u03a8 ( s | f ( u ) )", "entities": []}
{"text": "\u2212 log p ( s ) ] .", "entities": []}
{"text": "[ log p\u03a8 ( s | f ( u ) )", "entities": []}
{"text": "\u2212 log p ( s ) ] .", "entities": []}
{"text": "[ log p\u03a8 ( s | f ( u ) ) ] .", "entities": []}
{"text": "Adversarial training is widely used to protect sensitive features in natural language processing ( Elazar and Goldberg , 2018 ;", "entities": []}
{"text": "Coavoux et al , 2018 ; Li", "entities": []}
{"text": "et al , 2018 ) .", "entities": []}
{"text": "Using the persona predictor model", "entities": []}
{"text": "( Ap ( f ( u ) ) , s ) .", "entities": []}
{"text": "+ \u03bb2Lmi , ( 13 ) where \u03bb 1 and \u03bb 2 are hyper - parameters and \u03bb 1 \u2265 10\u03bb 2 to flatten the distribution of A p .", "entities": []}
{"text": "In this section , we conduct experiments to evaluate the performance of privacy and utility for the proposed defense learning strategies .", "entities": []}
{"text": "In Section 5.1 , we give our experimental settings in detail .", "entities": []}
{"text": "In Section 5.2 , we show the attacking performance with and without defense .", "entities": []}
{"text": "In Section 5.3 , we perform ablation study on defense objectives .", "entities": []}
{"text": "In Section 5.4 , we use automatic metrics to evaluate chatbots ' utility .", "entities": []}
{"text": "We conduct various attack setups in Section 5.5 and perform a case study in Section 5.6 .", "entities": []}
{"text": "Dataset .", "entities": []}
{"text": "To obtain annotated dataset D a for the adversary , we align personas to corresponding utterances through positive ( utterance , persona ) pairs provided in Dialogue NLI ( Welleck et al , 2019 ) dataset .", "entities": []}
{"text": "For those utterances with no annotations , we assign label \u22121 to them .", "entities": []}
{"text": "We reshuffle the dataset to balance the label distribution among train / val / test datasets with the ratio of 8 : 1 : 1 .", "entities": []}
{"text": "We first let the attacker and defender share the same training data .", "entities": []}
{"text": "In later sections , we will separate the annotated data for the adversary and defender with no overlap .", "entities": []}
{"text": "A summary statistics of D a is shown in Table 1 .", "entities": []}
{"text": "Attacker model .", "entities": []}
{"text": "Evaluation Metrics .", "entities": []}
{"text": "The evaluation metrics are based on privacy and utility .", "entities": []}
{"text": "We also use Bayesian Privacy ( BP )", "entities": []}
{"text": "Attacks without Defense .", "entities": []}
{"text": "Max - Ratio indicates the ratio that the most frequent prediction shares among all predictions .", "entities": []}
{"text": "The worse the attack model performs , the better privacy protection can be achieved .", "entities": []}
{"text": "distribution , then it can randomly guess over 4 , 332 labels ( Random Pred ) .", "entities": []}
{"text": "Otherwise the adversary can perform Best Guess that only guesses the most frequent persona in the dataset .", "entities": []}
{"text": "The huge performance gap between the attacker model and the baseline guess method indicates that simple language modeling objective has serious overlearning issues that unintentionally capture private personas of speakers .", "entities": []}
{"text": "Attacks on the Defensed LM .", "entities": []}
{"text": "This defense mechanism can even outperform Best Guess in terms of privacy protection .", "entities": []}
{"text": "That is , even if the adversary annotates its own dataset to train an attacker model , the attacking performance is still worse than simply guessing the most frequent label .", "entities": []}
{"text": "The adversary can not obtain any speaker 's persona from the embedding f ( u ) by training A. To learn why the proposed defenses work so well , we further examine the ratio of the most frequent predicted label ( Max - Ratio ) among all pre - dictions .", "entities": []}
{"text": "For LM+KL+MI , the Max - Ratio even occupies 81.87 % predictions .", "entities": []}
{"text": "This implies that the proposed defense strategies may have the potential to fool the attacker model to make wrong predictions on a single slot .", "entities": []}
{"text": "We will further investigate this implication in later sections .", "entities": []}
{"text": "Overall , the above experiment demonstrates that our proposed defense learning strategies can effectively mitigate the persona overlearning issue and avoid black - box persona inference attacks .", "entities": []}
{"text": "As discussed earlier , high Max - Ratio may also cause privacy leakage .", "entities": []}
{"text": "Suppose the adversary knows the persona with Max - Ratio , then it can improve its guess by not predicting this persona , which is a threat for fewer persona labels ( for example , binary classification ) .", "entities": []}
{"text": "Several automatic metrics are considered to evaluate the generation performance .", "entities": []}
{"text": "For generation , we use Table 4 : Evaluation on the privacy for 8 clusters .", "entities": []}
{"text": "Unseen shows the results only for the first 3 persona labels that defender has never seen .", "entities": []}
{"text": "Overall refers to the results on all 8 labels .", "entities": []}
{"text": "Still , the worse the attack model performs , the better privacy protection can be achieved .", "entities": []}
{"text": "the second speaker ( Human B in Figure 1 ) with all previous turns as context .", "entities": []}
{"text": "Then we compared the generated model outputs with ground truth replies .", "entities": []}
{"text": "We use Dist - 1 and Dist - 2 to count ratios of distinct unigrams and bigrams .", "entities": []}
{"text": "The evaluation result is shown in Table 3 , where same models from Table 2 are evaluated .", "entities": []}
{"text": "For Distinct and BERTScore , there are only minor differences between LM and defensed LMs .", "entities": []}
{"text": "In summary , there is almost no negative influence on the utility after applying the proposed defense strategies .", "entities": []}
{"text": "Attacks on Imbalanced Data Distribution .", "entities": []}
{"text": "Previous black - box attacks usually assume that the annotated dataset D a must share similar data distri - bution with the defender 's training data .", "entities": []}
{"text": "To examine the performance of defense strategies on unseen personas , we assign the adversary 's dataset D a with labels that the defender can not acquire .", "entities": []}
{"text": "We split data with 500 persona labels that are uniquely held by the adversary .", "entities": []}
{"text": "Attacks on Fewer Persona Labels .", "entities": []}
{"text": "The above experiments are based on 4 , 332 persona labels .", "entities": []}
{"text": "In fact , many personas share similar meanings and can be further clustered .", "entities": []}
{"text": "Therefore , it is necessary to consider defense performance on a smaller label space .", "entities": []}
{"text": "We manually checked these clusters and classified them as cars , food , animals For both conversations , the \" context \" is fixed and used as the first four utterances .", "entities": []}
{"text": "Then the bot and the user start interactive conversations with the \" context \" .", "entities": []}
{"text": "Since there is no gold standard , the results are annotated by the authors .", "entities": []}
{"text": "( pets ) , family information , hobbies , jobs , personal information and music tastes respectively .", "entities": []}
{"text": "To evaluate how the clustering performs , we randomly sample 100 utterances with clustered labels and invite two volunteers to inspect those samples .", "entities": []}
{"text": "Both of them agree on 90 % of the clustered annotations .", "entities": []}
{"text": "After manual inspection of the remaining 10 % annotations , the clustering error rate is 8 % .", "entities": []}
{"text": "Following previous imbalanced data split , we assign data in the first 3 clusters only to the adversary to make the data distribution imbalanced .", "entities": []}
{"text": "The attacking performance for both unseen labels and all labels is displayed in Table 4 .", "entities": []}
{"text": "BP u measures the KL divergence D KL", "entities": []}
{"text": "This indicates that proposed defenses fail to protect privacy as we desired in the baselines .", "entities": []}
{"text": "For BP u , LM+KL+MI are around 10 times smaller than LM .", "entities": []}
{"text": "It means that after applying defense objectives , the attacker 's estimated distribution is much closer to the uniform distribution .", "entities": []}
{"text": "In addition , Max - Ratio with 8 clusters on Unseen is smaller than 4 , 332 labels even though the distribution of 8 clusters is obviously tighter .", "entities": []}
{"text": "Still , the Max - Ratio of 58.15 % accounts for a much larger fraction than other predictions .", "entities": []}
{"text": "In summary , the above results imply that for the smaller label space , our proposed defense objectives are still effective even on unseen persona labels .", "entities": []}
{"text": "We manually mark True / False on the predicted results .", "entities": []}
{"text": "As shown in the figure , there are several successful attacks on LM and no correct prediction on the defensed LM .", "entities": []}
{"text": "For attacks on LM , speakers ' hobbies and jobs can be inferred .", "entities": []}
{"text": "For incorrect predictions , the attacker model can still predict context - aware personas .", "entities": []}
{"text": "After applying proposed defense learning strategies , the predicted personas become irrelevant with context and mostly predict \" My favorite color is blue . \"", "entities": []}
{"text": "In fact , it is the most frequent prediction for LM+KL+MI over 4 , 332 persona labels .", "entities": []}
{"text": "Unlike other works that suffer from utility degradation , our defense learning strategies do no harm to the powerful generation ability of LM - based chatbots .", "entities": []}
{"text": "We conduct extensive experiments to evaluate both privacy and utility .", "entities": []}
{"text": "We perform black - box persona inference attacks under various setups to demonstrate the robustness of proposed defense learning strategies .", "entities": []}
{"text": "In addition , we use automatic metrics to show that proposed defense learning strategies maintain the utility .", "entities": []}
{"text": "For future work , we suggest working on flattening the distributions of attacker models .", "entities": []}
{"text": "This work essentially considers blackbox attacks on the private persona attributes and proposes effective learning strategies to prevent chatbots from overlearning private personas .", "entities": []}
{"text": "Dataset .", "entities": []}
{"text": "During our dataset collection , all the conversations and personas are collected from publicly available datasets including PersonaChat and DNLI .", "entities": []}
{"text": "All the speakers are anonymized and no identifiable personal information is included .", "entities": []}
{"text": "Model .", "entities": []}
{"text": "For training our LM - based chatbots , we follow standard methods .", "entities": []}
{"text": "We are well aware of the bias issue inside current language models .", "entities": []}
{"text": "In the future , if there are other fairer language models , we will extend our defenses on them .", "entities": []}
{"text": "formation about personas .", "entities": []}
{"text": "However , its attacking performance is poor .", "entities": []}
{"text": "The poor performance implies our proposed defense learning strategies may obfuscate Attacker for estimating single sample f ( u ) and finally make the wrong prediction .", "entities": []}
{"text": "To show an intuition view on utility , we provide one generation sample shown in Figure 5 .", "entities": []}
{"text": "Both LM and LM+KL+MI are able to generate fluent and proper relies .", "entities": []}
{"text": "Moreover , they tend to maintain coherence with previous contexts .", "entities": []}
{"text": "For example , it is mentioned in the context that Human B is a vegan and both chatbots respond that they do not eat meat for the food preference .", "entities": []}
{"text": "This generation example shows that proposed defense learning objectives preserve the model utility .", "entities": []}
{"text": "Here , we give two more examples of the persona inference attacks in Table 6 .", "entities": []}
{"text": "The first example shows one successful defense .", "entities": []}
{"text": "For the second example , both attackers with and without defense fail to predict the ground truth persona .", "entities": []}
{"text": "Still , we can see that LM+KL+MI predicts personas that are irrelevant to the context .", "entities": []}
{"text": "However , LM 's output \" I know how to play the guitar . \" is much closer to the context about music and instruments .", "entities": []}
{"text": "Without any defense , the above examples show that the attacker model can still predict context - aware personas even if its predictions are wrong .", "entities": []}
{"text": "After applying the proposed defenses , the attacker model can not predict meaningful personas relevant to the context .", "entities": []}
{"text": "As shown in Table 5 , our defense is much more robust than LM when k \u2264 50 .", "entities": []}
{"text": "When k is larger than 500 , the defense degrades rapidly as k increases .", "entities": []}
{"text": "This result implies that the ground truth personas mostly lie in the top 2 , 000 predictions even if the defense is applied .", "entities": []}
{"text": "For a smaller k , our proposed defense learning strategies are still effective .", "entities": []}
{"text": "The authors of this paper were supported by the NSFC Fund ( U20B2053 ) from the NSFC of China , the RIF ( R6020 - 19 and R6021 - 20 ) and the GRF ( 16211520", "entities": []}
{"text": "To make predictions on personas , the arg max function is used for the estimated distribution of persona predictors .", "entities": []}
{"text": "However , the internal distribution conveys crucial information about how the persona predictors estimate f ( u ) .", "entities": []}
{"text": "We follow the setup of imbalanced data split of 8 clusters in Section 5.5 to examine persona predictors of attacker A and fake attacker", "entities": []}
{"text": "A p .", "entities": []}
{"text": "For attacker A , we consider the attack on LM and LM+KL+MI .", "entities": []}
{"text": "The defender A p tends to have a large difference with Data and tries to flatten its distribution among its own training set ( the last 5 labels ) .", "entities": []}
{"text": "For attacker A , distributions of both LM and LM+KL+MI seem close to the ground truth distribution .", "entities": []}
{"text": "This indicates that the attacker model", "entities": []}
{"text": "A can still learn statistical in -", "entities": []}
{"text": "Alignment verification to improve NMT translation towards highly inflectional languages with limited resources", "entities": []}
{"text": "The present article studies translation quality when limited training data is available to translate towards morphologically rich languages .", "entities": []}
{"text": "The starting point is a neural MT system , used to train translation models with only publicly available parallel data .", "entities": []}
{"text": "An initial analysis of the translation output has shown that quality is sub - optimal , mainly due to the insufficient amount of training data .", "entities": []}
{"text": "To improve translation , a hybridized solution is proposed , using an ensemble of relatively simple NMT systems trained with different metrics , combined with an open source module designed for low - resource MT that measures the alignment level .", "entities": []}
{"text": "A quantitative analysis based on established metrics is complemented by a qualitative analysis of translation results .", "entities": []}
{"text": "These show that over multiple test sets , the proposed hybridized method confers improvements over ( i ) both the best individual NMT and ( ii ) the ensemble system provided in the Marian - NMT package .", "entities": []}
{"text": "Improvements over Marian - NMT are in many cases statistically significant .", "entities": []}
{"text": "The state of the art in MT involves corpus - based systems developed with machine - learning methods .", "entities": []}
{"text": "These methods learn from corpora the models needed for translation .", "entities": []}
{"text": "A key strength of this approach is that the system is adapted specifically towards the data it is trained with .", "entities": []}
{"text": "For many years , the most successful data - driven approaches were phrase - based and syntax - based Statistical MT ( SMT ; Koehn , 2009 ) .", "entities": []}
{"text": "However , lately Neural MT ( NMT ) based on the encoderdecoder architecture and the concept of attention ( Sutskever et al , 2014 ; Bahdanau et al , 2016 ) has become very popular .", "entities": []}
{"text": "Indeed , since 2015 , in MT shared tasks ( Cettolo et al , 2015 ; Bojar et al , 2015 ; Bojar et al , 2016 ) most top - performing systems have been NMT systems .", "entities": []}
{"text": "This trend is confirmed in the most recent MT shared task ( Barrault et al , 2019 ) , where 80 % of participating systems are of NMT type .", "entities": []}
{"text": "Though NMT represents the state of the art for MT , specific weaknesses have been reported : NMT performance suffers from the lack of data resources ( Koehn and Knowles , 2017 ) , giving lower translation performance , especially when training with out - of - domain rather than in - domain data .", "entities": []}
{"text": "Recent advances in NMT models have been shown ( Sennrich and Zhang , 2019 ) to allow good translations to be achieved with smaller parallel corpora of typically 10 5 sentences , though substantial improvements are achieved when the corpus size reaches 10 6 sentences .", "entities": []}
{"text": "However , training sets of such sizes are not available for all languages .", "entities": []}
{"text": "The integration of multiple algorithms into an NMT system does not necessarily improve translation ( Denkowski and Neubig , 2017 ) .", "entities": []}
{"text": "The time complexity of training a new NMT system can be very high , with training sessions of the order of weeks .", "entities": []}
{"text": "NMT requires very large amounts of parallel data , measured in millions of parallel sentences .", "entities": []}
{"text": "This is reflected by the separate studies carried out for MT with limited resources , which includes initiatives such as Lorelei 1 .", "entities": []}
{"text": "In the case of morphologically - rich languages , the requirements for parallel corpora are further exacerbated .", "entities": []}
{"text": "In this paper , an effort to improve the translation quality is presented , when translating towards a morphologically - rich language , while reducing the training time .", "entities": []}
{"text": "This approach combines the output of multiple NMT systems with an NLP module developed for an example - based MT paradigm , resulting in a hybridized solution .", "entities": []}
{"text": "The latter module is fast and runs independently of its original MT system and thus the computational complexity of the proposed hybrid solution is not substantially increased over the base NMT system .", "entities": []}
{"text": "The idea of combining multiple MT models to produce a higher performing MT system has been studied extensively in the area of MT .", "entities": []}
{"text": "For instance in the recent shared task ( Barrault et al , 2019 ) more than 20 entries consist of ensembles of multiple NMT systems .", "entities": []}
{"text": "Ensembles of weaker NMT systems of the same general architecture have been proposed by Freitag et al ( 2017 ) to train a higher performing NMT system .", "entities": []}
{"text": "This base NMT system is described in section 2 .", "entities": []}
{"text": "The training data used is reported in section 3 .", "entities": []}
{"text": "The proposed hybridization is presented in section 4 , whilst the improvements attained are presented in section 5 .", "entities": []}
{"text": "Future developments are discussed in section 6 .", "entities": []}
{"text": "Since NMT systems have achieved the highest translation quality in recent evaluation contests , the Marian - NMT package ( Junczys - Dowmunt et al , 2018 ) is adopted for experimentation here .", "entities": []}
{"text": "Marian - NMT development was funded by the European Commission to consolidate NMT research and incorporates the most recent advances in NMT .", "entities": []}
{"text": "Its code is optimized to reduce the CPU / GPU time required to complete the simulations of NMT systems .", "entities": []}
{"text": "For creating NMT systems , three of the models provided by Marian - NMT were chosen , termed as the \" transformer \" , \" amun \" and \" s2s \" models .", "entities": []}
{"text": "The \" transformer \" model has been based on the work of Vaswani et al ( 2017 ) and uses a simple structure incorporating attention mechanisms and dispensing with recurrence to implement a fast NMT system .", "entities": []}
{"text": "The other two models are more conventional , using a recurrent neural network to implement the translation .", "entities": []}
{"text": "The \" amun \" model follows the approach of Bahdanau et al ( 2016 ) , employing a recurrent neural network but allowing the model to automatically search for wider ranges of the source language ( SL ) to connect with the target language side ( TL ) words .", "entities": []}
{"text": "Finally , \" s2s \" implements a recurrent neural network - based encoder - decoder model with attention mechanism , using the architecture proposed in ( Sennrich et al , 2017 ) .", "entities": []}
{"text": "Hereafter , the three models are identified via the names used within Marian - NMT , which are also used in evaluations ( cf .", "entities": []}
{"text": "Bojar et al , 2018 ) .", "entities": []}
{"text": "The main configuration parameters used for each model are depicted in Table 1 , to enable replication of experiments .", "entities": []}
{"text": "Regarding the main NMT parameters , all recurrent networks comprise 1 , 024 units in the hidden layer , an encoder depth of 6 layers and an embedding size of 512 .", "entities": []}
{"text": "The transformer dimension is set to 2 , 048 .", "entities": []}
{"text": "Initially , the three Marian - NMT models are trained to provide the base NMT systems .", "entities": []}
{"text": "This is equivalent to a ratio of 1:5:12 to train the respective systems .", "entities": []}
{"text": "When selecting the training corpora , it has been decided to refrain from using expensive language resources such as specialized or hand - built parallel corpora .", "entities": []}
{"text": "Instead , only standard publicly available parallel corpora have been adopted , namely the Europarl and DGT - Acquis corpora 2 , as listed in Table 2 .", "entities": []}
{"text": "The largest part of the Europarl corpus and the entire DGT - Acquis corpus are used to train the NMT system .", "entities": []}
{"text": "Three small portions of the Europarl corpus have been reserved for test and validation purposes .", "entities": []}
{"text": "More specifically , two independent sets of approx .", "entities": []}
{"text": "3 , 000 Europarl sentences each are excluded , to ensure that the NMT evaluation is unbiased .", "entities": []}
{"text": "In the present experiment , one of these sets is used for in - training validation .", "entities": []}
{"text": "The other set is reserved to allow additional cross - evaluation of experiments in the future , without invalidating the previously trained models .", "entities": []}
{"text": "Finally , a sample 2", "entities": []}
{"text": "The Europarl corpus ( ver.7 ) was retrieved from https://www.statmt.org/europarl .", "entities": []}
{"text": "The DGT - Acquis corpus was retrieved from https://ec.europa.eu/jrc/en/languagetechnologies/dgt - translation - memory of 1 , 000 sentences from Europarl ( Testset2 ) is retained to provide an unseen in - domain test set .", "entities": []}
{"text": "Another independent test set was drawn from the PRESEMT project resources , comprising 200 sentences which have not been used to either train an MT model or create any resources used herewith ( denoted as Testset1 ) .", "entities": []}
{"text": "A preliminary analysis of the NMT outputs has shown that translations are commendably fluent , though errors are evident .", "entities": []}
{"text": "A sample of amun translations is shown in Figure 1 .", "entities": []}
{"text": "In sentence # 1 , the term \" \u0391\u03bc\u03b5\u03c1\u03b9\u03ba\u03b1\u03bd\u03bf\u03af \" ( Transl .", "entities": []}
{"text": "\" Americans \" ) is erroneously used as a translation of the terms \" American \" , \" European \" , and \" Japanese monopolies \" .", "entities": []}
{"text": "Similarly , in sentence # 2 , the phrase \" \u03b7 \u03ba\u03b1\u03c4\u03b1\u03c0\u03bf\u03bb\u03ad\u03bc\u03b7\u03c3\u03b7 \u03c4\u03b7\u03c2 \u03c6\u03c4\u03ce\u03c7\u03b5\u03b9\u03b1\u03c2 \" ( meaning \" the reduction of poverty \" ) is used to translate semantically diverse phrases , including \" genetically modified organisms \" , and \" the negative social effects of unbridled , unregulated globalization \" .", "entities": []}
{"text": "Repetition is a widely reported weakness of NMT systems , most frequently attributed to insufficient training data .", "entities": []}
{"text": "An additional problem concerns the translation of rare words ( i.e. words with low frequency in the corpus ) , due to the limited vocabulary that NMT systems can directly handle .", "entities": []}
{"text": "This is especially severe when translating towards languages with complex morphology , which increases the effective vocabulary size .", "entities": []}
{"text": "For example the word \" ostensibly \" is translated into Greek as \" ostenfigher \" ( ungrammatical ) .", "entities": []}
{"text": "Similarly the word \" room \" is translated as \" \u03b4\u03c9\u03bc\u03b1\u03c4\u03b5\u03af\u03bf \" instead of the correct \" \u03b4\u03c9\u03bc\u03ac\u03c4\u03b9\u03bf \" ( meaning room ) , whilst the word \" indistinct \" is translated as \" \u03ac\u03c7\u03c9\u03c1\u03bf\u03c2 \" which is not a valid Greek word .", "entities": []}
{"text": "\u0391nother issue is that entire phrases present in the source text may be omitted in the translation .", "entities": []}
{"text": "For instance the sentence \" Businesses have undertaken the education \" is translated by a transformer NMT as \" H \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7 \u03ad\u03c7\u03b5\u03b9 \u03b1\u03bd\u03b1\u03bb\u03ac\u03b2\u03b5\u03b9 , [ meaning \" education has undertaken \" ] .", "entities": []}
{"text": "Hence , the subject \" business \" has been deleted .", "entities": []}
{"text": "Verification Method ( AVM )", "entities": []}
{"text": "On the contrary a high alignment score is indicative of a high likelihood that Figure 1 : Example translations generated by amun , with repetitions of texts highlighted in grey the NMT output is an accurate translation .", "entities": []}
{"text": "To this end a module will be added to implement alignment verification ( AVM ) , by determining the match between the input sentence and its translation .", "entities": []}
{"text": "For this research , MT software and tools released via open - source code have been surveyed and the Phrase Aligner Module ( PAM , cf .", "entities": []}
{"text": "Troullinos , 2013 ) has been selected .", "entities": []}
{"text": "The architecture of the proposal hybrid NMT is depicted in Figure 2 .", "entities": []}
{"text": "PAM was developed as part of the PRESEMT hybrid MT methodology ( Tambouratzis et al , 2017 ( Tambouratzis et al , 2017 ) .", "entities": []}
{"text": "PRESEMT was designed to create MT systems requiring only very limited amounts of specialized , expensive linguistic resources .", "entities": []}
{"text": "Frequently , the most expensive resource is the parallel corpus of SL - TL sentences .", "entities": []}
{"text": "PRESEMT uses parallel corpora of only a few hundred sentences , augmented by very extensive but comparatively inexpensive monolingual corpora .", "entities": []}
{"text": "Within the PRESEMT methodology , the small parallel corpus serves to establish the transformation from the SL structure to the TL one , using the Phrase Aligner module .", "entities": []}
{"text": "PAM utilizes a limited - size bilingual lexicon ( of typically 30 to 40 thousand token pairs ) together with a publicly available parser .", "entities": []}
{"text": "Details on these resources are reported in section 4.4 , as their choices are language - specific .", "entities": []}
{"text": "Based on these resources , PAM establishes for the set of parallel sentences the alignment of both words and phrases from SL to TL , in three hierarchically ordered stages : 1 .", "entities": []}
{"text": "Within the first stage , the alignment of words is based on equivalences provided by the bilingual lexicon .", "entities": []}
{"text": "Dedicated PAM processes resolve cases where ( i ) words have multiple appearances within a sentence and ( ii ) multiple potential translations of an SL word exist in the TL side .", "entities": []}
{"text": "2 .", "entities": []}
{"text": "Within the second stage , words are aligned by establishing statistical correspondences between grammatical features across the SL and TL pair .", "entities": []}
{"text": "These correspondences are automatically extracted from the lexicon .", "entities": []}
{"text": "3 .", "entities": []}
{"text": "Within the third stage , any remaining words are aligned and grouped into phrases on the basis of the alignments of their neighboring words that are successfully aligned .", "entities": []}
{"text": "To implement this , the principle of locality across languages is adopted ( words at a small distance to each other in SL also tend to be located close to each other in TL ) .", "entities": []}
{"text": "The key PAM principle is that decisions made at a later stage have a lower degree of confidence than those made at an earlier stage ( Troullinos , 2013 ) .", "entities": []}
{"text": "In the current application , PAM determines the suitability of each candidate translation , based on its match with the source sentence .", "entities": []}
{"text": "Thus , the assumption made is that the input sentence and the candidate translation represent the corresponding SL and TL entries of a parallel corpus and PAM determines their level of parallelism .", "entities": []}
{"text": "As the requirement is to grade various translations , the PAM operation is reversed , to identify the quality of match between the input sentence and the generated translations .", "entities": []}
{"text": "When PAM was used in PRESEMT , sentence pairs from the parallel corpus with a very low percentage of successful alignments were discarded without measuring their degree of parallelism , as poor exemplars of the structural transformations from SL to TL .", "entities": []}
{"text": "Here , PAM is modified so that for all pairs of input sentence and NMT - translation the word alignments and assignments of words to phrases are calculated .", "entities": []}
{"text": "This allows the re - roled PAM to grade any source / translation pair , no matter how poor the match of the two sentences is .", "entities": []}
{"text": "Two metrics have been established to calculate divergence between the SL sentence and its NMTderived translations .", "entities": []}
{"text": "The first metric ( Uscore ) calculates the number of unaligned words of the source sentence , after PAM is applied .", "entities": []}
{"text": "The aim is to have as few unaligned words as possible , so the lower Uscore is , the better the translation is .", "entities": []}
{"text": "U score = # unaligned words ( 1 )", "entities": []}
{"text": "The second metric ( Wscore ) is a weighted combination of several indicators of alignment between source sentence and candidate translation .", "entities": []}
{"text": "This summarizes in one measurement the type of alignments and the stage at which they were achieved .", "entities": []}
{"text": "Hence , for a sentence with K words , Wscore is defined as : W score = K i=1", "entities": []}
{"text": "( w", "entities": []}
{"text": "i * align stage i ) ( 2 )", "entities": []}
{"text": "In equation ( 2 ) , align - stage i denotes the stage ( cf . section 4.3 for the different stages ) at which the i - th SL word is aligned successfully to a TL word , and w i denotes the relevant weight for this stage .", "entities": []}
{"text": "In the case of the weighted metric Wscore , the higher the score , the more accurate the corresponding translation is .", "entities": []}
{"text": "The actual weight values must reward the establishment of alignments at an earlier rather than a later stage .", "entities": []}
{"text": "Thus , w i should be larger than w j , for i smaller than j. For the purposes of the present article , w i is set to integer values of 5 , 2 and 1 for the first , second and third stage respectively ( other sets of weight values that follow this reasoning produce similar results to those reported here ) .", "entities": []}
{"text": "al , 2002 ) and NIST ( Doddington , 2002 ) .", "entities": []}
{"text": "To calculate both these metrics , the mt - eval package ( version 13a ) is used .", "entities": []}
{"text": "For PAM , the PRESEMT bilingual lexicon from Greek to English is used , which contains approx .", "entities": []}
{"text": "8 , 000 lemmas and 40 , 000 Greek - English token pairs .", "entities": []}
{"text": "This lexicon is from the same domain as testset1 and is thus out - of - domain for testset2 , providing a more limited coverage for this testset .", "entities": []}
{"text": "Two different types of experiments are possible , depending on whether the ensemble comprises multiple NMT architectures , or only one type of architecture .", "entities": []}
{"text": "The first experiment reported here involves NMT ensembles that all share the same architecture , but are optimized with different criteria .", "entities": []}
{"text": "The second type of experiment studies ensembles which consist of systems with different architectures , to investigate if their combination results in a better translation quality .", "entities": []}
{"text": "The results obtained for testset1 of the Englishto - Greek translation pair are depicted in Table 3 , when running the single transformer , amun and s2s models respectively , as well as their ensembles .", "entities": []}
{"text": "The corresponding results for testset2 are depicted in Table 4 .", "entities": []}
{"text": "Marian - NMT implements a standard ensemble method , which allows the user to combine different models provided they use the same lexicon .", "entities": []}
{"text": "The user may specify weighting factors to boost selection of the models deemed to be better .", "entities": []}
{"text": "For this article , this ensemble combines the three aforementioned NMT models ( i ) , ( ii ) and ( iii ) , with equal weights for all NMTs .", "entities": []}
{"text": "A key difference of the Marian - NMT ensemble is that it is able to recombine partial results of the translation process from each NMT model and thus may generate a new translation that is different from all the translations of single - NMT systems .", "entities": []}
{"text": "On the contrary , Uscore and Wscore grade the translations generated by single - NMT models in the ensemble , and then select the highest - scoring translation to be the translation produced by the PAM - based ensemble .", "entities": []}
{"text": "To evaluate the quality of translations produced by the PAM - based ensembles , two baselines are selected .", "entities": []}
{"text": "The first baseline is the \" ce - mean \" option of the Marian - NMT translation system .", "entities": []}
{"text": "The second , and stronger , baseline is the Marian - NMT ensemble ( referred to as \" Marian - ensemble \" hereafter ) .", "entities": []}
{"text": "Entries that exceed the first baseline are depicted in bold .", "entities": []}
{"text": "Entries with scores that exceed the stronger Marian - ensemble baseline are annotated with an asterisk .", "entities": []}
{"text": "A broadly similar situation is found when using testset2 ( Table 4 ) .", "entities": []}
{"text": "Here , the improvement conferred by the ensemble methods over the three base models is much more marked .", "entities": []}
{"text": "One question is whether the improvements conferred by the ensembles are statistically significant .", "entities": []}
{"text": "Then the Wilcoxon and sign tests are used to determine if these populations have significant differences .", "entities": []}
{"text": "Applying the sign and Wilcoxon tests , Marian - ensemble produces statistically better NIST scores ( at a 0.05 level ) than the default Marian - NMT output for amun and s2s models , but not for the transformer model .", "entities": []}
{"text": "For the transformer and s2s models , the scores generated by PAM - Wscore are significantly better that those of single - model Marian - NMT , according to both the Wilcoxon and sign tests ( at a 0.05 level ) .", "entities": []}
{"text": "Similarly , PAM - Uscore gives statistically superior results to Marian - NMT ( ce - mean optimization ) for the s2s model ( at a significance level of 0.05 ) .", "entities": []}
{"text": "Comparing the ensembles to each other , Wscore consistently produces higher scores than Uscore .", "entities": []}
{"text": "This superiority is statistically significant at a 0.05 level according to both Wilcoxon and sign tests , for the transformer and the amun models .", "entities": []}
{"text": "Turning to testset2 , the results are more clearly separated .", "entities": []}
{"text": "This extends to all three NMT models ( amun , transformer and s2s ) , and indicates that both Marian - ensemble and the two PAM - based ensembles give substantially higher scores than single Marian - NMT models .", "entities": []}
{"text": "On the other hand , when comparing PAM - Wscore to PAM - Uscore for testset2 , no statistically significant difference ( at a 0.05 level of signifi - cance ) between the two systems is discerned by either the Wilcoxon or sign test .", "entities": []}
{"text": "Similarly , no statistically significant differences at a 0.05 level are found between the PAM - based ensembles and the Marian - ensemble and only small differences at a 0.10 level .", "entities": []}
{"text": "Thus , even though PAM - based ensembles achieve scores higher than Marian - ensemble , differences are not significant .", "entities": []}
{"text": "To quantize the improvements achieved by the proposed PAM - Wscore approach , in this section the computational requirements posed by each NMT system are also considered .", "entities": []}
{"text": "To this end , the most accurate NMT system is defined for each dataset and metric combination .", "entities": []}
{"text": "Two baselines are chosen , namely the most accurate NMT model and the most accurate Marian - ensemble .", "entities": []}
{"text": "We focus on the transformer model , which is the least expensive model to train .", "entities": []}
{"text": "For each ensemble using transformers , the aim is to determine how close to the Marian - ensemble baseline this is .", "entities": []}
{"text": "This equates to an increase of ca .", "entities": []}
{"text": "22 % in both scores , making the final result directly comparable to s2s , though GPU training requirements are reduced by a factor of 12 .", "entities": []}
{"text": "A second type of evaluation moves away from metrics to focus on analysing the translation errors by different models , with subjective methods .", "entities": []}
{"text": "The Wscore - ensemble reduces the ungrammatical words to 21 , improving translation .", "entities": []}
{"text": "The ungrammatical words were determined in all cases by visual inspection of the body of translations complemented by spell - checking tools to aid detection .", "entities": []}
{"text": "Further inspection of translation quality has involved comparing the Marian - ensemble and Wscore - ensemble outputs .", "entities": []}
{"text": "The length ( in words ) of translations per test sentence is found to differ substantially between the two ensembles , with the difference being more than 1/10 for 9 % of sentences , more than 1/4 for 2.5 % of sentences and more than 1/2 for 1 % of sentences ( close to identical results are obtained for testset1 and testset2 ) .", "entities": []}
{"text": "As such deviations are unexpectedly large , an analysis was performed , with typical examples being shown in Figure 3 .", "entities": []}
{"text": "As can be seen , PAM assists the Wscore - ensemble in retaining all phrases of the sentence .", "entities": []}
{"text": "On the contrary , Marian - ensemble fails to ensure this , and frequently discards portions of the input sentence .", "entities": []}
{"text": "In one case ( sentence # 774 )", "entities": []}
{"text": "Marianensemble results in a null - length translation , and in another ( sentence # 648 ) the final translation covers less than 10 % of the input text , radically distorting meaning .", "entities": []}
{"text": "Both PAM - ensembles are unaffected by such phenomena .", "entities": []}
{"text": "This article has studied the creation of translation systems towards highly inflectional languages , when the amount of in - domain training data is limited .", "entities": []}
{"text": "The Marian - NMT package has been chosen as the starting point to create NMT models for the English to Greek language pair .", "entities": []}
{"text": "Using only publicly available text corpora , the NMT models produce commendably fluent translations .", "entities": []}
{"text": "Identified errors in the NMT translations are typical of a lack of training data .", "entities": []}
{"text": "A hybrid methodology has been proposed that samples an ensemble of NMT models to select the final translation , chosen by a module calculating the alignment level between the input sentence and each translation .", "entities": []}
{"text": "This module was developed for resource - poor MT systems .", "entities": []}
{"text": "Improvements are in many cases statistically significant even over the ensemble system provided within the Marian - NMT package , indicating the promising nature of the hybrid approach .", "entities": []}
{"text": "Also , the translation process is found to be more robust , giving more consistent translations in comparison to the Marian - NMT ensemble system , which occasionally omits large portions of the input text from the translation .", "entities": []}
{"text": "One of the advantages of the proposed method is that it is general - purpose and does not rely on the use of ensembles of Neural MT systems with a specific architecture .", "entities": []}
{"text": "Instead , it can be used to combine the results of different types of Neural MT systems , or MT systems that belong to different paradigms , or even to combine human translations .", "entities": []}
{"text": "In addition the proposed method can be used to clean up a corpus of parallel sentences or several such corpora , by removing sentence pairs for which the source and target - language texts do not have a high degree of parallelism .", "entities": []}
{"text": "Similarly , the proposed method may be used to filter a corpus consisting of original text and its MT - derived translation , to produce a parallel corpus for training of other MT systems , fulfilling a role similar to that proposed by ( Rikters and Fishel , 2017 ) .", "entities": []}
{"text": "One point for future research is how effective a filtering system based on PAM would be , in comparison to already proposed systems .", "entities": []}
{"text": "Future work involves some relatively simple activities that can be imminently implemented , such as releasing the modified version of PAM for experimentation by interested parties .", "entities": []}
{"text": "Future experiments will investigate the effectiveness of this hybrid approach for other language pairs .", "entities": []}
{"text": "All these represent issues for the future .", "entities": []}
{"text": "It is also planned to study the approach using systematic optimisation of the PAM parameters , to identify in more detail configurations that produce more accurate translations .", "entities": []}
{"text": "Another possibility is to use PAM to detect sub - sentential parts of the translated sentences with particularly poor alignments between input and translation and seek better translations of only these specific parts .", "entities": []}
{"text": "Another direction is to investigate more extensively cases where the translation is not sufficiently close to the input sentence .", "entities": []}
{"text": "Then , comparisons to other low - scored translations are more difficult and result in a reduced level of confidence of the chosen translation .", "entities": []}
{"text": "Such a line of study will evaluate more thoroughly the robustness of the proposed method .", "entities": []}
{"text": "The authors acknowledge support of this work by the project \" DRASSI \" ( MIS5002437 ) which is implemented under the Action \" Reinforcement of the Research and Innovation Infrastructure \" , funded by the Operational Programme \" Competitiveness , Entrepreneurship and Innovation \" ( NSRF2014 - 2020 ) and co - financed by Greece and the European Commission ( European Regional Development Fund ) .", "entities": []}
{"text": "Following previous approaches , we machine - translate existing English training sets and manually translate development and test sets into Korean .", "entities": []}
{"text": "Our datasets are publicly available at https://github.com/ kakaobrain / KorNLUDatasets .", "entities": []}
{"text": "Following previous work ( Conneau et al , 2018 ) , we construct our datasets by machine - translating existing English training sets and by translating English development and test sets via human translators .", "entities": []}
{"text": "In an NLI task , a system receives a pair of sentences , a premise and a hypothesis , and classifies their relationship into one out of three categories : entailment , contradiction , and neutral .", "entities": []}
{"text": "There are several publicly available NLI datasets in English .", "entities": []}
{"text": "There are also publicly available NLI datasets in a few other non - English languages ( Fonseca et al , 2016 ; Real et al , 2019 ; Hayashibe , 2020 ) , but none exists for Korean at the time of publication .", "entities": []}
{"text": "Figure 1 : Data construction process .", "entities": []}
{"text": "We translate original English data into Korean using an internal translation engine .", "entities": []}
{"text": "The domain of input sentences covers image captions , news headlines , and user forums .", "entities": []}
{"text": "For details , we refer readers to Cer et al ( 2017 ) .", "entities": []}
{"text": "The overall construction process , which is applied identically to the two new datasets , is illustrated in Figure 1 .", "entities": []}
{"text": "Then , the translation results of the development and test sets are post - edited by professional translators in order to guarantee the quality of evaluation .", "entities": []}
{"text": "This multi - stage translation strategy aims not only to expedite the translators ' work , but also to help maintain the translation consistency between the training and evaluation datasets .", "entities": []}
{"text": "It is worth noting that the post - editing procedure does not simply mean proofreading .", "entities": []}
{"text": "To ensure translation quality , we hired two professional translators with at least seven years of experience who specialize in academic papers / books as well as business contracts .", "entities": []}
{"text": "The two translators each post - edited half of the dataset and cross - checked each other 's translation afterward .", "entities": []}
{"text": "This was further examined by one of the authors , who is fluent in both English and Korean .", "entities": []}
{"text": "We also note that the professional translators did not have to edit much during post - editing , suggesting that the machine - translated sentences were often good enough to begin with .", "entities": []}
{"text": "Finally , we note that translators did not see the English gold labels during post - editing , in order to expedite the post - editing process .", "entities": []}
{"text": "See Section 5 for a discussion on the effect of translation on data quality .", "entities": []}
{"text": "Examples Label P : \u1102 \u1165\u1102 \u1173 \u11ab", "entities": []}
{"text": "\u1100 \u1165\u1100 \u1175\u110b \u1166", "entities": []}
{"text": "\u110b \u1175 \u11bb\u110b", "entities": []}
{"text": "\u1173 \u11af", "entities": []}
{"text": "\u1111 \u1175 \u11af\u110b", "entities": []}
{"text": "\u116d", "entities": []}
{"text": "\u110b", "entities": []}
{"text": "\u1165 \u11b9\u110b \u1165 . E", "entities": []}
{"text": "\" You do n't have to stay there .", "entities": []}
{"text": "\" H : \u1100 \u1161\u1103 \u1169 \u1103 \u116b .", "entities": []}
{"text": "\" You can leave . \"", "entities": []}
{"text": "P : \u1102", "entities": []}
{"text": "\u1165\u1102 \u1173 \u11ab \u1100", "entities": []}
{"text": "\u1165\u1100 \u1175\u110b \u1166", "entities": []}
{"text": "\u110b \u1175 \u11bb\u110b", "entities": []}
{"text": "\u1173 \u11af", "entities": []}
{"text": "\u1111 \u1175 \u11af\u110b", "entities": []}
{"text": "\u116d", "entities": []}
{"text": "\u110b", "entities": []}
{"text": "\u1165 \u11b9\u110b \u1165 . C", "entities": []}
{"text": "\" You do n't have to stay there .", "entities": []}
{"text": "\" H : \u1102", "entities": []}
{"text": "\u1165 \u11ab", "entities": []}
{"text": "\u110c", "entities": []}
{"text": "\u1165 \u11bc\u1112 \u116a \u11a8\u1112 \u1175 \u1100", "entities": []}
{"text": "\u1173 \u110c \u1161\u1105 \u1175\u110b", "entities": []}
{"text": "\u1166", "entities": []}
{"text": "\u110b \u1175 \u11bb\u110b", "entities": []}
{"text": "\u1165\u110b \u1163 \u1112 \u1162 !", "entities": []}
{"text": "\" You need to stay in this place exactly ! \"", "entities": []}
{"text": "P : \u1102", "entities": []}
{"text": "\u1165\u1102 \u1173 \u11ab \u1100", "entities": []}
{"text": "\u1165\u1100 \u1175\u110b \u1166", "entities": []}
{"text": "\u110b \u1175 \u11bb\u110b", "entities": []}
{"text": "\u1173 \u11af", "entities": []}
{"text": "\u1111 \u1175 \u11af\u110b", "entities": []}
{"text": "\u116d", "entities": []}
{"text": "\u110b", "entities": []}
{"text": "\u1165 \u11b9\u110b \u1165 .", "entities": []}
{"text": "N \" You do n't have to stay there . \" are almost twice as long as the hypotheses , as reported in Conneau et al ( 2018 ) .", "entities": []}
{"text": "We present a few examples in Table 2 . H : \u1102 \u1166\u1100", "entities": []}
{"text": "\u1161 \u110b", "entities": []}
{"text": "\u116f", "entities": []}
{"text": "\u11ab\u1112 \u1161\u1106", "entities": []}
{"text": "\u1167 \u11ab \u1102", "entities": []}
{"text": "\u1165 \u11ab", "entities": []}
{"text": "\u110c", "entities": []}
{"text": "\u1175 \u11b8\u110b", "entities": []}
{"text": "\u1166", "entities": []}
{"text": "\u1100 \u1161\u1103 \u1169 \u1103 \u116b", "entities": []}
{"text": ".", "entities": []}
{"text": "\" You can go home if you like . \"", "entities": []}
{"text": "Because both tasks receive a pair of sentences as an input , there are two different approaches depending on whether the model encodes the sentences jointly ( \" cross - encoding \" ) or separately ( \" bi - encoding \" ) .", "entities": []}
{"text": "5", "entities": []}
{"text": "\u1173 \u11b7\u1109 \u1175 \u11a8\u110b \u1173", "entities": []}
{"text": "\u11af", "entities": []}
{"text": "\u1106", "entities": []}
{"text": "\u1165 \u11a8\u1100 \u1169", "entities": []}
{"text": "\u110b \u1175 \u11bb\u1103 \u1161 .", "entities": []}
{"text": "4.2 \" A man is eating food . \" B : \u1112 \u1161 \u11ab \u1102", "entities": []}
{"text": "\u1161 \u11b7\u110c", "entities": []}
{"text": "\u1161\u1100 \u1161 \u1106 \u116f \u11ab\u1100 \u1161\u1105 \u1173 \u11af", "entities": []}
{"text": "\u1106", "entities": []}
{"text": "\u1165 \u11a8\u1100 \u1169", "entities": []}
{"text": "\u110b \u1175 \u11bb\u1103 \u1161 .", "entities": []}
{"text": "\" A man is eating something .", "entities": []}
{"text": "\" A : \u1112 \u1161 \u11ab", "entities": []}
{"text": "\u110b \u1167\u1109", "entities": []}
{"text": "\u1165 \u11bc\u110b", "entities": []}
{"text": "\u1175 \u1100 \u1169\u1100 \u1175\u1105", "entities": []}
{"text": "\u1173 \u11af", "entities": []}
{"text": "\u110b \u116d\u1105", "entities": []}
{"text": "\u1175\u1112", "entities": []}
{"text": "\u1161\u1100 \u1169", "entities": []}
{"text": "\u110b \u1175 \u11bb\u1103 \u1161 .", "entities": []}
{"text": "0.0 \" A woman is cooking meat . \" B : \u1112 \u1161 \u11ab \u1102 \u1161 \u11b7\u110c", "entities": []}
{"text": "\u1161\u1100 \u1161 \u1106", "entities": []}
{"text": "\u1161 \u11af\u1112", "entities": []}
{"text": "\u1161\u1100 \u1169", "entities": []}
{"text": "\u110b \u1175 \u11bb\u1103 \u1161 .", "entities": []}
{"text": "\" A man is speaking . \"", "entities": []}
{"text": "approach , the pre - trained language model takes each sentence pair as a single input for fine - tuning .", "entities": []}
{"text": "These cross - encoding models typically achieve the state - of - the - art performance over bi - encoding models , which encode each input sentence separately .", "entities": []}
{"text": "We train our models using fairseq with 32 V100 GPUs for the base model ( 25 days ) and 64 for the large model ( 20 days ) .", "entities": []}
{"text": "For each model , the larger variant outperforms the base one , consistent with previous findings .", "entities": []}
{"text": "The bi - encoding approach bears practical importance in applications such as semantic search , where computing pairwise similarity among a large set of sentences is computationally expensive with cross - encoding .", "entities": []}
{"text": "We adopt the MEAN pooling strategy , i.e. , computing the sentence vector as the mean of all contextualized word vectors .", "entities": []}
{"text": "As noted in ( Conneau et al , 2018 ) , translation quality does not necessarily guarantee that the semantic relationships between sentences are preserved .", "entities": []}
{"text": "We also translated each sentence independently and took the gold labels from the original English pair , so the resulting label might no longer be \" gold , \" due to both incorrect translations and ( in rarer cases ) linguistic differences that make it difficult to translate specific concepts .", "entities": []}
{"text": "Nevertheless , we could also find some ( not many ) examples the gold label becomes incorrect after translating input sentences to Korean .", "entities": []}
{"text": "In another case , the English word sir appeared in the premise of an NLI example and was translated to \u1109", "entities": []}
{"text": "\u1165 \u11ab\u1109", "entities": []}
{"text": "As a result , when the hypothesis referencing the entity as the man got translated into \u1102 \u1161 \u11b7\u110c", "entities": []}
{"text": "\u1161 ( gender - specific ) , the English gold label ( entailment ) was no longer correct in the translated example .", "entities": []}
{"text": "More systematically analyzing these errors is an interesting future work , although the amount of human efforts involved in this analysis would match that of labeling a new dataset .", "entities": []}
{"text": "Looking forward , we hope that our datasets and baselines will facilitate future research on not only improving Korean NLU systems but also increasing language diversity in NLU research .", "entities": []}
{"text": "After training , the base and large models achieve validation perplexities of 2.55 and 2.39 respectively , where the validation set is a random 5 % subset of the entire corpora .", "entities": []}
{"text": "The fine - tuning hyperparameters are summarized in Table 8 .", "entities": []}
{"text": "We find that the hyperparameters used for English models and datasets give sufficiently good performances on the development set , so we do not perform an additional hyperparameter search .", "entities": []}
{"text": "After training each model for 10 epochs , we choose the model checkpoint that achieve the highest score on the development set and evaluate it on the test set to obtain our final results in 4.1 .", "entities": []}
{"text": "We also report the development set scores for the best checkpoint in Table 9 .", "entities": []}
{"text": "Unless described otherwise , we follow the experimental settings , including all hyperparameters , of SentenceBERT 10 .", "entities": []}
{"text": "These hyperparameters are shown in Table 10 .", "entities": []}
{"text": "We report the development set scores in Table 11 .", "entities": []}
{"text": "We thank Pulip Park for helping with hiring and contacting with the professional translators .", "entities": []}
{"text": "We would also like to acknowledge Kakao Brain Cloud , which we used for our baseline experiments .", "entities": []}
{"text": "DialPort , Gone Live : An Update After A Year of Development", "entities": []}
{"text": "DialPort collects user data for connected spoken dialog systems .", "entities": []}
{"text": "At present six systems are linked to a central portal that directs the user to the applicable system and suggests systems that the user may be interested in .", "entities": []}
{"text": "User data has started to flow into the system .", "entities": []}
{"text": "The goal of the DialPort spoken dialog portal is to gather large amounts of real user data for spoken dialog systems ( SDS ) .", "entities": []}
{"text": "Sophisticated statistical representations in state of the art SDS , require large amounts of data .", "entities": []}
{"text": "While industry has this , they can not share this treasure .", "entities": []}
{"text": "Academia has difficulty getting even small amounts of similar data .", "entities": []}
{"text": "With one central portal , connected to many different systems , the task of advertising and affording user access can be done in one centralized place that all systems can connect to .", "entities": []}
{"text": "DialPort provides a steady stream of data , allowing system creators to focus on developing their systems .", "entities": []}
{"text": "The portal decides what service the user wants and connects them to the appropriate system which carries on a dialog with the user , returning control to the portal at the end .", "entities": []}
{"text": "Later , more systems connected to the portal .", "entities": []}
{"text": "A flow of users has begun interacting with the portal .", "entities": []}
{"text": "In order to get a flow of users started , DialPort developers expanded the number of connected systems to make the portal offerings more attractive and relevant .", "entities": []}
{"text": "They also made the interface easier to use .", "entities": []}
{"text": "With few connected systems in previous versions it was difficult to assess the portal 's switching mechanisms .", "entities": []}
{"text": "The increased number of systems challenges the portal to make better decisions and have better a switching strategy .", "entities": []}
{"text": "It also demands changes in the frequency of recommendations to connected systems .", "entities": []}
{"text": "A short history of DialPort DialPort started with a call for research groups to link their SDS to the portal and a website listing SDS urls for users to try out .", "entities": []}
{"text": "System connections go through an API that sends them the ASR result ( Chrome at present ) .", "entities": []}
{"text": "The first assessment of the interface ( Lee et al , 2017 ) included five External Systems ( ESes , that is , systems that are joined to the portal and are thus not part of the central portal - they can be from CMU as well as from other sites ) :", "entities": []}
{"text": "The latter systems , by the CMU portal group , offer new services hoping to attract more diverse users and encourage them to become return users .", "entities": []}
{"text": "The current database has just over 100 restaurants and is implemented using the multi - domain statistical dialogue system toolkit PyDial .", "entities": []}
{"text": "To connect PyDial to Dialport , PyDial 's dialogue server interface is used .", "entities": []}
{"text": "It is implemented as an HTTP server expecting JSON messages from the Dialport client .", "entities": []}
{"text": "Mr. Clue Mr. Clue plays a simple wordguessing game ( Pincus and Traum , 2016 ) .", "entities": []}
{"text": "Mr. Clue is the clue - giver and the user plays the role of guesser .", "entities": []}
{"text": "Mr. Clue mines his clues from pre - existing web and database resources such as dictionary.com and WordNet .", "entities": []}
{"text": "Clue lists used are only clues that pass an automatic filter described in ( Pincus and Traum , 2016 ) .", "entities": []}
{"text": "The original Mr. Clue was updated to enable successful communication with Dialport .", "entities": []}
{"text": "First , since the original Mr. Clue listens for VH messages ( a variant of ActiveMQ messaging used by the Virtual Human Toolkit ( Hartholt et al , 2013 ) , we built an HTTP server that converts HTTP messages ( expected in JSON format ) to VH messages .", "entities": []}
{"text": "Mr. Clue is always in one of 2 states ( in - game or out - game ) .", "entities": []}
{"text": "The out - game state dialogue is limited to asking if the user wants to play another round ( and offering to give instructions in the beginning of a session ) .", "entities": []}
{"text": "The user can use goodbye keyword to exit the system at any time .", "entities": []}
{"text": "This sends an exit message to Di - alPort and allowing it to take back control .", "entities": []}
{"text": "For its 150 second rounds , timing information is kept on the back - end and sent to the front - end ( DialPort ) in every message .", "entities": []}
{"text": "It keeps track of which lists have been used for a session so a user will never play the same round twice ( for a given session ) .", "entities": []}
{"text": "Let'sDiscuss LetsDiscuss responds to queries about a specific restaurant by finding relevant segments of user reviews .", "entities": []}
{"text": "It searches a database of restaurant reviews obtained from Zomato and Yelp .", "entities": []}
{"text": "We formed a list of general discussion points for restaurants ( service , atmosphere , etc ) .", "entities": []}
{"text": "For each discussion point , a list of relevant keywords was compiled using WordNet , thesaurus , and by categorizing the most frequently words found in reviews .", "entities": []}
{"text": "Let'sForecast , from CMU , uses the NOAA website .", "entities": []}
{"text": "Let 's Go , derived from the Let 's Go system ( Raux et al , 2005 ) , is based on an end - to - end recurrent neural network structure and a backend that covers cities other than Pittsburgh .", "entities": []}
{"text": "In informal trials , some aspects of the portal 's interaction were not effective for some users .", "entities": []}
{"text": "Some ES need graphics to supplement their verbal information .", "entities": []}
{"text": "Since Mr Clue keeps score and timing of users ' answers , its instructions and scores are shown on a blackboard .", "entities": []}
{"text": "Let 's Go shows a map with the bus trajectory from departure to arrival .", "entities": []}
{"text": "The portal gives users feedback for : available topics , system state , and present system state .", "entities": []}
{"text": "Skylar does n't interrupt the dialog with a list of topics .", "entities": []}
{"text": "Rather it suggests one topic every few turns .", "entities": []}
{"text": "This evenly steers users to all of the ES .", "entities": []}
{"text": "A banner at the bottom of the screen reminds users of all the topics that can be discussed .", "entities": []}
{"text": "Another box indicates the system state in order to avoid user confusion about who has the floor .", "entities": []}
{"text": "It shows , for example , whether the system is processing the speech or is still waiting for them to talk .", "entities": []}
{"text": "The box shows : idle ( either from timeout or from the user clicking on the box to pause the system ) ; listening ( this is shown from the instant the ASR begins to process speech to when it is finished ) ; speaking ( from when the TTS begins output to when it is finished ) ; thinking ( from when the ASR output is sent to the NLU to when the DM issues its action ) .", "entities": []}
{"text": "Finally , the system informs the user of the present state of the dialog .", "entities": []}
{"text": "Do you still want XX ( e.g. Pittsburgh ) ?", "entities": []}
{"text": "reveals that the user preference for Pittsburgh has not been used for a while , and Skylar 's forgetting curve is ready to eliminate it .", "entities": []}
{"text": "The dynamic choice of implicit or explicit confirmation covers the global dialog state .", "entities": []}
{"text": "As more ES join the portal , policies and strategies have become more flexible .", "entities": []}
{"text": "There are two major changes to the portal 's behavior : ES selection policy and ES recommendation policy .", "entities": []}
{"text": "or Yes / No - question ( Is it going to rain ? ) .", "entities": []}
{"text": "A natural dialog should answer a user according to the way in which they made their earlier requests ( Traum and Allen , 1994 ) .", "entities": []}
{"text": "For example , the weather system should produce the natural", "entities": []}
{"text": "Yes it 's going to rain instead of a full weather report , for the third question above .", "entities": []}
{"text": "We thus keep the user 's initial request intent in the global dialog context and share it with the relevant ESes .", "entities": []}
{"text": "The recommendation policy has been improved in two ways : 1 ) All participating system developers agreed that Skylar should give ES recommendations on a rotating basis so that all systems are recommended equally .", "entities": []}
{"text": "Skylar no longer makes a recommendation at the end of each system turn .", "entities": []}
{"text": "Recommendations are made about every four turns and , as mentioned above , are not for a system that the user recently interacted with .", "entities": []}
{"text": "2 ) Fine grained recommendation : As more ESes joined the portal , we began to exploit the relatedness among ESs in order to generate more targeted recommendations .", "entities": []}
{"text": "For instance , we tuned the policy to have a higher probability of recommending the Let'sDiscuss restaurant review function when users obtain restaurant information by prompting , do you want to hear a review about this place ?", "entities": []}
{"text": "The weighted average F - 1 score for multi - intent and multi - domain classification is 0.93 .", "entities": []}
{"text": "There are several types of portal users .", "entities": []}
{"text": "First , the developers themselves try out the system .", "entities": []}
{"text": "Then they ask friends and family to try it .", "entities": []}
{"text": "Users can be paid .", "entities": []}
{"text": "Finally we have users who really need the information or gaming pleasure .", "entities": []}
{"text": "We define two potential types of users ( using IP addresses ) :", "entities": []}
{"text": "explorers and real users .", "entities": []}
{"text": "Explorers are trying the system for the first time .", "entities": []}
{"text": "They explore several of the ESes , but they do not have any real gaming or information need .", "entities": []}
{"text": "Real users have returned to use the por - tal , asking for something they need or enjoy .", "entities": []}
{"text": "They may speak to less of the ESes during their visit , but have some real .", "entities": []}
{"text": "The following factors may explain why users did not have a dialog with the system : presence of human study consent form ; not using Chrome browser ( solved by making a typing - only version ) ; user did n't want any portal services ; user did n't have a microphone ; user did n't understand the purpose of the portal ( we gave Skylar an opening monologue explaining what the data is for ) .", "entities": []}
{"text": "The AdWord experience lead us to published a Facebook page on April 12 , 2017 .", "entities": []}
{"text": "The page was to attract both explorers and real users through both organic ( friends and friends of friends ) and paid distribution .", "entities": []}
{"text": "Despite the short time ( 4 - 12 to 4 - 20 ) that it has been published , there have been a total of 51 dialogs ( excluding all dialogs from participating research teams ) .", "entities": []}
{"text": "As of April 20 , Dial - Port spent about $ 52 in advertising to reach 1776 individuals getting 147 page views , 47 likes and 346 engagements ( shares or clicks ) .", "entities": []}
{"text": "About 40 % of the clicks were from mobile devices as opposed to computers .", "entities": []}
{"text": "This underlines the need for mobile versions of DialPort .", "entities": []}
{"text": "The average length of a dialog is 8.7turns ( 7.18 stdev ) and 129.51s ( stdev 138.03 ) .", "entities": []}
{"text": "There were 14.9 % return users , although another person could be using that computer and some places have automatic IP assignment .", "entities": []}
{"text": "52.9 % of the dialogs were spoken as opposed to typed .", "entities": []}
{"text": "The average ASR delay was 925.03ms .", "entities": []}
{"text": "On average , users tried 4.8 systems per dialog .", "entities": []}
{"text": "The distribution of dialog turns per ES and for the portal over time is shown on Figure 1 .", "entities": []}
{"text": "Some systems are getting less use than others .", "entities": []}
{"text": "This will be countered by paid advertising campaigns that promote each specific system .", "entities": []}
{"text": "This paper has presented a novel portal that collects spoken dialog data for connected systems .", "entities": []}
{"text": "It has begun to collect data for the present seven systems .", "entities": []}
{"text": "In order to improve service an audio server is under construction as are smartphone and tablet versions .", "entities": []}
{"text": "The portal welcomes new external systems .", "entities": []}
{"text": "This work is partly funded by National Science Foundation grant CNS - 1512973 .", "entities": []}
{"text": "The opinions expressed in this paper do not necessarily reflect those of the National Science Foundation .", "entities": []}
{"text": "We propose a novel transition - based algorithm that straightforwardly parses sentences from left to right by building n attachments , with n being the length of the input sentence .", "entities": []}
{"text": "Most of the models providing competitive accuracies fall into two broad families of approaches : graph - based ( Mc - Donald et al , 2005a , b ) and transition - based ( Yamada and Matsumoto , 2003 ; Nivre , 2003 ) dependency parsers .", "entities": []}
{"text": "Given an input sentence , a graph - based parser scores trees by decomposing them into factors , and performs a search for the highest - scoring tree .", "entities": []}
{"text": "This tendency recently changed , since a transition - based parser developed by Ma et al ( 2018 ) managed to outperform the best graphbased model in the majority of datasets tested .", "entities": []}
{"text": "Transition - based parsers incrementally build a dependency graph for an input sentence by applying a sequence of transitions .", "entities": []}
{"text": "This results in more efficient parsers with linear time complexity for parsing projective sentences , or quadratic for handling non - projective structures , when implemented with greedy or beam search .", "entities": []}
{"text": "However , their main weakness is the lack of access to global context information when transitions are greedily chosen .", "entities": []}
{"text": "This favours error propagation , mainly affecting long dependencies that require a larger number of transitions to be built ( McDonald and Nivre , 2011 ) .", "entities": []}
{"text": "The proposed framework provides a global view of the input sentence by capturing information from the whole sentence and all the arcs previously built , crucial for reducing the effect of error propagation ; and , thanks to an attention mechanism ( Bahdanau et al , 2014 ; Luong et al , 2015 ) , is able to return a position in that sentence that corresponds to a word related to the word currently on top of the stack .", "entities": []}
{"text": "They take advantage of this and propose a novel transition system that follows a top - down depth - first strategy to perform the syntactic analysis .", "entities": []}
{"text": "Concretely , it considers the word pointed by the neural network as the child of the word on top of the stack , and builds the corresponding dependency relation between them .", "entities": []}
{"text": "This results in a transition - based algorithm that can process unrestricted non - projective sentences in O ( n 2 ) time complexity and requires 2n - 1 actions to successfully parse a sentence with n words .", "entities": []}
{"text": "This results in a straightforward transition system that can parse a sentence in just n actions , without the need of any additional data structure and by just attaching each word from the sentence to another word ( including the root node ) .", "entities": []}
{"text": "This kind of neural networks are able to learn the conditional probability of a sequence of discrete numbers that correspond to positions in an input sequence ( in this case , indexes of words in a sentence ) and , by means of attention ( Bahdanau et al , 2014 ; Luong et al , 2015 ) , implement a pointer that selects a position from the input at decoding time .", "entities": []}
{"text": "Their approach initially reads the whole sentence , composed of the n words w 1 , . . .", "entities": []}
{"text": ",", "entities": []}
{"text": "w n , and encodes each", "entities": []}
{"text": "w i one by one into an encoder hidden state e i .", "entities": []}
{"text": "As encoder , they employ a combination of CNNs and bi - directional LSTMs ( Chiu and Nichols , 2016 ; Ma and Hovy , 2016 ) .", "entities": []}
{"text": "For each word , CNNs are used to obtain its character - level representation that is concatenated to the word and PoS embeddings to finally be fed into BiLSTMs that encode word context information .", "entities": []}
{"text": "As decoder they present a top - down transition system , where parsing configurations use the classic data structures ( Nivre , 2008 ) : a buffer ( that contains unattached words ) and a stack ( that holds partially processed words ) .", "entities": []}
{"text": "The available parser actions are two transitions that we call Shift - Attach - p and Reduce .", "entities": []}
{"text": "Given a configuration with word", "entities": []}
{"text": "If p", "entities": []}
{"text": "= i , then the pointed word w p is considered as a child of w i ; so the parser chooses a Shift - Attach - p transition to move w p from the buffer to the stack and build an arc", "entities": []}
{"text": "w i", "entities": []}
{"text": "w p .", "entities": []}
{"text": "On the other hand , if p = i , then w i is considered to have found all its children , and a Reduce transition is applied to pop the stack .", "entities": []}
{"text": "The parsing process starts with a dummy root $ on the stack and , by applying 2n - 1 transitions , a dependency tree is built for the input in a top - down depth - first fashion , where multiple children of a same word are forced during training to be created in an inside - out manner .", "entities": []}
{"text": "w i on top of the stack to generate a decoder hidden state d t .", "entities": []}
{"text": "After that , d t , together with the sequence s i of encoder hidden states from words still in the buffer plus e i , are used to compute the attention vector a t as follows :", "entities": []}
{"text": "v t", "entities": []}
{"text": "As attention scoring function ( score ( ) ) , they adopt the biaffine attention mechanism described in ( Luong et al , 2015 ; Dozat and Manning , 2016 ) .", "entities": []}
{"text": "Finally , the attention vector a t will be used to return the highest - scoring position p and choose the next transition .", "entities": []}
{"text": "The parsing process ends when only the root remains on the stack .", "entities": []}
{"text": "As extra high - order features , Ma et al ( 2018 ) add grandparent and sibling information , whose encoder hidden states are added to that of the word on top of the stack to generate the corresponding decoder hidden state d t .", "entities": []}
{"text": "According to the authors , the original stackpointer network is trained to maximize the likelihood of choosing the correct word for each possible top - down path from the root to a leaf .", "entities": []}
{"text": "More in detail , a dependency tree can be represented as a sequence of top - down paths p 1 , . . .", "entities": []}
{"text": ", p k ,", "entities": []}
{"text": "where each path p i corresponds to a sequence of words $ , w i , 1 ,", "entities": []}
{"text": "w i , 2 , . . .", "entities": []}
{"text": ", w i , l i from the root to a leaf .", "entities": []}
{"text": "= k i=1 P", "entities": []}
{"text": "p", "entities": []}
{"text": "< i , x )", "entities": []}
{"text": "=", "entities": []}
{"text": "p i and w i , < j represents all the previous words on p i .", "entities": []}
{"text": "We take advantage of the neural network architecture designed by Ma et al ( 2018 ) and introduce a simpler left - to - right transition system that requires neither a stack nor a buffer to process the input sentence and where , instead of selecting a child of the word on top of the stack , the network points to the parent of the current focus word .", "entities": []}
{"text": "In particular , in our proposed approach , the parsing configuration just corresponds to a focus word pointer i , that is used to point to the word currently being processed .", "entities": []}
{"text": "The decoding process starts with i pointing at the first word of the sentence and , at each parsing configuration , only one action is available : the parameterized Attach - p transition , that links the focus word w i to the head word w p in position p of the sentence ( producing the dependency arc w p w i ) and moves i one position to the right .", "entities": []}
{"text": "The parsing process ends when the last word from the sentence is attached .", "entities": []}
{"text": "This can be easily represented as a loop that traverses the input sentence from left to right , linking each word to another from the same sentence or to the dummy root .", "entities": []}
{"text": "Therefore , we just need n steps to process the n words of a given sentence and build a dependency tree .", "entities": []}
{"text": "While our novel transition system intrinsically holds the single - head constraint ( since , after attaching the word w", "entities": []}
{"text": "i , i points to the next word w i+1", "entities": []}
{"text": "in the sentence ) , it can produce an output with cycles .", "entities": []}
{"text": "1", "entities": []}
{"text": "Therefore , in order to build a wellformed dependency tree during decoding , attachments that generate cycles in the already - built dependency graph must be forbidden .", "entities": []}
{"text": "Please note that the need of a cycle - checking extension does not increase the overall quadratic runtime complexity of the original implementation by Ma et al ( 2018 ) since , as in other transition - based parsers such as ( Covington , 2001 ; G\u00f3mez - Rodr\u00edguez and Nivre , 2010 ) , cycles can be incrementally identified in amortized constant time by keeping track of connected components using path compression and union by rank .", "entities": []}
{"text": "Therefore , the left - to - right algorithm requires n steps to produce a parse .", "entities": []}
{"text": "In addition , at each step , the attention vector a t needs to be computed and cycles must be checked , both in O ( n )", "entities": []}
{"text": "+ O ( n )", "entities": []}
{"text": "= O ( n ) runtime .", "entities": []}
{"text": "This results in a O ( n 2 ) time complexity for decoding .", "entities": []}
{"text": "2 On the other hand , while in the top - down decoding only available words in the buffer ( plus the word on top of the stack ) can be pointed to by the network and they are reduced as arcs are created ( basically to keep the single - head constraint ) ; our proposed approach is less rigid : all words from the sentence ( including the root node and excluding w i ) can be pointed to , as long as they satisfy the acyclicity constraint .", "entities": []}
{"text": "This is necessary because two different words might be attached to the same head node and the latter can be located in the sentence either before or after w i .", "entities": []}
{"text": "Therefore , the sequence s i , required by the attention score function ( Eq .", "entities": []}
{"text": "( 1 ) ) , is composed of the encoder hidden states of all words from the input , excluding e i , and prepending a special vector representation denoting the root node .", "entities": []}
{"text": "We also add extra features to represent the current focus word .", "entities": []}
{"text": "Instead of using grandparent and sibling information ( more beneficial for a topdown approach ) , we just add the encoder hidden states of the previous and next words in the sentence to generate d t , which seems to be more suitable for a left - to - right decoding .", "entities": []}
{"text": ", l n .", "entities": []}
{"text": "Each link l", "entities": []}
{"text": "i is characterized by the word", "entities": []}
{"text": "w i in position i in the sentence and its head word w h , resulting in a pair ( w i , w h ) .", "entities": []}
{"text": "=", "entities": []}
{"text": "l", "entities": []}
{"text": "< i , x )", "entities": []}
{"text": "=", "entities": []}
{"text": "| w i ,", "entities": []}
{"text": "l < i , x )", "entities": []}
{"text": "Therefore , the left - to - right parser is trained by maximizing the likelihood of choosing the correct head word w h for the word w", "entities": []}
{"text": "i in position i , given the previous predicted links l < i .", "entities": []}
{"text": "Finally , following a widely - used approach ( also implemented in ( Ma et al , 2018 ) ) , dependency labels are predicted by a multiclass classifier , which is trained in parallel with the parser by optimizing the sum of their objectives .", "entities": []}
{"text": "UAS LAS Chen and Manning ( 2014 ) 91.8 89.6", "entities": []}
{"text": "Dyer et al ( 2015 ) 93.1 90.9 93.99 92.05 93.56 91.42 Kiperwasser and Goldberg ( 2016 ) 93.9 91.9 94.23 92.36 94.3 92.2 Fern\u00e1ndez - G and G\u00f3mez - R ( 2018 ) Systems marked with * , including the improved variant described in ( Ma et al , 2018 ) of the graph - based parser by ( Dozat and Manning , 2016 ) , are implemented under the same framework as our approach and use the same training settings .", "entities": []}
{"text": "In addition , in Table 2 we can see how , under the exactly same conditions , the left - to - right algorithm improves over the original top - down variant in nine out of twelve languages in terms of LAS , obtaining competitive results in the remaining three datasets .", "entities": []}
{"text": "7", "entities": []}
{"text": "During decoding , a postprocessing step is needed to produce well - formed trees by means of a maximum spanning tree algorithm .", "entities": []}
{"text": "Like our approach , they also use the capabilities provided by pointer networks to undertake the parsing task as a simple process of attaching each word as dependent of another .", "entities": []}
{"text": "They also try to improve the network performance with POS tag prediction as auxiliary task and with different approaches to perform label prediction .", "entities": []}
{"text": "They do not exclude cycles , neither by forbidding them at parsing time or by removing them by post - processing , as they report that their system produces parses with a negligible amount of cycles , even with greedy decoding ( matching our observation for our own system , in our case with beam - search decoding ) .", "entities": []}
{"text": "Finally , the system developed by Chorowski et al ( 2017 ) is constrained to projective dependencies , while our approach can handle unrestricted non - projective structures .", "entities": []}
{"text": "We present a novel left - to - right dependency parser based on pointer networks .", "entities": []}
{"text": "We follow the same neural network architecture as the stack - pointerbased approach developed by Ma et al ( 2018 ) , but just using a focus word index instead of a buffer and a stack .", "entities": []}
{"text": "The good performance of our algorithm can be explained by the shortening of the transition sequence length .", "entities": []}
{"text": "In fact , it has been proved by several studies ( Fern\u00e1ndez - Gonz\u00e1lez and G\u00f3mez - Rodr\u00edguez , 2012 ; Fern\u00e1ndez - Gonz\u00e1lez and G\u00f3mez - Rodr\u00edguez , 2018 ) that by reducing the number of applied transitions , the impact of error propagation is alleviated , yielding more accurate parsers .", "entities": []}
{"text": "Our system 's source code is freely available at https://github.com/danifg/ Left2Right - Pointer - Parser .", "entities": []}
{"text": "This work has received funding from the European Research Council ( ERC ) , under the European Union 's Horizon 2020 research and innovation programme ( FASTPARSE , grant agreement", "entities": []}
{"text": "No 714150 ) , from MINECO ( FFI2014 - 51978 - C2 - 2 - R , TIN2017 - 85160 - C2 - 1 - R ) and from Xunta de Galicia ( ED431B 2017/01 ) .", "entities": []}
{"text": "These models are believed to be reasoning in some way ; however , it has been shown that these same results , or better , can be achieved without considering the claim at all - only the evidence .", "entities": []}
{"text": "We provide quantifiable results that prove our hypothesis that manipulable features are being used for fact - checking .", "entities": []}
{"text": "Recent events such as the last two U.S. presidential elections have been greatly affected by fake news , defined as \" fabricated information that disseminates deceptive content , or grossly distort actual news reports , shared on social media platforms \" ( Allcott and Gentzkow , 2017 ) .", "entities": []}
{"text": "In fact , the World Economic Forum 2013 report designates massive digital misinformation as a major technological and geopolitical risk ( Bovet and Makse , 2019 ) .", "entities": []}
{"text": "As daily social media usage increases ( Statista Research Department , 2021 ) , manual fact - checking can not keep up with this deluge of information .", "entities": []}
{"text": "Automatic fact - checking models are therefore a necessity , and most of them function using a system of claims and evidence ( Hassan et al , 2017 ) .", "entities": []}
{"text": "Given a specific claim , the models use external knowledge as evidence .", "entities": []}
{"text": "Typically , a web search query is treated as the claim , and a subset of the top search results is treated as the evidence .", "entities": []}
{"text": "There is an implicit assumption that the fact - checking models are reasoning in some way , using the evidence to confirm or refute the claim .", "entities": []}
{"text": "Recent research ( Hansen et al , 2021 ) found this conclusion may be premature ; current models can show improved performance when considering evidence alone , essentially fact - checking an unasked question .", "entities": []}
{"text": "While this might seem reasonable given that the evidence is conditioned on the claims by the search engine , this can be exploited as illustrated in Figure 1 , which shows that evidence returned using a ridiculous claim can still appear reasonable if we view the evidence alone without the claim .", "entities": []}
{"text": "Furthermore , textual entailment requires both a text and a hypothesis ; if we have a result without a hypothesis , we are performing a different , unknown task .", "entities": []}
{"text": "We use a variety of pre - processing steps , including neural and non - neural ones , to attempt to reduce the affectations common in evidence : Stemming , stopword removal , negation , and POS - filtering ( Babanejad et al , 2020 ) .", "entities": []}
{"text": "Although the returned evidence appearing reputable , it is clear that it has little relevance to deciding the veracity of the claim that \" all Canadians have eaten at least one bear . \"", "entities": []}
{"text": "et al , 2019 ) , adding an \" emotional attention \" layer to weight the most relevant emotional signals in a given evidence snippet .", "entities": []}
{"text": "We make our code publicly available .", "entities": []}
{"text": "1", "entities": []}
{"text": "With each of these methods , we focus on scores where the models perform better using both the claims and the evidence combined , S C&E , rather than with the evidence alone , S E .", "entities": []}
{"text": "Going forward , we will refer to the difference between these dataset combinations as the delta of the pre - processing step , where delta = S C&E \u2212 S E .", "entities": []}
{"text": "A positive delta score indicates that the claim was useful and helped yield an increase in performance .", "entities": []}
{"text": "Since we are removing indicators that the current models rely on , some of the models perform worse at the task than they did previously .", "entities": []}
{"text": "However , a surprising result is that many improved , and the need to consider the claim and the evidence together is a sign of using reasoning rather than manipulable indicators .", "entities": []}
{"text": "Under current fact - checking models , adversarial data can subvert these detectors .", "entities": []}
{"text": "Paraphrasing can be performed by inserting fictitious statements into otherwise truthful evidence with little effect on the model 's output .", "entities": []}
{"text": "For example , an article titled \" Is the GOP losing Walmart ? \" , could have \" Walmart \" substituted with \" Apple , \" and the predictions are nearly identical despite the news now being fictitious ( Zhou et al , 2019 ) .", "entities": []}
{"text": "1", "entities": []}
{"text": "There has been significant work with automatic fact - checking models using RNNs and Transformers ( Shaar et al , 2020a ; Alam et al , 2020 ; Shaar et al , 2020b ) as well as non - neural machine learning using TF - IDF vectors ( Reddy et al , 2018 ) .", "entities": []}
{"text": "Additionally , models may in fact simply memorize biases within data ( Gururangan et al , 2018 ) .", "entities": []}
{"text": "Improvements can be made when using human - identified justifications for fact - checking ( Alhindi et al , 2018 ; Vo and Lee , 2020 ) , and making use of textual entailment can offer improvements ( Saikh et al , 2019 ) .", "entities": []}
{"text": "Emotional text can signal low credibility ( Rashkin et al , 2017 ) , characterizing fake news as a task where pre - processing can be used effectively to diminish bias ( Giachanou et al , 2019 ; Babanejad et al , 2020 ) .", "entities": []}
{"text": "A framework to both categorize fake news and to identify features that differentiate fake news from real news has been described by Molina et al ( 2021 ) , and debiasing inappropriate subjectivity in text can be accomplished by replacing a single biased word in each sentence ( Pryzant et al , 2020 ) .", "entities": []}
{"text": "Figure 2 : Ablation studies where evidence was sequentially removed for training and evaluation of models .", "entities": []}
{"text": "On the far left , we show the most effective non - neural pre - processing compared to the baseline of none .", "entities": []}
{"text": "Performance generally worsens as the ablation increases .", "entities": []}
{"text": "For example , \" interrupt \" and \" rage \" are both categorized as anger words , but with the respective intensity values of 0.333 and 0.911 .", "entities": []}
{"text": "There is also a snippet attention layer at - tending to which evidence itself should be weighted most heavily for the given claim .", "entities": []}
{"text": "Our goal is to separate affect - based properties from factual content of the text .", "entities": []}
{"text": "Toward this , we run a large number of permutations of the following four simple pre - processing steps ( see Figure 4 in Appendix B for results ) .", "entities": []}
{"text": "In some cases we used a modified form - such as removing adverbs for POS pre - processing .", "entities": []}
{"text": "Negation ( NEG ) :", "entities": []}
{"text": "A mechanism that transforms a negated statement into its inverse ( Benamara et al , 2012 ) .", "entities": []}
{"text": "An example , \" I am not happy \" would have \" not \" removed and \" happy \" replaced by its antonym , forming the sentence \" I am sad .", "entities": []}
{"text": "\" Parts - of - Speech ( POS ) :", "entities": []}
{"text": "We keep only three parts of speech : nouns , verbs , and adjectives .", "entities": []}
{"text": "We initially included adverbs but found removing them improved results .", "entities": []}
{"text": "This could be due to some adverbs being emotionally charged .", "entities": []}
{"text": "Stopwords ( STOP ) :", "entities": []}
{"text": "These are generally the most common words in a language , such as function words and prepositions .", "entities": []}
{"text": "We use the NLTK library .", "entities": []}
{"text": "Stemming ( STEM ) :", "entities": []}
{"text": "Reducing a word to its root form .", "entities": []}
{"text": "We use the NLTK Snowball Stemmer .", "entities": []}
{"text": "As well , it removes punctuation and alters phrasing that might be understood as sarcasm , such as \" Melania Trump said that Native Americans upset about the Dakota Access Pipeline should ' go back to India \" ' to \" Melania Trump told Native Americans that was upset by the Dakota Access Pipeline , that they should travel to India . \"", "entities": []}
{"text": "The informalto - formal model lowercases everything and also changes the text significantly .", "entities": []}
{"text": "We chose this paraphrasing model based on the idea that fake news - especially that which is frequently posted on social media - has a certain polarizing style that might be neutralized by altering the formality of the text .", "entities": []}
{"text": "Rather surprisingly , we received better results transforming the style from formal - to - informal than we did with informal - toformal .", "entities": []}
{"text": "The EmoCred systems of EmoLexi and EmoInt use a lexicon to determine emotional word counts and intensities , respectively ( Giachanou et al , 2019 ) .", "entities": []}
{"text": "We use the NRC Affect Intensity Lexicon , a \" highcoverage lexicons that captures word - affect intensities \" for eight basic emotions , which were created using a technique called best - worst scaling ( Mohammad , 2017 ) .", "entities": []}
{"text": "As an example , a sentence that contains the word \" suffering \" conveys sadness with an NRC Affect Intensity Lexicon intensity of 0.844 , whereas the word \" affection \" indicates joy with an intensity of 0.647 .", "entities": []}
{"text": "The combination of POS+STOP steps come closest to parity , followed by EmoInt , then POS and STOP .", "entities": []}
{"text": "Further , they yield better deltas for S C&E versus S E , implying that the model now requires the claims to reason .", "entities": []}
{"text": "EmoAttention generates our best predictions and deltas , confirming our suspicion that the models rely on emotionally charged style as a predictive feature .", "entities": []}
{"text": "This is further narrowed to emotional intensity : the EmoInt intensity score - based model performs much better than its count - based counterpart EmoLexi .", "entities": []}
{"text": "Thus , evidence containing emotions associated with fake news will be considered more when scoring the claim .", "entities": []}
{"text": "One surprising result is the effectiveness of the simple POS and STOP pre - processing steps .", "entities": []}
{"text": "POS only included nouns , verbs , and adjectives ( i.e. , a superset of STOP ) .", "entities": []}
{"text": "This could explain why it has the best delta between S C&E vs. S E .", "entities": []}
{"text": "Future research could investigate if stopwords , which are often discarded , actually contain signals such as anaphora : a repetitive rhetoric style which can affect NLP analyses ( Liddy , 1990 ) .", "entities": []}
{"text": "As an example , Donald Trump makes heavy use of anaphora in his 2017 inauguration speech : \" Together , we will make America strong again .", "entities": []}
{"text": "We will make America wealthy again .", "entities": []}
{"text": "We will make America proud again .", "entities": []}
{"text": "We will make America safe again .", "entities": []}
{"text": "And , yes , together , we will make america great again . \"", "entities": []}
{"text": "( Trump Inauguration Address , 2017 )", "entities": []}
{"text": "By removing stopwords \" we \" , \" will \" and \" again \" , the model relies less on the text 's rhetoric style and more on the entailment we are seeking .", "entities": []}
{"text": "We propose further study on the effects of STOP and POS , as well as experimenting with different emotional vectors and EmoAttention to make factchecking models more robust .", "entities": []}
{"text": "Disinformation is much more than just a mild inconvenience for society ; it has resulted in needless deaths in the COVID - 19 pandemic , and has fomented violence and political instability all over the globe ( van der Linden et al , 2020 ) .", "entities": []}
{"text": "Our goal in this paper is to discover exploitable weaknesses in current fact - checking models and recommend that such models not be relied upon in their current form .", "entities": []}
{"text": "We point out how the models are dependent on emotional signals in the texts instead of exclusively performing textual entailment , and that additional research needs to be done to ensure they are performing the proper task .", "entities": []}
{"text": "Harm Minimization Our quantifying of the effects of pre - processing on fact - checking models does not cause any harm to real - world users or companies .", "entities": []}
{"text": "Research has demonstrated that adversarial attacks could result in disinformation being labeled as factual news .", "entities": []}
{"text": "Disinformation has become increasingly present in global politics , as some nation - states with significant resources have disseminated propaganda to create political dissent in other countries ( Zhou et al , 2019 ) .", "entities": []}
{"text": "Our research here has demonstrated potential risks : emotional writing could be used as an exploit to circumvent fact - checking models .", "entities": []}
{"text": "Thus , we urge others to further illuminate such vulnerabilities , to minimize potential harms , and to encourage improvements with new models .", "entities": []}
{"text": "Deployment Social media companies often deal with fake news by placing highly visible labels .", "entities": []}
{"text": "However , simply tagging stories as false can make readers more willing to believe and share other false , untagged stories .", "entities": []}
{"text": "This unintended consequence - in which the selective labeling of false news makes other news stories seem more legitimate - has been called the \" implied - truth effect \" ( Pennycook et al , 2019 ) .", "entities": []}
{"text": "Thus , unless these models become so accurate that they catch all fake news presented to them , the entire basis of their use is called into question .", "entities": []}
{"text": "Despite the significant progress in developing models to correctly identify fake news , the real elephant in the room is that many people simply ignore the labels ( Molina et al , 2021 ) .", "entities": []}
{"text": "There is , however , prior work supporting the idea that if people are warned that a headline is false , they will be less likely to believe it ( Ecker et al , 2010 ; Lewandowsky et al , 2012 ) .", "entities": []}
{"text": "Because of this , we believe this research represents a net benefit for humanity .", "entities": []}
{"text": "Warning labels are just one way of dealing with properly identified fake news , and publishers can choose to simply not allow it on their platforms .", "entities": []}
{"text": "Of course , this issue leads to questions of censorship .", "entities": []}
{"text": "In Figure 4 , we report all results for each preprocessing step .", "entities": []}
{"text": "The darkest green colors indicate the best results , while the red indicates the worst .", "entities": []}
{"text": "Multiple pre - processing steps such as ( pos , stop ) were performed in the order written .", "entities": []}
{"text": "Estonian as a Second Language Teacher 's Tools", "entities": []}
{"text": "The paper describes \" Teacher 's Tools \" ( et Opetaja t\u00f6\u00f6riistad ) developed by the Institute of the Estonian Language for teachers and specialists of Estonian as a Second Language .", "entities": []}
{"text": "The toolbox includes four modules : vocabulary , grammar , communicative language activities and text evaluation .", "entities": []}
{"text": "The vocabulary module provides graded word lists for young ( CEFR levels pre - A1 - C1 ) and adult ( CEFR levels A1 - C1 ) learners .", "entities": []}
{"text": "The grammar module provides descriptions of young learners ' grammar competence .", "entities": []}
{"text": "The communicative language activities module gives teachers an overview of the typical situations that learners should be able to cope with .", "entities": []}
{"text": "The text evaluation module marks lemmas in texts according to their CEFR assignment in the vocabulary module .", "entities": []}
{"text": "So far the grammar and the communicative language activities modules have been developed only for young learners ( CEFR levels pre - A1 - B2 ) .", "entities": []}
{"text": "The toolbox is aimed at providing support for the development of Estonian as an L2 courses , educational materials , exercises and tests within a CEFR - based framework .", "entities": []}
{"text": "The project started in 2017 and is ongoing .", "entities": []}
{"text": "1 Introduction \" Teacher 's Tools \" is a compatible toolkit developed for teachers and specialists of Estonian as a Second Language , providing an extensive overview of the development of lexical and grammatical competence in Estonian among L2 learners .", "entities": []}
{"text": "The toolkit is accessible as a sub - page of the language portal S\u00f5naveeb 1 .", "entities": []}
{"text": "The framework of the project is based on the Common European Framework of Reference for Languages : Learning , Teaching , Assessment ( CEFR , 2001 ) , its Companion Volume with New Descriptors ( CEFR / CV , 2018 ) and the Collated Representative Samples of Descriptors of Language Competences Developed for Young 1 https://sonaveeb.ee/teacher - tools Learners for Ages 7 - 10 ( Szabo , 2018a ) and 11 - 15 ( Szabo , 2018b ) .", "entities": []}
{"text": "The toolkit distinguishes between adult ( CEFR levels A1 - C1 ) and young learners ( CEFR levels pre - A1 - C1 ) .", "entities": []}
{"text": "The methodology of the project is adapted from similar projects for other languages ( e.g. Capel , 2010Capel , , 2012O'Keeffe and Geraldine 2017 ; Alfter et al , 2019 ) .", "entities": []}
{"text": "There are quite a few corpora available for the research of the Estonian Language ( the biggest Estonian corpus is the \" Estonian National Corpus 2019 \" ( 1 , 5 billions words ) ) 2 .", "entities": []}
{"text": "However , at the beginning of the project in 2017 there was a clear lack of resources available for the research of Estonian as an L2 .", "entities": []}
{"text": "In order to fill this gap , the Institute of Estonian Language compiled two types of corpora : 1 ) coursebook corpora and 2 ) learner corpora .", "entities": []}
{"text": "As Volodina and Kokkinakis ( 2013 ) point out , the first type of corpora provides information about what and when education professionals found important for students to learn ( passive linguistic competence ) , while learner corpora provide an indication of active linguistic competence .", "entities": []}
{"text": "Both need to be studied since the second is directly influenced by the first .", "entities": []}
{"text": "The coursebook corpora were compiled in several stages and resulted in two groups of corpora : \" Estonian as a Second Language Coursebook Content Corpus 2017 \" 3 ( contains eight coursebooks for adult learners at levels A1 - C1 ; 500 000 tokens ) and \" Estonian as a Second Language School Coursebook Content Corpus 2021 \" 4 ( contains 27 school coursebooks for grades 1 - 12 ; 1 300 000 tokens ) .", "entities": []}
{"text": "The learner corpus was compiled in 2019 - 2021 and contains 6700 Estonian as an L2 young learner 's texts .", "entities": []}
{"text": "These are texts written mostly as state standard - determining tests ( grades 3 and 6 ) , basic school final examinations The data is available for analysis through the interface of the Estonian Learner Language Corpus EMMA 5 .", "entities": []}
{"text": "The texts were not corrected or altered .", "entities": []}
{"text": "Both types of corpora were used for the development of \" Teacher 's Tools \" .", "entities": []}
{"text": "The coursebook corpora were used primarily to create passive vocabulary word lists and for the analysis of explicit and implicit grammar teaching .", "entities": []}
{"text": "The learner corpus was used for the creation and analysis of active vocabulary word lists and the dynamics of grammar acquisition .", "entities": []}
{"text": "3 Modules of \" Teacher 's Tools \" \" Teacher 's Tools \" consists of four modules : vocabulary , grammar , communicative language activities and text evaluation .", "entities": []}
{"text": "The vocabulary module includes young and adult learners ' word lists compiled on the basis of coursebook and learner corpus word lists in comparison with frequency distribution in the \" Estonian National Corpus 2017 \" 6 .", "entities": []}
{"text": "Currently , the vocabulary list for adult learners includes about 12 500 words for levels A1 - C1 .", "entities": []}
{"text": "The young learners ' word list includes approx .", "entities": []}
{"text": "9000 words for levels pre - A1 - B2 .", "entities": []}
{"text": "All words are presented as lemmas .", "entities": []}
{"text": "Users can generate word lists based on learner type ( young or adult ) and POS .", "entities": []}
{"text": "In addition , 24 topics ( clothes , furniture , birds etc . ) are compiled .", "entities": []}
{"text": "The search results can be sorted alphabetically , by level or by POS ( see Figure 1 ) .", "entities": []}
{"text": "The grammar module is the first attempt to create a systematic overview of Estonian L2 learner 's grammar competence development .", "entities": []}
{"text": "The general methodology was adopted from the English Grammar Profile 7 project ( O'Keeffe and Geraldine , 2017 ) .", "entities": []}
{"text": "Currently , the grammar module provides descriptions of grammar competence on the morphology , derivation , phrase and sentence levels , from level pre - A1 up to level B2 ( see Figure 2 ) .", "entities": []}
{"text": "The same grammatical category ( e.g. the use of the genetive case for nouns or the use of imperative mood for verbs ) may be described at all levels , but the functions and usage ( how often the learner makes mistakes and what kinds of mistakes ) are level - specific .", "entities": []}
{"text": "Search options offer users choices of four main categories : morphology , derivation , phrase formation and sentence formation .", "entities": []}
{"text": "The main categories contain 19 subcategories .", "entities": []}
{"text": "For example , subcategories of morphology are verb , noun , adjective , numeral , pronoun , adverb .", "entities": []}
{"text": "Every subcategory has grammatical markers ( e.g. number and case for noun ) .", "entities": []}
{"text": "The grammatical markers have values , for example number has values singular and plural .", "entities": []}
{"text": "All descriptions are equipped with example sentences compiled either by experts or taken from coursebook and learner 's corpora .", "entities": []}
{"text": "The communicative language activities module is based on CEFR / CV descriptor scales , and examples are taken from Szabo ( 2018a , b ) , which include reception processes and strategies of written and spoken texts ( listening and reading ) , written and oral text production and production strategies , and written and spoken interaction and interaction strategies .", "entities": []}
{"text": "It covers young learners at levels A1 - B2 .", "entities": []}
{"text": "The architecture of the module is similar to that of the grammar module , containing main categories ( reception , production , interaction and mediation ) and subcategories , which are followed by detailed descriptions , including limitations : when and how well the language user can manage certain communicative activities ( see Figure 3 ) .", "entities": []}
{"text": "For example , by choosing the main category reception and the subcategory overall listening comprehension , a teacher can see that at the A1 level the learner should be able to understand a simple description with short sentences , for example about the weather or how big a certain object is .", "entities": []}
{"text": "The text evaluation tool helps to assess Estonian texts for their degrees of complexity according to the CERF ( see Figure 4 ) .", "entities": []}
{"text": "Similar tools have also been developed for many other languages ( see e.g. Alfter , 2021 ) .", "entities": []}
{"text": "The tool needs to be developed further .", "entities": []}
{"text": "First , there is a need to implement methods for the improvement of the analysis of homonyms ( for example tamm can mean either an oak or a dam , which is assigned different levels in word lists ) and multiword expressions .", "entities": []}
{"text": "So far , the analysis is based only on single word lists , which is not sufficient .", "entities": []}
{"text": "4 Summary \" Teacher 's Tools \" is the first attempt to provide a systematic overview of the development of lexical and grammatical competence in Estonian as a Second Language .", "entities": []}
{"text": "The project is a work in progress and further development of the toolkit is foreseen .", "entities": []}
{"text": "We plan to add descriptions of the development of grammar competence and communicative language activities for adult learners .", "entities": []}
{"text": "The enrichment of the text evaluation module with the possibility of measuring grammatical difficulty and readability ( by for example adding Lix - index value ) is under development .", "entities": []}
{"text": "The use of NLP for the development of such computerassisted tools has enormous potential for enhancing the teaching and learning of Estonian as an L2 .", "entities": []}
{"text": "Supporting Spoken Assistant Systems with a Graphical User Interface that Signals Incremental Understanding and Prediction State", "entities": []}
{"text": "We explore the use of a graphical output modality for signalling incremental understanding and prediction state of the dialogue system .", "entities": []}
{"text": "We evaluate our system with real users and report that they found the system intuitive and easy to use , and that incremental and adaptive settings enable users to accomplish more tasks .", "entities": []}
{"text": "Current virtual personal assistants ( PAs ) require users to either formulate complex intents in one utterance ( e.g. , \" call Peter Miller on his mobile phone \" ) or go through tedious sub - dialogues ( e.g. , \" phone call \" - who would you like to call ?", "entities": []}
{"text": "- \" Peter Miller \" - I have a mobile number and a work number .", "entities": []}
{"text": "Which one do you want ? ) .", "entities": []}
{"text": "This is not how one would interact with a human assistant , where the request would be naturally structured into smaller chunks that individually get acknowledged ( e.g. , \" Can you make a connection for me ? \"", "entities": []}
{"text": "- sure - \" with Peter Miller \" - uh huh - \" on his mobile \" - dialling now ) .", "entities": []}
{"text": "Current PAs signal ongoing understanding by displaying the state of the recognised speech ( ASR ) to the user , but not their semantic interpretation of it .", "entities": []}
{"text": "Another type of assistant system forgoes enquiring user intent altogether and infers likely intents from context .", "entities": []}
{"text": "GoogleNow , for example , might present traffic information to a user picking up their mobile phone at their typical commute time .", "entities": []}
{"text": "These systems display their \" understanding \" state , but do not allow any type of interaction with it apart from dismissing the provided information .", "entities": []}
{"text": "In this work , we explore adding a graphical user interface ( GUI ) modality that makes it possible to see these interaction styles as extremes on a continuum , and to realise positions between these extremes and present a mixed graphical / voice enabled PA that can provide feedback of understanding to the user incrementally as the user 's utterance unfolds - allowing users to make requests in instalments instead of fully thought - out requests .", "entities": []}
{"text": "It does this by signalling ongoing understanding in an intuitive tree - like GUI that can be displayed on a mobile device .", "entities": []}
{"text": "We evaluate our system by directing users to perform tasks using it under nonincremental ( i.e. , ASR endpointing ) and incremental conditions and then compare the two conditions .", "entities": []}
{"text": "We further compare a non - adaptive with an adaptive ( i.e. , infers likely events ) version of our system .", "entities": []}
{"text": "We report that the users found the interface intuitive and easy to use , and that users were able to perform tasks more efficiently with incremental as well as adaptive variants of the system .", "entities": []}
{"text": "This work builds upon several threads of previous research :", "entities": []}
{"text": "Chai et al ( 2014 ) addressed misalignments in understanding ( i.e. , common ground ( Clark and Schaefer , 1989 ) ) between robots and humans by informing the human of the internal system state via speech .", "entities": []}
{"text": "We take this idea and ap - ply it to a PA by displaying the internal state of the system to the user via a GUI ( explained in Section 3.5 ) , allowing the user to determine if system understanding has taken place - a way of providing feedback and backchannels to the user .", "entities": []}
{"text": "Dethlefs et al ( 2016 ) provide a good review of work that show how backchannels facilitate grounding , feedback , and clarifications in human spoken dialogue , and apply an information density approach to determine when to backchannel using speech .", "entities": []}
{"text": "Because we do n't backchannel using speech here , there is no potential overlap between the user and the system ; rather , our system can display backchannels and ask clarifications without frustrating the user through inadvertent overlaps .", "entities": []}
{"text": "Though different in many ways , our work is similar in some regards to Larsson et al ( 2011 ) , which displays information to the user and allows the user to navigate the display itself ( e.g. , by saying up or down in a menu list ) - functionality that we intend to apply to our GUI in future work .", "entities": []}
{"text": "Our work is also comparable to SDS toolkits such as IrisTK ( Skantze and Moubayed , 2012 ) and", "entities": []}
{"text": "Open - Dial ( Lison , 2015 ) which enable SDS designers to visualise the internal state of their systems , though not for end user interpretability .", "entities": []}
{"text": "Some of the work here is inspired by the Microsoft Language Understanding Intelligent Service ( LUIS ) project ( Williams et al , 2015 ) .", "entities": []}
{"text": "While our system by no means achieves the scale that LUIS does , we offer here an additional contribution of an open source LUIS - like system ( with the important addition of the graphical interface ) that is authorable ( using JSON files ; we leave authoring using a web interface like that of LUIS to future work ) , extensible ( affordances can be easily added ) , incremental ( in that respect going beyond LUIS ) , trainable ( i.e. , can learn from examples , but can still function well without examples ) , and can learn through interacting ( here we apply a user model that learns during interaction ) .", "entities": []}
{"text": "The overall system is represented in Figure 1 .", "entities": []}
{"text": "For the remainder of this section , each module is explained in turn .", "entities": []}
{"text": "As each module processes input incrementally ( i.e. , word for word ) , we first explain our framework for incremental processing .", "entities": []}
{"text": "DM w 1 ... w n w 1 ... w n d 1 d m s 1 : v 1 s m : v m ... s 1 : v 1 s m : v m ... ...", "entities": []}
{"text": "Figure 1 : Overview of system made up of ASR which takes in a speech signal and produces transcribed words , NLU , which takes words and produces a slots in a frame , DM which takes slots and produces a decision for each , and the GUI which displays the state of the system .", "entities": []}
{"text": "An aspect of our SDS that sets it apart from others is the requirement that it process incrementally .", "entities": []}
{"text": "One potential concern with incremental processing is regarding informativeness : why act early when waiting might provide additional information , resulting in better - informed decisions ?", "entities": []}
{"text": "The trade off is naturalness as perceived by the user who is interacting with the SDS .", "entities": []}
{"text": "Indeed , it has been shown that human users perceive incremental systems as being more natural than traditional , turn - based systems ( Aist et al , 2006 ; Skantze and Schlangen , 2009 ; Skantze and Hjalmarsson , 1991 ; Asri et al , 2014 ) , offer a more human - like experience ( Edlund et al , 2008 ) and are more satisfying to interact with than non - incremental systems ( Aist et al , 2007 ) .", "entities": []}
{"text": "Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process ( Tanenhaus et al , 1995 ; Spivey et al , 2002 ) .", "entities": []}
{"text": "The trade - off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired .", "entities": []}
{"text": "Such mechanisms are offered by the incremental unit ( IU ) framework for SDS ( Schlangen and Skantze , 2011 ) , which we apply here .", "entities": []}
{"text": "Following Kennington et al ( 2014 ) , the IU framework consists of a network of processing modules .", "entities": []}
{"text": "A typical module takes input , performs some kind of processing on that data , and produces output .", "entities": []}
{"text": "The data are packaged as the payload of incremental units ( IUs ) which are passed between modules .", "entities": []}
{"text": "Thus IUs can be added , but can be later revoked and replaced in light of new information .", "entities": []}
{"text": "The IU framework can take advantage of up - to - date information , but have the potential to function in such a way that users perceive as more natural .", "entities": []}
{"text": "The modules explained in the remainder of this section are implemented as IU - modules and process incrementally .", "entities": []}
{"text": "Each will now be explained .", "entities": []}
{"text": "The module that takes speech input from the user in our SDS is the ASR component .", "entities": []}
{"text": "Incremental ASR must transcribe uttered speech into words which must be forthcoming from the ASR as early as possible ( i.e. , the ASR must not wait for endpointing to produce output ) .", "entities": []}
{"text": "Each module that follows must also process incrementally , acting in lock - step upon input as it is received .", "entities": []}
{"text": "Incremental ASR is not new ( Baumann et al , 2009 ) and many of the current freely - accessible ASR systems can produce output ( semi - ) incrementally .", "entities": []}
{"text": "We approach the task of NLU as a slot - filling task ( a very common approach ; see Tur et al ( 2012 ) ) where an intent is complete when all slots of a frame are filled .", "entities": []}
{"text": "The main driver of the NLU in our SDS is the SIUM model of NLU introduced in Kennington et al ( 2013 ) .", "entities": []}
{"text": "SIUM has been used in several systems which have reported substantial results in various domains , languages , and tasks ( Han et al , 2015 ; Kennington and Schlangen , 2017 )", "entities": []}
{"text": "Though originally a model of reference resolution , it was always intended to be used for general NLU , which we do here .", "entities": []}
{"text": "The model is formalised as follows : P ( I | U )", "entities": []}
{"text": "= 1 P ( U ) P ( I ) r R P ( U | R = r ) P ( R = r | I ) ( 1 )", "entities": []}
{"text": "That is , P ( I | U ) is the probability of the intent I ( i.e. , a frame slot ) behind the speaker 's ( ongoing ) utterance U .", "entities": []}
{"text": "This is recovered using the mediating variable R , a set of properties which map between aspects of U and aspects of I. We opt for abstract properties here ( e.g. , the frame for restaurant might be filled by a certain type of cuisine intent such as italian which has properties like pasta , mediterranean , vegetarian , etc . ) .", "entities": []}
{"text": "Properties are pre - defined by a system designer and can match words that might be uttered to describe the intent in question .", "entities": []}
{"text": "For P ( R | I ) , probability is distributed uniformly over all properties that a given intent is specified to have .", "entities": []}
{"text": "( If other information is available , more informative priors could be used as well . )", "entities": []}
{"text": "The mapping between properties and aspects of U can be learned from data .", "entities": []}
{"text": "During application , R is marginalised over , resulting in a distribution over possible intents .", "entities": []}
{"text": "1 This occurs at each word increment , where the distribution from the previous increment is combined via P ( I ) , keeping track of the distribution over time .", "entities": []}
{"text": "We further apply a simple rule to add in apriori knowledge : if some r R and w U are such that r .", "entities": []}
{"text": "= w ( where .", "entities": []}
{"text": "= is string equality ; e.g. , an intent has the property of pasta and the word pasta is uttered ) , then we set C", "entities": []}
{"text": "( U = w | R = r ) = 1 .", "entities": []}
{"text": "To allow for possible ASR confusions , we also apply C", "entities": []}
{"text": "( U = w | R = r ) = 1 \u2212 ld ( w , r ) /max", "entities": []}
{"text": "( len ( w ) , len ( r ) ) , where ld is the Levenshtein distance ( but we only apply this if the calculated value is above a threshold of 0.6 ; i.e. , the two strings are mostly similar ) .", "entities": []}
{"text": "This results in a distribution C , which we renormalise and blend with learned distribution to yield P ( U | R ) .", "entities": []}
{"text": "We apply an instantiation of SIUM for each slot .", "entities": []}
{"text": "The candidate slots which are processed depends on the state of the dialogue ; only slots represented by visible nodes are considered , thereby reducing the possible frames that could be predicted .", "entities": []}
{"text": "At each word increment , the updated slots ( and their corresponding ) distributions are given to the DM , which will now be explained .", "entities": []}
{"text": "The DM plays a crucial role in our SDS : as well as determining how to act , the DM is called upon to decide when to act , effectively giving the DM the control over timing of actions rather than relying on ASR endpointing - further separating our SDS from other systems .", "entities": []}
{"text": "The DM policy is based on a confidence score derived from the NLU ( in this case , we used the distribution 's argmax value ) using thresholds for the actions ( see below ) , set by hand ( i.e. , trial and error ) .", "entities": []}
{"text": "At each word and resulting distribution from NLU , the DM needs to choose one of the following : wait - wait for more information ( i.e. , for the next word ) select - as the NLU is confident enough , fill the slot can with the argmax from NLU request - signal a ( yes / no ) clarification request on the current slot and the proposed filler confirm - act on the confirmation of the user ; in effect , select the proposed slot value Though the thresholds are statically set , we applied OpenDial ( Lison , 2015 ) as an IU - module to perform the task of the DM with the future goal that these values could be adjusted through reinforcement learning ( which OpenDial could provide ) .", "entities": []}
{"text": "The DM processes and makes a decision for each slot , with the assumption that only one slot out of all that are processed will result in an non - wait action ( though this is not enforced ) .", "entities": []}
{"text": "The goal of the GUI is to intuitively inform the user about the internal state of the ongoing understanding .", "entities": []}
{"text": "One motivation for this is that the user can determine if the system understood the user 's intent before providing the user with a response ( e.g. , a list of restaurants of a certain type ) ; i.e. , if any misunderstanding takes place , it happens before the system commits to an action and is potentially more easily repaired .", "entities": []}
{"text": "The display is a rightbranching tree , where the branches directly off the root node display the affordances of the system ( i.e. , what domains of things it can understand and do something about ) .", "entities": []}
{"text": "When the first tree is displayed , it represents a state of the NLU where none of the slots are filled , as in Figure 3 .", "entities": []}
{"text": "When a user verbally selects a domain to ask about , the tree is adjusted to make that domain the only one displayed and the slots that are required for that domain are shown as branches .", "entities": []}
{"text": "The user can then fill those slots ( i.e. , branches ) by uttering the displayed name , or , alternatively , by uttering the item to fill the slot directly .", "entities": []}
{"text": "For example , at a minimum , the user could utter the name of the domain then an item for each slot ( e.g. , food Thai downtown ) or the speech could be more natural ( e.g. , I 'm quite hungry , I am looking for some Thai food maybe in the downtown area ) .", "entities": []}
{"text": "Crucially , the user can also hesitate within and between chunks , as advancement is not triggered by silence thresholding , but rather semantically .", "entities": []}
{"text": "When something is uttered that falls into the request state of the DM as explained above , the display expands the subtree under question and marks the item with a question mark ( see Figure 4 ) .", "entities": []}
{"text": "At this point , the user can utter any kind of confirmation .", "entities": []}
{"text": "A positive confirmation fills the slot with the item in question .", "entities": []}
{"text": "A negative confirmation retracts the question , but leaves the branch expanded .", "entities": []}
{"text": "The expanded branches are displayed according to their rank as given by the NLU 's probability distribution .", "entities": []}
{"text": "Though a branch in the display can theoretically display an unlimited number of children , we opted to only show 7 children ; if a branch had more , the final child displayed as an ellipsis .", "entities": []}
{"text": "A completed branch is collapsed , visually marking its corresponding slot as filled .", "entities": []}
{"text": "At any time , a user can backtrack by saying no ( or equivalent ) or start the entire interaction over from the beginning with a keyword , e.g. , restart .", "entities": []}
{"text": "To aid the user 's attention , the node under question is marked in red , where completed slots are represented by outlined nodes , and filled nodes represent candidates for the current slot in question ( see examples of all three in Figure 4 ) .", "entities": []}
{"text": "For cases where the system is in the wait state for several words ( during which there is no change in the tree ) , the system signals activity at each word by causing the red node in question to temporarily change to white , then back to red ( i.e. , appearing as a blinking node to the user ) .", "entities": []}
{"text": "Figure 5 shows a filled frame , represented as tree with one branch for each filled slot .", "entities": []}
{"text": "Such an interface clearly shows the internal state of the SDS and whether or not it has understood the request so far .", "entities": []}
{"text": "It is designed to aid the user 's attention to the slot in question , and clearly indicates the affordances that the system has .", "entities": []}
{"text": "The interface is currently a read - only display that is purely speech - driven , but it could be augmented with additional functionalities , such as tapping a node for expansion or typing input that the system might not yet display .", "entities": []}
{"text": "Adaptive Branching The GUI as explained affords an additional straight - forward extension : in order to move our system towards adaptivity on the above - mentioned continuum , the GUI can be used to signal what the system thinks the user might say next .", "entities": []}
{"text": "This is done by expanding a branch and displaying a confirmation on that branch , signalling that the system predicts that the user will choose that particular branch .", "entities": []}
{"text": "Alternatively , if the system is confident that a user will fill a slot with a particular value , that particular slot can be filled without confirmation .", "entities": []}
{"text": "This is displayed as a collapsed tree branch .", "entities": []}
{"text": "A system that perfectly predicts a user 's intent would fill an entire tree ( i.e. , all slots ) only requiring the user to confirm once .", "entities": []}
{"text": "A more careful system would confirm at each step ( such an interaction would only require the user to utter confirmations and nothing else ) .", "entities": []}
{"text": "We applied this adaptive variant of the tree in one of our experiments explained below .", "entities": []}
{"text": "In this section , we describe two experiments where we evaluated our system .", "entities": []}
{"text": "It is our primary goal to show that our GUI is useful and signals understanding to the user .", "entities": []}
{"text": "We also wish to show that incremental presentation of such a GUI is more effective than an endpointed system .", "entities": []}
{"text": "We further want to show that an adaptive system is more effective than a non - adaptive system ( though both would process incrementally ) .", "entities": []}
{"text": "In order to best evaluate our system , we recruited participants to interact with our system in varied settings to compare endpointed ( i.e. , non - incremental ) and nonadaptive as well as adaptive versions .", "entities": []}
{"text": "We describe how the data were collected from the participants , then explain each experiment and give results .", "entities": []}
{"text": "The participants were seated at a desk and given written instructions indicating that they were to use the system to perform as many tasks as possible in the allotted time .", "entities": []}
{"text": "Figure 6 shows some example tasks as they would be displayed ( one at a time ) to the user .", "entities": []}
{"text": "A screen , tablet , and keyboard were on the desk in front of the user ( see Figure 7 ) .", "entities": []}
{"text": "2", "entities": []}
{"text": "The user was instructed to convey the task presented on the screen to the system such that the GUI on the tablet would have a completed tree ( e.g. , as in Figure 5 ) .", "entities": []}
{"text": "When the participant was satisfied that the system understood her intent , she was to press space bar on the keyboard which triggered a new task to be displayed on the screen and reset the tree to its start state on the tablet ( as in Figure 3 ) .", "entities": []}
{"text": "The possible task domains were call , which had a single slot for name to be filled ( i.e. , one out of the 22 most common German given names ) ; message which had a slot for name and a slot for the message ( which , when invoked , would simply fill in directly from the ASR until 1 second of silence was detected ) ; eat which had slots for type ( in this case , 6 possible types ) and location ( in this case , 6 locations based around the city of Bielefeld ) ; route which had slots for source city and the destination city ( which shared the same list of the top 100 most populous German cities ) ; and reminder which had a slot for message .", "entities": []}
{"text": "For each task , the domain was first randomly chosen from the 5 possible domains , and then each slot value to be filled was randomly chosen ( the message slot for the name and message domains was randomly selected from a list of 6 possible \" messages \" , each with 2 - 3 words ; e.g. , feed the cat , visit grandma , etc . ) .", "entities": []}
{"text": "The system kept track of which tasks were already presented to the participant .", "entities": []}
{"text": "At any time after the first task , the system could choose a task that was previously presented and present it again to the participant ( with a 50 % chance )", "entities": []}
{"text": "so the user would often see tasks that she had seen before ( with the assumption that humans who use PAs often do perform similar , if not the same , tasks more than once ) .", "entities": []}
{"text": "The participant was told that she would interact with the system in three different phases , each for 4 minutes , and to accomplish as many tasks as possible in that time allotment .", "entities": []}
{"text": "The participant was not told what the different phases were .", "entities": []}
{"text": "The experiments described in Sections 4.2 and screen tablet keyboard participant 4.3 respectively describe and report a comparison first between the Phase 1 and 2 ( denoted as the endpointed and incremental variants of the system ) in order to establish whether or not the incremental variant produced better results than the endpointed variant .", "entities": []}
{"text": "We also report a comparison between Phase 2 and 3 ( incremental and incremental - adaptive phases ) .", "entities": []}
{"text": "Phase 1 and Phase 3 are not directly comparable to each other as Phase 3 is really a variant of Phase 2 .", "entities": []}
{"text": "Because of this , we fixed the order of the phase presentation for all participants .", "entities": []}
{"text": "Each of these phases are described below .", "entities": []}
{"text": "Before the participant began Phase 1 , they were able to try it out for up to 4 minutes ( in Phase 1 settings ) and ask for help from the experimenter , allowing them to get used to the Phase 1 interface before the actual experiment began .", "entities": []}
{"text": "After this trial phase , the experiment began with Phase 1 . Phase 1 : Non - incremental In this phase , the system did not appear to work incrementally ; i.e. , the system displayed tree updates after ASR endpointing ( of 1.2 seconds - a reasonable amount of time to expect a response from a commercial spoken PA ) .", "entities": []}
{"text": "The system displayed the ongoing ASR on the tablet as it was recognised ( as is often done in commercial PAs ) .", "entities": []}
{"text": "At the end of Phase 1 , a pop up window notified the user that the phase was complete .", "entities": []}
{"text": "They then moved onto Phase 2 . Phase 2 : Incremental", "entities": []}
{"text": "In this phase , the system displayed the tree information incrementally without endpointing .", "entities": []}
{"text": "The ASR was no longer displayed ; only the tree provided feedback in understanding , as explained in Section 3.5 .", "entities": []}
{"text": "After Phase 2 , a 10 - question questionnaire was displayed on the screen for the participant to fill out comparing Phase 1 and Phase 2 .", "entities": []}
{"text": "For each question , they had the choice of Phase 1 , Phase 2 , Both , and Neither .", "entities": []}
{"text": "( See Appendix for full list of questions . )", "entities": []}
{"text": "After completing the questionnaire , they moved onto Phase 3 . Phase 3 : Incremental - adaptive In this phase , the incremental system was again presented to the participant with an added user model that \" learned \" about the user .", "entities": []}
{"text": "If the user saw a task more than once , the user model would predict that , if the user chose that task domain again ( e.g. , route ) then the system would automatically ask a clarification using the previously filled values ( except for the message slot , which the user always had to fill ) .", "entities": []}
{"text": "If the user saw a task more than 3 times , the system skipped asking for clarifications and filled in the domain slots completely , requiring the user only to press the space bar to confirm it was the correct one ( i.e. , to complete the task ) .", "entities": []}
{"text": "An example progression might be as follows : a participant is presented with the task route from Bielefeld to Berlin , then the user would attempt to get the system to fill in the tree ( i.e. , slots ) with those values .", "entities": []}
{"text": "After some interaction in other domains , the user sees the same task again , and now after indicating the intent type route , the user must only say \" yes \" for each slot to confirm the system 's prediction .", "entities": []}
{"text": "Later , if the task is presented a third time , when entering that domain ( i.e , route ) , the two slots would already be filled .", "entities": []}
{"text": "If later a different route task was presented , e.g. , route from Bielefeld to Hamburg , the system would already have the two slots filled , but the user could backtrack by saying \" no , to Hamburg \" which would trigger the system to fill the appropriate slot with the corrected value .", "entities": []}
{"text": "Later interactions within the route domain would ask for a clarification on the destination slot since it has had several possible values given by the participant , but continue to fill the from slot with Bielefeld .", "entities": []}
{"text": "After Phase 3 , the participants were presented with another questionnaire on the screen to fill out with the same questions ( plus two additional questions ) , this time comparing Phase 2 and Phase 3 .", "entities": []}
{"text": "For each item , they had the choice of Phase 2 , Phase 3 , Both , and Neither .", "entities": []}
{"text": "At the end of the three phases and questionnaires , the participants were given a final questionnaire to fill out by hand on their general impressions of the systems .", "entities": []}
{"text": "We recruited 14 participants for the evaluation .", "entities": []}
{"text": "Due to some technical issues , one of the participants did not log interactions .", "entities": []}
{"text": "We collected data from 13 participants , post - Phase 2 questionnaires from 12 participants , post - Phase 3 questionnaires from all 14 participants , and general questionnaires from all 14 participants .", "entities": []}
{"text": "In the experiments that follow , we report objective and subjective measures to determine the settings that produced superior results .", "entities": []}
{"text": "Metrics We report the subjective results of the participant questionnaires .", "entities": []}
{"text": "We only report those items that were statistically significant ( see Appendix for a full list of the questions ) .", "entities": []}
{"text": "We further report objective measures for each system variant : total number of completed tasks , fully correct frames , average frame f - score , and average time elapsed ( averages are taken over all participants for each variant ; we only used the 10 participants who fully interacted with all three phases ) .", "entities": []}
{"text": "Discussion is left to the end of this section .", "entities": []}
{"text": "In this section we report the results of the evaluation between the endpointed ( i.e. , nonincremental ; Phase 1 ) variant vs the incremental ( Phase 2 ) variant of our system .", "entities": []}
{"text": "We applied a multinomial test of significance to the results , treating all four possible answers as equally likely ( with Bonferroni correction of 10 ) .", "entities": []}
{"text": "The item The interface was useful and easy to understand with the answer of Both was significant ( \u03c7 2 ( 4 , N = 12 )", "entities": []}
{"text": "= 9.0 , p < .005 ) , as was The assistant was easy and intuitive to use also with the answer Both ( \u03c7 2 ( 4 , N = 12 )", "entities": []}
{"text": "= 9.0 , p < .005 ) .", "entities": []}
{"text": "The item I always understood what the system wanted from me was also answered Both significantly more times than other answers ( \u03c7 2 ( 4 , N = 14 ) = 9.0 , p < .005 ) , similarly for It was sometimes unclear to me if the assistant understood me with the answer of Both ( \u03c7 2 ( 4 , N = 12 )", "entities": []}
{"text": "= 10.0 , p < .005 ) .", "entities": []}
{"text": "These responses tell us that though the participants did not report preference for either system variant , they reported a general positive impression of the GUI ( in both variants ) .", "entities": []}
{"text": "This is a nice result ; the GUI could be used in either system with benefit to the users .", "entities": []}
{"text": "The endpointed ( Phase 1 ) and incremental ( Phase 2 ) columns in Table 1 show the results of the objective evaluation .", "entities": []}
{"text": "incremental variant , the total number of tasks for the incremental variant was higher .", "entities": []}
{"text": "Manual inspection of logs indicate that participants took advantage of the system 's flexibility of understanding instalments ( i.e. , filling frames incrementally ) .", "entities": []}
{"text": "This is evidenced in that participants often uttered words understood by the system as being negative ( e.g. , nein / no ) , either as a result of an explicit confirmation request by the system ( e.g. , Thai ? )", "entities": []}
{"text": "or after a slot was incorrectly filled ( something very easily determined through the GUI ) .", "entities": []}
{"text": "This is a desired outcome of using our system ; participants were able to repair local areas of misunderstanding as they took place instead of needing to correct an entire intent ( i.e. , frame ) .", "entities": []}
{"text": "However , we can not fully empirically measure these tendencies given our data .", "entities": []}
{"text": "Incremental - Adaptive In this section we report results for the evaluation between the incremental ( Phase 2 ) and incremental - adaptive ( henceforth just adaptive ; Phase 3 ) systems .", "entities": []}
{"text": "We applied the same significance test as Experiment 1 ( with Bonferroni correction of 12 ) .", "entities": []}
{"text": "The item The interface was useful and easy to understand was answered with Both significantly ( \u03c7 2 ( 4 , N = 14 )", "entities": []}
{"text": "= 10.0 , p < .0042 ) ,", "entities": []}
{"text": "The item I had the feeling that the assistant attempted to learn about me was answered with Neither ( \u03c7 2 ( 4 , N = 14 ) = 8.0 , p < .0042 ) , though Phase 3 was also marked ( 6 times ) .", "entities": []}
{"text": "All other items were not significant .", "entities": []}
{"text": "Here again we see that there is a general positive impression of the GUI under all conditions .", "entities": []}
{"text": "If anyone noticed that a system variant was attempting to learn a user model at all , they noticed that it was in Phase 3 , as expected .", "entities": []}
{"text": "The incremental ( Phase 2 ) and adaptive ( Phase 3 ) columns in Table 1 show the results for the objective evaluation for this experiment .", "entities": []}
{"text": "There is a clear difference between the two variants , with the adaptive showing more completed tasks , more fully correct frames , and a higher average fscore ( all three likely due to the fact that frames were potentially pre - filled ) .", "entities": []}
{"text": "While the responses do n't express any preference for a particular system variant , the overall impression of the GUI was positive .", "entities": []}
{"text": "The objective measures show that there are gains to be made when the system signals understanding at a more finegrained interval than at the utterance level , due to the higher number of completed tasks and locallymade repairs .", "entities": []}
{"text": "There are further gains to be made when the system applies simple user modelling ( i.e. , adaptivity ) by attempting to predict what the user might want to do in a chosen domain , decreasing the possibility of user error and allowing the system to accurately and quickly complete more tasks .", "entities": []}
{"text": "Participants also did n't just get used to the system over time , as the average time per episode was fairly similar in all three phases .", "entities": []}
{"text": "The open - ended questionnaire sheds additional light .", "entities": []}
{"text": "Most of the suggestions for improvement related to ASR misrecognition and speed ( i.e. , not about the system itself ) .", "entities": []}
{"text": "Two participants suggested an ability to add \" free input \" or select alternatives from the tree .", "entities": []}
{"text": "Two participants suggested that the system be more responsive ( i.e. , in wait states ) , and give more feedback ( i.e. , backchannels ) more often .", "entities": []}
{"text": "For those participants that expressed preference to the non - incremental system ( Phase 1 ) , none of them had used a speech - based PA before , whereas those that expressed preference to the incremental versions ( Phases 2 and 3 ) use them regularly .", "entities": []}
{"text": "We conjecture that people without SDS experience equate understanding with ASR , whereas those that are more familiar with PAs know that perfect ASR does n't translate to perfect understanding - hence the need for a GUI .", "entities": []}
{"text": "A potential remedy would be to display ASR with the tree , signalling understanding despite ASR errors .", "entities": []}
{"text": "Given the results and analysis , we conclude that an intuitive presentation that signals a system 's ongoing understanding benefits end users who perform simple tasks which might be performed by a PA .", "entities": []}
{"text": "The GUI that we provided , using a right - branching tree , worked well ; indeed , the participants who used it found it intuitive and easy to understand .", "entities": []}
{"text": "There are gains to be made when the system signals understanding at finer - grained levels than just at the end of a pre - formulated utterance .", "entities": []}
{"text": "There are further gains to be made when a PA attempts to learn ( even a rudimentary ) user model to predict what the user might want to do next .", "entities": []}
{"text": "For future work , we intend to provide simple authoring tools for the system to make building simple PAs using our GUI easy .", "entities": []}
{"text": "We want to improve the NLU and scale to larger domains .", "entities": []}
{"text": "3 We also plan on implementing this as a standalone application that could be run on a mobile device , which could actually perform the tasks .", "entities": []}
{"text": "It would further be beneficial to compare the GUI with a system that responds with speech ( i.e. , without a GUI ) .", "entities": []}
{"text": "Lastly , we will investigate using touch as an additional input modality to select between possible alternatives that are offered by the system .", "entities": []}
{"text": "It was sometimes unclear to me if the assistant understood me .", "entities": []}
{"text": "The assistant responded while I spoke .", "entities": []}
{"text": "The assistant sometimes did things that I did not expect .", "entities": []}
{"text": "When the assistant made mistakes , it was easy for me to correct them .", "entities": []}
{"text": "In addition to the above 10 questions , the following were also asked on the questionnaire following Phase 3 : I had the feeling that the assistant attempted to learn about me .", "entities": []}
{"text": "I had the feeling that the assistant made incorrect guesses .", "entities": []}
{"text": "Yes / No What was your general impression of our personal assistants ?", "entities": []}
{"text": "Would you use one of these assistants on a smart phone or tablet if it were available ?", "entities": []}
{"text": "If yes , which one ? Do you have suggestions that you think would help us improve our assistants ?", "entities": []}
{"text": "If you have used other speech - based interfaces before , do you prefer this interface ?", "entities": []}
{"text": "Acknowledgements Thanks to the anonymous reviewers who provided useful comments and suggestions .", "entities": []}
{"text": "Thanks also to Julian Hough for helping with experiments .", "entities": []}
{"text": "We acknowledge support by the Cluster of Excellence \" Cognitive Interaction Technology \" ( CITEC ; EXC 277 ) at Bielefeld University , which is funded by the German Research Foundation ( DFG ) , and the BMBF Kogni - Home project .", "entities": []}
{"text": "The following questions were asked on both questionnaires following Phase 2 and Phase 3 ( comparing the two most latest used system versions ; as translated into English ) :", "entities": []}
{"text": "The interface was useful and easy to understand .", "entities": []}
{"text": "The assistant was easy and intuitive to use .", "entities": []}
{"text": "The assistant understood what I wanted to say .", "entities": []}
{"text": "I always understood what the system wanted from me .", "entities": []}
{"text": "The assistant made many mistakes .", "entities": []}
{"text": "The assistant did not respond while I spoke .", "entities": []}
{"text": "However , the data for children 's semantic knowledge across development is scarce .", "entities": []}
{"text": "We apply our methods to bag - of - words models , and find that ( 1 ) children acquire words with fewer semantic neighbours earlier , and ( 2 ) young learners only attend to very local context .", "entities": []}
{"text": "These findings provide converging evidence for validity of our methods in understanding the prerequisite features for a distributional model of word learning .", "entities": []}
{"text": "These embeddings are often evaluated either extrinsically , on how well they boost performance on a certain task , or intrinsically , by comparing representations against behavioral data from tests of semantic sim - ilarity , synonymity , analogy or word association ( Pereira et al , 2016 ) .", "entities": []}
{"text": "However , the evaluation of emergent word representations is far from straightforward , as there is no availability of the kind of semantic judgements that we have for adults .", "entities": []}
{"text": "We apply our methods to two bag - of - words models , and evaluate them on the acquisition of nouns in English - speaking children 1 .", "entities": []}
{"text": "Our goal by selecting these two approaches is to increase the variability of model performance within the bag - of - words paradigm .", "entities": []}
{"text": "We restrict our analyses to vectors of size 100 .", "entities": []}
{"text": "We use the Hyperwords package from Levy et al ( 2015 ) .", "entities": []}
{"text": "We trained the models on transcriptions of childdirected speech , i.e. samples of naturalistic productions in the linguistic environment of a child .", "entities": []}
{"text": "We used the childesr library to extract the child - directed utterances ( Sanchez et al , 2019 ) 2 .", "entities": []}
{"text": "The resulting dataset contains a total number of 3 , 135 , 822 sentences , 34 , 961 word types , and 12 , 975 , 520 word tokens .", "entities": []}
{"text": "To evaluate the models , we used data collected with the MacArthur - Bates Communicative Development Inventory forms ( CDI ) .", "entities": []}
{"text": "These are forms , given to parents of young children , that contain checklists of common early acquired words .", "entities": []}
{"text": "Parents complete the forms according to whether their child understands or produces each of those words .", "entities": []}
{"text": "These forms are collected at different ages , and thus can be used to estimate the Age of Acquisition ( AoA ) of words .", "entities": []}
{"text": "We used all the variants of English ' Words & Sentences ' CDIs from the Wordbank database ( Frank et al , 2017 ) , with the exception of those involving twins ( as significant differences have been observed in the language development of twins and singletons ,", "entities": []}
{"text": "Tomasello et al , 1986 ) .", "entities": []}
{"text": "We estimated the AoA of a word by considering that a word is acquired at the age at which at least 50 % of the children in the sample produced a given word .", "entities": []}
{"text": "2 http://childes - db.stanford.edu/about .", "entities": []}
{"text": "html 4 Method 1 : Neighbourhood Density Our first evaluation method is inspired by prior work on human word learning , presented in Hills et al ( 2010 ) .", "entities": []}
{"text": "Their model consists of a simple word co - occurrence matrix , where all the counts greater than zero are flattened into a count of one , resulting in a binary matrix .", "entities": []}
{"text": "The authors view the resulting matrix as a network of associations , where words are connected only if they have co - occurred .", "entities": []}
{"text": "The number of connections of each word is then used as an index , which the authors call Contextual Diversity ( CD ) .", "entities": []}
{"text": "Stella et al , 2017 ) and individual differences between typically developing children and late talkers ( Beckage et al , 2011 ) .", "entities": []}
{"text": "We propose a variant evaluation method that takes token co - occurrences into account .", "entities": []}
{"text": "The models we work with , on the contrary , are sensitive to co - occurrence frequencies , providing a more fine - grained characterization of the semantic space .", "entities": []}
{"text": "Our method works as follows .", "entities": []}
{"text": "First , we derived the semantic networks based on the cosine distance between representations .", "entities": []}
{"text": "Second , given this network , we counted the number of neighbours of each word as the number of other words connected to it .", "entities": []}
{"text": "We refer to this index as neighbourhood density ( ND ) .", "entities": []}
{"text": "Third , we computed the Pearson 's r correlation between this index and the AoA norms .", "entities": []}
{"text": "Figure 1 shows the distribution of the computed metric .", "entities": []}
{"text": "Note that these correlations can not be expected to be of the same order as those found when evaluating against adult ratings , since age of acquisition is predicted by a variety of factors , of which distributional information is only one , and it is subject to greater individual differences than adult semantic knowledge .", "entities": []}
{"text": "Therefore , moderate but significant correlations are generally consid - ered meaningful .", "entities": []}
{"text": "As a reference , the CD index , has a correlation of r = 0.32 in our dataset 3 .", "entities": []}
{"text": "As can be seen , the SGNS model is more likely to provide a semantic space that correlates with AoA , and some configurations yield an effect size comparable ( even larger ) than the CD metric .", "entities": []}
{"text": "This indicates that the SGNS model builds word representations in a way that reflects the relative difficulty of each word , and thus offers a good starting point for understanding how children use distributional context for vocabulary acquisition .", "entities": []}
{"text": "The fact that the correlation is positive prompts the prediction that , when co - occurrence frequency is incorporated in the model , words inhabiting less dense neighbourhoods are acquired earlier .", "entities": []}
{"text": "This finding suggests that semantic neighbours may act as competitors in the process of word learning .", "entities": []}
{"text": "To investigate this , we took the best model of our previous analyses ( SGNS with window size 1 , negative sampling 15 , frequency threshold 10 ) , and varied only the window size .", "entities": []}
{"text": "Results are in figure 2 .", "entities": []}
{"text": "As can be seen , smaller window sizes have better correlation with the data , indicating that the exploited context at this age is very local .", "entities": []}
{"text": "Such a result makes intuitive sense in the context of children 's immature verbal memory spans , which only improve as they acquire more language .", "entities": []}
{"text": "Now we turn our attention to the specific lexical items and their position in the semantic space .", "entities": []}
{"text": "Children tend to under - and overextend word meaning in the first stages of acquisition , and over time they become more precise on capturing the semantics of words .", "entities": []}
{"text": "A logical assumption then , is that words learnt earlier also converge earlier to adult - like semantic representations ( assuming that early and late words take , on average , approximately the same amount of time to converge ) .", "entities": []}
{"text": "We incorporated this idea in our second method by relating the AoA of words with adult free word association norms .", "entities": []}
{"text": "Note that this method can be applied to other semantic tasks , but we focus on word association because it does not impose the specific type of semantic relation that words need to have ( i.e. there is no distinction between similarity , analogy or others ) .", "entities": []}
{"text": "The dataset of free word association that we used is known as Small World of Worlds ( SWOW , De Deyne et al , 2019 ) , and it is the largest dataset of word associations in English , containing responses to over 12 , 000 cue words .", "entities": []}
{"text": "We filtered the preprocessed version of the dataset to include only words that have been acquired before 60 months old .", "entities": []}
{"text": "This results in 613 cue words , and 1839 responses ( word associates ) to these cues .", "entities": []}
{"text": "We then performed a similar cue - response experiment , with the best model from the previous section : for each cue , we retrieved the closest n neighbours .", "entities": []}
{"text": "As in Pereira et al ( 2016 ) , we used n = 50 , and then computed how many of these neighbours overlap with the word associates ( responses ) provided by human adults .", "entities": []}
{"text": "However , unlike that work , our evaluation is not based directly on the number of overlaps .", "entities": []}
{"text": "Figure 3 shows the result of this procedure .", "entities": []}
{"text": "As can be seen , there is a statistically significant rank correlation ( \u03c1 = \u22120.378 , p < 0.001 ) .", "entities": []}
{"text": "The negative direction confirms that words acquired earlier have a network of word associates that is more similar to those of adults , suggesting that convergence to adult semantic knowledge is at a more advanced state .", "entities": []}
{"text": "One limitation of this procedure is that it requires a choice on the number of neighbours to be retrieved .", "entities": []}
{"text": "In order to see how much the metric is affected by this parameter , we report the rank correlations of the previous model for several values of n. As can be seen in Figure 4 , this number stabilizes after n = 25 .", "entities": []}
{"text": "The graph shows that this model is consistently worse on our second evaluation method as well .", "entities": []}
{"text": "The use of these metrics already prompted the discovery that ( 1 ) words with fewer neighbours are easier to acquire , suggesting competition of neighbouring words , and ( 2 ) at young age , infants only attend to very local context .", "entities": []}
{"text": "The application of these methods to distributional models that incorporate additional assumptions ( e.g. knowledge of word order ) holds promise for further understanding of the role of distributional information in word learning .", "entities": []}
{"text": "Instead of operating on unannotated sentence pairs , our system uses pre - trained tagging systems to add linguistic features to source and target sentences .", "entities": []}
{"text": "Our proposed neural architecture learns a combined embedding of tokens and tags in the encoder , and simultaneous token and tag prediction in the decoder .", "entities": []}
{"text": "This demonstrates that certain token - level tag outputs from off - theshelf tagging systems can improve the output of neural translation systems using our combined embedding and simultaneous decoding extensions .", "entities": []}
{"text": "Consider the following examples : 1 ) Titanic struggles between good and evil .", "entities": []}
{"text": "\uc120\uacfc \uc545 \uc0ac\uc774\uc758 \uc5c4\uccad\ub09c \ud22c\uc7c1 .", "entities": []}
{"text": "big fight between good and evil \ud0c0\uc774\ud0c0\ub2c9\uc740 \uc120\uacfc \uc545 \uc0ac\uc774\uc5d0\uc11c \ud22c\uc7c1 \uc911\uc774\ub2e4 .", "entities": []}
{"text": "The Titanic is fighting between good and evil 2 ) Titanic struggles to stay afloat .", "entities": []}
{"text": "\ud0c0\uc774\ud0c0\ub2c9\uc740 \uce68\ubab0\ud558\uc9c0 \uc54a\ub3c4\ub85d \uace0\uad70\ubd84\ud22c \uc911\uc774\ub2e4 .", "entities": []}
{"text": "The Titanic is struggling not to sink \uce68\ubab0\ud558\uc9c0 \uc54a\uae30 \uc704\ud55c \uc5c4\uccad\ub09c \ud22c\uc7c1 .", "entities": []}
{"text": "big fight not to sink In ( 1 ) , \" Titanic \" is best translated as a common adjective ; in ( 2 ) , it most likely refers to a named entity , the famous ship .", "entities": []}
{"text": "Natural language processing ( NLP ) tools have benefited from the same explosion in deep learning and neural network developments that has spurred NMT .", "entities": []}
{"text": "al , 2020 )", "entities": []}
{"text": ".", "entities": []}
{"text": "We aim to use tags from publicly available pre - trained tagging systems as additional features to improve NMT training and output .", "entities": []}
{"text": "Tag assisted NMT requires modifications to the neural architecture to accommodate a tag at each token position .", "entities": []}
{"text": "The encoder must learn an embedding that combines information from each token and its tag , then compute a hidden state from these embeddings .", "entities": []}
{"text": "The decoder must learn to predict tokens and their tags simultaneously from the decoder state .", "entities": []}
{"text": "Subword tokenization reduced these effects to +0.22 points and - 0.22 points respectively .", "entities": []}
{"text": "Nonetheless , this demonstrates the feasibility of using certain pre - trained tagging outputs to improve translation quality .", "entities": []}
{"text": "Very early work addressed named entity translation by treating automatically identified named entities with a special translation system , usually a transliterator ( Babych and Hartley , 2003 ) .", "entities": []}
{"text": "This work did not attempt to integrate the translation models for one to benefit from information learned by the other .", "entities": []}
{"text": "This approach can also be adopted on the target - side , as presented here or in ( Hoang et al , 2016a ( Hoang et al , , 2018Nguyen et al , 2018 ) .", "entities": []}
{"text": "However , these methods only add linguistic feature information to the input , without encouraging the system to model that information in any particular way .", "entities": []}
{"text": "Garcia - Martinez et al , 2016 , 2017Tan et al , 2020 ) .", "entities": []}
{"text": "These systems also use a rule - based morphology toolkit in post - processing to generate the output surface forms from predicted output features , requiring knowledge of appropriate rule systems for the output language .", "entities": []}
{"text": "An additional tagged architecture ( N\u0203dejde et al , 2017 ) predicted syntax - tagged surface forms , but did so by appending the tags to the surface form tokens directly , rather than predicting separate factors .", "entities": []}
{"text": "In general , the focus of factored models has been to increase vocabulary coverage , for example of highly agglutitanative languages with rich morphologies , rather than our goal of disambiguating polysemous of polysyntactic words or otherwise handling named entities in a more nuanced way .", "entities": []}
{"text": "Finally , one previous work does consider a fully tagged ( both source and target ) factored neural model predicting tags with surface forms with independent layers in much the same way as presented here ( Wagner , 2017 ) .", "entities": []}
{"text": "1", "entities": []}
{"text": "By combining token and tag embeddings in the input and simultaneously predicting tokens and tags in the output , the NMT system learned to translate tagged source sentences to tagged target sentences ( Figure 1 ) .", "entities": []}
{"text": "Tags are added to the data as a preprocessing step .", "entities": []}
{"text": "Learning an embedding for every possible token and tag combination would enormously increase the model 's learnable parameter count .", "entities": []}
{"text": "Furthermore , training data is likely to be sparse in its coverage of all possible pairs , but not in its coverage of the token and tag vocabularies separately .", "entities": []}
{"text": "Therefore , we instead learn a separate embedding vector for each possible token and each possible tag , effectively concatenating these two vocabularies ( rather than taking the product space ) .", "entities": []}
{"text": "The embedding vectors for the token and tag at each position are then added to combine information from both channels into a single vector , so as not to increase the size of subsequent model layers and the capacity of the model , apart from the additional tag embedding vectors .", "entities": []}
{"text": "The decoder state d i at each step is conditioned on the target prefix and the encoded source sentence ( 3 ) .", "entities": []}
{"text": "d", "entities": []}
{"text": "i = Decoder ( prefix , src ) ( 3 ) This shared decoder state is used to predict both the next token and the next tag , with token and tag feature projections T and \u03c4 ( 4 and 5 ) .", "entities": []}
{"text": "P ( token , tag | prefix ; src ) = P ( token | pre . ; src ) P ( tag | pre . ; src ) ( 6 )", "entities": []}
{"text": "4 Data Preparation", "entities": []}
{"text": "Our experiments focused on film subtitles in German and English .", "entities": []}
{"text": "This data was cleaned with some rudimentary sentence length filtering , and randomly divided into a 3 million sentence - pair training split ( about 49 million tokens ) , along with 100 , 000 pair validation and test splits ( about 1.6 million tokens each ) .", "entities": []}
{"text": "We further divided the test split based on whether any named entities were found in either the source or the target sentence .", "entities": []}
{"text": "Out of 100 , 000 test pairs , 79 , 201 had no named entities , and 20 , 799 had some .", "entities": []}
{"text": "Word tokenization , as used by the tagging systems , is most straightforward for maintaining one - to - one alignments between tokens and their assigned tags .", "entities": []}
{"text": "For word tokenization experiments , vocabularies of size 35 , 012 for German and 17 , 196 for English were selected , resulting in an unknown word replacement rate of 3 % .", "entities": []}
{"text": "This unknown word replacement was considerably higher on rare word categories , for example named entities saw a 25 - 30 % rate of unknown words outside the selected word vocabulary .", "entities": []}
{"text": "To alleviate this it is also possible to consider subword ( Kudo , 2018 ) vocabulary of 32 , 000 subwords , built from the training split and used to tokenize both languages .", "entities": []}
{"text": "After subword tokenization , the BIOES structure of named entity spans was propagated across subword tokens in the natural way to maintain spans .", "entities": []}
{"text": "For POS tags , subwords received the same tag as their parent word .", "entities": []}
{"text": "This suggests that given O tag information the model can also treat common words with confidence that they are not named entities and should not be translated as such .", "entities": []}
{"text": "Adding in only source - side tag embeddings could be considered an enhanced baseline , since this kind of ( Sennrich and Haddow , 2016 ; Hoang et al , 2016b ) .", "entities": []}
{"text": "Our results show that this source - only tagging does not provide significant benefits compared to training on untagged data ( Table 1 ) , although for POS tagging this remains the best result .", "entities": []}
{"text": "On the other hand , adding in target - side tags while also predicting them from the decoder , without adding in source - side tag embeddings could be considered an ablation test to isolate the effects of our main contribution : target - side tag decoding .", "entities": []}
{"text": "Our results show that this target tagging provides the same benefit as the fully tagged training regime , demonstrating that it is the simultaneous tag decoding that accounts for the entire effect observed .", "entities": []}
{"text": "Experiments with subword tokenized data showed similar effects , but of a significantly reduced size .", "entities": []}
{"text": "Adding POS tags hurt results , decreasing the score by 0.22 , and again we see that source - only tagging is best case for POS tagging ( Table 4 ) .", "entities": []}
{"text": "It would appear that subword tokenization interferes with the benefits of tagging the data .", "entities": []}
{"text": "Since tags are aligned one - to - one with the input words , subword tokenization destroys this alignment , and copying tags across a word 's constituent subwords may interfere with the model 's ability to make sense the of tag information .", "entities": []}
{"text": "In particular for named entities , rare words are likely to tokenized into a larger number of subword tokens , exacerbating this effect .", "entities": []}
{"text": "The set of embeddings for the subwords in a word may not be as useful to the model for translating a named entity or other rare category as the single embedding learned specifically for the full word in a word tokenization setting , and further these subword embeddings may be affected by other contexts unrelated to the larger word .", "entities": []}
{"text": "Specifically for the named entity case , subword tokenization algorithms might prioritize the atomicity of certain rare words tagged as named entities in order to counteract this .", "entities": []}
{"text": "This shows how both tag types can add disambiguating information to the token prediction process , with POS tags naturally add more of such information , since they carry syntactic information .", "entities": []}
{"text": "That the beam decoding algorithm ( and autoregressive likelihood model ) used here for tags was unable to account for ( be conditioned on ) the as - yet uncomputed right context was cause for much apprehension before experimental results became available .", "entities": []}
{"text": "These positive results notwithstanding , future work could explore how to better incorporate the full tagging context in tag de - coding , perhaps , for example , by predicting the sequence more wholistically with non - autoregressive decoding ( Gu et al , 2018 ) .", "entities": []}
{"text": "A standard encoder - decoder architecture was extended to include tag embeddings and tag prediction at each token position .", "entities": []}
{"text": "At model input , token and tag embedding vectors were added to produce a combined embedding .", "entities": []}
{"text": "This tag assisted translation system was tested against baseline token - only systems on a German to English film subtitle corpus with both word and subword tokenization .", "entities": []}
{"text": "Subword tokenization reduced the size of the effect , suggesting the need for specialized subword tokenization to prioritize the integrity of important word categories .", "entities": []}
{"text": "Further examination of the cross - entropy showed that adding tags reduced the token cross - entropy thereby improving token modeling .", "entities": []}
{"text": "Future experiments can explore the use of other types of tag data as well as other decoding paradigms .", "entities": []}
{"text": "Many thanks go to my colleagues Jeesoo Bang , Jaehun Shin , and Baikjin Jung in the Knowledge and Language Engineering Lab ( POSTECH ) for their many hours generously spent discussing these research topics .", "entities": []}
{"text": "These results would not have been possible without their support .", "entities": []}
{"text": "This", "entities": []}
{"text": "M\u0101ori Loanwords : A Corpus of New Zealand English Tweets", "entities": []}
{"text": "M\u0101ori loanwords are widely used in New Zealand English for various social functions by New Zealanders within and outside of the M\u0101ori community .", "entities": []}
{"text": "Motivated by the lack of linguistic resources for studying how M\u0101ori loanwords are used in social media , we present a new corpus of New Zealand English tweets .", "entities": []}
{"text": "We collected tweets containing selected M\u0101ori words that are likely to be known by New Zealanders who do not speak M\u0101ori .", "entities": []}
{"text": "Since over 30 % of these words turned out to be irrelevant ( e.g. , mana is a popular gaming term , Moana is a character from a Disney movie ) , we manually annotated a sample of our tweets into relevant and irrelevant categories .", "entities": []}
{"text": "This data was used to train machine learning models to automatically filter out irrelevant tweets .", "entities": []}
{"text": "One of the most salient features of New Zealand English ( NZE ) is the widespread use of M\u0101ori words ( loanwords ) , such as aroha ( love ) , kai ( food ) and Aotearoa ( New Zealand ) .", "entities": []}
{"text": "See ex .", "entities": []}
{"text": "( 1 ) specifically from Twitter ( note the informal , conversational style and the M\u0101ori loanwords emphasised in bold ) .", "entities": []}
{"text": "( 1 ) Led the waiata for the manuhiri at the p\u014dwhiri for new staff for induction week .", "entities": []}
{"text": "Was told by the kaum\u0101tua I did it with mana and integrity .", "entities": []}
{"text": "The use of M\u0101ori words has been studied intensively over the past thirty years , offering a comprehensive insight into the evolution of one of the youngest dialects of English - New Zealand English ( Calude et al , 2017 ; Daly , 2007Daly , , 2016 ; Davies and Maclagan , 2006 ; De Bres , 2006 ;", "entities": []}
{"text": "Degani and Onysko , 2010 ; Kennedy and Yamazaki , 1999 ; Macalister , 2009Macalister , , 2006aOnysko and Calude , 2013 ) .", "entities": []}
{"text": "One aspect which is missing in this body of work is the online discourse presence of the loanwords - almost all studies come from ( collaborative ) written language ( highly edited , revised and scrutinised newspaper language , Davies and Maclagan 2006 ; Macalister 2009Macalister , 2006aOnysko and Calude 2013 , and picture - books , Daly 2007 , 2016 , or from spoken language collected in the late 1990s ( Kennedy and Yamazaki , 1999 ) .", "entities": []}
{"text": "In this paper , we build a corpus of New Zealand English tweets containing M\u0101ori loanwords .", "entities": []}
{"text": "Building such a corpus has its challenges ( as discussed in Section 3.1 ) .", "entities": []}
{"text": "Before we discuss these , it is important to highlight the uniqueness of the language contact situation between M\u0101ori and ( NZ ) English .", "entities": []}
{"text": "The language contact situation in New Zealand provides a unique case - study for loanwords because of a number of factors .", "entities": []}
{"text": "We list three particularly relevant here .", "entities": []}
{"text": "First , the direction of lexical transfer is highly unusual , namely , from an endangered indigenous language ( M\u0101ori ) into a dominant lingua franca ( English ) .", "entities": []}
{"text": "The large - scale lexical transfer of this type has virtually never been documented elsewhere , to the best of our knowledge ( see summary of current language contact situations in Stammers and Deuchar 2012 , particularly Table 1 , p. 634 ) .", "entities": []}
{"text": "Secondly , because M\u0101ori loanwords are \" New Zealand 's and New Zealand 's alone \" ( Deverson , 1991 , p. 18 - 19 ) , and above speakers ' consciousness , their ardent study over the years provides a fruitful comparison of the use of loanwords across genres , contexts and time .", "entities": []}
{"text": "Finally , the aforementioned body of previous research on the topic is rich and detailed , and still rapidly changing , with loanword use being an increasing trend ( Macalister , 2006a ; Kennedy and Yamazaki , 1999 ) .", "entities": []}
{"text": "However , the jury is still out regarding the reasons for the loanword use ( some hypotheses have been put forward ) , and the pat - terns of use across different genres ( it is unclear how language formality influences loanword use ) .", "entities": []}
{"text": "We find that Twitter data complements the growing body of work on M\u0101ori loanwords in NZE , by adding a combination of institutional and individual linguistic exchanges , in a non - editable online platform .", "entities": []}
{"text": "Social media language shares properties with both spoken and written language , but is not exactly like either .", "entities": []}
{"text": "More specifically , Twitter allows for creative expression and lexical innovation ( Grieve et al , 2017 ) .", "entities": []}
{"text": "Our Twitter corpus was created by following three main steps : collecting tweets over a ten - year period using \" query words \" ( Section 3.1 ) , manually labelling thousands of randomly - sampled tweets as \" relevant \" or \" irrelevant \" ( Section 3.2 ) , and then training a classifier to obtain automatic predictions for the relevance of each tweet and deploying this model on our target tweets , in a bid to filter out all those which are \" irrelevant \" ( Section 3.3 ) .", "entities": []}
{"text": "As will be discussed in Section 2 , our corpus is not the first of its kind but is the first corpus of New Zealand English tweets and the first collection of online discourse built specifically to analyse the use of M\u0101ori loanwords in NZE .", "entities": []}
{"text": "Section 4 outlines some preliminary findings from our corpus and Section 5 lays out plans for future work .", "entities": []}
{"text": "It is uncontroversial that M\u0101ori loanwords are both productively used in NZE and increasing in popularity ( Macalister , 2006a ) .", "entities": []}
{"text": "The corpora analysed previously indicate that loanword use is highly skewed , with some language users leading the way - specifically M\u0101ori women ( Calude et al , 2017 ; Kennedy and Yamazaki , 1999 ) , and with certain topics of discourse drawing significantly higher counts of loanwords than others - specifically those related to M\u0101ori people and M\u0101ori affairs , M\u0101oritanga ( Degani , 2010 ) .", "entities": []}
{"text": "The type of loanwords being borrowed from M\u0101ori is also changing .", "entities": []}
{"text": "During the first wave of borrowing , some two - hundred years ago , many flora and fauna words were being borrowed ; today , it is social culture terms that are increasingly adopted , e.g. , aroha ( love ) , whaea ( woman , teacher ) , and tangi ( M\u0101ori funeral ) , see Macalister ( 2006a ) .", "entities": []}
{"text": "However , the data available for loanword analysis is either outdated ( Calude et al , 2017 ; Kennedy and Yamazaki , 1999 ) , or exclusively formal and highly edited ( mainly newspaper language , Macalister 2006a ; Davies and Maclagan 2006 ;", "entities": []}
{"text": "Degani 2010 ) , so little is known about M\u0101ori loanwords in recent informal NZE interactions - a gap we hope to address here .", "entities": []}
{"text": "With the availability of vast amounts of data , building Twitter corpora has been a fruitful endeavour in various languages , including Turkish ( \u015e im\u015fek and\u00d6zdemir , 2012 ; \u00c7 etinoglu , 2016 ) , Greek ( Sifianou , 2015 ) , German ( Scheffler , 2014 ; Cieliebak et al , 2017 ) , and ( American ) English ( Huang et al , 2016 )", "entities": []}
{"text": "( though notably , not New Zealand English , while a modest corpus of te reo M\u0101ori tweets does exist , Keegan et al 2015 ) .", "entities": []}
{"text": "Twitter corpora of mixed languages are tougher to collect because it is not straightforward to detect mixed language data automatically .", "entities": []}
{"text": "Geolocations can help to some extent , but they have limitations ( most users do not use them to begin with ) .", "entities": []}
{"text": "Recent work on Arabic has leveraged the presence of distinct scripts - the Roman and Arabic alphabet - to create a mixed language corpus ( Voss et al , 2014 ) , but this option is not available to us .", "entities": []}
{"text": "M\u0101ori has traditionally been a spoken ( only ) language , and was first written down in the early 1800s by European missionaries in conjunction with M\u0101ori language scholars , using the Roman alphabet ( Smyth , 1946 ) .", "entities": []}
{"text": "Our task is more similar to studies such as Das andGamb\u00e4ck ( 2014 ) and\u00c7 etinoglu ( 2016 ) , who aim to find a mix of two languages which share the same script ( in their case , Hindi and English , and Turkish and German , respectively ) , but our method for collecting tweets is not user - based ; instead we use a set of target query words , as detailed in Section 3.1 .", "entities": []}
{"text": "In this section , we describe the process of building the M\u0101ori Loanword Twitter Corpus ( hereafter , the MLT Corpus ) 1 .", "entities": []}
{"text": "This process consists of three main steps , as depicted in Figure 1 .", "entities": []}
{"text": "In order to facilitate the collection of relevant data for the MLT Corpus , we compiled a list of 116 target loanwords , which we will call \" query words \" .", "entities": []}
{"text": "Most of these are individual words but some are short phrasal units ( tangata whenua , people of the land ; kapa haka , cultural performance ) .", "entities": []}
{"text": "The list is largely derived from Hay ( 2018 ) but was modified to exclude function words ( such as numerals ) and most proper nouns , except five that have native English counterparts : Aotearoa ( New Zealand ) , Kiwi ( s ) ( New Zealander ( s ) ) , M\u0101ori ( indigenous New Zealander ) , P\u0101keh\u0101 ( European New Zealander ) , non - M\u0101ori ( non - indigenous New Zealander ) .", "entities": []}
{"text": "We also added three further loanwords which we deemed useful for increasing our data , namely haurangi ( drunk ) , wairangi ( drugged , confused ) , and p\u014drangi ( crazy ) .", "entities": []}
{"text": "Using the Twitter Search API , we harvested 8 million tweets containing at least one query word ( after converting all characters to lowercase ) .", "entities": []}
{"text": "The tweets were collected diachronically over an eleven year period , between 2007 - 2018 .", "entities": []}
{"text": "We ensured that tweets were ( mostly ) written in English by using the lang : en parameter .", "entities": []}
{"text": "A number of exclusions and further adjustments were made .", "entities": []}
{"text": "With the aim of avoiding redundancy and uninformative data , retweets and tweets with URLs were discarded .", "entities": []}
{"text": "Tweets in which the query word was used as part of a username or mention ( e.g. , @happy kiwi ) were also discarded .", "entities": []}
{"text": "For those query words which contained macrons , we found that users were inconsistent in their macron use .", "entities": []}
{"text": "Consequently , we consolidated the data by adjusting our search to include both the macron and the non - macron version ( e.g. , both M\u0101ori and Maori ) .", "entities": []}
{"text": "We also removed all tweets containing fewer than five tokens ( words ) , due to insufficient context of analysis .", "entities": []}
{"text": "Owing to relaxed spelling conventions on Twitter ( and also the use of hashtags ) , certain query words comprising multiple lexical items were stripped of spaces in order to harvest all variants of the phrasal units ( e.g. , kai moana and kaimoana ) .", "entities": []}
{"text": "As kai was itself a query word ( in its own right ) , we excluded tweets containing kai moana when searching for tweets containing kai ( and repeated this process with similar items ) .", "entities": []}
{"text": "After inspecting these tweets , it was clear that a large number of our query words were polysemous ( or otherwise unrelated to NZE ) , and had introduced a significant amount of noise into the data .", "entities": []}
{"text": "The four main challenges we encountered are described below .", "entities": []}
{"text": "First , because Twitter contains many different varieties of English , NZE being just one of these , it is not always straightforward to disentangle the dialect of English spoken in New Zealand from other dialects of English .", "entities": []}
{"text": "This could be a problem when , for instance , a M\u0101ori word like Moana ( sea ) is used in American English tweets to denote the Disney movie ( or its main character ) .", "entities": []}
{"text": "Secondly , M\u0101ori words have cognate forms with other Austronesian languages , such as Hawaiian , Samoan and Tongan , and many speakers of these languages live and work ( and tweet ) in New Zealand .", "entities": []}
{"text": "For instance , the word wahine ( woman ) has the same written form in M\u0101ori and in Hawaiian .", "entities": []}
{"text": "But cognates are not the only problematic words .", "entities": []}
{"text": "Homographs with other , genealogically - unrelated languages can also pose problems .", "entities": []}
{"text": "For instance , the M\u0101ori word hui ( meeting ) is sometimes used as a proper noun in Chinese , as can be seen in the following tweet : \" Yo is Tay Peng Hui", "entities": []}
{"text": "okay with the tip of his finger ? \" .", "entities": []}
{"text": "Proper nouns constitute a third problematic aspect in our data .", "entities": []}
{"text": "As is typical for many language contact situations where an indigenous language shares the same geographical space as an incoming language , M\u0101ori has contributed many place names and personal names to NZE , such as Timaru , Aoraki , Titirangi , H\u0113mi , Mere and so on .", "entities": []}
{"text": "While these proper nouns theoretically count as loanwords , we are less interested in them than in content words because the use of the former does not constitute a choice , whereas the use of the latter does ( in many cases ) .", "entities": []}
{"text": "The \" choice \" of whether to use a loanword or whether to use a native English word ( or sometimes a native English phrase ) is interesting to study because it provides insights into idiolectal lexical preferences ( which words different speakers or writers prefer in given contexts ) and relative borrowing success rates ( Calude et al , 2017 ;", "entities": []}
{"text": "Zenner et al , 2012 ) .", "entities": []}
{"text": "Finally , given the impromptu and spontaneous nature of Twitter in general , we found that certain M\u0101ori words coincided with misspelled versions of intended native English words , e.g. , whare ( house ) instead of where .", "entities": []}
{"text": "The resulting collection of tweets , termed the Original Dataset , was used to create the Raw Corpus , as explained below .", "entities": []}
{"text": "We decided to address the \" noisy \" tweets in our data using supervised machine learning .", "entities": []}
{"text": "Two coders manually inspected a random sample of 30 tweets for each query word , by checking the word 's context of use , and labelled each tweet as \" relevant \" or \" irrelevant \" .", "entities": []}
{"text": "For example , a tweet like that in example ( 1 ) would be coded as relevant and one like \" awesome ! !", "entities": []}
{"text": "Congrats to Tangi : ) \" , would be coded as irrelevant ( because the query word tangi is used as a proper noun ) .", "entities": []}
{"text": "Since 39 of the query words consistently yielded irrelevant tweets ( at least 90 % of the time ) , these ( and the tweets they occurred in ) were removed altogether from the data .", "entities": []}
{"text": "Our annotators produced a total of 3 , 685 labelled tweets for the remaining 77 query words , which comprise the Labelled Corpus ( see Tables 1 and 4 ; note that irrelevant tweets have been removed from the latter for linguistic analysis ) .", "entities": []}
{"text": "Assuming our coded samples are representative of the real distribution of relevant / irrelevant tweets that occur with each query word , it makes sense to also discard the 39 \" noisy \" query words from our Original Dataset .", "entities": []}
{"text": "In this way , we created the ( unlabelled ) Raw Corpus , which is a fifth of the size ( see Table 4 ) .", "entities": []}
{"text": "We computed an inter - rater reliability score for our two coders , based on a random sample of 200 tweets .", "entities": []}
{"text": "Using Cohen 's Kappa , we calculated this value to be 0.87 ( \" strong \" ) .", "entities": []}
{"text": "In light of the strong agreement between the initial coders , no further coders were enlisted for the task .", "entities": []}
{"text": "The next step was to train a classifier using the Labelled Corpus as training data , so that the resulting model could be deployed on the Raw Corpus .", "entities": []}
{"text": "Our goal is to obtain automatic predictions for the relevance of each tweet in this corpus , according to probabilities given by our model .", "entities": []}
{"text": "We created ( stratified ) test and training sets that maintain the same proportion of relevant and irrelevant tweets associated with each query word in the Labelled Corpus .", "entities": []}
{"text": "We chose to include 80 % of these tweets in the training set and 20 % in the test set ( see Table 1 Using the AffectiveTweets package ( Bravo - Marquez et al , 2019 ) , our labelled tweets were transformed into feature vectors based on the word", "entities": []}
{"text": "n - grams they contain .", "entities": []}
{"text": "We then trained various classification models on this transformed data in Weka ( Hall et al , 2009 ) .", "entities": []}
{"text": "After deploying the Multinomial Naive Bayes model on the Raw Corpus , we found that 1 , 179 , 390 tweets were classified as relevant and 448 , 652 as irrelevant ( with probability threshold = 0.5 ) .", "entities": []}
{"text": "Table 3 shows examples from our corpus of each type of classification .", "entities": []}
{"text": "Some tweets were falsely classified as \" irrelevant \" and some were falsely classified as \" relevant \" .", "entities": []}
{"text": "A short explanation why the irrelevant tweets were coded as such is given in brackets at the end of each tweet .", "entities": []}
{"text": "We removed all tweets classified as irrelevant , thereby producing the Processed Corpus .", "entities": []}
{"text": "A summary of all three corpora is given in Table 4 .", "entities": []}
{"text": "As we are only just beginning to sift through the MLT Corpus , we note two particular sets of preliminary findings .", "entities": []}
{"text": "First , even though our corpus was primarily geared up to investigate loanword use , we are finding that , unlike other NZE genres analysed , the Twitter data exhibits use of M\u0101ori which is more in line with code - switching than with loanword use , see ex .", "entities": []}
{"text": "( 2 - 3 ) .", "entities": []}
{"text": "This is particularly interesting in light of the reported increase in te reo M\u0101ori language tweets ( Keegan et al , 2015 ) .", "entities": []}
{"text": "( 2 ) M\u014drena e hoa !", "entities": []}
{"text": "We must really meet IRL when I get back to T\u0101maki Makaurau !", "entities": []}
{"text": "You have a fab day too !", "entities": []}
{"text": "( 3 ) Heh !", "entities": []}
{"text": "He porangi toku ngeru - especially at 5 in the morning ! !", "entities": []}
{"text": "Ata marie e hoa ma .", "entities": []}
{"text": "I am well thank you .", "entities": []}
{"text": "Secondly , we also report the use of hybrid hashtags , that is , hashtags which contain a M\u0101ori part and an English part , for example # mycrazywhanau , # reostories , # Matarikistar , # bringitonmana , # growingupkiwi , # kaitoputinmyfridge .", "entities": []}
{"text": "To our knowledge , these hybrid hashtags have never been analysed in the current literature .", "entities": []}
{"text": "Hybrid hashtags parallel the phenomenon of hybrid compounds discussed by Degani and Onysko ( 2010 ) .", "entities": []}
{"text": "Degani and Onysko report that hybrid compounds are both productive and semantically novel , showing that the borrowed words take on reconceptualised meanings in their adoptive language ( 2010 , p.231 ) .", "entities": []}
{"text": "Relevant tweets f", "entities": []}
{"text": "( x ) < 0.5 Classified irrelevant Haka ne !", "entities": []}
{"text": "And i know even the good guys get blood for body ( 0.282 , foreign language ) son did nt get my chop ciggies 2day so stopped talking 2 him .", "entities": []}
{"text": "he just walked past and gave me the maori eyebrow lift and a smile .", "entities": []}
{"text": "were friends ( 0.337 )", "entities": []}
{"text": "Whare has the year gone ( 0.36 , misspelling ) Shorts and bare feet in this whare ( 0.41 ) chegar", "entities": []}
{"text": "na morena e falar can i be your girlfriend can i ( 0.384 , foreign language )", "entities": []}
{"text": "This paper introduced the first purpose - built corpus of M\u0101ori loanwords on Twitter , as well as a methodology for automatically filtering out irrelevant data via machine learning .", "entities": []}
{"text": "The MLT Corpus opens up a myriad of opportunities for future work .", "entities": []}
{"text": "Since our corpus is a diachronic one ( i.e. , all tweets are time - stamped ) , we are planning to use it for testing hypotheses about language change .", "entities": []}
{"text": "This is especially desirable in the context of New Zealand English , which has recently undergone considerable change as it comes into the final stage of dialect formation ( Schneider , 2003 ) .", "entities": []}
{"text": "Another avenue of future research is to automatically identify other M\u0101ori loanwords that are not part of our initial list of query words .", "entities": []}
{"text": "This could be achieved by deploying a language detector tool on every unique word in the corpus ( Martins and Silva , 2005 ) .", "entities": []}
{"text": "The \" discovered \" words could be used as new query words to further expand our corpus .", "entities": []}
{"text": "In addition , we intend to explore the meaning of our M\u0101ori loanwords using distributional semantic models .", "entities": []}
{"text": "We predict that these neighbouring words will enable us to understand the semantic make - up of our loanwords according to their usage .", "entities": []}
{"text": "This has great potential for enriching our understanding of how M\u0101ori loanwords are used in social media .", "entities": []}
{"text": "The authors would like to thank former Honours student Nicole Chan for a preliminary study on M\u0101ori Loanwords in Twitter .", "entities": []}
{"text": "Felipe Bravo - Marquez was funded by Millennium Institute for Foundational Research on Data .", "entities": []}
{"text": "Andreea S. Calude acknowledges the support of the NZ Royal Society Marsden Grant .", "entities": []}
{"text": "David Trye acknowledges the generous support of the Computing and Mathematical Sciences group at the University of Waikato .", "entities": []}
{"text": "Neural Maximum Subgraph Parsing for Cross - Domain Semantic Dependency Analysis", "entities": []}
{"text": "We present experiments for cross - domain semantic dependency analysis with a neural Maximum Subgraph parser .", "entities": []}
{"text": "Our parser targets 1 - endpoint - crossing , pagenumber - 2 graphs which are a good fit to semantic dependency graphs , and utilizes an efficient dynamic programming algorithm for decoding .", "entities": []}
{"text": "Our parser achieves very competitive results for both English and Chinese .", "entities": []}
{"text": "To improve the parsing performance on cross - domain texts , we propose a data - oriented method to explore the linguistic generality encoded in English Resource Grammar , which is a precisionoriented , hand - crafted HPSG grammar , in an implicit way .", "entities": []}
{"text": "Experiments demonstrate the effectiveness of our data - oriented method across a wide range of conditions .", "entities": []}
{"text": "Such sentence - level semantic analysis of text is concerned with the characterization of events and is therefore important to understand the essential meaning of a natural language sentence .", "entities": []}
{"text": "With the advent of many supporting resources , SDP has become a well - defined task with a substantial body of work and comparative evaluation .", "entities": []}
{"text": "( Almeida and Martins , 2015 ; Du et al , 2015a ; Zhang et al , 2016 ; Peng et al , 2017 ; Wang et al , 2018 ) .", "entities": []}
{"text": "Two SDP shared tasks have been run as part of the 2014 and 2015 International Workshops on Semantic Evaluation ( SemEval ) ( Oepen et al , 2014 ( Oepen et al , , 2015 .", "entities": []}
{"text": "Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types : transition - based ( Zhang et al , 2016 ; Wang et al , 2018 ) and graph - based , i.e. , Maximum Subgraph ( Kuhlmann and Jonsson , 2015 ; Cao et al , 2017a ) approaches .", "entities": []}
{"text": "From the above two perspectives , i.e. , the decoding and disambiguation frameworks , we find that what is still underexploited is neural Maximum Subgraph parsing for highly constrained graph classes , e.g. , noncrossing graphs .", "entities": []}
{"text": "In this paper , we fill this gap in the literature by developing a neural Maximum Subgraph parser .", "entities": []}
{"text": "Previous work showed that the 1 - endpointcrossing , pagenumber - 2 ( 1EC / P2 ) graphs are an appropriate graph class for modeling semantic dependency structures ( Cao et al , 2017a ) .", "entities": []}
{"text": "In this paper , we build a parser that targets 1EC / P2 graphs .", "entities": []}
{"text": "Our parser plays equally well for Chinese , resulting in an error reduction of 23.5 % and 9.4 % over the best published result reported in Zhang et al ( 2016 ) and Du et al ( 2015b ) .", "entities": []}
{"text": "How to build robust semantic dependency parsers that can learn across domains remains an under - addressed problem .", "entities": []}
{"text": "To improve the cross - domain parsing performance , we propose a data - oriented model to explore the linguistic generality encoded in a hand - crafted , domainindependent , linguistically - precise English grammar , namely English Resource Grammar ( ERG ; Flickinger , 2000 ) .", "entities": []}
{"text": "In particular , we introduce a cost - sensitive training model to learn crossdomain semantic information implicitly encoded in WikiWoods ( Flickinger et al , 2010 ) , i.e. , a corpus that collects the wikipedia 1 texts as well as their automatic syntactico - semantic annotations produced by ERG .", "entities": []}
{"text": "Evaluation demonstrates the usefulness of the imperfect annotations automatically created by ERG .", "entities": []}
{"text": "Our parser is available at https://github . com / draplater / msg - parser .", "entities": []}
{"text": "SDP is the task of mapping a natural language sentence into a formal meaning representation in the form of a dependency graph .", "entities": []}
{"text": "In this example , the semantic analysis is represented as a labeled directed graph in which the vertices are tokens in the sentence .", "entities": []}
{"text": "The graph abstracts away from syntactic analysis ( e.g. , the complementizer - thatand passive construction are excluded ) and includes most semantically relevant non - anaphoric local ( e.g. , from \" wants \" to \" Mark \" ) and longdistance ( e.g. , from \" buy \" to \" company \" ) dependencies .", "entities": []}
{"text": "The arc labels encode linguisticallymotivated , broadly - applicable semantic relations that are grounded under the type - driven semantics .", "entities": []}
{"text": "It is worth noting that semantic dependency graphs are not necessarily trees : ( 1 ) a token may be multiply headed because a word can be the arguments of more than one predicate ; ( 2 ) cycles are allowed if the direction of arcs are not taken into account .", "entities": []}
{"text": "Some recent work on parsing targets the graphstructured semantic representations that are more general than the tree representation .", "entities": []}
{"text": "Existing approaches can be categorized into two dominant types : the transition - based ( Zhang et al , 2016 ; Wang et al , 2018 ) and graph - based , i.e. , Maximum Subgraph ( Kuhlmann and Jonsson , 2015 ; Cao et al , 2017a ) , approaches .", "entities": []}
{"text": "Previous investigations on transition - based string - to - semantic - graph parsing adopt many ideas from syntactic string - totree parsing , such as how to handle crossing arcs and how to perform neural disambiguation .", "entities": []}
{"text": "Zhang et al ( 2016 ) introduced two transition systems that can generate arbitrary graphs and augmented them into practical semantic dependency parsers with a structured perceptron model .", "entities": []}
{"text": "Wang et al ( 2018 ) evaluated the effectiveness of deep learning techniques for transition - based SDP .", "entities": []}
{"text": "Kuhlmann and Jonsson ( 2015 ) proposed to formulate SDP as the search for the maximum subgraphs for some particular graph classes .", "entities": []}
{"text": "This proposal is called Maximum Subgraph parsing , which is a generalization of the graph - based parsing framework for syntactic parsing .", "entities": []}
{"text": "For arbitrary graphs , Du et al ( 2015a ) proved that the secondorder Maximum Subgraph problem is an NPhard problem .", "entities": []}
{"text": "Nevertheless , Almeida and Martins ( 2015 ) and Du et al ( 2015a ) showed that dual decomposition is a practical technique to solve the problem .", "entities": []}
{"text": "Considering more restricted graph classes , Kuhlmann and Jonsson ( 2015 ) introduced a dynamic programming algorithem for parsing to noncrossing graphs .", "entities": []}
{"text": "Cao et al ( 2017a ; 2017b ) showed that 1EC / P2 graphs are more suitable for describing semantic graphs than the noncrossing graphs , and they also allow low - degree dynamic programming algorithms for decoding .", "entities": []}
{"text": "Usually , syntactic dependency analysis employs the tree - shaped representation .", "entities": []}
{"text": "For SDP where the target representation are no longer trees , Kuhlmann and Jonsson ( 2015 ) proposed to generalize the MST model to other types of subgraphs .", "entities": []}
{"text": "G : Given a graph G = ( V , A ) , find a subset A \u2286 A with maximum total weight such that the induced subgraph G =", "entities": []}
{"text": "(", "entities": []}
{"text": "V , A ) belongs to G. Formally , we have the following optimization problem : G ( s ) = arg max H G ( s , G ) SCORE ( H )", "entities": []}
{"text": "= arg max H G ( s , G ) p in H SCOREPART ( s , p ) ( 1 ) Here , G ( s , G ) is the set of all graphs that belong to G and are compatible with s and G.", "entities": []}
{"text": "For parsing , G is usually a complete graph .", "entities": []}
{"text": "SCOREPART ( s , p ) evaluates whether a small subgraph p of a candidate graph H is a good partial analysis for sentence s.", "entities": []}
{"text": "For some graph classes and some types of score functions , there exists efficient algorithms for solving ( 1 ) .", "entities": []}
{"text": "For example , when G is the set of noncrossing graphs and SCOREPART is limited to handle individual dependencies , ( 1 ) can be solved in cubic - time ( Kuhlmann and Jonsson , 2015 ) .", "entities": []}
{"text": "Previous work showed that the Maximum Subgraph framework is not only elegant in theory but also effective in practice ( Kuhlmann and Jonsson , 2015 ; Cao et al , 2017a , b ) .", "entities": []}
{"text": "In particular , 1EC / P2 graphs are an appropriate graph class for modeling semantic dependency structures ( Cao et al , 2017a ) .", "entities": []}
{"text": "Figure 2 presents an example to illustrate the 1 - endpoint - crossing property , while Figure 3 shows a case for pagenumber - 2 .", "entities": []}
{"text": "Below we present the formal description of the two properties that are adopted from Pitler et al ( 2013 ) and Kuhlmann and Jonsson ( 2015 ) respectively .", "entities": []}
{"text": "Definition 1", "entities": []}
{"text": "A dependency graph is 1 - Endpoint - Crossing if for any edge e , all edges that cross e share an endpoint p named pencil point .", "entities": []}
{"text": "Definition 2", "entities": []}
{"text": "A pagenumber - k graph means it consists at most k half - planes , and arcs on each half - plane are noncrossing .", "entities": []}
{"text": "If G is the set of 1 - endpoint - crossing graphs or more restricted 1EC / P2 graphs , the optimization problem ( 1 ) in the first - order case can be solved in quintic - time ( Cao et al , 2017a ) by using dynamic programming .", "entities": []}
{"text": "Furthermore , ignoring one linguistically - rare structure in 1EC / P2 graphs descreases the complexity to O ( n 4 ) ( Cao et al , 2017a ) .", "entities": []}
{"text": "In this paper , we implement Cao et al Cao et al ( 2017a ) 's algorithm as the basis of our parser .", "entities": []}
{"text": "A semantic graph mainly consists of two parts : the structural part and the label part .", "entities": []}
{"text": "The former describes the predicate - argument relation in the sentence , and the latter describes the type of this relation .", "entities": []}
{"text": "In our model , the structural part and the label part are regarded as independent of each other .", "entities": []}
{"text": "We use a coarse - to - fine strategy : finding the maximum unlabeled subgraph first and assigning a label for every edge in this subgraph then .", "entities": []}
{"text": "The motivation is to avoid the calculation of a number of unnecessary label scores in order to improve the processing efficiency .", "entities": []}
{"text": "candidate dependencies as well as their relation types .", "entities": []}
{"text": "Figure 4 shows the architecture of our system .", "entities": []}
{"text": "We use words as well as POS tags as clues for scoring an individual arc .", "entities": []}
{"text": "In particular , we transform all of them into continuous and dense vectors .", "entities": []}
{"text": "The word - based embedding module applies the common lookup - table mechanism , while the character - based word embedding w i is implemented by extracting the features ( denoted as c 1 , c 2 , . .", "entities": []}
{"text": ". ,", "entities": []}
{"text": "w i", "entities": []}
{"text": "= x 1", "entities": []}
{"text": "+ x n", "entities": []}
{"text": "The concatenation of word embedding w i and POS - tag embedding p i of each word in specific sentence is used as the input of BiLSTMs to extract context - related feature vectors r i for each position i. a i =", "entities": []}
{"text": "w i", "entities": []}
{"text": "p", "entities": []}
{"text": "In our first order model , the SCORE function evaluates the preference of a semantic dependency graph by considering every bilexical relation in this graph one by one .", "entities": []}
{"text": "In particular , the corresponding SCOREPART function assigns a score to a candidate arc between word i and word j using a non - linear transform from the two feature vectors , viz .", "entities": []}
{"text": "r i and r j , associated to the two words : SCOREPART ( i , j )", "entities": []}
{"text": "The assignment task for dependency labels can be regarded as a classification task .", "entities": []}
{"text": "Our label scoring process is similar to the prediction of dependencies : LABEL ( i , j )", "entities": []}
{"text": "It is similar to the firstorder factorization as defined in a number of linear parsing models , e.g. , the models defined by Martins and Almeida ( 2014 ) and Cao et al ( 2017a ) .", "entities": []}
{"text": "+ SCORE ( \u011c ) )", "entities": []}
{"text": "The margin objective \u2206 measures the similarity between the gold graph G * and the prediction G. Follow Peng et al ( 2017 ) 's approach , we define \u2206 as weighted Hamming to trade off between precision and recall .", "entities": []}
{"text": "Banarescu et al , 2013 ) .", "entities": []}
{"text": "Such a model applies a rich set of precise linguistic rules to constrain their search for a preferable syntactic or semantic analysis .", "entities": []}
{"text": "In recent years , several of these linguistically motivated parsing systems achieved high performances that are comparable or even superior to the treebank - based purely data - driven parsers .", "entities": []}
{"text": "The main weakness of the precision grammarguided parsers is their robustness with respect to both coverage and efficiency .", "entities": []}
{"text": "For the texts from the web , e.g. , tweets , this problem is much more serious .", "entities": []}
{"text": "Moreover , checking all linguistic constraints makes a grammar - guided parser too slow for many realistic NLP applications .", "entities": []}
{"text": "On the contrary , light - weight , data - driven parsers usually have complementary strengthes in terms of both coverage and efficiency .", "entities": []}
{"text": "Intuitively , a hand - crafted precision grammar , e.g. , ERG , reflects highly generalized properties of a particular language and is thus highly resilient to domain shifts .", "entities": []}
{"text": "Accordingly , one should expect that a precision grammar - guided parser which guarantees the a rich set of domain - independent linguistic constraints to be met can be more robust to domain shifts than a purely data - driven parser .", "entities": []}
{"text": "In related work for syntactic parsing , Ivanova et al ( 2013 ) showed that the ERG - based parser was more robust to domain variation than several representative data - driven parsers .", "entities": []}
{"text": "However , there are at least two drawbacks of their ERG - guided parser based method : 1 .", "entities": []}
{"text": "2 . This method fails to take parsing efficiency into account .", "entities": []}
{"text": "In this paper , we introduce a new data - oriented strategy to consume a precision grammar .", "entities": []}
{"text": "The key idea is to take a grammar as an imperfect annotator : We let a precision grammar - guided parser parse large - scale raw texts in an offline way , and then utilize the automatically generated analysis as imperfect training data .", "entities": []}
{"text": "Because we only need raw texts to be parsed once , even if this process takes much time , it is still reasonable .", "entities": []}
{"text": "A grammarguided parser can not parse a considerable portion of data , but this will not cause serious problems because we can take an enormous amount of sentences as annotation candidates .", "entities": []}
{"text": "Just considering the wikipedia , we can collect at least dozens of millions of comparatively high - quality sentences .", "entities": []}
{"text": "An essential problem of this method is that such imperfect annotations bring in annotation errors which may hurt parser training .", "entities": []}
{"text": "To deal with this problem , we adopted a cost - sensitive training method to train our model on the extended training data .", "entities": []}
{"text": "In each epoch , we trained on imperfect corpus first and then on gold - standard corpus .", "entities": []}
{"text": "As for label assigning , we exclude losses less than 0.5 .", "entities": []}
{"text": "These threshold numbers are tuned on the development data .", "entities": []}
{"text": "To evaluate neural Maximum Subgraph parsing in practice , we first conduct experiments on the three English data sets , namely DM , PAS and PSD 4 , which are from the SemEval 2015 Task18 ( Oepen et al , 2015 ) .", "entities": []}
{"text": "We use the \" standard \" training , validation , and test splits to facilitate comparisons .", "entities": []}
{"text": "In other words , the data splitting policy follows the shared task .", "entities": []}
{"text": "In addition to English parsing , we consider Chinese SDP and use two data sets : ( 1 ) Chinese PAS data provided by SemEval 2015 , and", "entities": []}
{"text": "( Tse and Curran , 2010 ) to evaluate the cross - lingual ability of our model .", "entities": []}
{"text": "All the SemEval data sets are publicly available from LDC ( Oepen et al , 2016 ) .", "entities": []}
{"text": "We use DyNet 5 to implement our neural models .", "entities": []}
{"text": "We use the automatic batch technique ( Neubig et al , 2017 ) in DyNet to perform mini - batch gradient descent training .", "entities": []}
{"text": "The detailed network hyper - parameters are summarized in Table 2 .", "entities": []}
{"text": "We use the same pre - trained word embedding as Kiperwasser and Goldberg ( 2016 ) .", "entities": []}
{"text": "Results from other papers are of different yet representative decoding or disambiguation frameworks .", "entities": []}
{"text": "Du et al ( 2015a ) sion of the two linear model - based parsers is comparable or even superior to our neural parser , but the recall is far behind .", "entities": []}
{"text": "We evaluate two ensemble methods , voting and score averaging .", "entities": []}
{"text": "In the voting method , each model parses the sentence to graph respectively .", "entities": []}
{"text": "An edge will exist on the combined graph only if more than half output graphs of these models contain this edge .", "entities": []}
{"text": "The label of this edge will be the most common label .", "entities": []}
{"text": "In the score averaging method , we use averaged score parts to get a maximum graph and classify labels .", "entities": []}
{"text": "We choose 3/10 kind of different initial parameters to train models for ensemble .", "entities": []}
{"text": "Figure 5 shows the result of the two ensemble methods .", "entities": []}
{"text": "The averaging method has slightly better performance on the 3 datasets .", "entities": []}
{"text": "The performance of this method on test data is shown on Table 1 .", "entities": []}
{"text": "Since around 2001 , the ERG has been accompanied by syntactico - semantic annotations , where for each sentence an annotator has selected the intended analysis among all alternatives licensed by the grammar .", "entities": []}
{"text": "This derived resource , namly Redwoods 6 ( Oepen et al , 2002 ;", "entities": []}
{"text": "Flickinger et al , 2017 ) , is a collection of hand - annotated corpora and consists of data sets from several distinct domains .", "entities": []}
{"text": "The WSJ part is also known as Deep - Bank .", "entities": []}
{"text": "The Brown corpus part is used as the out - of - domain test data by Se - mEval 2015 .", "entities": []}
{"text": "The DM data sets for both SemEval 2014 and 2015 SDP shared tasks are based on the RedWoods corpus .", "entities": []}
{"text": "Besides gold standard annoations , Flickinger et al ( 2010 ) built the WikiWoods corpus 7 , which provides automatically created annotations for the texts from wikipedia .", "entities": []}
{"text": "The annotations are disambiguated using the MaxEnt model trained using redwoods without DeepBank .", "entities": []}
{"text": "We use a small portion of Wikiwoods , which contains 857 , 329 sentences in total .", "entities": []}
{"text": "To evaluate the ( positive ) impact of ERG on out - of - domain parsing , we conduct experiments on the DM data .", "entities": []}
{"text": "The first group of experiments are designed to be comparable with the results obtained by various participant systems of SemEval 2015 .", "entities": []}
{"text": "The detailed data set - up is as follows : Test Data .", "entities": []}
{"text": "We use the Brown corpus section which is provided by SemEval 2015 .", "entities": []}
{"text": "Training Data .", "entities": []}
{"text": "We use three data sets for training : ( 1 ) DeepBank , ( 2 ) RedWoods and ( 3 ) a small portion of WikiWoods reparsed using the MaxEnt model trained on Deep - Bank .", "entities": []}
{"text": "We denote this reparsed WikiWoods as WikiWoods - ACE , since the HPSG analysis is provided by the ACE parser .", "entities": []}
{"text": "To extract the semantic dependency graph , we use the pydelphin tool 8 .", "entities": []}
{"text": "For the second group of experiments , we use the section wsj21 from the DeepBank as test data , which is the official in - domain test of the SemEval 2015 .", "entities": []}
{"text": "The training data includes the \" RedWoods minus DeepBank \" annotations ( RedwoodsWOD for short ) as well as the official WikiWoods annotations .", "entities": []}
{"text": "Note that the MaxEnt model used to obtain the official WikiWoods annotations are compatible with RedwoodswWOD .", "entities": []}
{"text": "Due to the diversity of the RedwoodsWOD and DeepBank sentences , this set - up can also be viewed as an outof - domain evaluation .", "entities": []}
{"text": "WikiWoods to train another model , and leave out other parts of Redwoods .", "entities": []}
{"text": "The performance improvement is more remarkable when providing more data , even though such data contains annotation errors .", "entities": []}
{"text": "For the second group of experiments , we use the RedwoodsWOD sentences for training and the DeepBank WSJ sentences for evaluation .", "entities": []}
{"text": "For this set - up , consistent improvements of the parser quality are observed .", "entities": []}
{"text": "To test the ability for cross - lingual parsing , we conduct experiments on HPSG and CCG grounded semantic analyses respectively .", "entities": []}
{"text": "The HPSG grounded analysis is provided by SemEval 2015 and the underlying framework is the same to the English PAS data .", "entities": []}
{"text": "We use the same set - up as Zhang et al ( 2016 ) .", "entities": []}
{"text": "Table 4 and 5 Chinese POS tagging has a great impact on parsing .", "entities": []}
{"text": "From Table 5 , we can see that POS information is very important to Chinese SDP .", "entities": []}
{"text": "Mandarin Chinese is recognized as a morphology - poor language : POS tags are defined mainly according to words ' distributional rather than morphological properties .", "entities": []}
{"text": "the power of the RNN architecture to learn nonlocal dependencies and thus benefit our semantic dependency parser a lot .", "entities": []}
{"text": "We introduce a new parser for semantic dependency analysis , which combines two promising parsing techniques , i.e. , decoding based on Maximum Subgraph algorithms and disambiguation based on BiLSTMs .", "entities": []}
{"text": "To our knowledge , this is the first neural Maximum Subgraph parser .", "entities": []}
{"text": "We also propose a new data - oriented method to leverage ERG , a linguistically - motivated , hand - crafted grammar , to improve cross - domain performance .", "entities": []}
{"text": "Experiments demonstrate the effectiveness of taking ERG as an imperfect annotator .", "entities": []}
{"text": "This work was supported by the National Natural Science Foundation of China ( 61772036 , 61331011 ) and the Key Laboratory of Science , Technology and Standard in Press Industry ( Key Laboratory of Intelligent Press Media Technology ) .", "entities": []}
{"text": "We thank the anonymous reviewers for their helpful comments .", "entities": []}
{"text": "Weiwei Sun is the corresponding author .", "entities": []}
{"text": "To this end , it discovers relation patterns between named entities and then clusters those semantically equivalent patterns into a united relation cluster .", "entities": []}
{"text": "Most OpenRE methods typically confine themselves to unsupervised paradigms , without taking advantage of existing relational facts in knowledge bases ( KBs ) and their high - quality labeled instances .", "entities": []}
{"text": "To address this issue , we propose Relational Siamese Networks ( RSNs ) to learn similarity metrics of relations from labeled data of pre - defined relations , and then transfer the relational knowledge to identify novel relations in unlabeled data .", "entities": []}
{"text": "Experiment results on two real - world datasets show that our framework can achieve significant improvements as compared with other state - of - the - art methods .", "entities": []}
{"text": "Our code is available at https://github . com / thunlp / RSN .", "entities": []}
{"text": "For example , with the sentence \" Hayao Miyazaki is the director of the film ' The Wind Rises ' \" , we can extract a relation \" director_of \" between two entities \" Hayao Miyazaki \" and \" The Wind Rises \" .", "entities": []}
{"text": "Recent progress in supervised methods to RE has achieved great successes .", "entities": []}
{"text": "Supervised methods can effectively learn significant relation semantic patterns based on existing labeled data , but the data constructions are time - consuming and human - intensive .", "entities": []}
{"text": "Mintz ( 2009 ) also proposes distant supervision to generate training data automatically .", "entities": []}
{"text": "It assumes that if two entities have a relation in KBs , all sentences that contain these two entities will express this relation .", "entities": []}
{"text": "Still , all these approaches can only extract pre - defined relations that have already appeared either in human - annotated datasets or KBs .", "entities": []}
{"text": "It is hard for them to cover the great variety of novel relational facts in the open - domain corpora .", "entities": []}
{"text": "There are some efforts concentrating on extracting triples with new relation types .", "entities": []}
{"text": "Banko ( 2008 ) directly extracts words or phrases in sentences to represent new relation types .", "entities": []}
{"text": "However , some relations can not be explicitly represented with tokens in sentences , and it is hard to align different relational tokens that exactly have the same meanings .", "entities": []}
{"text": "Yao ( 2011 ) consid - ers OpenRE as a clustering task for extracting triples with new relation types .", "entities": []}
{"text": "However , previous clustering - based OpenRE methods ( Yao et al , 2011 ( Yao et al , , 2012Marcheggiani and Titov , 2016 ; Elsahar et al , 2017 ) are mostly unsupervised , and can not effectively select meaningful relation patterns and discard irrelevant information .", "entities": []}
{"text": "In this paper , we propose to take advantage of high - quality supervised data of pre - defined relations for OpenRE .", "entities": []}
{"text": "The approach is non - trivial , however , due to the considerable gap between the pre - defined relations and novel relations of interest in open domain .", "entities": []}
{"text": "To bridge the gap , we propose Relational Siamese Networks ( RSNs ) to learn transferable relational knowledge from supervised data for OpenRE .", "entities": []}
{"text": "Specifically , RSNs learn relational similarity metrics from labeled data of pre - defined relations , and then transfer the metrics to measure the similarity of unlabeled sentences for open relation clustering .", "entities": []}
{"text": "We describe the flowchart of our framework in Figure 1 .", "entities": []}
{"text": "Moreover , we show that RSNs can also be generalized to various weakly - supervised scenarios .", "entities": []}
{"text": "We propose Semi - supervised RSN to learn from both supervised data of pre - defined relations and unsupervised data with novel relations , and Distantly - supervised RSN to learn from distantly - supervised data and unsupervised data .", "entities": []}
{"text": "The results demonstrate that our models significantly outperform state - of - the - art baseline methods in all scenarios without using external linguistic tools .", "entities": []}
{"text": "To summarize , the main contributions of this work are as follows : ( 1 ) We develop a novel relational knowledge transfer framework RSN for OpenRE , which can effectively transfer existing relational knowledge to novel - relation data and accurately identify novel relations .", "entities": []}
{"text": "To the best of our knowledge , RSN is the first model to consider knowledge transfer in clustering - based OpenRE task .", "entities": []}
{"text": "( 2 ) We further propose Semi - supervised RSNs and Distantly - supervised RSNs that can learn from various weakly supervised scenarios .", "entities": []}
{"text": "Traditional RE methods mainly concentrate on classifying relational facts into pre - defined relation types ( Mintz et al , 2009 ; Yu et al , 2017 ) .", "entities": []}
{"text": "Zeng ( 2014 ) utilizes CNN encoders to build sentence representations with the help of position embeddings .", "entities": []}
{"text": "Lin ( 2016 ) further improves RE performance on distantlysupervised data via instance - level attention .", "entities": []}
{"text": "These methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed representations , and have achieved promising results .", "entities": []}
{"text": "However , these methods can not handle the open - ended growth of new relation types in the open - domain corpora .", "entities": []}
{"text": "OpenRE methods can be roughly divided into two categories : taggingbased and clustering - based .", "entities": []}
{"text": "Tagging - based methods cast OpenRE as a sequence labeling problem , and extract relational phrases consisting of words from sentences in unsupervised ( Banko et al , 2007 ; Banko and Etzioni , 2008 ) or supervised paradigms ( Jia et al , 2018 ; Cui et al , 2018 ; Stanovsky et al , 2018 ) .", "entities": []}
{"text": "However , tagging - based methods often extract multiple overly - specific relational phrases for the same relation type , and can not be readily utilized for downstream tasks .", "entities": []}
{"text": "In comparison , conventional clustering - based OpenRE methods extract rich features for relation instances via external linguistic tools , and cluster semantic patterns into several relation types ( Lin and Pantel , 2001 ;", "entities": []}
{"text": "Yao et al , 2011Yao et al , , 2012 .", "entities": []}
{"text": "Elsahar ( 2017 ) utilizes a clustering algorithm over linguistic features .", "entities": []}
{"text": "In this paper , we focus on the clustering - based OpenRE methods , which have the advantage of discovering highly distinguishable relation types .", "entities": []}
{"text": "Semi - supervised Clustering .", "entities": []}
{"text": "Differently , our proposed Semi - supervised RSN only leverages labeled instances of pre - defined relations , and does not need any seed of new relations .", "entities": []}
{"text": "Our OpenRE framework mainly consists of two modules , the relation similarity calculation module and the relation clustering module .", "entities": []}
{"text": "For relation similarity calculation , we propose Relational Siamese Networks ( RSNs ) , which learn to predict whether two sentences mention the same relation .", "entities": []}
{"text": "To utilize large - scale unsupervised data and distantly - supervised data , we further propose Semi - supervised RSN and Distantly - supervised RSN .", "entities": []}
{"text": "Finally , in the relation clustering module , with the learned relation metric , we utilize hierarchical agglomerative clustering ( HAC ) and Louvain clustering algorithms to cluster target relation instances of new relation types .", "entities": []}
{"text": "The architecture of our Relational Siamese Networks is shown in Figure 2 .", "entities": []}
{"text": "CNN modules encode a pair of relational instances into vectors , and several shared layers compute their similarity .", "entities": []}
{"text": "Sentence Encoder .", "entities": []}
{"text": "We use a CNN module as the sentence encoder .", "entities": []}
{"text": "The CNN module includes an embedding layer , a convolutional layer , a max - pooling layer , and a fully - connected ( FC ) layer .", "entities": []}
{"text": "Following ( Zeng et al , 2014 ) , we concatenate these embeddings to form a vector sequence .", "entities": []}
{"text": "Next , a one - dimensional convolutional layer and a maxpooling layer transform the vector sequence into features .", "entities": []}
{"text": "And with paired input relational instances , we have : vl = CNN ( s l ) , vr = CNN ( sr ) , ( 2 ) in which two CNN modules are identical and share all the parameters .", "entities": []}
{"text": "Similarity Computation .", "entities": []}
{"text": "Next , to measure the similarity of two relational vectors , we calculate their absolute distance and transform it into a realnumber similarity", "entities": []}
{"text": "First , a distance layer computes the element - wise absolute distance of two vectors : vd", "entities": []}
{"text": "= | vl \u2212 vr | .", "entities": []}
{"text": "( 3 ) Then , a classifier layer calculates a metric p for relation similarity .", "entities": []}
{"text": "To summarize , we obtain a good similarity metric p of relational instances .", "entities": []}
{"text": "Cross Entropy Loss .", "entities": []}
{"text": "The output of RSN p can also be explained as the probability of two sentences mentioning two different relations .", "entities": []}
{"text": "To discover relation clusters in the open - domain corpus , it is beneficial to not only learn from labeled data , but also capture the manifold of unlabeled data in the semantic space .", "entities": []}
{"text": "To this end , we need to push the decision boundaries away from high - density areas , which is known as the cluster assumption ( Chapelle and Zien , 2005 ) .", "entities": []}
{"text": "In the following paragraphs , we denote the labeled training dataset as D l and a couple of labeled relational instances as d l .", "entities": []}
{"text": "Similarly , we denote the unlabeled training dataset as D u and a couple of unlabeled instances as d u .", "entities": []}
{"text": "Conditional Entropy Loss .", "entities": []}
{"text": "In classification problems , a well - classified embedding space usually reserves large margins between different classified clusters , and optimizing margin can be a promising way to facilitate training .", "entities": []}
{"text": "However , in clustering problems , type labels are not available during training .", "entities": []}
{"text": "To optimize margin without explicit supervision , we can push the data points away from the decision boundaries .", "entities": []}
{"text": "Intuitively , when the distance similarity p between two relational instances equals 0.5 , there is a high prob - ability that at least one of two instances is near the decision boundary between relation clusters .", "entities": []}
{"text": "( 6 ) Virtual Adversarial Loss .", "entities": []}
{"text": "Despite its theoretical promise , conditional entropy minimization suffers from shortcomings in practice .", "entities": []}
{"text": "Due to neural networks ' strong fitting ability , a very complex decision hyperplane might be learned so as to keep away from all the training samples , which lacks generalizability .", "entities": []}
{"text": "As a solution , we can smooth the relational representation space with locally - Lipschitz constraint .", "entities": []}
{"text": "To satisfy this constraint , we introduce virtual adversarial training ( Miyato et al , 2016 ) on both branches of RSN .", "entities": []}
{"text": "Virtual adversarial training can search through data point neighborhoods , and penalize most sharp changes in distance prediction .", "entities": []}
{"text": "Empirically , we approximate the perturbations the same as the original paper ( Miyato et al , 2016 ) .", "entities": []}
{"text": "Specifically , we first add a random noise to the input , and calculate the gradient of the KL - divergence between the outputs of the original input and the noisy input .", "entities": []}
{"text": "We then add the normalized gradient to the original input and get the perturbed input .", "entities": []}
{"text": "L all = L l + \u03bbvL vl + \u03bbu", "entities": []}
{"text": "( Lu + \u03bbvLvu ) , ( 9 ) in which \u03bb v and \u03bb u are two hyperparameters .", "entities": []}
{"text": "To alleviate the intensive human labor for annotation , the topic of distantly - supervised learning has attracted much attention in RE .", "entities": []}
{"text": "Here , we propose Distantly - supervised RSN , which can learn from both distantly - supervised data and unsupervised data for relational knowledge transfer .", "entities": []}
{"text": "L all = L l + \u03bbu", "entities": []}
{"text": "Firstly , the noise will be overwhelmed by the large proportion of negative sampling during training .", "entities": []}
{"text": "Secondly , during clustering , the prediction of a new relation cluster is based on areas where the density of relational instances is high .", "entities": []}
{"text": "Outliers from noise , as a result , will not influence the prediction process so much .", "entities": []}
{"text": "After RSN is learned , we can use RSN to calculate the similarity matrix of testing instances .", "entities": []}
{"text": "With this matrix , several clustering methods can be applied to extract new relation clusters .", "entities": []}
{"text": "Hierarchical Agglomerative Clustering .", "entities": []}
{"text": "The first clustering method we adopt is hierarchical agglomerative clustering ( HAC ) .", "entities": []}
{"text": "HAC is a bottomup clustering algorithm .", "entities": []}
{"text": "At the start , every testing instance is regarded as a cluster .", "entities": []}
{"text": "For every step , it agglomerates two closest instances .", "entities": []}
{"text": "There are several criteria to evaluate the distance between two clusters .", "entities": []}
{"text": "Here , we adopt the complete - linkage criterion , which is more robust to extreme instances .", "entities": []}
{"text": "However , there is a significant shortcoming of HAC : it needs the exact number of clusters in advance .", "entities": []}
{"text": "A potential solution is to stop agglomerating according to an empirical distance threshold , but it is hard to determine such a threshold .", "entities": []}
{"text": "This problem leads us to consider another clustering algorithm Louvain ( Blondel et al , 2008 ) .", "entities": []}
{"text": "Louvain .", "entities": []}
{"text": "Louvain is a graph - based clustering algorithm traditionally used for detecting communities .", "entities": []}
{"text": "The advantage of Louvain is that it does not need the number of potential clusters beforehand .", "entities": []}
{"text": "It will automatically find proper sizes of clusters by optimizing community modularity .", "entities": []}
{"text": "According to the experiments we conduct , Louvain performs better than HAC .", "entities": []}
{"text": "After running , Louvain might produce a number of singleton clusters with few instances .", "entities": []}
{"text": "It is not proper to call these clusters new relation types , so we label these instances the same as their closest labeled neighbors .", "entities": []}
{"text": "Finally , we want to explain the reason why we do not use some other common clustering methods like K - Means , Mean - Shift and Ward 's ( Ward Jr , 1963 ) method of HAC : these methods calculate the centroid of several points during clustering by merely averaging them .", "entities": []}
{"text": "Consequently , it is not proper to calculate the centroid by simply averaging the vectors .", "entities": []}
{"text": "In this section , we conduct several experiments on real - world RE datasets to show the effectiveness of our models , and give a detailed analysis to show its advantages .", "entities": []}
{"text": "We then randomly choose 1 , 600 instances from the unlabeled set as the test set , with the rest labeled and unlabeled instances considered as the train set .", "entities": []}
{"text": "Finally , the auto - labeled train set contains 323 , 549 relational instances , and the unlabeled train set contains 60 , 581 instances .", "entities": []}
{"text": "A previous OpenRE work reports performance on an unpublic dataset called NYT - FB ( Marcheggiani and Titov , 2016 ) .", "entities": []}
{"text": "First , NTY - FB 's test set is distantly - supervised and is noisy for instance - level RE .", "entities": []}
{"text": "Moreover , instances in NYT - FB often share entity pairs or relational phrases , which makes it much easier for relation clustering .", "entities": []}
{"text": "Data Sampling .", "entities": []}
{"text": "The input of RSN should be a pair of sampled instances .", "entities": []}
{"text": "For the unlabeled set , the only possible sampling method is to select two instances randomly .", "entities": []}
{"text": "For the labeled set , however , random selection would result in too many different - relation pairs , and cause severe biases for RSN .", "entities": []}
{"text": "To solve this problem , we use downsampling .", "entities": []}
{"text": "In our experiments , we fix the percentage of same - relation pairs in every labeled data batch as 6 % .", "entities": []}
{"text": "Let us denote this percentage number as the sample ratio for convenience .", "entities": []}
{"text": "Experimental results show that the sample ratio decides RSN 's tendency to predict larger or smaller clusters .", "entities": []}
{"text": "In other words , it controls the granularity of the predicted relation types .", "entities": []}
{"text": "However , we leave any serious discussion to future work .", "entities": []}
{"text": "Hyperparameter Settings .", "entities": []}
{"text": "Following ( Lin et al , 2016 ) and ( Zeng et al , 2014 ) , we fix the less influencing hyperparameters for sentence encoding as their reported optimal values .", "entities": []}
{"text": "For position embeddings , we use randominitialized 5 - dimensional position embeddings .", "entities": []}
{"text": "During training , all the embeddings are trainable .", "entities": []}
{"text": "For the neural network , the number of feature maps in the convolutional layer is 230 .", "entities": []}
{"text": "The filter length is 3 .", "entities": []}
{"text": "Besides , we adopt two regularization methods in the CNN module .", "entities": []}
{"text": "We put a dropout layer right after the embedding layer as ( Miyato et al , 2016 ) .", "entities": []}
{"text": "The dropout rate is 0.2 .", "entities": []}
{"text": "Hyperparameters for virtual adversarial training are just the same as ( Miyato et al , 2016 ) proposed .", "entities": []}
{"text": "At the same time , major hyperparameters are selected with grid search according to the model performance on a validation set .", "entities": []}
{"text": "Specifically , the validation set contains 10 , 000 randomly chosen sentence pairs from the unlabeled set ( i.e. 16 novel relations ) and does not overlap with the test set .", "entities": []}
{"text": "The model is evaluated according to the precision of binary classification of sentence pairs on the validation set , which is an estimation for models ' clustering ability .", "entities": []}
{"text": "For hyperparameters in Equation 9 and Equation 10 , \u03bb v is 1.0 selected from { 0.1 , 0.5 , 1.0 , 2.0 } and \u03bb u is 0.03 selected from { 0.01 , 0.02 , 0.03 , 0.04 , 0.05 } .", "entities": []}
{"text": "For baseline models , original papers do grid search for all possible hyperparameters and report the best result during testing .", "entities": []}
{"text": "We follow their settings and do grid search directly on the test set .", "entities": []}
{"text": "In this section , we demonstrate the effectiveness of our RSN models by comparing our models with state - of - the - art clustering - based OpenRE methods .", "entities": []}
{"text": "We also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of Semi - supervised RSN and Distantly - supervised RSN . Baselines .", "entities": []}
{"text": "Conventional clustering - based OpenRE models usually cluster instances by either clustering their linguistic features ( Lin and Pantel , 2001 ;", "entities": []}
{"text": "Yao et al , 2012 ; Elsahar et al , 2017 ) or imposing reconstruction constraints ( Yao et al , 2011 ; Marcheggiani and Titov , 2016 ) .", "entities": []}
{"text": "( Elsahar et al , 2017 ) : RW - HAC is the state - of - the - art feature clustering model for OpenRE .", "entities": []}
{"text": "It optimizes a relation classifier by reconstructing entities from pairing entities and predicted relation types .", "entities": []}
{"text": "Rich features including entity words , context words , trigger words , dependency paths , and context POS tags are used to predict the relation type .", "entities": []}
{"text": "Specifically , we first align entities to Wikidata and get their KB types .", "entities": []}
{"text": "It is worth noting that these features are only used by baseline models .", "entities": []}
{"text": "Our models , in contrast , only use sentences and entity pairs as inputs .", "entities": []}
{"text": "Evaluation Protocol .", "entities": []}
{"text": "In evaluation , we use B 3 metric ( Bagga and Baldwin , 1998 ) as the scoring function .", "entities": []}
{"text": "B 3 metric is a standard measure to balance the precision and recall of clustering tasks , and is commonly used in previous OpenRE works ( Marcheggiani and Titov , 2016 ;", "entities": []}
{"text": "Elsahar et al , 2017 ) .", "entities": []}
{"text": "To be specific , we use F 1 measure , the harmonic mean of precision and recall .", "entities": []}
{"text": "First , we report the result of supervised RSN with different clustering methods .", "entities": []}
{"text": "Specifically , SN represents the original RSN structure , HAC and L indicate HAC and Louvain clustering introduced in Sec .", "entities": []}
{"text": "3.3 .", "entities": []}
{"text": "The result shows that Louvain performs better than HAC , so in the following experiments we focus on using Louvain clustering .", "entities": []}
{"text": "Next , for Semi - supervised and Distantlysupervised RSN , we conduct various combinations of different mechanisms to verify the contribution of each part .", "entities": []}
{"text": "( + C ) indicates that the model is powered up with conditional entropy minimization , while ( + V ) indicates that the model is pow - Experimental Result Analysis .", "entities": []}
{"text": "This indicates that RSN is capable of understanding new relations ' semantic meanings within sentences .", "entities": []}
{"text": "( 2 ) Supervised and distantly - supervised relational representations improve clustering performances .", "entities": []}
{"text": "Compared with RW - HAC , SN - HAC achieves better clustering results because of its supervised relational representation and similarity metric .", "entities": []}
{"text": "Specifically , unsupervised baselines mainly use sparse one - hot features .", "entities": []}
{"text": "In contrast , RSN uses distributed feature representations , and can optimize information integration process according to supervision .", "entities": []}
{"text": "( 3 ) Louvain outperforms HAC for clustering with RSN , comparing SN - HAC with SN - L. One explanation is that our model does not put additional constraints on the prior distribution of relational vectors , and therefore the relation clusters might have odd shapes in violation of HAC 's assumption .", "entities": []}
{"text": "Moreover , when representations are not distinguishable enough , forcing HAC to find finegrained clusters may harm recall while contributing minimally to precision .", "entities": []}
{"text": "In practice , we do observe that the number of relations SN - L extracts is constantly less than the true number 16 .", "entities": []}
{"text": "( 4 ) Both SN - L+V and SN - L+C improve the performance of supervised or distantly - supervised RSN by further utilizing unsupervised corpora .", "entities": []}
{"text": "Both semi - supervised approaches bring significant improvements for F 1 scores by increasing the precision and recall , and combining both can further increase the F 1 score .", "entities": []}
{"text": "( 5 ) One interesting observation is that SN - L+V does not outperform SN - L so much on FewReldistant .", "entities": []}
{"text": "This is probably because VAT on the noisy data might amplify the noise .", "entities": []}
{"text": "In further experiments , we perform VAT only on unlabeled set and observe improvements on F 1 , with SN - L+V from 45.8 % to 49.2 % and SN - L+CV from 52.0 % to 52.6 % , which proves this conjecture .", "entities": []}
{"text": "In this subsection , we mainly focus on analyzing the influence of pre - defined relation diversity , i.e. , the number of relations in the labeled train set .", "entities": []}
{"text": "Several conclusions can be drawn according to Figure 5 .", "entities": []}
{"text": "Firstly , a rich variety of labeled relations do improve the performance of our models , especially RSN .", "entities": []}
{"text": "The models trained on 64 relations perform better than those trained on 40 relations constantly .", "entities": []}
{"text": "Secondly , while the performance of supervised RSN is very sensitive to pre - defined relation diversity , its semi - supervised counterparts suffer much less from the relation number limit .", "entities": []}
{"text": "This phenomenon suggests that Semi - supervised RSNs succeed in learning from unlabeled novelrelation data and are more generalizable to novel relations .", "entities": []}
{"text": "To intuitively evaluate the knowledge transfer effects of RSN and Semi - supervised RSN , we visualize their relational knowledge representation spaces in the last layer of CNN encoders with t - SNE ( Maaten and Hinton , 2008 ) in Figure 4 .", "entities": []}
{"text": "We also compare with a supervised CNN trained on 9 , 600 labeled instances of novel relations , which suggests the optimal relational knowledge representation .", "entities": []}
{"text": "In each figure , we plot 402 relation instances of 4 randomly - chosen relation types in the test set , and points are colored according to their ground - truth labels .", "entities": []}
{"text": "As we can see from Figure 4 , RSN is able to roughly distinguish different relations , and Semi - supervised RSN further facilitated knowledge transfer by optimizing the margin between potential relation clusters during training .", "entities": []}
{"text": "As a result , Semi - supervised RSN can extract more distinguishable novel relations , and gains comparable relational knowledge representation ability with supervised CNN .", "entities": []}
{"text": "Different from conventional unsupervised models , our model learns to measure relational similarity from supervised / distantly - supervised data of predefined relations , as well as unsupervised data of novel relations .", "entities": []}
{"text": "There are mainly two innovative points in our model .", "entities": []}
{"text": "First , we propose to transfer relational similarity knowledge with RSN structure .", "entities": []}
{"text": "To the best of our knowledge , we are the first to propose knowledge transfer for OpenRE .", "entities": []}
{"text": "Experiments show that our models significantly surpass conventional OpenRE models and achieve new state - of - the - art performance .", "entities": []}
{"text": "In the future , we can try different sentence encoders in our model .", "entities": []}
{"text": "( 2 ) As mentioned above , our model has the potential ability to discover the hierarchical structure of relations .", "entities": []}
{"text": "In the future , we will try to explore this application with additional experiments .", "entities": []}
{"text": "KALA : Knowledge - Augmented Language Model Adaptation", "entities": []}
{"text": "Simple fine - tuning of PLMs , on the other hand , might be suboptimal for domain - specific tasks because they can not possibly cover knowledge from all domains .", "entities": []}
{"text": "While adaptive pre - training of PLMs can help them obtain domain - specific knowledge , it requires a large training cost .", "entities": []}
{"text": "To overcome such limitations of adaptive pre - training for PLM adaption , we propose a novel domain adaption framework for PLMs coined as Knowledge - Augmented Language model Adaptation ( KALA ) , which modulates the intermediate hidden representations of PLMs with domain knowledge , consisting of entities and their relational facts .", "entities": []}
{"text": "The results show that , despite being computationally efficient , our KALA largely outperforms adaptive pre - training .", "entities": []}
{"text": "Code is available at : https://github.com/Nardien/KALA .", "entities": []}
{"text": "Pre - trained Language Models ( PLMs )", "entities": []}
{"text": "This continual pre - training of a PLM on the target domain corpus allows it to learn the distribution of the target domain , resulting in improved performance on domain - specific tasks ( Howard and Ruder , 2018 ; Han and Eisenstein , 2019 ) .", "entities": []}
{"text": "While it has shown to be effective , adaptive pretraining has obvious drawbacks .", "entities": []}
{"text": "First , it is computationally inefficient .", "entities": []}
{"text": "Although a PLM becomes more powerful with the increasing amount of pretraining data ( Gururangan et al , 2020 ) , further pre - training on the additional data requires larger memory and computational cost as the dataset size grows ( Bai et al , 2021 ) .", "entities": []}
{"text": "This catastrophic forgetting of the task - Our KALA framework embeds the unseen entities on the embedding space of seen entities by representing them with their relational knowledge over the graph , while the strong DAPT baseline ( Gururangan et al , 2020 ) can not appropriately handle unseen entities that are not given for task fine - tuning .", "entities": []}
{"text": "In Figure 1 , we show that adaptive pre - training with more training steps could lead to performance degeneration .", "entities": []}
{"text": "Thus , it would be preferable if we could adapt the PLM to the domain - specific task without costly adaptive pre - training .", "entities": []}
{"text": "To this end , we aim to integrate the domain - specific knowledge into the PLM directly during the task - specific fine - tuning step , as shown in Figure 2b , eliminating the adaptive pre - training stage .", "entities": []}
{"text": "Specifically , we first note that entities and relations are core building blocks of the domain - specific knowledge that are required to solve for the domain - specific downstream tasks .", "entities": []}
{"text": "Clinical domain experts , for example , are familiar with medical terminologies and their complex relations .", "entities": []}
{"text": "Then , we further exploit the relational structures of the entities by utilizing a Knowledge Graph ( KG ) , which denotes the factual relationships between entities , as shown in Knowledge Graph of Figure 2b .", "entities": []}
{"text": "The remaining step is how to integrate the knowledge into the PLM during fine - tuning .", "entities": []}
{"text": "To this end , we propose a novel layer named Knowledgeconditioned Feature Modulation ( KFM , 3.2 ) , which scales and shifts the intermediate hidden representations of PLMs by conditioning them with retrieved knowledge representations .", "entities": []}
{"text": "This knowledge integration scheme has several advantages .", "entities": []}
{"text": "First , it does not modify the original PLM architecture , and thus could be integrated into any PLMs regardless of their architectures .", "entities": []}
{"text": "Also , it only re - quires marginal computational and memory overhead , while eliminating the need of excessive further pre - training ( Figure 1 ) .", "entities": []}
{"text": "Finally , it can effectively handle unseen entities with relational knowledge from the KG , which are suboptimally embedded by adaptive pre - training .", "entities": []}
{"text": "For example , as shown in Figure 2 , an entity restenosis does not appear in the training dataset for fine - tuning , thus adaptive pre - training only implicitly infers them within the context from the broad domain corpus .", "entities": []}
{"text": "However , we can explicitly represent the unknown entity by aggregating the representations of known entities in the entity memory ( i.e. , in Figure 2 , neighboring entities , such as asthma and pethidine , are used to represent the unseen entity restenosis ) .", "entities": []}
{"text": "We combine all the previously described components into a novel language model adaptation framework , coined as Knowledge - Augmented Language model Adaptation ( KALA ) ( Figure 3 ) .", "entities": []}
{"text": "We empirically verify that KALA improves the performance of the PLM over adaptive pre - training on various domains with two knowledge - intensive tasks :", "entities": []}
{"text": "Our contribution is threefold : We propose a novel LM adaptation framework , which augments PLMs with entities and their relations from the target domain , during fine - tuning without any further pre - training .", "entities": []}
{"text": "To our knowledge , this is the first work that utilizes the structured knowledge for language model adaptation .", "entities": []}
{"text": "To reflect structural knowledge into the PLM , we introduce a novel layer which scales and shifts the intermediate PLM representations with the entity representations contextualized by their related entities according to the KG .", "entities": []}
{"text": "This strategy first pretrains a Language Model ( LM ) on a large and unlabeled corpus , then fine - tunes it on downstream tasks with labeled data ( Devlin et al , 2019 ) .", "entities": []}
{"text": "While this scheme alone achieves impressive performance on various NLU tasks , adaptive pre - training of the PLM on a domain - specific corpus helps the PLM achieve better performance on the domain - specific tasks .", "entities": []}
{"text": "For example , demonstrated that a further pre - trained LM on biomedical documents outperforms the original LM on biomedical NLU tasks .", "entities": []}
{"text": "Also , Gururangan et al ( 2020 ) showed that adaptive pre - training of the PLM on the corpus of a target domain ( Domain - adaptive Pre - training ; DAPT ) or a target task ( Task - adaptive Pre - training ; TAPT ) improves its performance on domain - specific tasks .", "entities": []}
{"text": "However , above approaches generally require a large amount of computational costs for pre - training .", "entities": []}
{"text": "Knowledge - aware LM Accompanied with increasing sources of knowledge ( Vrandecic and Kr\u00f6tzsch , 2014 ) , some prior works have proposed to integrate external knowledge into PLMs , to enhance their performance on tasks that require structured knowledge .", "entities": []}
{"text": "Entity - as - Experts ( F\u00e9vry et al , 2020 ) and LUKE ( Yamada et al , 2020 ) use the entity memory that is pre - trained along with the LMs from scratch .", "entities": []}
{"text": "ERICA", "entities": []}
{"text": "( Qin et al , 2021 )", "entities": []}
{"text": "further uses the fact consisting of entities and their relations in the pre - training stage of LMs from scratch .", "entities": []}
{"text": "Previous works aim to integrate external knowledge into the LMs during the pre - training step to obtain a universal knowledge - aware LM that requires additional parameters for millions of entities .", "entities": []}
{"text": "In contrast to this , our framework aims to efficiently modify a general PLM for the domain - specific task with a linear modulation layer scheme discussed in Section 3.2 , during fine - tuning .", "entities": []}
{"text": "We first introduce the NLU tasks we target , followed by the descriptions of the proposed knowledgeaugmented LM .", "entities": []}
{"text": "After that , we formally define the ingredients for structured knowledge integration .", "entities": []}
{"text": "The goal of an NLU task is to predict the label y of the given input instance x , where the input x contains the sequence of tokens ( Devlin et al , 2019 ) :", "entities": []}
{"text": "x = [ w 1 , w 2 , . . .", "entities": []}
{"text": ", w | x | ] .", "entities": []}
{"text": "( y |", "entities": []}
{"text": "|", "entities": []}
{"text": "=", "entities": []}
{"text": "If the LM is composed of L - layers of transformer blocks ( Devlin et al , 2019 ) , the function f is decomposed to multiple functions", "entities": []}
{"text": "f =", "entities": []}
{"text": ". ,", "entities": []}
{"text": "f L ] , where each block gets the output of the previous block as the input :", "entities": []}
{"text": "H l = f l", "entities": []}
{"text": "( H l\u22121 ) .", "entities": []}
{"text": "Thus , contextualizing the texts by the domain knowledge , captured by the domain - specific entities and their relations , is more appropriate for handling such domain - specific problems .", "entities": []}
{"text": "To this end , we propose a function h ( ; \u03c6 ) which augments PLMs conditioned on the domain knowledge .", "entities": []}
{"text": "|", "entities": []}
{"text": "In the following , we will describe the definition of the knowledgerelated inputs E , M , G , and the details of h ( , \u03c6 ) .", "entities": []}
{"text": "Definition 1 ( Entity and Mention ) .", "entities": []}
{"text": "Given a sequence of tokens x = [ w 1 , . . .", "entities": []}
{"text": ",", "entities": []}
{"text": ", w m \u03c9 ] x 2 .", "entities": []}
{"text": "Consequently , for each given input x ( i ) , there are a set of entities E ( i ) = { e 1 , . . .", "entities": []}
{"text": ", e K } and their corresponding mentions M ( i ) = { m 1 , . . .", "entities": []}
{"text": ", m K } .", "entities": []}
{"text": "For example , given an input x =", "entities": []}
{"text": "[", "entities": []}
{"text": "New , York , is , a , city ] , we have two entities E = { New_York , city } and their associated mentions M = { ( 1 , 2 ) , ( 4 , 4 ) } .", "entities": []}
{"text": "We further construct the entity vocabulary E train = N i=1 E", "entities": []}
{"text": "( i ) , which consists of all entities appearing in the training dataset .", "entities": []}
{"text": "However , at test time , we may encounter unseen entities that are not in E train .", "entities": []}
{"text": "To tackle this , we regard unknown entities as the null entity e , so that \u2200e E train \u222a { e } .", "entities": []}
{"text": "Definition 2 ( Entity Memory ) .", "entities": []}
{"text": "In order to implement this , we define the entity memory E R ( | E train | +1 ) \u00d7d that comprises of an entity e R as a key and its embedding e R d as its value .", "entities": []}
{"text": "Also , to access the value in the entity memory , we define the point - wise memory access function EntEmbed which takes an entity as an input .", "entities": []}
{"text": "For instance , e = EntEmbed ( New_York ) returns the embedding of the New_York entity , and e = EntEmbed ( e ) returns the zero embedding .", "entities": []}
{"text": "This entity memory E is the part of the parameter \u03c6 used in function h. Definition 3 ( Knowledge Graph ) .", "entities": []}
{"text": "Since the entity memory alone can not represent relational information between entities , we further define a Knowledge Graph ( KG ) G that consists of a set of factual triplets { ( h , r , t ) } , where the head and the tail entities , h and t , are the elements of E , and a relation r is an element of a set of relations R : h , t E and r R. We assume that a preconstructed KG G ( i ) is given for each input", "entities": []}
{"text": "x ( i ) , and provide the details of the KGs and how to construct them in Appendix A.", "entities": []}
{"text": "The remaining problem is how to augment a PLM by conditioning it on the domain - specific knowledge , through the function h.", "entities": []}
{"text": "An effective approach to do so without stacking additional layers on top of the LM is to interleave the knowledge from h with the pre - trained parameters of the language model ( Devlin et al , 2019 ) consisting of transformer layers ( Vaswani et al , 2017 ) .", "entities": []}
{"text": "[ h l\u22121 1 , . . .", "entities": []}
{"text": ", h l\u22121 |", "entities": []}
{"text": "x | ] R | x | \u00d7d from the layer l \u2212 1 where d is the embedding size , each transformer block outputs the contextualized representations for all tokens .", "entities": []}
{"text": "In detail , the l - th block consists of the multi - head self - attention ( Attn ) layer and the residual feed - forward ( FF ) layer as follows : H l = LN ( H l\u22121 + Attn ( H l\u22121 ) )", "entities": []}
{"text": "We omit the bias term for brevity .", "entities": []}
{"text": "+ B , F F ( \u0124 l ) =", "entities": []}
{"text": "+ B , ( 1 )", "entities": []}
{"text": "[", "entities": []}
{"text": ",", "entities": []}
{"text": "One notable advantage of our KFM is that multiple tokens associated to the identical entity are affected by the same modulation ( e.g. , ' New ' and ' York ' in Figure 3 ) , which allows the PLM to know which adjacent tokens are in the same entity .", "entities": []}
{"text": "This is important for representing the tokens of the domain entity ( e.g. , ' cod ' and ' on ' ) , since the original PLM might regard them as separate , unrelated tokens ( See analysis in 5.5 with Figure 5 ) .", "entities": []}
{"text": "However , with our KFM , the PLM can identify associated tokens and embed them to be close to each other .", "entities": []}
{"text": "Then , how can we design such functional operations in h ?", "entities": []}
{"text": "If so , all unseen entities are inevitably modulated by the same parameters even if they have essentially different meaning .", "entities": []}
{"text": "To tackle these limitations , we further consider the relational information between two entities that are linked with a particular relation .", "entities": []}
{"text": "For example , the entity New_York alone will not give meaningful information .", "entities": []}
{"text": "However , with two associated facts ( New_York , instance of , city ) and ( New_York , country , USA ) , it is clear that New_York is a city in the USA .", "entities": []}
{"text": "More specifically , our goal is to effectively utilize the relations among entities in G , to improve the EntEmbed function in equation 2 .", "entities": []}
{"text": "We tackle this objective by utilizing a Graph Neural Network ( GNN ) which learns feature representations of each node using a neighborhood aggregation scheme ( Hamilton et al , 2017 ) , as follows : v = UPDATE ( EntEmbed ( e ) , AGG ( { EntEmbed ( \u00ea ) : \u2200\u00ea N ( e ; G ) } ) ) , where N ( e ; G ) is a set of neighboring entities of the entity e , AGG is the function that aggregates embeddings of neighboring entities of e , and UPDATE is the function that updates the representation of e with the aggregated messages from AGG .", "entities": []}
{"text": "However , simple aggregation ( e.g. , mean ) can not reflect the relative importance on neighboring nodes , thus we consider the attentive scheme ( Velickovic et al , 2018 ; Brody et al , 2021 ) for neighborhood aggregation , to allocate weights to the target entity 's neighbors by their importance .", "entities": []}
{"text": "This scheme is helpful in filtering out less useful relations .", "entities": []}
{"text": "Formally , we first define a scoring function \u03c8 that calculates a score for every triplet ( e i , r ij , e j ) , which is then used to weigh each node during aggregation : e i = EntEmbed ( e i ) , e j = EntEmbed ( e j ) , e", "entities": []}
{"text": "* = [ e i r ij e", "entities": []}
{"text": "j h", "entities": []}
{"text": "e", "entities": []}
{"text": "i ] , \u03c8", "entities": []}
{"text": "( e i , r ij , e j , h e i )", "entities": []}
{"text": "= a \u03c3 ( W e * ) , where \u03c3 is a nonlinear activation , e * R 4d is concatenated vector where denotes the concatenation , a R d and W R d\u00d74d are learnable parameters , r ij R d is a embedding of the relation , and h e i R d is a context representation of the entity e i obtained from the intermediate hidden states of the LM 3 .", "entities": []}
{"text": "The scores obtained from \u03c8 are normalized across all neighbors", "entities": []}
{"text": "= exp ( \u03c8 ( e i , r ij , e j ) )", "entities": []}
{"text": "e j N ( e i ; G ) exp ( \u03c8 ( e i , r ij , e j ) ) .", "entities": []}
{"text": "( 3 ) 1 m", "entities": []}
{"text": "\u03c9", "entities": []}
{"text": "m", "entities": []}
{"text": "\u03c9", "entities": []}
{"text": "i By replacing the EntEmbed function in equation 2 with the above GNN in equation 3 , we now represent each entity with its relational information in KG .", "entities": []}
{"text": "This relational retrieval has several advantages over simple retrieval of a single entity from the entity memory .", "entities": []}
{"text": "First , the relational retrieval with KG can consider richer interactions among entities , as described in Figure 3 .", "entities": []}
{"text": "In addition , we can naturally represent an unseen entity - which is not seen during training but appears at test time - through neighboring aggregation , which is impossible only with the entity memory .", "entities": []}
{"text": "In Figure 2 , we provide an illustrative example of the unseen entity representation , where the unseen entity restenosis is represented with a weighted sum of representations of its neighboring entities myocardial_infarction , asthma , and pethidine , which is beneficial when the set of entities for training and test datasets have small overlaps .", "entities": []}
{"text": "We evaluate our model on two NLU tasks :", "entities": []}
{"text": "al , 2018 )", "entities": []}
{"text": ".", "entities": []}
{"text": "We report statistics and detailed descriptions of each dataset in Appendix B.2 .", "entities": []}
{"text": "A direct baseline of our KALA is the adaptive pre - training , which is commonly used to adapt the PLM independent to the choice of a domain and task .", "entities": []}
{"text": "Also , to compare ours against a more powerful baseline , we modify a recent method that alleviates forgetting of PLM during fine - tuning .", "entities": []}
{"text": "Details for each baseline we use are described as follows :", "entities": []}
{"text": "1 .", "entities": []}
{"text": "Vanilla Fine - Tuning ( FT ) :", "entities": []}
{"text": "A baseline that directly fine - tunes the LM on downstream tasks .", "entities": []}
{"text": "The numbers in bold fonts denote the best score .", "entities": []}
{"text": "\u2020 indicates the method under an extremely high computational resource setting ( See Figure 1 ) .", "entities": []}
{"text": "LM .", "entities": []}
{"text": "We use this baseline to show that the performance gain of our model does not come from the use of additional parameters .", "entities": []}
{"text": "6 . KALA ( pointwise ) :", "entities": []}
{"text": "7 . KALA ( relational ) :", "entities": []}
{"text": "Our full model that uses KGs to perform relational retrieval from the entity memory .", "entities": []}
{"text": "For more details on training and implementation , please see the Appendix B.", "entities": []}
{"text": "These results show that our KALA is highly effective for the language model adaptation task .", "entities": []}
{"text": "KALA also largely outperforms DAPT ( Gururangan et al , 2020 ) which is trained with extra data and requires a significantly higher computational cost compare to KALA ( See Figure 1 for the plot of efficiency , discussed in Section 5.3 ) .", "entities": []}
{"text": "Effect of Using more Parameters One may suspect whether the performance of our KALA comes from the increment of parameters .", "entities": []}
{"text": "The performance gap between KALA ( relational ) and KALA ( point - wise ) shows the effectiveness of relational retrieval for language model adaptation , which allows us to incorporate relational knowledge into the PLM .", "entities": []}
{"text": "The relational retrieval also helps address unseen entities , as discussed in Section 5.4 .", "entities": []}
{"text": "We perform an ablation study to see how much each component contributes to the performance gain .", "entities": []}
{"text": "Architectural Variants We now examine the effectiveness of the proposed knowledge conditioning scheme in our KALA framework .", "entities": []}
{"text": "To this end , we use or adapt the knowledge integration methods from previous literature , to compare their effectiveness .", "entities": []}
{"text": "( Qin et al , 2021 ) .", "entities": []}
{"text": "Note that , most of them were proposed for improving pre - training from scratch , while we adapt them for fine - tuning under our KALA framework ( The details are given in Appendix B.4 ) .", "entities": []}
{"text": "As shown in Table 4 , our KFM used in KALA outperforms all variants , demonstrating the effectiveness of feature modulation in the middle of transformer layers for fine - tuning .", "entities": []}
{"text": "Thus , we believe that our KALA would be useful to any PLMs , not depending on specific PLMs .", "entities": []}
{"text": "On the other hand , our KALA outperforms DAPT without using external data , while requiring 17 times fewer computational costs , which shows that KALA is not only effective but also highly efficient .", "entities": []}
{"text": "To further compare the efficiency in various aspects , we report GPU memory , training wall time , and training FLOPs for baselines and ours in Table 6 .", "entities": []}
{"text": "Through this , we verify that our KALA is more efficient to train for language model adaptation settings than baselines .", "entities": []}
{"text": "Note that the resource requirement of KALA could be further reduced by adjusting the size of the entity memory ( e.g. , removing less frequent entities ) .", "entities": []}
{"text": "Therefore , to show the flexibility of our KALA on the typical resource constraint , we provide the experimental results on two different settings ( i.e. , tuning the number of entities in the entity memory ) - KALA with memory size of 200 and 62.8k ( full memory ) in Appendix C.6 .", "entities": []}
{"text": "One remarkable advantage of our KALA is its ability to represent an unseen entity by aggregating features of its neighbors from a given KG .", "entities": []}
{"text": "To analyze this , we first divide all contexts into one of Seen and Unseen , where Seen denotes the context with less than 3 unseen entities , and then measure the performance on the two subsets .", "entities": []}
{"text": "As shown in Figure 4 , we observe that the performance gain of KALA over the baselines is much larger on the Unseen subset , which demonstrates the effectiveness of KALA 's relational retrieval scheme to represent unseen entities .", "entities": []}
{"text": "DAPT also largely outperforms fine - tuning and TAPT as it is trained on an extremely large external corpus for adaptive pre - training .", "entities": []}
{"text": "However , KALA even outperforms DAPT in most cases , verifying that our knowledgeaugmentation method is more effective for tackling domain - specific tasks .", "entities": []}
{"text": "The visualization of embeddings of seen and unseen entities in Figure 2 shows that KALA embeds the unseen entities more closely to the seen entities 4 , which explains KALA 's good performance on the Unseen subset .", "entities": []}
{"text": "To better see how our KFM ( 3.2 ) works , we show the context and its fact , and then visualize representations from the PLM modulated by the KFM .", "entities": []}
{"text": "As shown in Figure 5 right , the token ' # # on ' is not aligned with their corresponding tokens , such as ' ex ' ( for exon ) and ' cod ' ( for codon ) , in the baseline .", "entities": []}
{"text": "However , with our feature modulation that transforms multiple tokens associated with the single entity equally , the two tokens ( e.g. , ( ' ex ' , ' # # on ' ) ) , composing one entity , are closely embedded .", "entities": []}
{"text": "Also , while the baseline can not handle the unseen entity consisting of three tokens : 're ' , ' # # tina ' , and ' # # l ' , KALA embeds them closely by representing the unseen retinal from the representation of its neighborhood gene derived by the domain knowledge - ( retinal , instance of , gene ) .", "entities": []}
{"text": "Our KALA framework is also applicable to encoder - decoder PLMs by applying the KFM to the encoder .", "entities": []}
{"text": "Therefore , we further validate KALA 's effectiveness on the encoder - decoder PLMs on the generative QA task ( Lee et al , 2021 ) with T5small ( Raffel et al , 2020 ) .", "entities": []}
{"text": "Table 7 shows that KALA largely outperforms baselines even with such a generative PLM .", "entities": []}
{"text": "In this paper , we introduced KALA , a novel framework for language model adaptation , which modulates the intermediate representations of a PLM by conditioning it with the entity memory and the relational facts from KGs .", "entities": []}
{"text": "We demonstrate that the success of KALA comes from both KFM and relational retrieval , allowing the PLM to recognize entities but also handle unseen ones that might frequently appear in domain - specific tasks .", "entities": []}
{"text": "There are many other avenues for future work , including the application of KALA on pre - training of knowledge - augmented PLMs from scratch .", "entities": []}
{"text": "Enhancing the domain converge of pre - traind language models ( PLMs ) with external knowledge is increasingly important , since the PLMs can not observe all the data during training and can not memorize all the necessary knowledge for solving down - stream tasks .", "entities": []}
{"text": "However , we have to still consider the accurateness of knowledge , i.e. , the fact in the knowledge graph may not be correct , which affects the model to generate incorrect answers .", "entities": []}
{"text": "Also , the model 's prediction performance is still far from optimal .", "entities": []}
{"text": "Thus , we should be aware of model 's failure from errors in knowledge and prediction , especially on high - risk domains ( e.g. , biomedicine ) .", "entities": []}
{"text": "Fine - tuned model \" text \" : \" Arvane Rezai \" , \" start \" : 30 , \" end \" : 43 , \" i d \" : 228998 \" h \" : 11578 , \" r \" : \" P3373 \" , \" t \" : 228998", "entities": []}
{"text": "In this work , we propose to use the Knowledge Graph ( KG ) that can define the relational information among entities that only appear in each dataset .", "entities": []}
{"text": "However , unfortunately , most of the task datasets do not contain such relational facts on its context , thus we need to construct them manually to obtain the knowledge graph .", "entities": []}
{"text": "In this section , we explain the way of constructing the knowledge graph that we used , consisting of facts of entities for each context in the task dataset .", "entities": []}
{"text": "To do so , we first need to extract entities and their corresponding mentions from the text , and then link it to the existing entities in wikidata ( Vrandecic and Kr\u00f6tzsch , 2014 ) .", "entities": []}
{"text": "In order to do this , we use the existing library named as spaCy 5 , and opensourced implementation of Entity Linker 6 .", "entities": []}
{"text": "To sum up , in our work , a set of entities E ( i ) and corresponding mentions M ( i ) for the given input x ( i ) are obtained through this step .", "entities": []}
{"text": "Regarding a concrete example , please see format ( a ) in Figure 6 .", "entities": []}
{"text": "In other words , we use the trained 5 https://spacy.io/ 6 https://github.com/egerber/spaCy - entity - linker RE model to build our own knowledge base ( KB ) instead of using the existing KG directly from the existing general - domain KB 7 .", "entities": []}
{"text": "For an example of the extracted fact , please see format ( b ) in Figure 6 .", "entities": []}
{"text": "In the example , \" h \" denotes the wikidata i d of the head entity , \" r \" denotes the wikidata i d of the extracted relation , and \" t \" denotes the wikidata", "entities": []}
{"text": "i d of the tail entity .", "entities": []}
{"text": "In general , the relation of top - 1 probability is used as the relation for the corresponding entity pair .", "entities": []}
{"text": "However , this approach sometimes results in predicting no_relation on most entity pairs .", "entities": []}
{"text": "Thus , to obtain more relations , we further use the relation of top - 2 probability in the case where no_relation has a top - 1 probability but the top - 2 probability is larger than a certain threshold ( e.g. , > 0.1 ) .", "entities": []}
{"text": "In Figure 6 , we summarize our KG construction pipeline .", "entities": []}
{"text": "In Table 8 , we report the hyperparameters related to our KG construction .", "entities": []}
{"text": "In this section , we introduce the detailed setups for our models and baselines used in Table 1 , 2 , and 4 .", "entities": []}
{"text": "We use the Pytorch ( Paszke et al , 2019 ) for the implementation of all models .", "entities": []}
{"text": "Also , to easily implement the language model , we use the huggingface library", "entities": []}
{"text": "( Wolf et al , 2020 ) containing various transformer - based pre - trained language models ( PLMs ) and their checkpoints .", "entities": []}
{"text": "Details for KALA In this paragraph , we describe the implementation details of the components , such as four linear layers in the proposed KFM , architectural specifications in the attentionbased GNN , and initialization of both the entity memory and relational embeddings , in the following .", "entities": []}
{"text": "For attention in our GNN , we mask the nodes of the null entity , so that the attention score becomes zero for them .", "entities": []}
{"text": "Moreover , to obtain the context representation of the entity ( See Footnote 3 in the main paper ) used in the GNN attention , we use the scatter operation 8 for reduced computational cost .", "entities": []}
{"text": "For Entity Memory , we experimentally found that initializing the embeddings of the entity memory with the contextualized features obtained from 8 https://github.com/rusty1s/pytorch_scatter the pre - trained language model could be helpful .", "entities": []}
{"text": "Therefore , the dimension of the entity embedding is set to the same as the language model d", "entities": []}
{"text": "= 768 .", "entities": []}
{"text": "For relation embeddings , we randomly initialize them , where the dimension size is set to 128 .", "entities": []}
{"text": "Location of KLM in the PLM Note that , the number and location of the KFM layers inside the PLM are hyperparameters .", "entities": []}
{"text": "However , Yue et al ( 2020 ) extract two major subsets by dividing the entire dataset into Relation and Medication and suggest the usage of sampled questions from the original em - rQA dataset .", "entities": []}
{"text": "Following the suggestion of Yue et al ( 2020 ) , we use only 1 % of generated questions of Relation for training , validation , and testing .", "entities": []}
{"text": "Also , we only use 1 % of generated questions of Medication for training and use 5 % of generated questions of Medication for validation and testing .", "entities": []}
{"text": "To overcome this limitation , Yue et al ( 2020 ) suggests two ways to make the task more difficult .", "entities": []}
{"text": "First , they divide the question templates into easy and hard versions and then use the hard question only .", "entities": []}
{"text": "We use both methods to Relation and Medication datasets to report the performance of every model .", "entities": []}
{"text": "For more details on Relation and Medication datasets , please refer to the original paper ( Yue et al , 2020 ) .", "entities": []}
{"text": "The statistics of training , validation , and test sets on all QA datasets are provided in Table 9 .", "entities": []}
{"text": "al , 2017 )", "entities": []}
{"text": "All experiments are constrained to be done with a single 12 GB Geforce RTX", "entities": []}
{"text": "2080 Ti GPU for fairness in terms of memory and the availability on the academic budget , except for the DAPT and generative QA which use a single 48 GB Quadro 8000 GPU .", "entities": []}
{"text": "KALA training needs 3 hours in wall time with a single GPU .", "entities": []}
{"text": "For all experiments , we select the best checkpoint on the validation set .", "entities": []}
{"text": "For the summary of training setups , please see Table 10 and 12 .", "entities": []}
{"text": "Fine - tuning Setup", "entities": []}
{"text": "Adaptive Pre - training Setup In this paragraph , we describe the experimental settings of adaptive pre - training baselines , namely TAPT , TAPT ( + RecAdam ) , and DAPT .", "entities": []}
{"text": "For QA tasks , we further pre - train the PLM for { 1 , 3 , 5 , 10 } epochs and then report the best performance among them .", "entities": []}
{"text": "For training with DAPT , we need an external corpus having a large amount of data for adaptive pre - training .", "entities": []}
{"text": "Thus , we first choose the datasets of two domains - News and Medical .", "entities": []}
{"text": "The size of pre - training data used in DAPT is much larger than TAPT .", "entities": []}
{"text": "Other experimental details are the same as TAPT described above .", "entities": []}
{"text": "In this subsection , we describe the details of architectural variants reported in Section 5.1 .", "entities": []}
{"text": "For all variants , we use the same KGs used in KALA .", "entities": []}
{"text": "Entity - as - Experts ( F\u00e9vry et al ( 2020 ) ; EaE ) utilizes the entity memory similar to our work , but they use the parametric dense retrieval more like the memory neural network ( Sukhbaatar et al , 2015 ) .", "entities": []}
{"text": "Similar to F\u00e9vry et al ( 2020 ) ;", "entities": []}
{"text": "Verga et al ( 2021 ) , we change the formulation of query and memory retrieval by using the mention representation of the entity from the intermediate hidden states of PLMs , which is formally defined as follows : h e = 1 m \u03c9 \u2212 m", "entities": []}
{"text": "\u03c9 i", "entities": []}
{"text": "With this retrieval , EaE also can represent the unseen entity e / E train if we know the mention boundary of the given entity on the context .", "entities": []}
{"text": "We believe it is expected to work well , if the entity memory is pre - trained on the enormous text along with the pre - training of the language model from the scratch .", "entities": []}
{"text": "However , it might underperform for the language model adaptation scenario , since it can fall into the problem of circular reasoning - the PLM does not properly represent the unseen entity , but it should predict which entity it is similar from the representation .", "entities": []}
{"text": "Regarding the integration of the knowledge from the entity memory into the PLM , the retrieved entity representation v is simply added ( Peters et al , 2019 ) to the hidden representations H after the transformer block as follows : H l = H l + h ( v ) ( 5 ) where h is Multi - Layer Perceptrons ( MLPs ) .", "entities": []}
{"text": "Also , instead of only providing the LM hidden states as an input , we concatenate the knowledge representation in Equation 3 to the LM hidden states .", "entities": []}
{"text": "ERNIE ( Zhang et al , 2019 ) is a notable PLM model that utilizes the external KB as an input for the language model .", "entities": []}
{"text": "The key feature of ERNIE can be summarized into two folds .", "entities": []}
{"text": "First , they use the multi - head self - attention scheme ( Vaswani et al , 2017 ) to contextualize the input entities .", "entities": []}
{"text": "Second , ERNIE fuses the entity representation at the end of the PLM by adding it to the corresponding language representation .", "entities": []}
{"text": "We assume that those two features are important points of ERNIE .", "entities": []}
{"text": "Then , we add it to a representation of the entity from the PLM , which is the same as the design in equation 5 . KT - Net ( Yang et al , 2019 ) uses knowledge as an external input in the fine - tuning stage for extractive QA .", "entities": []}
{"text": "Since they have a typical layer for integrating existing KB ( Miller , 1995 ; Carlson et al , 2010 ) with the PLM , we only adopt the self - matching layer as the architecture variant of the KFM layer used in our KALA framework .", "entities": []}
{"text": "The computation of the self - matching matrix in KT - Net is costly , i.e. , it requires a large computational cost that is approximately 12 times larger than KALA .", "entities": []}
{"text": "ERICA", "entities": []}
{"text": "We use the Entity Discrimination task from ERICA on the primary task of fine - tuning .", "entities": []}
{"text": "We would like to note that , as reported in Section 5 of the original paper ( Qin et al , 2021 ) , the use of ERICA on fine - tuning has no effect , since the size and diversity of entities and relations in downstream training data are limited .", "entities": []}
{"text": "Such limited information rather harms the performance , as it can hinder the generalization .", "entities": []}
{"text": "In this subsection , we give detailed descriptions of how the FLOPs in Figure 1 are measured .", "entities": []}
{"text": "For FLOPs computation of our KALA , we additionally include the FLOPs of the entity embedding layer , linear layers for h 1 , h 2 , h 3 , h 4 , and GNN layer .", "entities": []}
{"text": "Since the GNN layer is implemented based on the sparse implementation , we first calculate the FLOPs of the message propagation over one edge , and then multiply it to the average number of edges per node .", "entities": []}
{"text": "Also , in terms of the computation on mentions , we consider the maximum sequence length of the context rather than the average number of mentions , to set the upper bound of FLOPs for our KALA .", "entities": []}
{"text": "In this section , we provide the analyses on the forgetting of TAPT , entity memory , number of entities and facts , location of the KLM layer , and values of Gamma and Beta .", "entities": []}
{"text": "In Figure 1 , we observe that the performance of TAPT decreases as the number of training steps increases .", "entities": []}
{"text": "To get a concrete intuition on this particular phenomenon , we analysis what happens in the Pre - trained Language Model ( PLM ) , when we further pre - train it on the task - specific corpus .", "entities": []}
{"text": "Specifically , in Figure 7", "entities": []}
{"text": "In this subsection , we analyze how the size of entity memory affects the performance of our KALA .", "entities": []}
{"text": "Note that , we reduce the size of the entity memory by eliminating the entity appearing fewer times .", "entities": []}
{"text": "As shown in Figure 8 , we observe that the size of the entity memory is larger , the performance of our KALA is better in general .", "entities": []}
{"text": "However , interestingly , we also observe that the smallest size of the entity memory shows decent performance , which might be due to the fact that some parameters in the entity memory are stale .", "entities": []}
{"text": "For more discussions on it including visualization , please refer to Appendix D.2 .", "entities": []}
{"text": "Specifically , we first collect the contexts having more than or equal to the k number of entities ( or facts ) , and then calculate the performance difference from our KALA to the fine - tuning baseline .", "entities": []}
{"text": "As shown in Figure 9 , while there are no obvious patterns , performance improvements from the baseline are consistent across a varying number of entities and facts .", "entities": []}
{"text": "This result suggests that our KALA is indeed beneficial when entities and facts are given to the model , whereas the appropriate number of entities and facts to obtain the best performance against the baseline is different across datasets .", "entities": []}
{"text": "In the main paper and Appendix B.1 , we describe that the location of the KFM layer inside the PLM architecture is the hyperparameter .", "entities": []}
{"text": "However , someone might wonder which location of KFM yields the best performance , and what is the reason for this .", "entities": []}
{"text": "Also , we notice that the scale of values remain nearly around the mean point , which suggests that the small amount of shifting to intermediate hidden representations on transformer layers is enough to contribute to the performance gain , as we can see in the main results of Table 1 , 2 .", "entities": []}
{"text": "While we provide the efficiency on FLOPs in Figure 1 , we further provide the efficiency on GPU memory , wall time , and FLOPs for training each method in Table 6 .", "entities": []}
{"text": "Ti GPU on the same machine .", "entities": []}
{"text": "For our KALA , as we can flexibly manage the cost of GPU memory by reducing the number of entities in entity memory ( See Figure 8 with Appendix C.2 for more analysis on the effects of the size of entity memory ) , we provide the experimental results on two settings - KALA with memory size 0.2k and 62.8k ( full memory ) .", "entities": []}
{"text": "Here we provide the frequency distribution of entities , additional case studies , and more illustrations of textual examples and embedding spaces .", "entities": []}
{"text": "Similar to Figure 2 , we observe that all baselines fail to closely embed the unseen entities in the representation space of seen entities .", "entities": []}
{"text": "While this visualization result does not give a strong evidence of why our KALA outperforms other baselines , we clearly observe that KALA is beneficial to represent unseen entities in the feature space of seen entities , which suggests that such an advantage of our KALA helps the PLM to generalize over the test dataset , where the context contains unseen entities .", "entities": []}
{"text": "We visualize the frequency of entities in Figure 13 and 14 .", "entities": []}
{"text": "The entity frequency denotes the number of mentions of their associated entities within the entire text corpus of the training dataset .", "entities": []}
{"text": "This observation suggests that most of the elements in the entity memory are not utilized frequently .", "entities": []}
{"text": "In other words , only few entities are accurately trained with many training instances , whereas there exists the stale embeddings which are rarely updated .", "entities": []}
{"text": "This observation raises an interesting research question on the efficient usage of the entity memory , as we can see in Figure 8 that the small size of entity memory could result in the better performance ( See Appendix C.2 ) .", "entities": []}
{"text": "We leave the more in - depth analysis on the entity memory as the future work .", "entities": []}
{"text": "where almost all entities appear less than 10 times , while an extremely few numbers of entities appear very frequently .", "entities": []}
{"text": "The question in the example is \" who was kidnapped because of her neighbor \" .", "entities": []}
{"text": "We observe that DAPT answers this question as Araceli Valencia .", "entities": []}
{"text": "This prediction may come from matching the word ' her ' in the question to the feminine name ' Araceli Valencia ' in the context .", "entities": []}
{"text": "In contrast , our KALA predicts the Jaime Andrade as an answer , which is the ground truth .", "entities": []}
{"text": "We suspect that this might be because of the fact \" ( Jaime Andrade , spouse , Valencia ) \" in the knowledge graph , which relates the ' Valencia ' to the ' Jaime Andrade ' .", "entities": []}
{"text": "Although it is not clear how it directly affects the model 's performance , we can reason that KALA can successfully answer the question by utilizing the existing facts .", "entities": []}
{"text": "In Figure 16 and 17 , we visualize the examples of the context with its seen and unseen entities and its relational facts .", "entities": []}
{"text": "We first confirm that the quality of facts is moderate to use .", "entities": []}
{"text": "For instance , in the first example of Figure 16 , the fact in the context that Omar_bin_Laden is son of Osama_bin_Laden , is also appeared in the knowledge graph .", "entities": []}
{"text": "In addition , we observe that there are facts that link unseen entities to the seen entities in both Figure 16 and 17 .", "entities": []}
{"text": "Thus , while some of the facts in the knowledge graph are not accurate , we can represent the unseen entities with their relation to the seen entities .", "entities": []}
{"text": "We expect that there is a still room to improve in terms of the quality of KGs , allowing our KALA to modulate the entity representation more accurately .", "entities": []}
{"text": "We leave the study on this as the future work .", "entities": []}
{"text": "The adenomatous polyposis coli ( APC ) tumour - suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK - 3beta ) , axin / conductin and betacatenin .", "entities": []}
{"text": "( complex , subclass of , protein ) ( GSK , instance of , protein ) ( glycogen , instance of , protein ) ( APC , instance of , protein ) Context HLA typing for HLA - B27 , HLA - B60 , and HLA - DR1 was performed by polymerase chain reaction with sequence - specific primers , and zygosity was assessed using microsatellite markers .", "entities": []}
{"text": "( microsatellite , subclass of , primers ) ( DR1 , instance of , microsatellite ) ( microsatellite , subclass of , typing )", "entities": []}
{"text": "We identified four germline mutations in three breast cancer families and in one breast - ovarian cancer family .", "entities": []}
{"text": "among these were one frameshift mutation , one nonsense mutation , one novel splice site mutation , and one missense mutation .", "entities": []}
{"text": "The development of online media platforms has given users more opportunities to post and comment freely , but the negative impact of offensive language has become increasingly apparent .", "entities": []}
{"text": "It is very necessary for the automatic identification system of offensive language .", "entities": []}
{"text": "The test results on the official test data set confirm the effectiveness of our system .", "entities": []}
{"text": "With the development of the information society , people have become accustomed to uploading content on social media platforms in the form of text , pictures , or videos .", "entities": []}
{"text": "At the same time , they also comment on the content uploaded by other users and interact with each other , thus increasing the activity of social media platforms Mahesan , 2019 , 2020a , b ) .", "entities": []}
{"text": "Inevitably , however , some users will post offensive posts or comments .", "entities": []}
{"text": "The use of offensive discourse is a kind of impolite phenomenon which has negative effects on the civilization of the network community ( Chakravarthi , 2020 ) .", "entities": []}
{"text": "It usually has the characteristics of causing conflicts and the purpose of publishing intentionally .", "entities": []}
{"text": "The publisher of offensive language may use reproach , sarcasm , swear and other language means to achieve intentional offense , and express a variety of intentions , such as disturbing , provoking , and expressing negative emotions ( Chakravarthi and Muralidaran , 2021 ; Suryawanshi and Chakravarthi , 2021 ) .", "entities": []}
{"text": "Most people will take measures to respond to offensive words .", "entities": []}
{"text": "The way to respond to the direct conflict of offensive words is mainly rhetorical questions , swear , sarcasm and threat , so as to express dissatisfaction , deny and satirize the other party and provoke the other party .", "entities": []}
{"text": "This will further cause conflicts and destroy the harmony of the network environment .", "entities": []}
{"text": "Many social media platforms use a content review process , in which human reviewers check users ' comments for offensive language and other infractions , and which comments have been removed from the platform because of the violation ( Mandl et al , 2020 ) .", "entities": []}
{"text": "It is up to the moderator to decide which comments will be removed from the platform due to violations and which ones will be kept .", "entities": []}
{"text": "As the number of network users increases and user activity increases , the manual approach is undoubtedly inefficient .", "entities": []}
{"text": "Therefore , the automatic detection and identification of offensive content are very necessary .", "entities": []}
{"text": "However , offensive words often depend on the emotions and psychology of the listener , and some seemingly innocuous words can be potentially offensive , and words that often seem offensive are watered down by the emotions of the listener .", "entities": []}
{"text": "This kind of language phenomenon is not uncommon in real life , either unintentionally or deliberately used to achieve the speaker 's expected purpose , which is a challenging work for the current detection system .", "entities": []}
{"text": "This is a classification task at the comment / post level .", "entities": []}
{"text": "The goal of this task is to identify offensive language content of the code - mixed dataset of comments / posts in Dravidian Languages ( ( Tamil - English , Malayalam - English , and Kannada - English ) ) collected from social media .", "entities": []}
{"text": "Tamil language is the oldest language in Indian languages , Malayalam and Kannada evolved from Tamil language .", "entities": []}
{"text": "For a comment on Youtube , the system must classify it into not - offensive , offensive - untargeted , offensive - targeted - individual , offensive - targeted - group , offensive - targeted - other , or not - in - indented - language .", "entities": []}
{"text": "This method can combine the advantages of the two models to achieve a better classification effect .", "entities": []}
{"text": "The rest of the paper is divided into the following parts .", "entities": []}
{"text": "In the third part , we introduce the model structure and the composition of our training data .", "entities": []}
{"text": "The fourth part introduces our experimental setup and results .", "entities": []}
{"text": "The fifth part is the conclusion .", "entities": []}
{"text": "Due to the harm of offensive language to the network environment , the identification of offensive language has been carried out for a long time .", "entities": []}
{"text": "Research so far has focused on automating the decision - making process in the form of supervised machine learning for classification tasks ( Sun et al , 2019 ) .", "entities": []}
{"text": "As far back as 2012 , Chen et al ( 2012 ) proposed a lexical syntactic feature ( LSF ) framework to detect offensive content in social media , distinguished the roles of derogatory / profane and obscenity in identifying offensive content , and introduced handwritten syntax rules to identify abusive harassment .", "entities": []}
{"text": "Although English is currently one of the most commonly spoken languages in the world , work is ongoing to identify the offensive language in other languages that are less widely spoken .", "entities": []}
{"text": "Razavi et al ( 2010 ) proposed to extract features at different conceptual levels and apply multilevel classification for offensive language detection .", "entities": []}
{"text": "Pitsilis et", "entities": []}
{"text": "al ( 2018 ) proposed the ensemble of a recursive neural network ( RNN ) classifier , which combines various characteristics related to user - related information , such as the user 's sexist or racist tendencies , and was then fed to the classifier as input along with a word frequency vector derived from the text content .", "entities": []}
{"text": "When there is a large amount of labeled data , increasing the size and parameters of the model will definitely improve the performance of the model .", "entities": []}
{"text": "However , when the amount of training is relatively small , the large - scale model may not be able to achieve good results , so solving the problem of model training under the condition of a small amount of target data has become a research hotspot .", "entities": []}
{"text": "English was used as the source language and German , French , and Japanese were used as the target language to conduct the experiment in the field of cross - language sentiment classification .", "entities": []}
{"text": "We count the number of each type of tag in the training set and the validation set , and obtain the data distribution of Not - offensive , offensive - untargeted , offensive - targeted - individual , offensive - targeted - group , offensive - targeted - other , and Not - in - indented - language in Tamil , Malayalam , and Kannada .", "entities": []}
{"text": "as shown in Table 1 .", "entities": []}
{"text": "Specifically , a preprocessed CommonCrawl dataset of more than 2 TB based on 100 languages is used to train crosslanguage representations in a self - supervised manner .", "entities": []}
{"text": "This includes generating new unlabeled corpora for low - resource languages and expanding the amount of training data available for these languages by two orders of magnitude .", "entities": []}
{"text": "In the finetuning period , the multi - language tagging data is used based on the ability of the multi - language model to improve the performance of the downstream tasks .", "entities": []}
{"text": "Tune the parameters of the model to address cases where extending the model to more languages using cross - language migration limits the ability of the model to understand each language .", "entities": []}
{"text": "DPCNN ( Deep Pyramid Convolutional Neural Networks ) is a kind of deep word level CNN structure , the calculation amount of each layer of the structure decreases exponentially .", "entities": []}
{"text": "At the same time , the pyramid structure also enables the model to discover long - term dependencies in the text .", "entities": []}
{"text": "As shown in Figure 1 .", "entities": []}
{"text": "After adding the DPCNN module , we began to set the experimental parameters .", "entities": []}
{"text": "In the training process , we used five - fold stratified cross - validation to make the proportion of data of each category in each subsample the same as that in the original data and finally obtained the optimal result through the voting ( Onan et al , 2016 ) system , as shown in Figure 2 .", "entities": []}
{"text": "At the same time , the hierarchical crossvalidation method is used to improve the training effect .", "entities": []}
{"text": "The final results show that our model achieves satisfactory performance .", "entities": []}
{"text": "In future work , we will try to adjust the structure of the new model , so as to improve its effect more significantly .", "entities": []}
{"text": "Right for the Right Reason : Evidence Extraction for Trustworthy Tabular Reasoning", "entities": []}
{"text": "When pre - trained contextualized embeddingbased models developed for unstructured data are adapted for structured tabular data , they perform admirably .", "entities": []}
{"text": "However , recent probing studies show that these models use spurious correlations , and often predict inference labels by focusing on false evidence or ignoring it altogether .", "entities": []}
{"text": "To study this issue , we introduce the task of Trustworthy Tabular Reasoning , where a model needs to extract evidence to be used for reasoning , in addition to predicting the label .", "entities": []}
{"text": "As a case study , we propose a twostage sequential prediction approach , which includes an evidence extraction and an inference stage .", "entities": []}
{"text": "Our evidence extraction strategy outperforms earlier baselines .", "entities": []}
{"text": "On the downstream tabular inference task , using only the automatically extracted evidence as the premise , our approach outperforms prior benchmarks .", "entities": []}
{"text": "Reasoning on tabular or semi - structured knowledge is a fundamental challenge for today 's natural language processing ( NLP ) systems .", "entities": []}
{"text": "Today 's state - of - the - art for NLI over unstructured text uses contextualized embeddings ( e.g. , Devlin et al , 2019 ; Liu et al , 2019b ) .", "entities": []}
{"text": "When adapted for tabular NLI by flattening tables into synthetic sentences using heuristics , these models achieve remarkable performance on the datasets .", "entities": []}
{"text": "However , a recent study demonstrates that these models fail to reason prop - * Work done during an internship at Bloomberg Peter Henderson , Supertramp 1 H1 H1 :", "entities": []}
{"text": "Supertramp produced 1 an album that was less than an hour long 2 .", "entities": []}
{"text": "H2 :", "entities": []}
{"text": "H3 :", "entities": []}
{"text": "Hypotheses H1 are entailed by it , H2 is neither entailed nor contradictory , and H3 is a contradiction .", "entities": []}
{"text": "The Relevant column shows the hypotheses that use the corresponding row .", "entities": []}
{"text": "The colored text ( and superscripts ) in the table and hypothesis highlights relevance token level alignment .", "entities": []}
{"text": "erly on the semi - structured inputs in many cases .", "entities": []}
{"text": "For example , they can ignore relevant rows , and ( a ) focus on the irrelevant rows ( Neeraja et al , 2021 ) , ( b ) use only the hypothesis sentence ( Poliak et al , 2018 ; Gururangan et al , 2018 ) , or ( c ) knowledge acquired during pre - training ( Jain et al , 2021 ; .", "entities": []}
{"text": "In essence , they use spurious correlations between irrelevant rows , the hypothesis , and the inference label to predict labels .", "entities": []}
{"text": "This paper argues that existing NLI systems optimized solely for label prediction can not be trusted .", "entities": []}
{"text": "It is not sufficient for a model to be merely Right", "entities": []}
{"text": "but also Right for the Right Reasons .", "entities": []}
{"text": "In particular , at least identifying the relevant elements of inputs as the ' Right Reasons ' is essential for trustworthy reasoning 1 .", "entities": []}
{"text": "We address this issue by introducing the task of Trustworthy Tabular Inference , where the goal is to extract relevant rows as evidence and predict inference labels .", "entities": []}
{"text": "The figure also marks the rows needed to make decisions about each hypothesis , and also indicates the relevant tokens for each hypothesis .", "entities": []}
{"text": "For trustworthy tabular reasoning , in addition to predicting the label ENTAIL for H1 , CONTRADICT for H2 and NEU - TRAL for H3 , the model should also identify the evidence rows - namely , the rows Producer and Length for hypothesis H1 , Recorded for hypothesis H2 , Released and Recorded for hypothesis H3 .", "entities": []}
{"text": "As a first step , we propose a two - stage sequential prediction approach for the task , comprising of an evidence extraction stage , followed by an inference stage .", "entities": []}
{"text": "In the evidence extraction stage , the model extracts the necessary information needed for the second stage .", "entities": []}
{"text": "In the inference stage , the NLI model uses only the extracted evidence as the premise for the label prediction task .", "entities": []}
{"text": "Our best unsupervised evidence extraction method outperforms a previously developed baseline by 4.3 % , 2.5 % and 5.4 % absolute score on the three test sets .", "entities": []}
{"text": "The supervised model improves the evidence extraction performance by 8.7 % , 10.8 % , and 4.2 % absolute scores on the three test sets over the unsupervised approach .", "entities": []}
{"text": "Finally , for the full inference task , we demonstrate that our two - stage approach with best extraction , outperforms the earlier baseline by 1.6 % , 3.8 % , and 4.2 % on the three test sets .", "entities": []}
{"text": "In summary , our contributions are as follows 2 : We introduce the problem of trustworthy tabular reasoning and study a two - stage prediction approach that first extracts evidence and then predicts the NLI label .", "entities": []}
{"text": "We investigate a variety of unsupervised evidence extraction techniques .", "entities": []}
{"text": "Our unsupervised approach for evidence extraction outperforms the previous methods .", "entities": []}
{"text": "We demonstrate that our two - stage technique with best extraction outperforms all the prior benchmarks on the downstream NLI task .", "entities": []}
{"text": "We begin by introducing the task and the datasets we use .", "entities": []}
{"text": "Tabular Inference is a reasoning task that , like conventional NLI ( Dagan et al , 2013 ; Bowman et al , 2015 ; Williams et al , 2018 ) , asks whether a natural language hypothesis can be inferred from a tabular premise .", "entities": []}
{"text": "Concretely , given a premise table T with m rows { r 1 , r 2 , . . . , r m } , and a hypothesis sentence H , the task maps them to ENTAIL ( E ) , CONTRADICT ( C ) or NEUTRAL ( N ) .", "entities": []}
{"text": "We can denote the mapping as f ( T , H ) y ( 1 ) where , y { E , N , C } .", "entities": []}
{"text": "For example , for the tabular premise in Figure 1 , the model should predict E , C , and N for the hypotheses H1 , H2 , and H3 , respectively .", "entities": []}
{"text": "Trustworthy Tabular Inference is a table reasoning problem that seeks not just the NLI label , but also relevant evidence from the input table that supports the label prediction .", "entities": []}
{"text": "We use T R , a subset of T , to denote the relevant rows or evidence .", "entities": []}
{"text": "Then , the task is defined as follows .", "entities": []}
{"text": "f ( T , H ) { T R , y } ( 2 )", "entities": []}
{"text": "In our example table , this task will also indicate the evidence rows T R of Producer and Length for hypothesis H1 , Recorded for hypothesis H2 , and Released and Recorded for hypothesis H3 .", "entities": []}
{"text": "While the notion of evidence is well - defined for the ENTAIL and CONTRADICT labels , the NEU - TRAL label requires explanation .", "entities": []}
{"text": "To decide on the NEUTRAL label , one must first search for relevant rows ( if any ) , i.e. , identify evidence in the premise tables .", "entities": []}
{"text": "In fact , this is a causally correct sequential approach .", "entities": []}
{"text": "For example , in our example table , the premise table indicates that the album was recorded in 1978 , emphasizing the importance of the Recorded row for the hypothesis H2 .", "entities": []}
{"text": "For NEUTRAL examples , we refer to any such pertinent rows as evidence .", "entities": []}
{"text": "Dataset Details .", "entities": []}
{"text": "There are several datasets for tabular NLI :", "entities": []}
{"text": "The dataset consists of 23 , 738 premisehypothesis pairs collected via crowdsourcing on Amazon MTurk .", "entities": []}
{"text": "The tabular premises are based on 2 , 540 Wikipedia Infoboxes representing twelve diverse domains , and the hypotheses are short statements paired with NLI labels .", "entities": []}
{"text": "All tables contain a title followed by two columns ( cf .", "entities": []}
{"text": "Figure 1 ) ; the left columns are keys and the right ones are values ) .", "entities": []}
{"text": "The dev and test set , comprising of 7200 table - hypothesis pairs , were recently extended with crowdsourced evidence rows .", "entities": []}
{"text": "As one of our contributions , we describe the evidence rows annotation for the training set in the next Section 3 .", "entities": []}
{"text": "We followed the protocol of : one table and three distinct hypotheses formed a HIT .", "entities": []}
{"text": "For each of the hypotheses , five annotators would select the evidence rows .", "entities": []}
{"text": "We divide the tasks equally into 110 batches , each batch having 51 HITs each having three examples .", "entities": []}
{"text": "To reduce bias induced by a link between the NLI label and row selection , we do not reveal the labels to the annotators .", "entities": []}
{"text": "The rows of an Infobox table are semantically distinct , though all connected to the title entity .", "entities": []}
{"text": "Each row can be considered a separate and uniquely distinct source of information about the title entity .", "entities": []}
{"text": "Because of this property , the problem of evidence extraction is well - formed as relevant row selection .", "entities": []}
{"text": "The same is not valid for unstructured text , whose units of information may be tokens , phrases , sentences or entire paragraphs , and is typically unavailable ( Ribeiro et al , 2020 ;", "entities": []}
{"text": "Goel et al , 2021 ; Mishra et al , 2021 ; Yin et", "entities": []}
{"text": "al , 2021 ) .", "entities": []}
{"text": "Trustworthy inference has an intrinsic sequential causal structure : extract evidence first , then predict the inference label using the extracted evidence data , knowledge / common sense , and perhaps formal reasoning ( Herzig et al , 2021 ; Paranjape et al , 2020 ) 7 .", "entities": []}
{"text": "To operationalize this intuition , we chose a two - stage sequential approach which consists of an evidence extraction followed by the NLI classi - fication , as shown in Figure 2 . Notation .", "entities": []}
{"text": "The function f in Eq . 2 can be rewritten with functions g and h , f ( . )", "entities": []}
{"text": "=", "entities": []}
{"text": "g ( . ) , h g ( . ) , as f ( T , H ) = { g ( T , H ) , h ( g ( T , H ) , H ) } ( 3 )", "entities": []}
{"text": "Here , g extracts the evidence rows T R subset of T , and h uses the extracted evidence T R and the hypothesis H to predict the inference label y , as g", "entities": []}
{"text": "( T , H ) T R h ( T R , H ) y ( 4 )", "entities": []}
{"text": "To obtain f , we need to define the functions g and h , and a flexible representation of a semi - structured .", "entities": []}
{"text": "We explore unsupervised ( $ 4.1 ) and supervised ( $ 4.2 ) methods for the evidence row extractor g.", "entities": []}
{"text": "The unsupervised approaches extract Top - K rows are based on relevance scores , where K is a hyperparameter .", "entities": []}
{"text": "We use the cosine similarity between the row and the hypothesis sentence representations to score rows .", "entities": []}
{"text": "We study three ways to define relevance described next .", "entities": []}
{"text": "We employ three modifications to improve DRR .", "entities": []}
{"text": "For most instances , the number of relevant rows ( K ) is much lower than the total number of rows ( m ) ; most examples have only one or two relevant rows .", "entities": []}
{"text": "We constrained the sparsity in the extraction by capping the value of K to S m.", "entities": []}
{"text": "We use a threshold \u03c4 to select rows dynamically Top - K \u03c4 based on the hypothesis , rather than always selecting fixed K rows .", "entities": []}
{"text": "We only select rows whose similarity ( after Re - Ranking ) to the hypothesis sentence representations is greater than a threshold \u03c4 .", "entities": []}
{"text": "We adopt this strategy because ( a ) the number of rows in the premise table can vary across examples , and ( b ) different hypotheses may require a differing number of evidence rows .", "entities": []}
{"text": "This approach consists of two parts ( a ) aligning rows and hypothesis words , and ( b ) then computing cosine similarity between the aligned words .", "entities": []}
{"text": "Specifically , we use the SimAlign ( Jalili Sabet et al , 2020 ) method for word - level alignment .", "entities": []}
{"text": "SimAlign uses static and contextualized embeddings without parallel training data to get word alignments .", "entities": []}
{"text": "Among the approaches explored by SimAlign , we use the Match ( mwmf ) method , which uses maximum - weight maximal matching in the bipartite weighted network formed by the word level similarity matrix .", "entities": []}
{"text": "Our choice of this approach over the other greedy methods ( Itermax and Argmax ) is motivated by the fact that it finds the global optimum matching , while the other two do not .", "entities": []}
{"text": "Furthermore , because all rows use the same title , we assign title matching terms zero weight .", "entities": []}
{"text": "This paper refers to this method as SimAlign ( Match ( mwmf ) ) .", "entities": []}
{"text": "The approach we saw in $ 4.1.2 defines rowhypothesis similarity using word alignments .", "entities": []}
{"text": "We explore two options here .", "entities": []}
{"text": "Thakur et al , 2021 ; Wang et al , 2021a ) , which use Siamese neural networks ( Koch et al , 2015 ; Chicco , 2021 ) .", "entities": []}
{"text": "We explore several pre - trained sentence transformers models 9 for sentence representation .", "entities": []}
{"text": "These models differ in ( a ) the data used for pre - training , ( b ) the main model type and it size , and ( c ) the maximum sequence length .", "entities": []}
{"text": "The former is trained to take an input sentence and reconstruct it using standard dropout as noise .", "entities": []}
{"text": "To avoid misleading matches between the hypothesis tokens and those in the premise title , we swap the hypothesis title tokens with a single token title from another randomly selected table of the same category .", "entities": []}
{"text": "We again use the sparsity and dynamic selection as earlier .", "entities": []}
{"text": "The supervised evidence extraction procedure consists of three aspects : Dataset Construction .", "entities": []}
{"text": "We use the annotated relevant row data ( $ 3 ) to construct a supervised extraction training dataset .", "entities": []}
{"text": "Every row in the table , paired with the hypothesis , is associated with a binary label signifying whether the row is relevant or not .", "entities": []}
{"text": "As before , we use the sentences from Better Paragraph Representation ( BPR ) ( Neeraja et al , 2021 ) to represent each row .", "entities": []}
{"text": "Label Balancing .", "entities": []}
{"text": "Our annotation , and the perturbation probing analysis of 10 , show that the number of irrelevant rows can be much larger than the relevant ones for a tablehypothesis pair .", "entities": []}
{"text": "Therefore , if we use all irrelevant rows from tables as negative examples , the resulting training set would be imbalanced , with about 6\u00d7 more irrelevant rows than relevant rows .", "entities": []}
{"text": "We investigate several label balancing strategies by sub - sampling irrelevant rows for training .", "entities": []}
{"text": "We explore the following schemes : ( a ) taking all irrelevant rows from the table without sub - sampling ( on average 6\u00d7 more irrelevant rows ) referred to as Without Sample ( 6\u00d7 ) , ( b ) randomly sampling unrelated rowsin the same proportion as relevant rows , referred to as Random Negative ( 1\u00d7 ) , ( c ) using the unsupervised DRR ( Re - Rank + Top - S \u03c4 ) method to pick the irrelevant rows that are most similar to the hypothesis , in equal proportion as the relevant rows , referred to as Hard Negative ( 1\u00d7 ) , and ( d ) same as ( c ) , except picking three times as many irrelevant rows , referred to as Hard Negative ( 3\u00d7 ) 11 .", "entities": []}
{"text": "Classifier Training .", "entities": []}
{"text": "For the downstream NLI task , the function h is a two - sentence classifier whose inputs are T R ( the rows selected by g ) and the hypothesis H. We use BPR to represent T R as we did for the full table T.", "entities": []}
{"text": "Our experiments assess the efficacy of evidence extraction ( $ 4 ) and its impact on the downstream NLI task by studying the following questions : RQ1 : What is the efficacy of unsupervised approaches for evidence extraction ?", "entities": []}
{"text": "( $ 5.2 )", "entities": []}
{"text": "First , we briefly summarize the models used in our experiments .", "entities": []}
{"text": "We investigate both unsupervised ( $ 4.1 ) and supervised ( $ 4.2 ) evidence extraction methods .", "entities": []}
{"text": "We use only the extracted evidence as the premise for the tabular inference task ( $ 4.3 ) .", "entities": []}
{"text": "We compare both tasks against human performance .", "entities": []}
{"text": "As baselines , we use the Word Mover Distance ( WMD ) of and the original DRR ( Neeraja et al , 2021 ) with Top - 4 extracted evidence rows .", "entities": []}
{"text": "For DRR ( Re - Rank + Top - S \u03c4 ) , which uses static embeddings , we set the sparsity parameter S = 2 , and the dynamic row selection parameter \u03c4 = 1.0 .", "entities": []}
{"text": "This choice is based on performance on the development set .", "entities": []}
{"text": "We compare the following settings : ( a ) WMD Top - 3 from , ( b )", "entities": []}
{"text": "No extraction i.e. using the full premise table with the \" para \" representation from , ( c ) DRR Top - 4 , ( d ) DRR ( Re - Rank + Top - 2", "entities": []}
{"text": "( \u03c4 = 1 ) ) for training , de - velopment and test sets , ( e ) training a supervised classifier with a human oracle i.e. annotated evidence extraction as discussed in $ 3 , and using the best extraction model , i.e. supervised evidence extraction with Hard Negative ( 3\u00d7 ) for the test sets , and ( f ) the human oracle across the training , development , and test sets .", "entities": []}
{"text": "Unsupervised evidence extraction .", "entities": []}
{"text": "For RQ1 , Table 2 shows the performance of unsupervised methods .", "entities": []}
{"text": "Among the static embedding cases , DRR ( Re - Rank + Top - 2 ( \u03c4 = 1 ) ) sees substantial performance improvement over the original DRR baseline .", "entities": []}
{"text": "Overall , the idea of using Top - S \u03c4 , i.e. , using the dynamic number of rows prediction and Re - Rank ( exact - match based re - ranking ) is beneficial .", "entities": []}
{"text": "Furthermore , introducing sparsity with Top - S \u03c4 , i.e. considering only the Top - 2 rows ( S=2 ) and dynamic row selection ( \u03c4 = 1 ) substantially enhances evidence extraction precision .", "entities": []}
{"text": "outperforms DRR ( Re - Rank + Top - 2", "entities": []}
{"text": "Supervised evidence extraction .", "entities": []}
{"text": "For RQ2 , Table 4 shows the performance of the supervised relevant row extraction approaches that use binary classifiers trained with several sampling techniques for irrelevant rows .", "entities": []}
{"text": "Overall , adding supervision is advantageous 13 .", "entities": []}
{"text": "Furthermore , we observe that using the unsupervised DRR technique to extract challenging irrelevant rows , i.e. , Hard Negative , is more effective than random sampling .", "entities": []}
{"text": "Indeed , using random negative examples as the irrelevant rows performs the worst .", "entities": []}
{"text": "Not sampling ( 6\u00d7 ) or using only one irrelevant row , namely Hard Negative ( 1\u00d7 ) , also underperforms .", "entities": []}
{"text": "We see that employing moderate sampling , i.e. , Hard Negative ( 3\u00d7 ) , performs best across all test sets .", "entities": []}
{"text": "14", "entities": []}
{"text": "13", "entities": []}
{"text": "We investigate \" How much supervision is adequate ? \" in Appendix A. 14", "entities": []}
{"text": "Although \u03b12 is adversarial owing to label flipping , rendering the NLI task more difficult , both \u03b11 and \u03b12 have instances with the same domain tables and hypotheses with similar reasoning types , making the relevant row extraction task equally challenging .", "entities": []}
{"text": "For RQ3 , we investigate how using only extracted evidence as a premise impacts the performance of the tabular NLI task .", "entities": []}
{"text": "Table 3 shows the results .", "entities": []}
{"text": "Furthermore , using human extracted ( Oracle ) rows for both training and testing sets outperforms all models - based extraction methods .", "entities": []}
{"text": "Overall , these findings indicate that extracting evidence is beneficial for reasoning in tabular inference task .", "entities": []}
{"text": "Despite using human extracted ( Oracle ) rows for both training and testing , the NLI model still falls far behind human reasoning ( Human NLI ) .", "entities": []}
{"text": "We perform an error analysis of how well our proposed supervised extraction model ( Hard Negative ( 3x ) ) performs compared to the human annotators .", "entities": []}
{"text": "The model makes two types of errors : a Type I error occurs when an evidence row is marked as irrelevant , whereas Type II error occurs when an irrelevant row is marked as evidence .", "entities": []}
{"text": "A Type I error will reduce the model 's precision for the extraction model , whereas a Type II error will decrease the model 's recall .", "entities": []}
{"text": "Type I errors are especially concerning for the downstream NLI task .", "entities": []}
{"text": "Since mislabeled evidence rows will be absent from the extracted premise , necessary evidence will be omitted , leading to inaccurate entailment labels .", "entities": []}
{"text": "On the other hand , with Type II errors , when an irrelevant row is labeled as evidence , the model has to deal with from extra noise in the premise .", "entities": []}
{"text": "However , all the required evidence remains .", "entities": []}
{"text": "Table 5 shows a comparison of the supervised extraction ( Hard Negative ( 3x ) )", "entities": []}
{"text": "approach with the ground truth human labels on all the three test sets for both error types .", "entities": []}
{"text": "Appendix C provides several examples of both types of errors .", "entities": []}
{"text": "Why Sequential Prediction ?", "entities": []}
{"text": "Our choice of the sequential paradigm is motivated by the observation that it enforces a causal structure .", "entities": []}
{"text": "Of course , a joint or a multi - task model may make better predictions .", "entities": []}
{"text": "However , these models ignore the causal relationship between evidence selection and label prediction ( Herzig et al , 2021 ; Paranjape et al , 2020 ) .", "entities": []}
{"text": "Ideally , each row is independent and , its relevance to the hypothesis can be determined on its own .", "entities": []}
{"text": "In a joint or a multi - task model that exploits correlations across rows and the final label , irrelevant rows and the NLI label , can erroneously influence row selection .", "entities": []}
{"text": "Future Directions .", "entities": []}
{"text": "Based on the observations and discussions , we identify the future directions as follows .", "entities": []}
{"text": "( 1 ) Joint Causal Model .", "entities": []}
{"text": "To build a joint or a multi - task model that follows the causal reasoning structure , significant changes in model architecture are required .", "entities": []}
{"text": "Such a model would first identify important rows and then use them for NLI predictions , but without risking spurious correlations .", "entities": []}
{"text": "( 2 ) How much Supervision is Needed ?", "entities": []}
{"text": "But do we need full supervision for all examples ?", "entities": []}
{"text": "Is there any lower limit to supervision ?", "entities": []}
{"text": "We partially answered this question in the affirmative by training the evidence extraction model with limited supervision ( semi - supervised setting ) , but a deeper analysis is needed to understand the limits .", "entities": []}
{"text": "See Appendix A for details .", "entities": []}
{"text": "( 3 ) Improving Zero - shot Domain Performance .", "entities": []}
{"text": "( 4 ) Finally , inspired by Neeraja et al ( 2021 ) , we may be able to add explicit knowledge to improve evidence extraction .", "entities": []}
{"text": "Yu et al ( 2018 ; and Neeraja et al ( 2021 ) study pre - training for improving tabular inference .", "entities": []}
{"text": "Interpretability and Explainability Model interpretability can either be through explanations or by identifying the evidence for the predictions ( Feng et al , 2018 ; Serrano and Smith , 2019 ; Jain and Wallace , 2019 ; Wiegreffe and Pinter , 2019 ; DeYoung et al , 2020 ; Paranjape et al , 2020 ) .", "entities": []}
{"text": "Additionally , NLI models ( e.g. Ribeiro et al , 2016Ribeiro et al , , 2018aZhao et al , 2018 ; Glockner et al , 2018 ; Naik et al , 2018 ; McCoy et al , 2019 ; Nie et al , 2019 ; Liu et al , 2019a ) must be subjected to numerous test sets with adversarial settings .", "entities": []}
{"text": "Our work focuses on the extraction of label - independent evidence for correct inference , rather than on the generation of abstractive explanations for a given label .", "entities": []}
{"text": "Comparison with Shared Tasks The Se - mEval'21 Task 9 ( Wang et al , 2021b ) and FEVEROUS'21 shared task ( Aly et al , 2021 ) are conceptually close to this work .", "entities": []}
{"text": "The SemEval task focuses on statement verification and evidence extraction using relational tables from scientific articles .", "entities": []}
{"text": "The FEVEROUS'21 shared task focuses on verifying information using unstructured and structured evidence from open - domain Wikipedia .", "entities": []}
{"text": "Our approach concerns evidence extraction from a single table rather than open - domain document , table or paragraph retrieval .", "entities": []}
{"text": "In this paper , we introduced the problem of Trustworthy Tabular Inference , where a reasoning model both extracts evidence from a table and predicts an inference label .", "entities": []}
{"text": "We studied a two - stage approach , comprising an evidence extraction and an inference stage .", "entities": []}
{"text": "We explored several unsupervised and supervised strategies for evidence extraction , several of which outperformed prior benchmarks .", "entities": []}
{"text": "Finally , we showed that by using only extracted evidence as the premise , our approach outperforms previous baselines on the downstream tabular inference task .", "entities": []}
{"text": "To simulate semi - supervision settings , we randomly sample 10 % , 20 % , 30 % , 40 % , and 50 % example instances of the train set in an incremental fashion for model training , where we repeat the random samplings three times .", "entities": []}
{"text": "We discovered that adding some supervision had advantages over not having any supervision .", "entities": []}
{"text": "One key issue we observe is the lack of a visible trend due to significant variation produced by random data sub - sampling .", "entities": []}
{"text": "Since many hypothesis sentences ( especially those with neutral labels ) require out - of - table information for inference , we introduced the option to choose out - of - table ( OOT ) pseudo rows , which are highlighted only when the hypothesis requires information that is not common ( i.e. common sense ) and missing from the table .", "entities": []}
{"text": "To reduce any possible bias due to unintended associations between the NLI label and the row selections ( e.g. , using OOT for neutral examples ) , we avoid showing inference labels to the annotators 15 .", "entities": []}
{"text": "To assess an annotator , we compare their annotations with the majority consensus of other annotators ' ( four ) annotations .", "entities": []}
{"text": "We perform this comparison at two levels : ( a ) local - consensus - score on the most recent batch , and ( b ) cumulative - consensusscore on all batches annotated thus far .", "entities": []}
{"text": "We use these consensus scores to temporarily ( local - consensus - score ) or permanently ( cumulative score ) block the poor annotators from the task .", "entities": []}
{"text": "We also review the annotations manually and provide feedback with more detailed instructions and personalized examples for annotators who were making mistakes due to ambiguity in the task .", "entities": []}
{"text": "We give incentives to annotators who received high consensus scores .", "entities": []}
{"text": "As in previous work , we removed certain annotators ' annotations that have a poor consensus score ( cumulative score ) and published a second validation HIT to double - check each data point if necessary .", "entities": []}
{"text": "We manually inspect the Type I and Type II error instances for the supervised model and human annotation for the development set .", "entities": []}
{"text": "Below , we show some of these examples where models conflict with ground - truth human annotation .", "entities": []}
{"text": "We also provide a possible reason behind the model mistakes .", "entities": []}
{"text": "Example III Row : The trainer of Caveat is Woody Stephens .", "entities": []}
{"text": "Hypothesis :", "entities": []}
{"text": "Caveat won more in winnings than it took to raise and train him .", "entities": []}
{"text": "Model Prediction : Relevant Evidence Human Ground Truth : Not Relevant .", "entities": []}
{"text": "Possible Reason :", "entities": []}
{"text": "Model connects the ' raise and train ' term with the trainer name which is unrelated and has no connection to overall , winning races money vs spending for animal .", "entities": []}
{"text": "Discussion Based on the observation from the above examples as also stated in $ 5.3 , the model fails on many examples due to its lack of knowledge and common - sense reasoning ability .", "entities": []}
{"text": "One possible solution to mitigate this is by the addition of implicit and explicit knowledge on - the - fly for evidence extraction , as done for inference task by Neeraja et al ( 2021 ) .", "entities": []}
{"text": "We manually examine the human - annotated evidence in the development set .", "entities": []}
{"text": "We discovered the existence of several relevant phrases / tokens which implicitly indicate the presence of evidence rows .", "entities": []}
{"text": "E.g. The existence of tokens such as married , husband , lesbian , and wife in hypothesis ( H ) is very suggestive of the row Spouse being the relevant evidence .", "entities": []}
{"text": "Learning such implicit relevance - based phrases and tokens connection is easy for humans and large pre - trained supervision models .", "entities": []}
{"text": "It is a challenging task for similarity - based unsupervised extraction methods .", "entities": []}
{"text": "Below , we show implicit relevance , indicating token and the corresponding relevant evidence rows .", "entities": []}
{"text": "Relevance Indicating Phrase ( H )", "entities": []}
{"text": "Relevant Evidence Rows Key ( T )", "entities": []}
{"text": "The authors thank Bloomberg 's AI Engineering team , especially Ketevan Tsereteli , Anju Kambadur , and Amanda Stent for helpful feedback and directions .", "entities": []}
{"text": "We also appreciate the useful feedback provided by Ellen Riloff and the Utah NLP group .", "entities": []}
{"text": "Additionally , we appreciate the helpful inputs provided by Atreya Ghosal , Riyaz A. Bhat , Manish Srivastava , and Maneesh Singh .", "entities": []}
{"text": "Vivek Gupta acknowledges support from Bloomberg 's Data Science Ph.D. Fellowship .", "entities": []}
{"text": "This work was supported in part by National Science Foundation grants # 1801446 ( SaTC ) and # 1822877 ( Cyberlearning ) .", "entities": []}
{"text": "Finally , we would like to express our gratitude to the reviewing team for their insightful comments .", "entities": []}
{"text": "X575 : writing rengas with web services", "entities": []}
{"text": "Our software system simulates the classical collaborative Japanese poetry form , renga , made of linked haikus .", "entities": []}
{"text": "We used NLP methods wrapped up as web services .", "entities": []}
{"text": "This approach is suitable for collaborative human - AI generation , as well as purely computer - generated poetry .", "entities": []}
{"text": "Evaluation included a blind survey comparing AI and human haiku .", "entities": []}
{"text": "To gather ideas for future work , we examine related research in semiotics , linguistics , and computing .", "entities": []}
{"text": "Computer haikus have been explored in practice at least since Lutz ( 1959 ) .", "entities": []}
{"text": "More recently , haikus have been used by Ventura ( 2016 ) as the testbed for a thought experiment on levels of computational creativity .", "entities": []}
{"text": "As we will discuss below , the classic haiku traditionally formed the starting verse of a longer poetry jam , resulting in a poem called a renga .", "entities": []}
{"text": "A computational exploration of renga writing allows us to return to some of the classical ideas in Japanese poetry via thoroughly modern ideas like concept blending and collaborative AI .", "entities": []}
{"text": "Ventura 's creative levels range from randomisation to plagiarisation , memorisation , generalisation , filtration , inception 1 and creation .", "entities": []}
{"text": "Further gradations and criteria could be advanced , for example , the fitness function used for filtration could be developed and refined as the system learns .", "entities": []}
{"text": "Creativity might be assessed in a social context , as we investigate how a system collaborates .", "entities": []}
{"text": "While self - play was a good way for the recently developed board game - playing system AlphaGo to transcend its training data ( Silver et al , 2016 ) , we do not yet have computationally robust qualitative evaluation measures for the poetry domain , where there is no obvious \" winning condition . \"", "entities": []}
{"text": "We began by creating a program for generating haikus , trained on a small corpus .", "entities": []}
{"text": "Our technical aim then was to simulate the collaborative creation of renga , i.e. , linked haikus .", "entities": []}
{"text": "There are several forms of renga with varying constraints ( Carley , 2015 ) , for example the 20 stanza \" Nijiun \" renga which alternates between twoline and three - line verses , with a focus on seasonal symbolism and rules against repetition .", "entities": []}
{"text": "2 Our initial effort was a technical success , however the rengas we produced fail to fully satisfy classical constraints .", "entities": []}
{"text": "A subsequent experiment is more convincing in this regard , but still leaves room for improvement .", "entities": []}
{"text": "Our discussion considers the aesthetics of the generated poems and outlines directions for future research .", "entities": []}
{"text": "Coleridge considered poetry to be \" the blossom and the fragrance of all human knowledge . \"", "entities": []}
{"text": "AI researcher Ruli Manurung defines poetry somewhat more drily : \" A poem is a natural language artefact which simultaneously fulfils the properties of meaningfulness , grammaticality and poeticness \" ( Manurung , 2004 , p. 8 ) .", "entities": []}
{"text": "The haiku as we know it was originally called hokku - \u767a\u53e5 , literally the \" starting verse \" of a collaboratively written poem , hakai no renga .", "entities": []}
{"text": "Typically each of following links in a renga take the familiar 5/7/5 syllable form .", "entities": []}
{"text": "Classical rengas vary in length from two to 100 links ( and , rarely , even 1000 ) .", "entities": []}
{"text": "The starting verse is traditionally comprised of two images , with a kireji - a sharp cut - between them .", "entities": []}
{"text": "The term haiku introduced by the 19th Century poet Masaoka Shiki supersedes the older term .", "entities": []}
{"text": "Stylistically , a haiku captures a moment .", "entities": []}
{"text": "In classical renga , all of the verses after the first have additional complex constraints , such as requiring certain images to be used at certain points , but disallowing repetition , with various proximity constraints .", "entities": []}
{"text": "The setting in which rengas were composed is also worth commenting on .", "entities": []}
{"text": "A few poets would compose together in party atmosphere , with one honoured guest proposing the starting haiku , then the next responding , and continuing in turn , subject to the oversight of a scribe and a renga master .", "entities": []}
{"text": "These poetry parties were once so popular and time consuming that they were viewed as a major decadence .", "entities": []}
{"text": "Jin'Ichi", "entities": []}
{"text": "et al ( 1975 ) offers a useful overview .", "entities": []}
{"text": "Because of the way we 've constructed our haiku generating system , it can take an entire haiku as its input topic - we just add the word vectors to make a topic model - and compose a response .", "entities": []}
{"text": "This affords AI - to - AI collaboration , or AI - human collaboration .", "entities": []}
{"text": "It can also blend two inputs - for example , the previous haiku and the current constraint from the renga ruleset ( e.g. , the requirement to allude to \" cherry blossoms \" or \" the moon \" ) .", "entities": []}
{"text": "Working with a small haiku corpus , we used a POS tagger to reveal the grammatical structure typical to haikus .", "entities": []}
{"text": "The CMU Pronouncing Dictionary is used to count syllables of words that fill in this structure .", "entities": []}
{"text": "3", "entities": []}
{"text": "The Brown corpus was used to generate n - grams , and the generation process prefers more common constructions in haikus .", "entities": []}
{"text": "5", "entities": []}
{"text": "Adding a web API turned the haiku generating system into a haiku server , and facilitated subsequent work with FloWr .", "entities": []}
{"text": "Details of a surveybased blind comparison of human and computerwritten haikus were written up by Aji ( 2015 ) .", "entities": []}
{"text": "The system was then extended with multiple inputs , in some cases producing interesting blends : e.g. , the following in response to \" frog pond \" and \" moon \" : that gull in the dressvivacious in statue from so many ebbs II .", "entities": []}
{"text": "Generation of rengas Here are two rengas generated by wrapping the haiku API inside the FloWr flowchart system ( Charnley et al , 2016", "entities": []}
{"text": "In each case , the prompt for the first link is \" flower blossom \" and each link is passed on to the next link along with a secondary prompt .", "entities": []}
{"text": "The secondary links are \" moon , \" \" autumn , \" and \" love , \" respectively .", "entities": []}
{"text": "For the first renga , we designed a flowchart that selects the \" most positive \" haiku from the ten that the haiku API returns , using the AFINN word list .", "entities": []}
{"text": "6 In the second renga , we designed a flowchart to select the haiku with the lowest word variety ( computed in terms of Levenshtein distance ) .", "entities": []}
{"text": "We made improvements to the use of the Brown corpus to utilise n - grams for word - flow and sense , as well as tuning the weightings given to sense and topic .", "entities": []}
{"text": "We implemented the injection of topics via by blending , as per classical constraints ( e.g. , required seasonal themes like \" winter , \" or \" flowers \" in the penultimate link ) .", "entities": []}
{"text": "At left , we quote the closing links of the first Nijiun renga generated by our software .", "entities": []}
{"text": "Towards automated evaluation Some aspects of the evaluation dimensions are built into the way the poems are constructed .", "entities": []}
{"text": "Surface form scales up well for rengas .", "entities": []}
{"text": "Sense : the haiku generating subsystem uses an ngram model of text likelihood , which will yield a higher score for constructions that match frequently observed phrases .", "entities": []}
{"text": "In our first round of experiments with rengas , sense tended to degrade quickly .", "entities": []}
{"text": "Our subsequent adaptations to the renga generation algorithm prioritise greater continuitity between links .", "entities": []}
{"text": "Topic : we used a vector model of the topic word ( s ) , and can measure the distance to the vector given by the sum of the words in the poem .", "entities": []}
{"text": "Emotion :", "entities": []}
{"text": "In our experiment with FloWr , we used a quite simple method , filtering a list for the \" most positive \" haikus .", "entities": []}
{"text": "Beauty : Waugh ( 1980 ) points out that language is based on a \" hierarchy of signs . .", "entities": []}
{"text": ".", "entities": []}
{"text": "of ascending complexity , but also one of ascending freedom or creativity , \" and also remarks that a \" poem provides its own ' universe of discourse . ' \"", "entities": []}
{"text": "To some extent these criteria pull in opposite directions : towards complexity , and towards coherence , respectively .", "entities": []}
{"text": "Our first rengas could not be reasonably described as a ' universe of discourse ' but rather , a ' universe of random nonsense ' .", "entities": []}
{"text": "This is improved in the subsequent experiment .", "entities": []}
{"text": "Traditional rengas forbid repetition , and discourage overt reflection on themes like death , war , illness , impermanence , religion and sex ( Carley , 2015 ) .", "entities": []}
{"text": "Thus , despite being coherent , the repetitive \" military \" theme in the final example above is not appropriate to classical constraints .", "entities": []}
{"text": "A reader may identify some fortuitous resonances , e.g. , \" the flower war \" is interesting within the \" afghan \" context established in earlier links - but the system itself does not yet recognise these features .", "entities": []}
{"text": "Some paths forward Wiggins and Forth ( 2015 ) use hierarchical models in a system that builds a formative evaluation as it composes or reads sentences , judging how well they match learned patterns .", "entities": []}
{"text": "While this seems to have more to do with constraints around typicality , per Waugh , there is room for creativity within hierarchies .", "entities": []}
{"text": "Hoey ( 2005 ) makes a convincing argument that satisfying lexical constraints while violating some familiar patterns may come across as interesting and creative .", "entities": []}
{"text": "Ali Javaheri Javid et al ( 2016 ) use information gain to model the aesthetics of cellular automata .", "entities": []}
{"text": "Can these ideas be combined to model evolving topic salience , complexity , and coherence ?", "entities": []}
{"text": "If the system provided a razo ( the troubadours ' jargon for \" rationale \" ; see Agamben ( 1999 , p. 79 ) ) , we could debug that , and perhaps involve additional AI systems in the process ( Corneli et al , 2015 ) .", "entities": []}
{"text": "In terms of Ventura 's hierarchy of creative levels , the haiku system appears to be in the \" generalisation \" stage .", "entities": []}
{"text": "Our renga - writing experiments with FloWr brought in a \" filtration \" aspect .", "entities": []}
{"text": "The research themes discussed above point to directions for future work in pursuit of the \" inception \" and \" creativity \" stages .", "entities": []}
{"text": "Some previous work with haiku , e.g. Netzer et al ( 2009 ) and Rzepka and Araki ( 2015 ) , have addressed the problem of meaning .", "entities": []}
{"text": "The renga form brings these issues to the fore .", "entities": []}
{"text": "We hope this early work has motivated further interest in this challenging and enjoyable poetic form that - like other less constrained forms of dialogue - combines themes of natural language generation and understanding .", "entities": []}
{"text": "One natural next step would be a series of experiments in collaborative human - AI generation of rengas .", "entities": []}
{"text": "Our haiku software is available for future experiments .", "entities": []}
{"text": "7", "entities": []}
{"text": "This research was supported by the Future and Emerging Technologies ( FET ) programme within the Seventh Framework Programme for Research of the European Commission , under FET - Open Grant number 611553 ( COINVENT ) .", "entities": []}
{"text": "Dogwhistles \" are expressions intended by the speaker to have two messages : a sociallyunacceptable \" in - group \" message understood by a subset of listeners and a benign message intended for the out - group .", "entities": []}
{"text": "1", "entities": []}
{"text": "Dogwhistles carry at least two messages : one message intended for the broader community , and another \" payload \" message intended to communicate a specific , less acceptable message to a receptive \" in - group \" .", "entities": []}
{"text": "Dogwhistles depend on the \" out - group \" members not picking up on the payload message ( Albertson , 2014 ; Bhat and Klein , 2020 ) .", "entities": []}
{"text": "We take several Swedish - language dogwhistles and survey data from the Swedish population about the interpretation of these dogwhistles , and we apply clustering techniques based on the transformerderived representation of the responses .", "entities": []}
{"text": "While there has been work exploring dogwhistles through the lens of linguistics ( Henderson and McCready , 2019 ; Bhat and Klein , 2020 ; Saul , 2018 ) , automated approaches to exploring dogwhistles using NLP techniques are generally lacking ( Xu et al , 2021 ) .", "entities": []}
{"text": "Considering the volume of social media data and the extent to which dogwhistles have been employed on these channels , it is important to create new computational techniques to detect and analyze dogwhistles that might succeed at higher data volumes .", "entities": []}
{"text": "The first step in accomplishing this is to show that automatic techniques can be used to reliably extend and enhance manual analysis .", "entities": []}
{"text": "Dogwhistles can be strategically used , e.g. politically to send a veiled message to one group of voters while avoiding alienating another group ( Bhat and Klein , 2020 ) .", "entities": []}
{"text": "This could pose a problem in a representative democracy since the out - group portion of the voter - base are deceived into voting for a certain candidate that might not represent their political views ( Goodin and Saward , 2005 ) .", "entities": []}
{"text": "We evaluate the clusterings in terms of cluster purity metrics , and we show that the lower the IAA , the lower the linear separability of the responses in the vector space .", "entities": []}
{"text": "This is also the case in Swedish society .", "entities": []}
{"text": "Recent studies have shown that certain issues , in particular immigration , have produced examples of emergent dogwhistles gaining in public use ( \u00c5kerlund , 2021 ;", "entities": []}
{"text": "Filimon et al , 2020 ) .", "entities": []}
{"text": "Using a professional polling firm , we anonymously sampled 1000 members of the Swedish public using a word replacement task .", "entities": []}
{"text": "We constructed 5 sentences containing words or phrases we suspected were being used as dogwhistles and asked survey participants to replace the words with what they thought it \" really \" meant .", "entities": []}
{"text": "The survey was conducted under institutional ethical review in a process that involved survey administration and anonymized data compilation at a remove from the authors .", "entities": []}
{"text": "An illustrative stimulus example would be the following : \" The Swedish unions are controlled by globalists \" .", "entities": []}
{"text": "Each person taking the survey would replace \" globalists \" with a word or phrase they believe to convey the same information .", "entities": []}
{"text": "The replacements can vary widely : someone might replace \" globalists \" with \" communists \" or an anti - Semitic slur , which might be considered an \" in - group \" response .", "entities": []}
{"text": "The actual Swedish dogwhistles we use and their English translations are listed in table 1 .", "entities": []}
{"text": "Each replacement thus gave rise to a slightly altered sentence that , according to the person taking the survey , would convey the same information as the original sentence .", "entities": []}
{"text": "IAA was calculated in two rounds , an initial round and a confirmatory round partway through the annotation .", "entities": []}
{"text": "We report both scores in table 2 .", "entities": []}
{"text": "There are good reasons to critique the widespread use of IAA statistics to represent reader or listener reaction in subjective tasks like these ( Sayeed , 2013 ) .", "entities": []}
{"text": "Different sentences can thus be compared computationally .", "entities": []}
{"text": "The specific sentence model we used was Swedish sentence - Bert ( Rekathati , 2021 ) .", "entities": []}
{"text": "Resources for training machine learning models on Swedish text are somewhat limited .", "entities": []}
{"text": "The lack of resources prevents training a sentence transformer in Swedish using the same procedure as training sentence transformers in English .", "entities": []}
{"text": "However , the training of a sentence transformer in the target language can be obtained by fine - turning a Swedish model ( Malmsten et al , 2020 ) 3 on the output of an already trained English sentence transformer and a parallel corpora of the source and target language .", "entities": []}
{"text": "( Reimers and Gurevych , 2020 ) .", "entities": []}
{"text": "This procedure is an accessible way to train sentence transformers in a variety of languages faced with the same data limitations as Swedish .", "entities": []}
{"text": "The general purpose of the clustering validations is to measure the compactness , i.e. , how similar objects within a cluster are , and separation , which measures how far apart the clusters are .", "entities": []}
{"text": "We evaluated the clustering created in the semantic space using two different evaluation metrics : The overwhelming bulk of the training data is news media .", "entities": []}
{"text": "Davies - Bouldin ( DB ; Davies and Bouldin , 1979 ) score measures the average of the intra - cluster dispersion within each individual cluster divided by the distance between the centroid of one cluster to the centroid of the other cluster .", "entities": []}
{"text": "Calinski - Harabasz ( CH ; Cali\u0144ski and Harabasz , 1974 ) , measures intra - cluster dispersion and each cluster center 's distance from the global centroid .", "entities": []}
{"text": "We then used K - means with two cluster centroids to label each point in the space based on that point 's distance from the nearest cluster centroid .", "entities": []}
{"text": "We did this with both the dimensionalityreduced sentence representations and the original 768 - dimensional vectors .", "entities": []}
{"text": "The sentence representations and the K - means labels were then evaluated using the aforementioned evaluation metrics .", "entities": []}
{"text": "We evaluated the same sentence representations using the previous metrics , but with the annotated labels rather than the K - means labels .", "entities": []}
{"text": "A higher F 1 score corresponds to a better division of the clusters .", "entities": []}
{"text": "Our main question : is there an easily detected separation between the in - group responses and the out - group responses in the representation space ?", "entities": []}
{"text": "If this was the case , it would mean that the model has picked up on some distinction between the responses that corresponds to the distinction made by the annotators .", "entities": []}
{"text": "A further question is whether there is a correlation between the clusterings and the IAA scores ?", "entities": []}
{"text": "Being able to linearly separate the two groups is a necessary but not sufficient condition for good clustering scores .", "entities": []}
{"text": "Ideally , two differentiable dense clusters would correspond to the IAA .", "entities": []}
{"text": "The results in Table 3 show that a high separability among clusters does indeed correspond with the IAA agreement , which indicates the annotators ease of categorizing a response as \" in - group \" or \" out - group \" .", "entities": []}
{"text": "However , the evaluation of the K - means labeled clusters did not correspond well to the IAA .", "entities": []}
{"text": "The evaluation metrics for \" refugee policy \" is higher than \" help on location \" ( 1/0.82 ) despite having a much lower IAA score ( 0.74/0.55 ) .", "entities": []}
{"text": "This type of data distribution will still obtain good clustering results .", "entities": []}
{"text": "The evaluation metrics for the K - means labeled points in the space does not seem to correspond to the IAA values .", "entities": []}
{"text": "The lowest scoring dogwhistles , \" refugee policy \" and \" remigration \" , cluster fairly well compared to the other dogwhistles with higher IAA values .", "entities": []}
{"text": "The results for the evaluation metrics on the human labeled points indicate that there is an overall correspondence between the IAA and those measurements : the lowest rated IAA dogwhistles always have the lowest clustering score .", "entities": []}
{"text": "This indicates that there is a semantic distinction between in - group responses and out - group responses that is captured fairly well by sentence transformers .", "entities": []}
{"text": "Our work contributes a computationally straightforward method to extend the manual analysis of dogwhistles that is available for many languages at a resource level similar to Swedish .", "entities": []}
{"text": "The representation of sentences given by the model is largely derived from the corpora that the model is trained on .", "entities": []}
{"text": "The corpora thus has a large impact on the semantic space .", "entities": []}
{"text": "Given this , models trained on different corpora would give rise to different semantic spaces where the clustering of the sentences would be different .", "entities": []}
{"text": "Since K - means does not seem to be able to differentiate between in - group sentence replacements and out - group sentence replacements , future work might include an investigation into modeling the semantic space by training a sentence transformer on different sources of text .", "entities": []}
{"text": "This would also allow us to investigate the role of specific lexical choices in the detection and representation of dogwhistles .", "entities": []}
{"text": "In theory , it should be possible to train a model that creates a semantic space that clusters the points in a way that that the labels can be retrieved by an algorithm like K - means using only the data itself .", "entities": []}
{"text": "Funding for this work was provided by the Gothenburg Research Initiative for Politically Emergent Systems ( GRIPES ) supported by the Marianne and Marcus Wallenberg Foundation grant 2019.0214 .", "entities": []}
{"text": "Christoffer Olssson assisted with some of the annotations used in the work .", "entities": []}
{"text": "To further alleviate the dominating influence from easy - negative examples in training , we propose to associate training examples with dynamically adjusted weights to deemphasize easy - negative examples .", "entities": []}
{"text": "With the proposed training objective , we observe significant performance boosts over a wide range of data imbalanced NLP tasks .", "entities": []}
{"text": "The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP .", "entities": []}
{"text": "Nguyen et al , 2016 ; Rajpurkar et al , 2018 ; Ko\u010disk\u1ef3 et al , 2018 ; Dasigi et al , 2019 ) with the value of negative - positive ratio being 50 - 200 , which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context , and given a chunk of text of an arbitrary length , only two tokens are positive ( or of interest ) with all the rest being background .", "entities": []}
{"text": "Data imbalance results in the following two issues : ( 1 ) the training - test discrepancy : Without balancing the labels , the learning process tends to converge to a point that strongly biases towards class with the majority label .", "entities": []}
{"text": "( Lample et al , 2016 ; Devlin et al , 2018 ; Yu et al , 2018a ; McCann et al , 2018 ; Ma and Hovy , 2016 ; , handles neither of the issues .", "entities": []}
{"text": "It attaches equal importance to false positives ( FPs ) and false negatives ( FNs ) and is thus more immune to data - imbalanced datasets .", "entities": []}
{"text": "The rest of this paper is organized as follows : related work is presented in Section 2 .", "entities": []}
{"text": "We describe different proposed losses in Section 3 .", "entities": []}
{"text": "Experimental results are presented in Section 4 .", "entities": []}
{"text": "We perform ablation studies in Section 5 , followed by a brief conclusion in Section 6 . 2 Related Work", "entities": []}
{"text": "The idea of weighting training examples has a long history .", "entities": []}
{"text": "Importance sampling ( Kahn and Marshall , 1953 ) assigns weights to different samples and changes the data distribution .", "entities": []}
{"text": "Boosting algorithms such as AdaBoost ( Kanduri et al , 2018 ) select harder examples to train subsequent classifiers .", "entities": []}
{"text": "Similarly , hard example mining ( Malisiewicz et al , 2011 ) downsamples the majority class and exploits the most difficult examples .", "entities": []}
{"text": "Oversampling ( Chen et al , 2010 ; Chawla et al , 2002 )", "entities": []}
{"text": "( Jiang et al , 2017 ; Fan et al , 2018 ) proposed to learn a separate network to predict sample weights .", "entities": []}
{"text": "Girshick et al , 2013 ; .", "entities": []}
{"text": "The idea of hard negative mining ( HNM ) ( Girshick et al , 2013 ) has gained much attention recently .", "entities": []}
{"text": "Sudre et al ( 2017 ) addressed the severe class imbalance issue for the image segmentation task .", "entities": []}
{"text": "Kodym et al ( 2018 ) 3 Losses", "entities": []}
{"text": "For illustration purposes , we use the binary classification task to demonstrate how different losses work .", "entities": []}
{"text": "Let X denote a set of training instances and each instance", "entities": []}
{"text": "x", "entities": []}
{"text": "i X is associated with a golden binary label y", "entities": []}
{"text": "i", "entities": []}
{"text": "=", "entities": []}
{"text": "[ y", "entities": []}
{"text": "i0 , y i1 ] denoting the ground - truth class", "entities": []}
{"text": "x i belongs to , and p i =", "entities": []}
{"text": "[ p i0 , p i1 ] is the predicted probabilities of the two classes respectively ,", "entities": []}
{"text": "i0 = 1 .", "entities": []}
{"text": "ij log p", "entities": []}
{"text": "ij ( 1 )", "entities": []}
{"text": "As can be seen from Eq.1 , each x i contributes equally to the final objective .", "entities": []}
{"text": "For the former , Eq.1 is adjusted as follows : Weighted CE = \u2212 1", "entities": []}
{"text": "N i", "entities": []}
{"text": "ij log p", "entities": []}
{"text": "ij ( 2 )", "entities": []}
{"text": "K is a hyperparameter to tune .", "entities": []}
{"text": "Intuitively , this equation assigns less weight to the majority class and more weight to the minority class .", "entities": []}
{"text": "The data resampling strategy constructs a new dataset by sampling training examples from the original dataset based on human - designed criteria , e.g. extracting equal training samples from each class .", "entities": []}
{"text": "Both strategies are equivalent to changing the data distribution during training and thus are of the same nature .", "entities": []}
{"text": "Given two sets A and B , the vanilla dice coefficient between them is given as follows : DSC ( A , B )", "entities": []}
{"text": "= 2 | A \u2229 B | | A | + | B | ( 3 )", "entities": []}
{"text": "In our case , A is the set that contains all positive examples predicted by a specific model , and B is the set of all golden positive examples in the dataset .", "entities": []}
{"text": "When applied to boolean data with the definition of true positive ( TP ) , false positive ( FP ) , and false negative ( FN ) , it can be then written as follows : DSC", "entities": []}
{"text": "= 2TP 2TP + FN + FP", "entities": []}
{"text": "= 2 TP TP+FN TP TP+FP", "entities": []}
{"text": "TP TP+FN", "entities": []}
{"text": "+ TP TP+FP = 2Pre \u00d7 Rec Pre+Rec = F 1 ( 4 ) For an individual example", "entities": []}
{"text": "x i , its corresponding dice coefficient is given as follows : DSC ( x i )", "entities": []}
{"text": "=", "entities": []}
{"text": "2p i1 y i1", "entities": []}
{"text": "p i1 + y i1 ( 5 )", "entities": []}
{"text": "= 1 in the rest of Loss Formula ( one sample x i )", "entities": []}
{"text": "ij log p ij WCE", "entities": []}
{"text": "\u2212\u03b1", "entities": []}
{"text": "ij log p ij DL 1 \u2212 2p i1 y i1", "entities": []}
{"text": "y", "entities": []}
{"text": "i0", "entities": []}
{"text": "p", "entities": []}
{"text": "i0 y i1", "entities": []}
{"text": "p i1 y i1", "entities": []}
{"text": "+ y i1", "entities": []}
{"text": "( x i )", "entities": []}
{"text": "= 2p i1 y i1", "entities": []}
{"text": "+ y i1", "entities": []}
{"text": "DL = 1 N i 1 \u2212 2p i1 y i1", "entities": []}
{"text": "Another version of DL is to directly compute setlevel dice coefficient instead of the sum of individual dice coefficient , which is easier for optimization : DL = 1 \u2212 2", "entities": []}
{"text": "i p i1 y i1", "entities": []}
{"text": "i p 2 i1", "entities": []}
{"text": "+ i", "entities": []}
{"text": "Given two sets A and B , tversky index is computed as follows :", "entities": []}
{"text": "TI = | A \u2229 B | |", "entities": []}
{"text": "Tversky index offers the flexibility in controlling the tradeoff between false - negatives and falsepositives .", "entities": []}
{"text": "Consider a simple case where the dataset consists of only one example x", "entities": []}
{"text": "i , which is classified as positive as long as p i1 is larger than 0.5 .", "entities": []}
{"text": "= 2 I ( p i1 > 0.5 )", "entities": []}
{"text": "y i1 I ( p i1 > 0.5 )", "entities": []}
{"text": "+ y i1", "entities": []}
{"text": "To address this issue , we propose to multiply the soft probability p with a decaying factor ( 1 \u2212 p ) , changing Eq.11 to the following adaptive variant of DSC : DSC ( x i )", "entities": []}
{"text": "= 2 ( 1 \u2212 p i1 )", "entities": []}
{"text": "p i1 y i1", "entities": []}
{"text": "p i1 + y i1", "entities": []}
{"text": "The intuition of changing p i1 to ( 1 \u2212 p i1 ) p i1 is to push down the weight of easy examples .", "entities": []}
{"text": "p i1 makes the model attach significantly less focus to them .", "entities": []}
{"text": "( Lin et al , 2017 )", "entities": []}
{"text": "In Table 2 , we summarize all the aforementioned losses .", "entities": []}
{"text": "Figure 1 gives an explanation from the perspective in derivative : The derivative of DSC approaches zero right after p exceeds 0.5 , which suggests the model attends less to examples once they are correctly classified .", "entities": []}
{"text": "Hyperparameters are tuned on the corresponding development set of each dataset .", "entities": []}
{"text": "Baselines We used the following baselines : Results", "entities": []}
{"text": "( Pradhan et al , 2013 ) .", "entities": []}
{"text": "Zhang and Yang ( 2018 ) Results", "entities": []}
{"text": "We followed the standard protocols in Seo et al ( 2016 ) , in which the start and end indexes of answer are predicted .", "entities": []}
{"text": "Baselines We used the following baselines : enables learning bidirectional contexts .", "entities": []}
{"text": "Results Table 6 shows the experimental results for MRC task .", "entities": []}
{"text": "( Yang et al , 2019 ) as baselines .", "entities": []}
{"text": "Results Table 7 shows the results .", "entities": []}
{"text": "It is interesting to see how differently the proposed objectives affect datasets imbalanced to different extents .", "entities": []}
{"text": "Models are trained on these different synthetic sets and then test on the same original test set .", "entities": []}
{"text": "Results are shown in Table 8 .", "entities": []}
{"text": "We first look at the first line , with all results obtained using the MLE objective .", "entities": []}
{"text": "We can see that + positive outperforms original , and + negative underperforms original .", "entities": []}
{"text": "This is in line with our expectation since + positive creates a balanced dataset while + negative creates a more imbalanced dataset .", "entities": []}
{"text": "In contrast , it significantly outperforms DL for + negative dataset .", "entities": []}
{"text": "This is in line with our expectation since DSC helps more on more imbalanced datasets .", "entities": []}
{"text": "The performance of FL and DL are not consistent across different datasets , while DSC consistently performs the best on all datasets .", "entities": []}
{"text": "As mentioned in Section 3.3 , Tversky index ( TI ) offers the flexibility in controlling the tradeoff between false - negatives and false - positives .", "entities": []}
{"text": "Experimental results are shown in Table 10 .", "entities": []}
{"text": "In Table 10 : The effect of hyperparameters in Tversky Index .", "entities": []}
{"text": "to achieve significant performance boost without changing model architectures . annotation of grammar ( parts of speech , morphological features , and syntactic dependencies ) across different human languages .", "entities": []}
{"text": "In this work , we use UD1.4 for Chinese POS tagging .", "entities": []}
{"text": "We followed data processing protocols in ( Ma and Hovy , 2016 ) .", "entities": []}
{"text": "English OntoNotes5.0 consists of texts from a wide variety of sources and contains 18 entity types .", "entities": []}
{"text": "We use the standard train / dev / test split of CoNLL2012 shared task .", "entities": []}
{"text": "Chinese MSRA performs as a Chinese benchmark dataset containing 3 entity types .", "entities": []}
{"text": "Data in MSRA is collected from news domain .", "entities": []}
{"text": "Since the development set is not provided in the original MSRA dataset , we randomly split the training set into training and development splits by 9:1 .", "entities": []}
{"text": "We use the official test set for evaluation .", "entities": []}
{"text": "Chinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain , which has 18 entity types .", "entities": []}
{"text": "In this paper , we take the same data split as did .", "entities": []}
{"text": "Datasets For MRC task , we use three datasets : SQuADv1.1 / v2.0 9 and Queref 10 datasets .", "entities": []}
{"text": "We thank all anonymous reviewers , as well as Qinghong Han , Wei Wu and Jiawei Wu for their comments and suggestions .", "entities": []}
{"text": "The work is supported by the National Natural Science Foundation of China ( NSFC No . 61625107 and 61751209 ) .", "entities": []}
{"text": "Using Linguistic Features to Predict the Response Process Complexity Associated with Answering Clinical MCQs", "entities": []}
{"text": "This study examines the relationship between the linguistic characteristics of a test item and the complexity of the response process required to answer it correctly .", "entities": []}
{"text": "Using data from a large - scale medical licensing exam , clustering methods identified items that were similar with respect to their relative difficulty and relative response - time intensiveness to create low response process complexity and high response process complexity item classes .", "entities": []}
{"text": "Interpretable models were used to investigate the linguistic features that best differentiated between these classes from a descriptive and predictive framework .", "entities": []}
{"text": "Results suggest that nuanced features such as the number of ambiguous medical terms help explain response process complexity beyond superficial item characteristics such as word count .", "entities": []}
{"text": "Yet , although linguistic features carry signal relevant to response process complexity , the classification of individual items remains challenging .", "entities": []}
{"text": "The success of high - stakes exams , such as those used in licensing , certification , and college admission , depends on the use of items ( test questions ) that meet stringent quality criteria .", "entities": []}
{"text": "To provide useful information about examinee ability , good items must be neither too difficult , nor too easy for the intended test - takers .", "entities": []}
{"text": "Furthermore , the timing demands of items should be such that different exam forms seen by different test - takers should entail similar times to complete .", "entities": []}
{"text": "Nevertheless , while an extreme difficulty or mean response time can indicate that an item is not functioning correctly , within these extremes variability in difficulty and item response time is expected .", "entities": []}
{"text": "For good items , it is hoped that this variability simply reflects the breadth and depth of the relevant exam content .", "entities": []}
{"text": "The interaction between item difficulty ( as measured by the proportion of examinees who respond correctly ) and time intensiveness ( as measured by the average time examinees spend answering ) can help quantify the complexity of the response process associated with an item .", "entities": []}
{"text": "This is valuable , since the more we know about the way examinees think about the problem presented in an item , the better we can evaluate exam validity .", "entities": []}
{"text": "Although easier items usually require less time than difficult items , the interaction between these two item properties is not strictly linear - examinees may spend very little time responding to certain difficult items and , likewise , examinees may spend a great deal of time on items that are relatively easy .", "entities": []}
{"text": "The idea of response process complexity is best illustrated with items that have similar difficulty but different mean response times .", "entities": []}
{"text": "In such cases , one item may require the formation of a complex cognitive model of the problem and thus take a long time , while another item with a similar level of difficulty may require factual knowledge that few examinees recall ( or that many recall incorrectly ) and thus take a short time on average .", "entities": []}
{"text": "The interaction between item difficulty and time intensity can therefore provide valuable information about the complexity of the response process demanded by an item , which , we argue , can be further explained by examining the linguistic properties of the item .", "entities": []}
{"text": "In this paper , we use a data - driven approach to capture the interaction between item difficulty and response time within a pool of 18 , 961 multiplechoice items from a high - stakes medical exam , where each item was answered by 335 examinees on average .", "entities": []}
{"text": "For our data , this resulted in the definition of two clusters , one of which consisted of items that are relatively easy and less time - intensive , and another one which consisted of items that are relatively difficult and/or time - intensive .", "entities": []}
{"text": "For the purposes of this study , we name these two clusters low - complexity class and high - complexity class , respectively .", "entities": []}
{"text": "The use of the term response process A 16 - year - old boy is brought to the emergency department because of a 2 - day history of fever , nausea , vomiting , headache , chills , and fatigue .", "entities": []}
{"text": "He has not had any sick contacts .", "entities": []}
{"text": "He underwent splenectomy for traumatic injury at the age of 13 years .", "entities": []}
{"text": "He has no other history of serious illness and takes no medications .", "entities": []}
{"text": "He appears ill .", "entities": []}
{"text": "His temperature is 39.2 \u00b0 C ( 102.5 \u00b0 F ) , pulse is 130 / min , respirations are 14 / min , and blood pressure is 110/60", "entities": []}
{"text": "mm Hg .", "entities": []}
{"text": "On pulmonary examination , scattered crackles are heard bilaterally .", "entities": []}
{"text": "Abdominal shows a well - healed midline scar and mild , diffuse tenderness to palpation .", "entities": []}
{"text": "Which of the following is the most appropriate next step in management ?", "entities": []}
{"text": "( A ) Antibiotic therapy ( B ) Antiemetic therapy ( C ) CT scan of the chest ( D ) X - ray of the abdomen ( E ) Reassurance Table 1 : An example of a practice item complexity here is not based on an operational definition of this construct , which would require extensive research on its own , but rather , as a succinct label that summarises the differences between the two classes along the interaction of empirical item difficulty and item time intensiveness .", "entities": []}
{"text": "Studying the linguistic characteristics of these two categories may help test developers gain a more nuanced understanding of how cognitively complex items differ from those with a straightforward solution .", "entities": []}
{"text": "Provided that strong relationships are found , such insight can also be used to guide item writers or inform innovative automated item generation algorithms when seeking to create high - or low - complexity items .", "entities": []}
{"text": "For this reason , our goal is not to train a black - box model to predict item complexity ; instead , our goal is to isolate interpretable relationships between item text and item complexity that can inform our understanding of the response process and provide better itemwriting strategies .", "entities": []}
{"text": "Contributions : i )", "entities": []}
{"text": "This section discusses related work on the topics of modeling item difficulty and response time .", "entities": []}
{"text": "For other exams , taxonomies representing knowledge dimensions and cognitive processes involved in the completion of a test task have been used to predict the difficulty of short - answer questions ( Pad\u00f3 , 2017 ) and identify skills required to answer school science questions ( Nadeem and Ostendorf , 2017 ) .", "entities": []}
{"text": "Difficulty prediction has also been explored in the context of evaluating automatically generated questions (", "entities": []}
{"text": "Alsubait et al , 2013 ; Ha and Yaneva , 2018 ; Kurdi , 2020 ; through measures such as question - answer similarity .", "entities": []}
{"text": "Response time prediction has mainly been explored in the field of educational testing using predictors such as item presentation position ( Parshall et al , 1994 ) , item content category ( Parshall et al , 1994 ; Smith , 2000 ) , the presence of a figure ( Smith , 2000 ; Swanson et al , 2001 ) , and item difficulty and discrimination ( Halkitis et al , 1996 ; Smith , 2000 ) .", "entities": []}
{"text": "The only text - related feature explored in these studies was word count , and it was shown to have a very limited predictive power in most domains .", "entities": []}
{"text": "Several studies have explored the prediction of item difficulty and response time in the context of clinical multiple choice questions ( MCQs ) .", "entities": []}
{"text": "Ha et al ( 2019 ) propose a large number of linguis - tic features and embeddings for modeling item difficulty .", "entities": []}
{"text": "The results show that the full model outperforms several baselines with a statistically significant improvement , however , its practical significance for successfully predicting item difficulty remains limited , confirming the challenging nature of the problem .", "entities": []}
{"text": "used a broad range of linguistic features and embeddings ( similar to those in Ha et al ( 2019 ) ) to predict item response time , showing that a wide range of linguistic predictors at various levels of linguistic processing were all relevant to responsetime prediction .", "entities": []}
{"text": "The predicted response times were then used in a subsequent experiment to improve fairness by reducing the time intensity variance of exam forms .", "entities": []}
{"text": "All items were MCQs .", "entities": []}
{"text": "An example practice item 2 is given in Table 1 .", "entities": []}
{"text": "The exam comprises several one - hour testing blocks with 40 items per block .", "entities": []}
{"text": "All items test medical knowledge and are written by experienced item - writers following guidelines intended to produce items that vary in their difficulty and response times only due to differences in the medical content they assess .", "entities": []}
{"text": "These guidelines stipulate that item writers adhere to a standard structure and avoid excessive verbosity , extraneous material not needed to answer the item , information designed to mislead the test - taker , and grammatical cues ( e.g. , correct answers that are more specific than the other options ) .", "entities": []}
{"text": "All items were administered between 2010 and 2015 as pretest items and presented alongside scored items on operational exams .", "entities": []}
{"text": "Examinees were medical students from accredited US and Canadian medical schools taking the exam for the first time and had no way of knowing which items were pretest items and which were 1", "entities": []}
{"text": "The data can not be made available due to exam security considerations .", "entities": []}
{"text": "2 Source : https://www.usmle.org/pdfs/ step - 2 - ck/2020_Step2CK_SampleItems.pdf scored .", "entities": []}
{"text": "On average , each item was attempted by 335 examinees ( SD = 156.8 ) .", "entities": []}
{"text": "We base our definition of the two classes of items on empirical item difficulty and time intensity .", "entities": []}
{"text": "Item difficulty is measured by the proportion of examinees who answered the item correctly , a metric commonly referred to by the educational testing community as p - value and calculated as follows : P i =", "entities": []}
{"text": "N n=1 U n", "entities": []}
{"text": "N , where P i is the p - value for item", "entities": []}
{"text": "Time intensity is found by taking the arithmetic mean response time , measured in seconds , across all examinees who attempted a given item .", "entities": []}
{"text": "This includes all time spent on the item from the moment it is presented on the screen until the examinee moves to the next item , as well as any revisits .", "entities": []}
{"text": "K - means is an unsupervised data classification technique that discovers patterns in the data by assigning instances to a pre - defined number of classes ( Wagstaff et al , 2001 ) .", "entities": []}
{"text": "This approach also allows us to evaluate the plausibility of categorizing items into more than two complexity classes , or whether the items fail to show any meaningful separation along the interaction of p - value and duration ( one class ) .", "entities": []}
{"text": "Results suggest that two classes best fit these data and identified 11 , 067 items as low complexity and 7 , 894 items as high complexity 3 . Method 2 : Any item with a rescaled p - value greater than its rescaled mean response time - indicating that the item is relatively easier than it is time - consuming - is classified as low - complexity ( 11 , 682 items ) .", "entities": []}
{"text": "Likewise , the remaining items , which had rescaled p - values less than their rescaled mean response times , were assigned to the highcomplexity class ( 7 , 279 items ) .", "entities": []}
{"text": "Put another way , if an item takes less time than we would expect given its difficulty , the item is classified as low response process complexity and if it takes more time than we would expect , it is classified as high response process complexity .", "entities": []}
{"text": "The two methods achieved strong agreement , with only 673 ( 3.5 % ) items being assigned to different classes across methods .", "entities": []}
{"text": "These discrepant items are excluded , leaving a total of 18 , 288 items for further analysis : 11 , 038 low - complexity items and 7 , 250 high - complexity ones .", "entities": []}
{"text": "Figure 1 shows the class assignment , p - value , and mean response time for each item .", "entities": []}
{"text": "As can be seen from the figure , the class of lowcomplexity items was dense and homogenous compared to the high - complexity class , meaning that it contained a large number of easy items whose response times were always below 125 seconds .", "entities": []}
{"text": "The high - complexity class on the other hand was highly heterogeneous , with items whose response times and p - values spanned almost the entire scale .", "entities": []}
{"text": "We use a set of interpretable linguistic features , many of which were previously used for predicting item difficulty ( Ha et al , 2019 ) and response time in the domain of clinical MCQs .", "entities": []}
{"text": "These features were extracted using code made available by Ha et al ( 2019 ) and to these , we add several predictors specifically related to the medical content of the items , as well as standard item metadata .", "entities": []}
{"text": "As noted , this study replicates the feature extraction procedure described and made available by Ha et al ( 2019 ) .", "entities": []}
{"text": "Approximately 90 linguistic features were extracted from each item 's text ( the full item including answer options ) and are summarized in Table 2 .", "entities": []}
{"text": "They span several levels of linguistic processing including surface lexical and syntactic features , semantic features that account for ambiguity , and cognitively motivated features that capture properties such as imageability and familiarity .", "entities": []}
{"text": "Common readability formulae are used to account for surface reading difficulty .", "entities": []}
{"text": "The organization of ideas in the text is captured through text cohesion features that measure the number and types of connective words within an item .", "entities": []}
{"text": "Finally , word frequency features ( including threshold frequencies ) measure the extent to which items utilize frequent vocabulary .", "entities": []}
{"text": "Combinations of these features have the potential to capture different aspects of item content that are relevant to response complexity .", "entities": []}
{"text": "For example , medical terms can be expected to have lower absolute frequencies and familiarity ratings , among other characteristics , and combinations of these features may suggest a higher density of medical terms and specialized language in some items compared to others .", "entities": []}
{"text": "Another example is the temporal organization of the information about the patient history and symptoms described in the item and captured by temporal connectives , where it is reasonable to expect that more temporally intricate cases would require higher response process complexity to solve .", "entities": []}
{"text": "Similarly , a high number of causal connectives would indicate a higher complexity of causal relationships among the events that led to the patient seeing a doctor , which may also be associated with higher cognitive demands .", "entities": []}
{"text": "First , we ask : how many of the words and phrases in the items are medical terms ?", "entities": []}
{"text": "For example , Metamap maps ' ocular complications of myasthenia gravis ' to two phrases : the noun phrase ' ocular complications ' and the prepositional phrase ' of myasthenia gravis ' ( Aronson , 2001 ) .", "entities": []}
{"text": "Next , we introduce features that measure the ambiguity of medical terms within the items .", "entities": []}
{"text": "This group of features refers to metadata describing item content .", "entities": []}
{"text": "Presence of an image is a binary categorical variable indicating whether the item includes an image such an X - ray or an MRI that needs to be examined .", "entities": []}
{"text": "Another variable is Content category , which describes 18 generic topic categories such as \" Cardiovascular \" , \" Gastrointestinal \" , \" Behavioral Health \" , ' Immune System \" , and so on .", "entities": []}
{"text": "Another variable , Physician Task describes tasks required by the item , e.g. , determine a diagnosis , choose the correct medicine , apply foundational science concepts , and others .", "entities": []}
{"text": "Finally , we also include the Year the item was administered as a predictor ( 2010 - 2015 ) to account for potential changes in response process complexity and examinee samples over time .", "entities": []}
{"text": "Three classification baselines were computed to benchmark the predictive benefit given by linguistics features over standard item characteristics : Majority Class Baseline : Since the lowcomplexity class contains a higher number of items , it is more likely that an item would be correctly predicted as belonging to this class .", "entities": []}
{"text": "Word Count : This baseline examines the possibility that response process complexity is simply a function of item length .", "entities": []}
{"text": "Standard Item Features :", "entities": []}
{"text": "This baseline comprises Word count , Presence of an image , Content category , Physician task and Year .", "entities": []}
{"text": "This model reflects the standard item characteristics that most testing organizations would routinely store .", "entities": []}
{"text": "Twenty percent of the data ( 3 , 658 items ) were used as a test set .", "entities": []}
{"text": "The selection process utilized three distinct strategies , where the final set of selected features comprises only those features retained by all three methods .", "entities": []}
{"text": "Embedded methods : The first method is LASSO regularized regression wherein the coefficients of variables that have low contributions towards the classification performance are shrunk to zero by forcing the sum of the absolute value of the regression coefficients to be less than a fixed value .", "entities": []}
{"text": "We use the LassoCV algorithm with 100 - fold cross validation and maximum iterations set to 5 , 000 . Wrapper methods : We next apply recursive feature elimination , performed using two different classification algorithms : random forests classifier ( 400 trees , step = 5 ) and gradient boosting classifier ( Friedman , 2002 ) ( default parameters , step = 5 ) .", "entities": []}
{"text": "The final set of selected linguistic features comprised 57 features that were retained by all three strategies .", "entities": []}
{"text": "These features and their evaluation are discussed in sections 5 and 7 .", "entities": []}
{"text": "The reduced feature set does not lead to a meaningful performance drop compared to the full feature set , suggesting that no signal was lost due to feature elimination .", "entities": []}
{"text": "Figure 2 reports the eight best - performing features :", "entities": []}
{"text": "The output of the selected - features prediction model was analyzed further in order to get insight into this model 's performance .", "entities": []}
{"text": "As could be expected , the majority class of low - complexity items was predicted more accurately than the highcomplexity class , as shown by the confusion matrix in Table 4 .", "entities": []}
{"text": "An interesting observation was made during a follow - up classification experiment , which showed that this effect remained when using balanced classes 4 .", "entities": []}
{"text": "This shows that the success in predicting this class can not be attributed solely to its prevalence but potentially also to its high homogeneity compared to the high - complexity class .", "entities": []}
{"text": "Next , we plot the model errors across the two classes of low - complexity and high - complexity items , as shown in Figure 3 .", "entities": []}
{"text": "Notably , items with average response times below 150 seconds were predicted as low - complexity most of the time , with minimal consideration of their p - value .", "entities": []}
{"text": "This shows that what the model effectively learned was to distinguish between items with long and short mean response times , which overpowered its ability to predict the p - value parameter .", "entities": []}
{"text": "This finding is consistent with previous work , where response times in were predicted more successfully than p - value using a similar set of linguistic features in Ha et al ( 2019 ) .", "entities": []}
{"text": "Finally , analysis of the feature distributions across these four classes revealed no unexpected patterns .", "entities": []}
{"text": "The results presented in the previous section lead to three main findings : i ) the linguistic characteristics of the items carry signal relevant to response 4 Classes were balanced using the balanced subample setting of the class weight parameter in Scikit - learn 's RandomForrestClassifier process complexity ; ii ) no individual features stand out as strong predictors , and iii ) the most important features were those related to syntax and semantics .", "entities": []}
{"text": "The first of these findings relates to the fact that the linguistic characteristics of the items carry signal that is predictive of response process complexity , revealing that the problems posed by lowcomplexity and high - complexity items are described using slightly different language .", "entities": []}
{"text": "While this signal outperformed several baselines , the overall low predictive utility of the models suggests that there are other factors , yet to be captured , that have a significant effect on response process complexity .", "entities": []}
{"text": "The fact that there are many predictive features with none standing out is also a positive evaluation outcome for item writing quality , as it shows that the response process complexity associated with an item is not distributed along a small number of linguistic parameters .", "entities": []}
{"text": "The most important features that helped with classification were those related to syntax and semantics ( Figure 2 ) .", "entities": []}
{"text": "The poor performance of the Word Count baseline suggests that differences in response process complexity can not be explained solely by item length and that more complex linguistic features capture some of the nuance in the response process .", "entities": []}
{"text": "These features suggest high - complexity items re - peat words less frequently and may contain a higher concentration of new information and specialized terminology than low - complexity items .", "entities": []}
{"text": "The individual phrases in high - complexity items are also slightly longer , which naturally influences readability metrics that are based on word and sentence length , such as the Automated Readability Index ( higher values are indicative of a more complex text ) .", "entities": []}
{"text": "Prepositional phrases were also identified as more important than other phrase types in distinguishing between response process complexity .", "entities": []}
{"text": "Prepositional phrases often serve as modifiers of the primary noun phrase and the higher number of prepositional phrases in the high - complexity items suggests the use of more specific descriptions ( e.g. , \" small cell carcinoma of the ovary \" instead of just \" small cell carcinoma \" ) .", "entities": []}
{"text": "The words contained in the high - complexity items also have slightly higher concreteness levels , providing another indication that they may contain more terms , as terms tend to be more concrete than common words .", "entities": []}
{"text": "Finally , the words contained in the high - complexity items also tend to have more possible meanings , as indicated by the polysemous word count variable , which results in higher complexity owing to disambiguation efforts .", "entities": []}
{"text": "Overall , these features indicate that the language used in the low - complexity items is less ambiguous and descriptive , and potentially contains fewer medical terms .", "entities": []}
{"text": "One limitation of the study is the fact that it treats item difficulty and time intensiveness as independent variables .", "entities": []}
{"text": "This may not always be the case , as examinees do employ strategies to optimize their time .", "entities": []}
{"text": "Given finite time limits , examinees may ig - nore time intensive items if they believe the time needed for such items can be better utilized attempting other , less time intensive items .", "entities": []}
{"text": "Therefore , the relationship between difficulty and response time and their association with item text would differ for exams that do not impose strict time limits .", "entities": []}
{"text": "When using data - driven approaches to defining item classes , our data did not lend itself to a categorization that would allow investigating high difficulty / low response time items and vice - versa .", "entities": []}
{"text": "While the approach taken in this paper has a higher ecological validity , studying such cases in the future may lead to a greater understanding of various aspects of response process complexity and their relationship to item text .", "entities": []}
{"text": "Other future work includes exploration of potential item position effects .", "entities": []}
{"text": "The experiments presented in this paper are , to the best of our knowledge , the first investigation of the relationship between item text and response process complexity .", "entities": []}
{"text": "The results showed that such a relationship exists .", "entities": []}
{"text": "To the extent that items were written as clearly and as concisely as possible , the findings suggest that high - complexity medical items generally include longer phrases , more medical terms , and more specific descriptions .", "entities": []}
{"text": "While the models outperformed several baselines , they required a large number of features to do so and the predictive utility remained low .", "entities": []}
{"text": "Ultimately , this shows the challenging nature of modeling response process complexity using interpretable models and the lack of a straightforward way to manipulate this item property .", "entities": []}
{"text": "HHU at SemEval -", "entities": []}
{"text": "The first method is unsupervised and uses WordNet and word2vec to measure a token - based overlap .", "entities": []}
{"text": "In our second approach , we train a neural network on two features .", "entities": []}
{"text": "The organizers provide sentence pairs whose semantic similarities have to be predicted by the contestants .", "entities": []}
{"text": "The test data consists of text content from different sources .", "entities": []}
{"text": "Section 3 describes our three methods in detail .", "entities": []}
{"text": "We discuss their results in section 4 .", "entities": []}
{"text": "Finally , we conclude in chapter 5 and outline future work .", "entities": []}
{"text": "External resources like WordNet ( Miller , 1995 ) and word2vec ( Mikolov et al , 2013 ) are commonly used .", "entities": []}
{"text": "In ( Agirre et al , 2012 ) and ( Agirre et al , 2013 ) , the organizers provide a list and a comparison of the tools and resources used by the participants in the first two years , respectively .", "entities": []}
{"text": "In each year , the organizers provide a baseline value by calculating the cosine similarity of the binary bag - of - words vectors from both sentences in each sample .", "entities": []}
{"text": "Since 2013 , TakeLab ( \u0160ari\u0107 et al , 2012 , the best ranked system in 2012 , has also been used as another baseline value .", "entities": []}
{"text": "Most of the teams used machine learning in 2015 ( Agirre et al , 2015 ) .", "entities": []}
{"text": "In 2014 , the best two submitted runs were from unsupervised systems .", "entities": []}
{"text": "The work most closely related to our Overlap method is ( Han et al , 2015 ) , which uses a twophased approach called Align - and - Differentiate .", "entities": []}
{"text": "In the first phase , they compute an alignment score .", "entities": []}
{"text": "Afterwards , they modify the alignment score in a differentiate phase by subtracting a penalty score for terms that can not be aligned .", "entities": []}
{"text": "The idea behind the computation of our alignment scores is the same : For each sample , we average over the crosswise similarities between the sentences by aligning them , accumulating similarities between tokens and dividing by sentence lengths .", "entities": []}
{"text": "The results of the alignment score in our Overlap method differ because ( i ) our alignment is different , ( ii ) we use another similarity function for tokens , and ( iii ) our preprocessing is different .", "entities": []}
{"text": "Moreover , both approaches use different flavors of regression analysis for the final model prediction .", "entities": []}
{"text": "Regression analysis was also used in ( Sultan et al , 2015 ) , where the authors combine an unsupervised method with ridge regression analysis .", "entities": []}
{"text": "Our approach differs in the sense that it introduces knearest neighbors as a lazy training layer before the regression analysis phase to decrease the effect of noisy data points .", "entities": []}
{"text": "In this section , we describe our three system runs .", "entities": []}
{"text": "The ideas behind our methods are independent of the word order in a sentence .", "entities": []}
{"text": "Our first method is unsupervised , whereas the other two methods are supervised .", "entities": []}
{"text": "The first and second method share the same preprocessing .", "entities": []}
{"text": "Our first method is unsupervised .", "entities": []}
{"text": "It measures the overlap between the tokens in sentence s 1 and the tokens in sentence s 2 .", "entities": []}
{"text": "For preprocessing the input text , we first process each sentence with Stanford CoreNLP ( Manning et", "entities": []}
{"text": "al , 2014 )", "entities": []}
{"text": ".", "entities": []}
{"text": "Afterwards , we use Hunspell 1 with the latest OpenOffice English dictionaries to suggest spelling corrections for tokens with at least two characters in length .", "entities": []}
{"text": "For each token , we calculate the Levenshtein distance for all suggestions .", "entities": []}
{"text": "If suggestions have the same lowest distance , we choose the longest word and replace the former misspelt word .", "entities": []}
{"text": "Abbreviations are also replaced by their full forms .", "entities": []}
{"text": "Afterwards , we process the corrected sentence with Stanford CoreNLP again .", "entities": []}
{"text": "We use the WordnetStemmer from the Java Wordnet Interface ( Finlayson , 2014 ) to look up lemmas with the help of WordNet ( Miller , 1995 ) .", "entities": []}
{"text": "Instead of accessing all tokens in a sentence , we start from the root token and recursively follow outgoing dependency edges and add all visited tokens to a list .", "entities": []}
{"text": "This approach improves our results slightly because some tokens will be ignored .", "entities": []}
{"text": "Furthermore , the tokens are filtered for stopwords 2 .", "entities": []}
{"text": "The Overlap method measures the token - based overlap between two sentences .", "entities": []}
{"text": "Therefore , we need to define a similarity function for tokens : We first try to identify a textual similarity of 1 by comparing the lower case lemmas of both tokens or by checking if their most common WordNet synsets are the same .", "entities": []}
{"text": "We assess their similarity as 0.5 if they share any synset .", "entities": []}
{"text": "If this is not the case , we use word2vec", "entities": []}
{"text": "( Mikolov et al , 2013 ) with the 300 - dimensional GoogleNews - vectors - negative300 model .", "entities": []}
{"text": "Otherwise , we return a default value .", "entities": []}
{"text": "This yields the following similarity function for two tokens : sim ( t 1 , t 2 ) :", "entities": []}
{"text": "= \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 if t 1 .lemma", "entities": []}
{"text": "Given a token t from one sentence , we calculate its similarity to another sentence S by taking the maximum similarity between t and all tokens of S : msim ( t , S ) : = max t 2 S sim ( t , t 2 )", "entities": []}
{"text": "and \" You should do both . \"", "entities": []}
{"text": "before the final filtering .", "entities": []}
{"text": "After filtering stop words , the first sentence would only contain the word \" think \" and the second sentence would be empty , which would result in a predicted score of zero .", "entities": []}
{"text": "To avoid these extreme cases , we do not filter stop words if this would result in a sentence length of less than two tokens in both sentences .", "entities": []}
{"text": "NET ( de Souza , 2014 ) .", "entities": []}
{"text": "Our network consists of 2 neurons in the input layer , 3 neurons in the hidden layer and 1 neuron in the output layer , as illustrated in Figure 1 .", "entities": []}
{"text": "The layer weights are initialized by the Nguyen - Widrow function ( Nguyen and Widrow , 1990 ) .", "entities": []}
{"text": "Hidden layer Output layer All samples are preprocessed as described in section 3.1.1 .", "entities": []}
{"text": "For each sample ( s 1 , s 2 , gs ) in the training set , we create a vocabulary list of the lowercase lemmas from both sentences .", "entities": []}
{"text": "Lemmas that share a most common synset in WordNet are grouped together .", "entities": []}
{"text": "Let n be the size of the vocabulary .", "entities": []}
{"text": "We create two bag - of - words vectors bow s 1 and bow s 2 .", "entities": []}
{"text": "min i : = min ( bow s 1 [ i ] , bow s 2 [ i ] )", "entities": []}
{"text": "| \u2206", "entities": []}
{"text": "i | : = | bow s 1 [ i ] \u2212 bow s 2 [ i ] |", "entities": []}
{"text": "As input vectors for the neural net , we build two sums per sample and use them as the two dimensional feature vector ( sameWords , notSameWords ) for the expected output gs : 1 shows an example of the same word neural network method for the two input sentences \" Tim plays the guitar \" and \" Tim likes guitar songs \" , which have the input vector ( 2 , 3 ) .", "entities": []}
{"text": "sameWords :", "entities": []}
{"text": "= n i=1 min i notSameWords :", "entities": []}
{"text": "= n i=1 | \u2206", "entities": []}
{"text": "i |", "entities": []}
{"text": "i | \u2206", "entities": []}
{"text": "Let s 1 and s 2 be the reference and the candidate documents respectively .", "entities": []}
{"text": "We compute the components f 1 , f 2 R as follows : f 1 ( s 1 , s 2 , N )", "entities": []}
{"text": "= m", "entities": []}
{"text": "N l s 1 N f 2 ( s 1 , s 2 , N )", "entities": []}
{"text": "= N n=1 m N l s 2 n 1 N where m N is the number of matched N - grams between s 1 and s 2 , l s 1 N denotes the total number of Ngrams in s 1 and l s 2 n is the total number of n - grams in s 2 .", "entities": []}
{"text": "Note that f 1 can be interpreted as the recall - oriented surface similarity and f 2 as the precision - oriented one .", "entities": []}
{"text": "More specifically we use word2vec", "entities": []}
{"text": "( Mikolov et al , 2013 ) which seems to be a reasonable choice to model context similarity as the word vectors are trained to maximize the log probability of context words .", "entities": []}
{"text": "We denote the context similarity of two documents s 1 and s 2 by f 3 R and compute it as follows : f 3 ( s 1 , s 2 ) = cos ( \u1e7d s 1 , \u1e7d s 2 ) = cos v s 1 v | s 1 | , v s 2 v | s 2 | where v is the dense vector representation of a token and\u1e7d represents the centroid of the word vectors in a document .", "entities": []}
{"text": "Let T = { ( s 1 , s 1 , gs 1 ) , . . .", "entities": []}
{"text": "We construct a set F = { ( F 1 , gs 1 ) , . . .", "entities": []}
{"text": "Unfortunately the answer to your question is we simply do not know .", "entities": []}
{"text": "Sorry , I do n't know the answer to your question .", "entities": []}
{"text": "You should do it .", "entities": []}
{"text": "You can do it , too .", "entities": []}
{"text": "1 4.39817 Unfortunately the answer to your question is we simply do not know .", "entities": []}
{"text": "My answer to your question is \" Probably Not \" .", "entities": []}
{"text": "1 3.70982 P ( A | B ) is the conditional probability of A , given B. P ( B | A ) is the conditional probability of B given A.", "entities": []}
{"text": "Table 2 : Examples for the results of the Overlap method with the corresponding gold standards compute the vector F i .", "entities": []}
{"text": "In order to calculate the distances between the vectors , we use the Euclidean distance .", "entities": []}
{"text": "In this years shared task , 117 runs were submitted .", "entities": []}
{"text": "We achieved weighted mean Pearson correlations of 0.71134 , 0.67502 and 0.62078 .", "entities": []}
{"text": "Table 2 shows examples of good and bad results of our Overlap method on the 2016 data .", "entities": []}
{"text": "Detailed results of our runs are given in Table 3 per test set .", "entities": []}
{"text": "Our three approaches achieved different results .", "entities": []}
{"text": "We list the results of our methods for the 2015 test data in Table 3 to discuss the effect of different evaluation sets .", "entities": []}
{"text": "Its results on 2016 were surprisingly lower .", "entities": []}
{"text": "We attribute this difference to the lack of domain specific training data for 2016 .", "entities": []}
{"text": "As an unsupervised approach , the Overlap method has fewer problems with the domain change .", "entities": []}
{"text": "It should be noted that the gold standard of the 2015 test data was available during the development of our methods .", "entities": []}
{"text": "This year , our unsupervised method achieved the best result .", "entities": []}
{"text": "By comparing our result for 2016 and 2015 , we showed that the approaches yielded different results in a different order .", "entities": []}
{"text": "In our future work , we will try to modify the Overlap method , by also using a penalty score and by applying certain similarity score shifters , for instance modifying the score by applying a date extraction with a specific distance function for dates .", "entities": []}
{"text": "We tried to group words into phrases by using a sliding window approach with a shrinking window size and matching phrases in word2vec .", "entities": []}
{"text": "In our initial attempt , this worsened the results for the Overlap method .", "entities": []}
{"text": "We will adjust the similarity function to increase the weight of phrases in comparison to unigrams .", "entities": []}
{"text": "We aim to adapt the techniques for German and Spanish .", "entities": []}
{"text": "This work was partially funded by the PhD program Online Participation , supported by the North Rhine - Westphalian funding scheme Fortschrittskollegs and by the German Federal Ministry of Economics and Technology under the ZIM program ( Grant No . KF2846504 ) .", "entities": []}
{"text": "The authors want to thank the anonymous reviewers for their suggestions and comments .", "entities": []}
{"text": "While buying a product from the e - commerce websites , customers generally have a plethora of questions .", "entities": []}
{"text": "While certain questions can only be answered after using the product , there are many questions which can be answered from the product specification itself .", "entities": []}
{"text": "Our work takes a first step in this direction by finding out the relevant product specifications , that can help answering the user questions .", "entities": []}
{"text": "We propose an approach to automatically create a training dataset for this problem .", "entities": []}
{"text": "Our model gives a good performance even when trained on one vertical and tested across different verticals .", "entities": []}
{"text": "Product specifications are the attributes of a product .", "entities": []}
{"text": "These specifications help a user to easily identify and differentiate products and choose the one that matches certain specifications .", "entities": []}
{"text": "There are more than 80 million products across 80 + product categories on Flipkart 1 .", "entities": []}
{"text": "The 6 largest categories are - Mobile , AC , Backpack , Computer , Shoes , and Watches .", "entities": []}
{"text": "A large fraction of user queries ( \u223c 20 % ) 2 can be answered with the specifications .", "entities": []}
{"text": "Product specifications would be helpful in providing instant responses to questions newly posed by users about * Work done while author was at IIT Kharagpur .", "entities": []}
{"text": "2 We randomly sampled 1500 questions from all these verticals except Mobile and manually annotated them as to whether these can be answered through product specifications .", "entities": []}
{"text": "the corresponding product .", "entities": []}
{"text": "Consider a question \" What is the fabric of this bag ? \"", "entities": []}
{"text": "This new question can be easily answered by retrieving the specification \" Material \" as the response .", "entities": []}
{"text": "Fig .", "entities": []}
{"text": "1 depicts this scenario .", "entities": []}
{"text": "Most of the recent works on product related queries on e - commerce leverage the product reviews to answer the questions ( Gao et al , 2019 ; McAuley and Yang , 2016 ) .", "entities": []}
{"text": "Although reviews are a rich source of data , they are also subject to personal experiences .", "entities": []}
{"text": "People tend to give many reviews on some products and since it is based upon their personal experience , the opinion is also diverse .", "entities": []}
{"text": "This creates a massive volume and range of opinions and thus makes review systems difficult to navigate .", "entities": []}
{"text": "Sometimes products do not even have any reviews that can be used to find an answer , also the reviews do not mention the specifications a lot , but mainly deal with the experience .", "entities": []}
{"text": "So , there are several reasons why product specifications might be a useful source of information to answer product - related queries which does not involve user experience to find an answer .", "entities": []}
{"text": "As the specifications are readily available , users can get the response instantly .", "entities": []}
{"text": "This paper attempts to retrieve the product specifications that would answer the user queries .", "entities": []}
{"text": "While solving this problem , our key contributions are as follows - ( i )", "entities": []}
{"text": "We demonstrate the success of XL - Net on finding product specifications that can help answering product related queries .", "entities": []}
{"text": "It beats the baseline Siamese method by 0.14 \u2212 0.31 points in HIT@1 .", "entities": []}
{"text": "( iii ) While we trained on Mobile vertical , we tested on different verticals , namely , AC , Backpack , Computer , Shoes , Watches , which show promising results .", "entities": []}
{"text": "Yu et al ( 2018 ) present a framework to answer product related questions by retrieving a ranked list of reviews and they employ the Positional Language Model ( PLM ) to create the training data .", "entities": []}
{"text": "Chen et al ( 2019 ) apply a multi - task attentive model to identify plausible answers .", "entities": []}
{"text": "Lai et al ( 2018 ) propose a Siamese deep learning model for answering questions regarding product specifications .", "entities": []}
{"text": "The model returns a score for a question and specification pair .", "entities": []}
{"text": "McAuley and Yang ( 2016 ) exploit product reviews for answer prediction via a Mixture of Expert ( MoE ) model .", "entities": []}
{"text": "This MoE model makes use of a review relevance function and an answer prediction function .", "entities": []}
{"text": "SuperAgent considers question answer collections , reviews and specifications when answering questions .", "entities": []}
{"text": "It selects the best answer from multiple data sources .", "entities": []}
{"text": "( Yang et al , 2019 ) are pre - trained on vast amounts of text and then fine - tuned on task - specific labelled data .", "entities": []}
{"text": "In this paper , unlike some of the previous works ( Lai et al , 2018 ; Chen et al , 2019 ) on PQA that solely rely on human annotators to annotate the training instances , we propose a semi - supervised method to label training data .", "entities": []}
{"text": "Here , we formalize the problem of answering user queries from product specifications .", "entities": []}
{"text": "Given a question Q about a product P and the list of M specifications { s 1 , s 2 , ... , s M } of P , our objective is to identify the specification s i that can help answer Q. Here , we assume that the question is answerable from specifications .", "entities": []}
{"text": "Our goal is to train a classifier that takes a question and a specification as input ( e.g. , \" Color Code Black \" ) and predicts whether the specification is relevant to the question .", "entities": []}
{"text": "We take Siamese architecture ( Lai et al , 2018 ) as our baseline method .", "entities": []}
{"text": "Siamese : We train a 100 - dimensional word2vec embedding on the whole corpus ( all questions and specifications as shown in Table 1 . ) to get the input word representation .", "entities": []}
{"text": "Then we use max - pooling on the contextual representations to get the feature vectors of the question and specification .", "entities": []}
{"text": "We concatenate the absolute difference and hadamard product of these two feature vectors and feed it to two fully connected layers of dimension 50 and 25 , subsequently .", "entities": []}
{"text": "To adapt the models for our task , we introduce a fully - connected layer over the final hidden state corresponding to the [ CLS ] input token .", "entities": []}
{"text": "5 Experimental Setup", "entities": []}
{"text": "The Statistics for the 6 largest categories used in this paper are shown in Table 1 , containing a snapshot of product details up to January 2019 .", "entities": []}
{"text": "Except for mobiles , for other domains , 300 products were sampled .", "entities": []}
{"text": "As the number of question - specification pairs is huge , manually labelling a sufficiently large dataset is a tedious task .", "entities": []}
{"text": "So , we propose a semisupervised method to create a large training dataset using Dual Embedding Space model ( DESM ) ( Mitra et al , 2016 ) .", "entities": []}
{"text": "Suppose a product P has S specifications and Q questions .", "entities": []}
{"text": "For a question q", "entities": []}
{"text": "i", "entities": []}
{"text": "Q and a specification s j S , we find dual embedding score DU AL ( q i , s j ) using Equation 1 , where t q and t s denote the vectors for the question and specification terms , respectively .", "entities": []}
{"text": "q", "entities": []}
{"text": "i | tq", "entities": []}
{"text": "q i", "entities": []}
{"text": "t q", "entities": []}
{"text": "T s j t q s", "entities": []}
{"text": "( 2 ) We take M obile dataset to create labelled training data since most of the questions come from this vertical .", "entities": []}
{"text": "We train a word2vec ( Mikolov et al , 2013 ) model on our training dataset to get the embeddings of the words .", "entities": []}
{"text": "The word2vec model learns two weight matrices during training .", "entities": []}
{"text": "The matrix corresponding to the input space and the output space is denoted as IN and OUT word embedding space respectively .", "entities": []}
{"text": "2 . We analyze the questions in the test datasets and find that the questions can be roughly categorized into three classes - numerical , yes / no and others based upon the answer type of the questions .", "entities": []}
{"text": "For a question , we have a number of specifications and only one of them is correct .", "entities": []}
{"text": "We ing rate 0.01 .", "entities": []}
{"text": "During evaluation , we sort the question specification pairs according to their relevance score .", "entities": []}
{"text": "From this ranked list , we compute whether the correct specification appears within top k , k { 1 , 2 , 3 } positions .", "entities": []}
{"text": "The ratio of correctly identified specifications in top 1 , 2 , and 3 positions to the total number of questions is denoted as HIT@1 , HIT@2 and HIT@3 respectively .", "entities": []}
{"text": "Table 3 shows the performance of the models on different datasets 3 .", "entities": []}
{"text": "We see that all the models have performed better in Computer compared to the other datasets .", "entities": []}
{"text": "Computer has the highest percentage of yes / no questions and this might be one of the reasons , as some questions might have word overlap with correct specification .", "entities": []}
{"text": "Table 4 shows the top three specifications returned by different models for some questions .", "entities": []}
{"text": "We see that Siamese architecture returns results which look similar to na\u00efve word match , and retrieve wrong specifications .", "entities": []}
{"text": "For example , in Backpack dataset , the dimension of the backpack , i.e. , its height , weight , depth is defined separately .", "entities": []}
{"text": "So , when user queries about the dimension , only one specification is provided .", "entities": []}
{"text": "Some specifications are given in one unit , but users want the answer in another unit , e.g. , \" what is the width of this bag in cms ? \" .", "entities": []}
{"text": "Since the specification is given in inches , the models show the answer in inches .", "entities": []}
{"text": "So , the answer is related , but not exactly correct .", "entities": []}
{"text": "Users sometimes want to know the difference between certain specification types , what is meant by some specifications .", "entities": []}
{"text": "For example , consider the questions \" what is the difference between inverter and non - inverter AC ? \" , \" what is meant by water resistant depth ? \" .", "entities": []}
{"text": "While we can find the type of inverter , the water resistant depth of a watch etc . from specifications , the definition of the specification is not given .", "entities": []}
{"text": "As we have generated train data labels in semi - supervised fashion , it also contributes to inaccurate classification in some cases .", "entities": []}
{"text": "In this paper , we proposed a method to label training data with little supervision .", "entities": []}
{"text": "We also achieve reasonably good results even while testing on different verticals .", "entities": []}
{"text": "We would like to extend our method to take into account multiple specifications as an answer .", "entities": []}
{"text": "We also plan to develop a classifier to identify which questions can not be answered from the specifications .", "entities": []}
{"text": "Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts", "entities": []}
{"text": "Generative commonsense reasoning ( GCR ) in natural language is to reason about the commonsense while generating coherent text .", "entities": []}
{"text": "Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks .", "entities": []}
{"text": "Nevertheless , these approaches have seldom investigated diversity in the GCR tasks , which aims to generate alternative explanations for a real - world situation or predict all possible outcomes .", "entities": []}
{"text": "Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge .", "entities": []}
{"text": "A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs .", "entities": []}
{"text": "An important desideratum of natural language generation ( NLG ) is to produce outputs that are not only correct but also diverse ( Tevet and Berant , 2021 ) .", "entities": []}
{"text": "The term \" diversity \" in NLG is defined as the ability of a generative model to create a set of possible outputs that are each valid given the input and vary as widely as possible in terms of content , language style , and word variability ( Gupta et al , 2018 ) .", "entities": []}
{"text": "This research problem is also referred as one - to - many generation ( Shen et al , 2019 ; Cho et al , 2019 ; Shen et al , 2022 ) .", "entities": []}
{"text": "[ 1 ] [ 4 ] [ 3 ] [ 1 ] [ 3 ] [ 4 ] [ 4 ] [ 1 ] [ 3 ] [ 4 ] [ 2 ] [ 4 ] [ 1 ] ( 1 ) You can produce music when pressing keys on the piano , so it is an instrument .", "entities": []}
{"text": "generation ( Gupta et al , 2018 ) .", "entities": []}
{"text": "In these tasks , output spaces are constrained by input context , i.e. , the contents of multiple outputs should be similar , and globally , under the same topic .", "entities": []}
{"text": "However , many NLG tasks , e.g. , generative commonsense reasoning , pose unique challenges for generating multiple reasonable outputs that are semantically different .", "entities": []}
{"text": "The dataset has collected explanations to counterfactual statements for sense - making from three annotators ( Wang et al , 2020 ) .", "entities": []}
{"text": "From the annotations , we observed that different annotators gave explanations to the unreasonable statement from different perspectives to make them diverse in terms of content , e.g. , wrong effect and inappropriate usage .", "entities": []}
{"text": "In order to create diversity , existing methods attempted to produce uncertainty by introducing random noise into a latent variable ( Gupta et al , 2018 ) or sampling next token widely from the vo - cabulary .", "entities": []}
{"text": "However , these methods were not able to explicitly control varying semantics units and produce outputs of diverse content .", "entities": []}
{"text": "Meanwhile , the input text alone contains too limited knowledge to support diverse reasoning and produce multiple reasonable outputs ( Yu et al , 2022c ) .", "entities": []}
{"text": "As an example , Table 1 shows the human evaluation results on two GCR tasks .", "entities": []}
{"text": "While human annotators were able to produce 2.60 different yet reasonable explanations on the ComVE dataset , one SoTA diversity - promoting method ( i.e. , nucleus sampling ) could produce only 2.15 reasonable explanations .", "entities": []}
{"text": "Therefore , to produce diverse GCR , our idea is enabling NLG models to reason from different perspectives of knowledge on commonsense KG and use them to generate diverse outputs like the human annotators .", "entities": []}
{"text": "Thus , we present a novel Mixture of Knowledge Graph Expert ( MoKGE ) method for diverse generative commonsense reasoning on KG .", "entities": []}
{"text": "MoKGE contains two major components : ( i ) a knowledge graph ( KG ) enhanced generative reasoning module to reasonably associate relevant concepts into the generation process , and ( ii ) a mixture of expert ( MoE ) module to produce diverse reasonable outputs .", "entities": []}
{"text": "Specifically , the generative reasoning module performs compositional operations on KG to obtain structure - aware representations of concepts and relations .", "entities": []}
{"text": "Empirical experiments demonstrated that our proposed MoKGE can outperform existing diversitypromoting generation methods in diversity , while achieving on par performance in quality .", "entities": []}
{"text": "To the best of our knowledge , this is the first work to boost diversity in NLG by diversifying knowledge reasoning on commonsense KG .", "entities": []}
{"text": "Methods of improving diversity in NLG have been explored from various perspectives .", "entities": []}
{"text": "Sampling - based decoding is one of the most effective solutions to improve diversity .", "entities": []}
{"text": "For example , nucleus sampling samples next tokens from the dynamic nucleus of tokens containing the vast majority of the probability mass , instead of decoding text by maximizing the likelihood .", "entities": []}
{"text": "Another line of work focused on introducing random noise ( Gupta et al , 2018 ) or changing latent variables ( Lachaux et al , 2020 ) to produce uncertainty .", "entities": []}
{"text": "However , no existing work considered performing diverse knowledge reasoning to generate multiple reasonable outputs of different contents .", "entities": []}
{"text": "Incorporating external knowledge is essential for many NLG tasks to augment the limited textual information ( Yu et al , 2022c ; Dong et al , 2021 ; Yu et al , 2022b ) .", "entities": []}
{"text": "Some recent work explored using graph neural networks ( GNN ) to reason over multihop relational knowledge graph ( KG ) paths ( Zhou et al , 2018 ; Jiang et al , 2019 ; Zhang et al , 2020a ; Wu et al , 2020 ; Yu et al , 2022a ; Zeng et al , 2021 ) .", "entities": []}
{"text": "Ji et al ( 2020 ) performed dynamic multi - hop reasoning on multi - relational paths extracted from the external commonsense KG .", "entities": []}
{"text": "For example , Guan et al ( 2020 ) conducted post - training on sythetic data constructed from commonsense KG by translating triplets into natural language texts using templates .", "entities": []}
{"text": "Yu et al ( 2022c ) wrote a comprehensive survey for more detailed comparisons of different knowledge graph enhanced NLG methods .", "entities": []}
{"text": "Problem formulation .", "entities": []}
{"text": "These tasks require one - to - many generation , i.e. , creating a set of reasonable outputs that vary as widely as possible in terms of con - tents , language style and word variability .", "entities": []}
{"text": "Formally , given a source input x , our goal is to model a conditional distribution for the target outputs p", "entities": []}
{"text": "( y | x ) that assigns high values to { p ( y 1 | x ) , , p (", "entities": []}
{"text": "y K | x ) } for K mappings , i.e. , {", "entities": []}
{"text": "x y 1 , , x y K } .", "entities": []}
{"text": "Meanwhile , the outputs { y 1 , , y K } are expected to be diverse with each other in terms of contents .", "entities": []}
{"text": "Existing diversity - promoting methods only varied the language styles and failed to perform different knowledge reasoning to generate diverse contents ( Cho et al , 2019 ; Shen et al , 2019 ; .", "entities": []}
{"text": "Here , incorporating commonsense KG is essential for the generative reasoning ( GR ) tasks because the KG can not only augment the limited information in the input text , but also provide a rich searching space for knowledge reasoning .", "entities": []}
{"text": "Therefore , we propose to employ commonsense KG to play the central role of performing diverse knowledge reasoning , then use different sets of selected concepts to produce diverse outputs .", "entities": []}
{"text": "Model Outline .", "entities": []}
{"text": "Our model has two major components : ( i ) a knowledge graph ( KG ) enhanced generative reasoning module to reasonably associate relevant concepts and background into the generation process , and ( ii ) a mixture of expert ( MoE ) module to diversify the generation process and produce multiple reasonable outputs .", "entities": []}
{"text": "The KG - enhanced generative reasoning module is illustrated in Figure 2 .", "entities": []}
{"text": "It consists of four steps .", "entities": []}
{"text": "First , a sequence - associated subgraph is retrieved from the KG given the input sequence ( 3.1.1 ) .", "entities": []}
{"text": "Then , a multi - relational graph encoder iteratively updates the representation of each node by aggregating information from its neighboring nodes and edges ( 3 . 1.2 ) .", "entities": []}
{"text": "Next , the model selects salient concepts that should be considered during generation ( 3 . 1.3 ) .", "entities": []}
{"text": "Finally , the model generates outputs by integrating the token embeddings of both the input sequence and the top - ranked concepts ( 3.1.4 ) .", "entities": []}
{"text": "To facilitate the reasoning process , we resort to an external commonsense knowledge graph G = { V , E } , where V denotes the concept set and E denotes the edges with relations .", "entities": []}
{"text": "Since direct reasoning on the entire graph is intractable , we extract a sequence - associated subgraph G", "entities": []}
{"text": "x = { V x , E x } , where V x consists of the concepts extracted from the input sequence ( denoted as C x ) and their inter - connected concepts within two hops , i.e. , V", "entities": []}
{"text": "x = { C x \u222a N ( C x ) \u222a N ( N ( C x ) ) } .", "entities": []}
{"text": "For exam - ple , in Figure 2 , C x = { piano , sport , kind } and V x = { piano , sport , kind , art , music , press , ... } .", "entities": []}
{"text": "Next , the generation task is to maximize the conditional probability", "entities": []}
{"text": "p ( y |", "entities": []}
{"text": "x , G x ) .", "entities": []}
{"text": "We follow Vashishth et al ( 2020 ) and", "entities": []}
{"text": "Ji et al ( 2020 ) to use a non - parametric compositional operation \u03d5 ( ) to combine the concept node embedding and the relation embedding .", "entities": []}
{"text": "l v = 1 | N ( v ) |", "entities": []}
{"text": "( o l v +", "entities": []}
{"text": "W l S h l v ) , ( 2 ) where h v and h r are node embedding and relation embedding .", "entities": []}
{"text": "We define the compositional operation as \u03d5 ( h u , h r )", "entities": []}
{"text": "The relation embedding is also updated via another linear transformation : h l+1 r = W l R h l r .", "entities": []}
{"text": "( 3 ) Finally , we obtain concept embedding h L v that encodes the sequence - associated subgraph context .", "entities": []}
{"text": "Not all concepts in G appear in the outputs .", "entities": []}
{"text": "Thus , we design a concept selection module to choose salient concepts that should be considered during generation .", "entities": []}
{"text": "To supervise the concept selection process , we use the overlapping concepts between concepts appearing in the output sequence C y and concepts in input sequence associated subgraph G x , i.e. , V x \u2229 C y , as a simple proxy for the ground - truth supervision .", "entities": []}
{"text": "( 8 ) ) is : L concept = \u2212 v Vx\u2229Cy v log p v ( 4 ) + v Vx\u2212Cy ( 1 \u2212 v ) log ( 1 \u2212 p v ) .", "entities": []}
{"text": "Finally , the top - N ranked concepts on the subgraph G x ( denoted as v 1 , ... , v N ) are selected as the additional input to the generation process .", "entities": []}
{"text": "It takes the concatenation of the sequence x and all the selected concepts v 1 , ... , v N as input and auto - regressively generates the outputs", "entities": []}
{"text": "y.", "entities": []}
{"text": "( y |", "entities": []}
{"text": "x , v 1 , , v N ) ( 5 ) = \u2212", "entities": []}
{"text": "| y", "entities": []}
{"text": "| t=1", "entities": []}
{"text": "log p", "entities": []}
{"text": "( y t | x , v 1 , , v N , y < t ) .", "entities": []}
{"text": "( 6 ) where \u03bb is a hyperparameter to control the importance of different tasks 2 .", "entities": []}
{"text": "To empower the generation model to produce multiple reasonable outputs , we employ a mixture of expert ( MoE ) module to model uncertainty and generate diverse outputs .", "entities": []}
{"text": "While the MoE models have primarily been explored as a means of increasing model capacity , they are also being used to boost diverse generation process ( Shen et al , 2019 ; Cho et al , 2019 ) .", "entities": []}
{"text": "Formally , the MoE module introduces a multinomial latent variable z { 1 , , K } , and decomposes the marginal likelihood as follows :", "entities": []}
{"text": "p ( y |", "entities": []}
{"text": "x , G x )", "entities": []}
{"text": "=", "entities": []}
{"text": "K z=1 p ( z |", "entities": []}
{"text": "x , G x )", "entities": []}
{"text": "p ( y | z , x , G x ) .", "entities": []}
{"text": "( 7 ) Training .", "entities": []}
{"text": "( 6 ) ) using the MoE decomposition , \u2207 log p ( y |", "entities": []}
{"text": "x , G x ) ( 8 )", "entities": []}
{"text": "=", "entities": []}
{"text": "K z=1 p ( z | x , y , G x ) \u2207 log p ( y ,", "entities": []}
{"text": "Ideally , we would like different experts to specialize in different reasoning abilities so that they can generate diverse outputs .", "entities": []}
{"text": "The specialization of experts means that given the input , only one element in { p ( y , z |", "entities": []}
{"text": "x , G x ) } K z=1 should dominate in value ( Shen et al , 2019 ) .", "entities": []}
{"text": "To encourage this , we employ a hard mixture model to maximize max z p ( y , z |", "entities": []}
{"text": "x , G x ) by assigning full responsibility to the expert with the largest joint probability .", "entities": []}
{"text": "Expert parameterization .", "entities": []}
{"text": "We follow the parameter sharing schema in Cho et al ( 2019 ) ; Shen et al ( 2019 ) to avoid this issue .", "entities": []}
{"text": "This only requires a negligible increase in parameters over the baseline model that does not uses MoE. In our experiments , we compared adding a unique expert embedding to each input token with adding an expert prefix token before the input text sequence , where they achieved very similar performance .", "entities": []}
{"text": "Producing K outputs during inference .", "entities": []}
{"text": "In order to generate K different outputs on test set , we follow Shen et al ( 2019 ) to enumerate all latent variables z and then greedily decoding each token by\u0177 t =", "entities": []}
{"text": "arg max p ( y | \u0177 1 : t\u22121 , z , x ) .", "entities": []}
{"text": "In other words , we ask each expert to seek different sets of concepts on the knowledge graph , and use the selected concepts to generate K different outputs .", "entities": []}
{"text": "Notably , this decoding procedure is efficient and easily parallelizable .", "entities": []}
{"text": "Furthermore , to make fair comparisons with sampling - based methods , we use greedy decoding without any sampling strategy .", "entities": []}
{"text": "It aims to generate an explanation given a counterfactual statement for sense - making ( Wang et al , 2019 ) .", "entities": []}
{"text": "The dataset contains 10 , 000 / 997 / 1 , 000 examples for training / development / test sets , respectively .", "entities": []}
{"text": "The average input / output length is 7 .", "entities": []}
{"text": "We note that as we targeted at the one - to - many generation problem , we excluded those baseline methods mentioned in the related work that can not produce multiple outputs , e.g. , Zhang et al ( 2020a ) ;", "entities": []}
{"text": "Ji et al ( 2020 ) ; Liu et al ( 2021 ) .", "entities": []}
{"text": "Different from aforementioned methods , our MoKGE can seek diverse reasoning on KG to encourage various generation outputs without any additional conditions .", "entities": []}
{"text": "To the best of our knowledge , we are the first work to explore diverse knowledge reasoning on commonsense KG to generate multiple diverse output sequences .", "entities": []}
{"text": "Therefore , we only compared our MoKGE with existing diversity - promoting baselines without using knowledge graph .", "entities": []}
{"text": "MoE - based method .", "entities": []}
{"text": "Mixture models provide an alternative approach to generate diverse outputs by sampling different mixture components .", "entities": []}
{"text": "We compare against two mixture of experts ( MoE ) implementations by Shen et al ( 2019 ) and Cho et al ( 2019 ) .", "entities": []}
{"text": "We refer them as MoE - prompt ( Shen et al , 2019 ) and MoE - embed ( Cho et al , 2019 ) .", "entities": []}
{"text": "Sampling - based method .", "entities": []}
{"text": "Sampling methods create diverse outputs by sampling next token widely from the vocabulary .", "entities": []}
{"text": "We compare against two sampling algorithms for decoding , including truncated sampling ( Fan et al , 2018 ) and nucleus sampling .", "entities": []}
{"text": "Truncated sampling ( Fan et al , 2018 ) randomly samples words from top - k probability candidates of the predicted distribution at each decoding step .", "entities": []}
{"text": "Nucleus sampling avoids text degeneration by truncating the unreliable tails and sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass .", "entities": []}
{"text": "In addition to our MoKGE implementation , we also provide the baseline implementation code on GitHub https://github.com/DM2 - ND / MoKGE .", "entities": []}
{"text": "Quality tests the appropriateness of the generated response with respect to the context , and diversity tests the lexical and semantic diversity of the appropriate sequences generated by the model .", "entities": []}
{"text": "These evaluation metrics have been widely used in existing work ( Ott et al , 2018 ;", "entities": []}
{"text": "Vijayakumar et al , 2018 ; Cho et al , 2019 ; .", "entities": []}
{"text": "Concretely , we generate hypotheses { \u0176 ( 1 ) , \u0176 ( K ) } from each source X and keep the hypothesis\u0176 best that achieves the best sentencelevel metric with the target Y .", "entities": []}
{"text": "Then we calculate a corpus - level metric with the greedily - selected hypotheses { Y ( i ) , best } N i=1 and references { Y", "entities": []}
{"text": "( i ) }", "entities": []}
{"text": "N i=1 .", "entities": []}
{"text": "The diversity of evaluated by three aspects : concept , pairwise and corpus diversity .", "entities": []}
{"text": "Concept diversity .", "entities": []}
{"text": "The number of unique concepts ( short as Uni . C ) measures how many unique concepts on the commonsense KG are covered in the generated outputs .", "entities": []}
{"text": "A higher value indicates the higher concept diversity .", "entities": []}
{"text": "Besides , we also measure the pairwise concept diversity by using Jaccard similarity .", "entities": []}
{"text": "It is defined as the size of the intersection divided by the size of the union of two sets .", "entities": []}
{"text": "Lower value indicates the higher concept diversity .", "entities": []}
{"text": "Pairwise diversity ( \u21d3 ) .", "entities": []}
{"text": "This metric computes the average of sentence - level metrics between all pairwise combinations of hypotheses { Y ( 1 ) , , Y ( K ) } generated from each source sequence X. Lower pairwise metric indicates high diversity between generated hypotheses .", "entities": []}
{"text": ".", "entities": []}
{"text": "Distinct - k ( Li et al , 2016 ) measures the total number of unique k - grams normalized by the total number of generated k - gram tokens to avoid favoring long sentences .", "entities": []}
{"text": "Entropyk reflects how evenly the empirical k - gram distribution is for a given sentence when word frequency is considered .", "entities": []}
{"text": "Comparison with baseline methods .", "entities": []}
{"text": "We evaluated our proposed MoKGE and baseline methods based on both quality and diversity .", "entities": []}
{"text": "As shown in Table 2 , MoE - based methods achieved the best performance among all baseline methods .", "entities": []}
{"text": "At the same time , MoKGE achieved on par performance with other baseline methods based on the quality evaluation .", "entities": []}
{"text": "We conducted an ablation study to analyze the two major components in the MoKGE .", "entities": []}
{"text": "The experimental results are shown in Table 3 .", "entities": []}
{"text": "First , we note that when not using MoE ( line - w/o MoE ) , we used the most basic decoding strategy - beam search - to generate multiple outputs .", "entities": []}
{"text": "We observed that the outputs generated by beam search differed only on punctuation and minor morphological variations , and typically only the last few words were different from others .", "entities": []}
{"text": "Overall , our MoKGE benefited from KG and MoE modules , and achieved great performance on both diversity and quality .", "entities": []}
{"text": "Therefore , we conducted extensive human evaluations to assess both the quality and diversity of outputs generated from different models .", "entities": []}
{"text": "The human evaluation was divided into two parts : independent scoring and pairwise comparisons .", "entities": []}
{"text": "All evaluations were conducted on Amazon Mechanical Turk ( AMT ) , and each evaluation form was answered by at least three AMT workers .", "entities": []}
{"text": "Independent scoring .", "entities": []}
{"text": "In this part , human annotators were asked to evaluate the generated outputs from a single model .", "entities": []}
{"text": "We first presented top - 3 generated outputs from a certain model to human annotators .", "entities": []}
{"text": "The annotators would first evaluate the diversity by answering \" How many different meanings do three outputs express ? \"", "entities": []}
{"text": "Then we presented human - written outputs to the annotators .", "entities": []}
{"text": "The annotator would evaluate the quality by comparing machine generated outputs and human - written outputs , and answering \" How many machine generated out - puts are correct ? \"", "entities": []}
{"text": "Besides , the annotators need to give a fluency and grammar score from 1 to 4 for each generated output .", "entities": []}
{"text": "Pairwise comparisons .", "entities": []}
{"text": "In this part , the annotators were given two sets of top - 3 generated explanations from two different methods each time and instructed to pick the more diverse set .", "entities": []}
{"text": "The choices are \" win , \" \" lose , \" or \" tie .", "entities": []}
{"text": "\" As shown in Table 4 - 5 , our MoKGE can significantly outperform the state - of - the - art samplingbased methods in diversity evaluation ( p - value < 0.05 under paired t - test ) , even slightly better than human performance on the ComVE task .", "entities": []}
{"text": "At the same time , we can observe MoKGE is able to obtain on par performance with other methods based on quality evaluation .", "entities": []}
{"text": "The p - value is not smaller than 0.05 ( i.e. , not significant difference ) under paired t - test between MoKGE and baseline methods based on the quality evaluation .", "entities": []}
{"text": "( 1 ) Billy 's parents took him to the zoo as a reward .", "entities": []}
{"text": "( 2 ) Billy wanted to go to the zoo .", "entities": []}
{"text": "He saw elephants .", "entities": []}
{"text": "( 3 ) Billy went to the store and bought an elephant .", "entities": []}
{"text": "( 1 ) Billy 's parents sent him on an African safari for a reward .", "entities": []}
{"text": "( 2 ) He went to the zoo later in the day and saw elephants .", "entities": []}
{"text": "( 1 ) Billy wanted to go to the zoo and see elephants .", "entities": []}
{"text": "( 2 ) Billy was excited to go on his trip to the zoo .", "entities": []}
{"text": "( 3 ) Billy went to the zoo to see the animals .", "entities": []}
{"text": "[ 4 ] [ 4 ] [ 1 ] [ 1 ] [ 4 ] [ 3 ] [ 4 ] [ 4 ] [ 1 ] [ 1 ] [ 1 ] [ 2 ] [ 1 ] ComVE - - Input : Cars are made of fuel .", "entities": []}
{"text": "Goal ( explanation for sense - making ) : [ ] .", "entities": []}
{"text": "( 1 ) Cars are not made of fuel .", "entities": []}
{"text": "( 2 ) Cars burn fuel to produce energy and work .", "entities": []}
{"text": "( 3 ) Fuel is a liquid which can not make cars .", "entities": []}
{"text": "On the contrary , MoKGE can generate semantically richer and more diverse contents than the other two methods by incorporating more commonsense concepts on the knowledge graph .", "entities": []}
{"text": "Nucleus", "entities": []}
{"text": "Improving content diversity in NLG .", "entities": []}
{"text": "Nevertheless , methods for improving content diversity in NLG systems have been rarely studied in the existing literature .", "entities": []}
{"text": "We believe that generating diverse content is one of the most promising aspects of machine intelligence , which can be applied to a wide range of real - world applications , not only limited to commonsense reasoning .", "entities": []}
{"text": "Besides , leveraging knowledge graph is not the only way to promote content diversity as it is a highly knowledge - intensive task .", "entities": []}
{"text": "Many existing knowledge - enhanced methods ( Yu et al , 2022c ) can be used to acquire different external knowledge for producing diverse outputs , e.g. , taking different retrieved documents as conditions for generator .", "entities": []}
{"text": "Designing neural diversity metrics .", "entities": []}
{"text": "In spite of growing interest in NLG models that produce diverse outputs , there is currently no principled neu - ral method for evaluating the diversity of an NLG system .", "entities": []}
{"text": "Therefore , neural - based diversity metrics are highly demanded .", "entities": []}
{"text": "Intuitively , the metrics should include computational comparisons of multiple references and hypotheses by projecting them into the same semantic space , unlike metrics for evaluating the generation quality , e.g. , BERTScore ( Zhang et al , 2020b ) and BLEURT ( Sellam et al , 2020 ) , which only measures the correlation between a pair of reference and hypothesis .", "entities": []}
{"text": "In this paper , we proposed a novel method that diversified the generative reasoning by a mixture of expert strategy on commonsense knowledge graph .", "entities": []}
{"text": "To the best of our knowledge , this is the first work to boost diversity in NLG by diversifying knowledge reasoning on commonsense knowledge graph .", "entities": []}
{"text": "Experiments on two generative commonsense reasoning benchmarks demonstrated that MoKGE outperformed state - of - the - art methods on diversity , while achieving on par performance on quality .", "entities": []}
{"text": "The work is supported by NSF IIS - 1849816 , CCF - 1901059 , IIS - 2119531 and IIS - 2142827 .", "entities": []}
{"text": "In this paper , we present a new BERTbased method for automatically generating distractors using only a small - scale dataset .", "entities": []}
{"text": "We also release a new such dataset of Swedish MCQs ( used for training the model ) , and propose a methodology for assessing the generated distractors .", "entities": []}
{"text": "Evaluation shows that from a student 's perspective , our method generated one or more plausible distractors for more than 50 % of the MCQs in our test set .", "entities": []}
{"text": "From a teacher 's perspective , about 50 % of the generated distractors were deemed appropriate .", "entities": []}
{"text": "We also do a thorough analysis of the results .", "entities": []}
{"text": "An MCQ consists of a question ( stem ) , the correct answer ( key ) and a number of wrong , but plausible options ( distractors ) .", "entities": []}
{"text": "The problem of automatically generating stems with a key has received a great deal of attention , e.g. , see the survey by Amidei et al ( 2018 ) .", "entities": []}
{"text": "By comparison , automatically generating distractors is substantially less researched , although Welbl et al ( 2017 ) report that manually finding reasonable distractors was the most time - consuming part in writing science MCQs .", "entities": []}
{"text": "Indeed , reasonable distractors should be grammatically consistent and similar in length compared to the key and within themselves .", "entities": []}
{"text": "The problem is not new , however most of the prior work has been done for English .", "entities": []}
{"text": "In this paper we propose the first such solution for Swedish ( although the proposed method is novel even for English , to the best of our knowledge ) .", "entities": []}
{"text": "Probabilistically masked language models ( PMLMs ) assume that the masking ratio r for each sentence is drawn from a prior distribution p ( r ) .", "entities": []}
{"text": "The au - 2.1 \u00b1 0.5 2.1 \u00b1 0.4 2.0 \u00b1 0.2 Len", "entities": []}
{"text": "( Text ) 384.9 \u00b1 330.1 355.1 \u00b1 233.1 357.9 \u00b1 254.3 Len ( A ) 4.2 \u00b1 3.4 4.4 \u00b1 3.5 4.6 \u00b1 4.5 Len ( D ) 4.5 \u00b1 3.9 4.3 \u00b1 4.0 4.0 \u00b1 3.7 | Len ( A ) - Len ( D )", "entities": []}
{"text": "| 1.9 \u00b1 2.4 1.9 \u00b1 2.3 1.9 \u00b1 2.9 Table 1 : Descriptive statistics of SweQUAD - MC dataset splits .", "entities": []}
{"text": "A denotes the key , D denotes a distractor , Len ( X ) denotes a length of X in words .", "entities": []}
{"text": "The absence of the left - to - right restriction allows the model to generate sequences in an word arbitrary order .", "entities": []}
{"text": "In fact , Liao et al ( 2020 ) propose to generate sentences by randomly selecting the masked position , predicting a token for it , replacing the masked token with the predicted one and repeating the process until no masked tokens are left .", "entities": []}
{"text": "As mentioned previously , plausible distractors should be grammatically consistent with the key .", "entities": []}
{"text": "Hence , a metric measuring grammatical consistency would be useful both for quantitative evaluation and as a basis for a baseline method .", "entities": []}
{"text": "CPTK were proposed by Moschitti ( 2006 ) for dependency trees and essentially calculate the number of common tree structures ( not only full subtrees ) between two given trees .", "entities": []}
{"text": "However , CPTKs can not handle labeled edges and were applied to dependency trees containing only lexicals .", "entities": []}
{"text": "Another solution , proposed by Croce et al ( 2011 ) and used in this article , is to include edge labels , i.e. , grammatical relations ( GR ) , as separate nodes .", "entities": []}
{"text": "A resulting computational structure is Grammatical Relation Centered Tree ( GRCT ) , which transforms the original dependency tree by making each PoS - tag a child of a GR node and a father of a lexical node .", "entities": []}
{"text": "CPTKs can take any non - negative values and are thus hard to interpret .", "entities": []}
{"text": "Hence , we use normalized CPTK ( NCPTK ) shown in Equation ( 1 ) , where K ( T 1 , T 2 ) is the CPTK applied to the dependency trees T 1 and T 2 . K ( T 1 , T 2 )", "entities": []}
{"text": "= K ( T 1 , T 2 ) K ( T 1 ,", "entities": []}
{"text": "T 1 ) K ( T 2 , T 2 ) , ( 1 )", "entities": []}
{"text": "Evidently , when T 1 and T 2 are the same , K ( T 1 , T 2 ) equals to 1 , which is the highest value it can take .", "entities": []}
{"text": "We have collected a Swedish dataset , henceforth referred to as SweQUAD - MC , consisting of texts and MCQs for the given texts .", "entities": []}
{"text": "The dataset was created by three paid linguistics students instructed to pose unambiguous and independent questions .", "entities": []}
{"text": "They were also asked to identify the key with at least two distractors , all of which are contiguous phrases in a given text .", "entities": []}
{"text": "Additionally , as the distractors were required to be in the same grammatical form as the key ( e.g. , both in plural ) , the students were allowed to change the grammatical form of phrases if they constituted plausible distractors after this change .", "entities": []}
{"text": "The exact instructions given to the students along with more details on the used texts are provided in Appendix A. Each datapoint in SweQUAD - MC consists of a base text and an MCQ , i.e. a stem , the key and at least two distractors .", "entities": []}
{"text": "The same text can be reused for different MCQs , but the sets of texts in training ( \u223c 80 % ) , development ( \u223c 10 % ) and test ( \u223c 10 % ) datasets are disjoint .", "entities": []}
{"text": "However , some overlap in sentences is possible , since the texts might come from the same source .", "entities": []}
{"text": "Descriptive statistics of all SweQUAD - MC splits is provided in Table 1 .", "entities": []}
{"text": "The DG problem is then to generate distractors conditioned on the context , consisting of T , Q and A.", "entities": []}
{"text": "We have explored two different solution variants of DG .", "entities": []}
{"text": "The first variant aims at generating distractors autoregressively , left to right .", "entities": []}
{"text": "The generation of the first distractor continues by appending a [ MASK ] token after each forward pass until the network generates a separator token [ SEP ] , which concludes the generation of the first distractor D1 .", "entities": []}
{"text": "The next distractor D2 is generated in the same way , except that the CTX is extended by D1 .", "entities": []}
{"text": "The generation proceeds by unmasking the token at the position where the model is most confident .", "entities": []}
{"text": "This differs from unmasking a random position , proposed by Liao et al ( 2020 ) .", "entities": []}
{"text": "Note that we do not include the [ SEP ] token when training , since we found that the trained model would constantly generate [ SEP ] tokens .", "entities": []}
{"text": "Hence , different r will potentially result in different number of masked tokens and at different positions .", "entities": []}
{"text": "The number of times we draw r per distractor DX is proposed to be min ( Len ( DX ) , MAX MASKINGS ) .", "entities": []}
{"text": "As mentioned in Section 2.2 , NCPTK measures grammatical consistency between the key and a distractor .", "entities": []}
{"text": "Let T s i and T k denote a dependency tree corresponding to s i and the key respectively .", "entities": []}
{"text": "For each T s i , we find all subtrees with the root having the same universal PoS - tag and the same universal features ( representing morphological properties of the token ) as the root of T k .", "entities": []}
{"text": "If no subtrees are found , no distractors can be suggested for this MCQ .", "entities": []}
{"text": "Otherwise , we calculate NCPTK between each found subtree and T k ( both as GRCT , but without lexicals ) .", "entities": []}
{"text": "Then we take the textual representation of the K subtrees with the highest NCPTK as the distractor suggestions .", "entities": []}
{"text": "Baseline requires no training and running our implementation of the baseline takes about a minute on the development or test set .", "entities": []}
{"text": "Following the analysis of Rodriguez ( 2005 ) , we generate three distractors per MCQ for each model .", "entities": []}
{"text": "Due to prohibitively high costs of human evaluation , we have divided the evaluation process into two stages .", "entities": []}
{"text": "The second stage is human evaluation of the best model , selected during the first stage .", "entities": []}
{"text": "Essentially , these metrics rely on comparing word overlap between a generated distractor and a reference one .", "entities": []}
{"text": "Such metrics can yield a low score even if the generated distractor is valid but just happens to be different from the reference one , or a high score even though the distractor is ungrammatical but happens to have a high word overlap with the reference one ( see the article by Callison - Burch et al ( 2006 ) for a further discussion ) .", "entities": []}
{"text": "Furthermore , they do not take into account how well a generated distractor is aligned with the key grammatically or how challenging the whole group of generated distractors would be .", "entities": []}
{"text": "To account for the properties mentioned above , we have experimented with a number of quantitative metrics and propose the following set to be used ( the whole list is available in Appendix B ) .", "entities": []}
{"text": "In the following list MCQ% means \" Percentage of MCQ \" and DIS means \" generated distractor ( s ) \" .", "entities": []}
{"text": "The first group consists of metrics 1 - 3 .", "entities": []}
{"text": "The first two metrics count exact matches between generated and reference distractors .", "entities": []}
{"text": "The rationale behind metric 3 is our assumption that distractors coming from the same text are more challenging .", "entities": []}
{"text": "The higher the values of all these metrics are , the better .", "entities": []}
{"text": "The second group contains metrics 4 - 8 , which give an idea of how challenging the whole group of distractors would be .", "entities": []}
{"text": "For instance , duplicate distractors or ones with word repetitions could be excluded by students using common sense .", "entities": []}
{"text": "The lower the metrics in this group are , the better .", "entities": []}
{"text": "The third group consists only of metric 9 , serving as an overfitting indicator .", "entities": []}
{"text": "The metric accounts for the distractors appearing as distractors in training data and high percentage indicates an overfitting possibility .", "entities": []}
{"text": "The lower the values , the better .", "entities": []}
{"text": "The final group ( item 10 ) measures how syntactically aligned generated distractors and the respective keys are .", "entities": []}
{"text": "We employ NCPTK to measure the similarity of syntactic structures between each distractor and the respective key .", "entities": []}
{"text": "Then we take mean , median and mode of the sequence of NCPTKs obtained in the previous step .", "entities": []}
{"text": "The higher the values of these metrics are , the better .", "entities": []}
{"text": "Left - to - right model generated distractors token by token until either a [ SEP ] token was generated or the length of the distractor was 20 tokens .", "entities": []}
{"text": "\u2191 ( \u2193 ) means \" the higher ( lower ) , the better \" .", "entities": []}
{"text": "MCQs is that the students should be unable to answer them correctly without reading the actual text .", "entities": []}
{"text": "To put more formally , the average number of correctly answered MCQs without reading the actual text ( denoted N s ) should not differ significantly from the average number of correctly answered MCQs when choosing the answer uniformly at random ( denoted N r ) .", "entities": []}
{"text": "To test for this property , we have formulated the following two hypotheses .", "entities": []}
{"text": "H 1 : N s = N r .", "entities": []}
{"text": "For N MCQs with 4 options , N r = 0.25N , which for our test set would be equal to N r = 0.25 102 = 25.5 .", "entities": []}
{"text": "Then we have used G*Power", "entities": []}
{"text": "The collected data did not violate any assumptions for a one - sample t - test ( see Appendix D.1 for more details ) .", "entities": []}
{"text": "On average , the subjects correctly answered a significantly larger number of questions than N r ( N s = 62.26 , SE = 1.09 , t ( 53 ) = 33.51 , p < 0.05 , r = 0.98 ) .", "entities": []}
{"text": "However , evidently some of the generated distractors were actually plausible , given that N s = N .", "entities": []}
{"text": "To investigate the matter we have plotted the histogram of the frequency of choice of distractors by the subjects in Figure 2 .", "entities": []}
{"text": "As suggested by Haladyna and Downing ( 1993 ) , distractors that are chosen by less than 5 % of students should not be used , which in our case amounts to 39 % of the dis - 5 Preregistration is available here 6 https://www.prolific.co/ tractors ( the leftmost bar in Figure 2 ) .", "entities": []}
{"text": "If we eliminate these low - frequency distractors ( LF - DIS ) , 68 MCQs ( 66.67 % ) will lose at least one distractor , 10 MCQs ( 9.8 % ) will lose all distractors and thus 34 MCQs ( 33.33 % ) will keep all 3 distractors .", "entities": []}
{"text": "A more relaxed question is how many MCQs had at least one plausible distractor , which can be estimated by calculating the entropy for each question as shown in Equation ( 2 ) , where A is the key , D is a distractor , Q is the stem , P Q ( A ) ( P Q ( D ) ) is the probability that the key ( any distractor ) is chosen for Q by a subject .", "entities": []}
{"text": "H ( Q ) =", "entities": []}
{"text": "\u2212 O { A , D } p Q ( O ) log ( p Q ( O ) )", "entities": []}
{"text": "( 2 ) The distribution of entropies per question is shown in Figure 3 .", "entities": []}
{"text": "Assuming the natural logarithm , the highest theoretically possible value for H ( Q ) is 0.69 , if p Q ( A ) = p Q ( D ) = 0.5 .", "entities": []}
{"text": "32 % of MCQs had an entropy larger than 0.65 , whereas 51 % had an entropy larger than 0.6 , which means that half of MCQs had at least one plausible distractor .", "entities": []}
{"text": "Bearing in mind the findings of Section 6.2.1 , it is interesting to see which of the proposed distractors ( especially , among LF - DIS ) teachers would mark as acceptable .", "entities": []}
{"text": "Given the complexity of such evaluation , using the whole test set was infeasible .", "entities": []}
{"text": "To get a representative sample , we used entropy per question ( shown in Figure 3 ) .", "entities": []}
{"text": "All MCQs were divided into 5 equally sized buckets by entropy and 9 MCQs were sampled uniformly at random from each bucket , resulting in 45 MCQs in total .", "entities": []}
{"text": "We asked 5 teachers to evaluate each MCQ ( presented in a random order for each of them ) .", "entities": []}
{"text": "Each MCQ contained the base text , the stem , the key and the generated distractors .", "entities": []}
{"text": "Additionally , we asked to provide their reasons for each rejected distractor in a free - text input .", "entities": []}
{"text": "On average , 1.47 distractors per MCQ were accepted by a teacher .", "entities": []}
{"text": "Their reasons for rejections are distributed as shown in Figure 4 .", "entities": []}
{"text": "All teachers accepted at least one generated distractor for 39 MCQs ( 86.7 % ) , whereas the majority of teachers did so for 27 MCQs ( 60 % ) .", "entities": []}
{"text": "Interestingly , there are no MCQs in which all 5 teachers have either accepted or rejected all generated distractors .", "entities": []}
{"text": "However , the majority of teachers has accepted or rejected all distractors for 4 MCQs ( 8.9 % ) and 6 MCQs ( 13.3 % ) respectively .", "entities": []}
{"text": "Out of 45 MCQs , 31 ( 68.9 % ) had at least one LF - DIS , as defined in Section 6.2.1 .", "entities": []}
{"text": "For these 31 MCQs we report a distribution of accepted / rejected LF - DIS by the majority of teachers in Figure 5 .", "entities": []}
{"text": "Let us call the 15 MCQs with all LF - DIS accepted by the majority of teachers as mismatch MCQs ( lowest row in Figure 5 ) .", "entities": []}
{"text": "Interestingly , 12 of the 15 mismatch MCQs had at least one more distractor in addition to LF - DIS being accepted by the majority of teachers .", "entities": []}
{"text": "Furthermore , all mismatch MCQs had entropy higher than 0.3 .", "entities": []}
{"text": "This entails that almost a half of LF - DIS should not necessarily be thrown away , since they were accepted by teachers , but the MCQs either happened to have more plausible distractors or subjects might have had relevant background knowledge to answer the questions .", "entities": []}
{"text": "We employed a systematic process to get a comprehensive overview of DG methods ( see Appendix E for more details ) .", "entities": []}
{"text": "Out of the resulting 28 articles ( see an overview in Table 4 ) , only 2 worked with a language other than English ( Chinese and Basque ) .", "entities": []}
{"text": "Two of these used rule - based approaches .", "entities": []}
{"text": "Majumder and Saha ( 2015 ) generated MCQs for cricket domain and used a number of hand - crafted rules based on gazeteers and Wikipedia entries to generate distractors .", "entities": []}
{"text": "Mitkov and Ha ( 2003 ) proposed to generate distractors for MCQs on electronic instructional documents using WordNet .", "entities": []}
{"text": "Six of these relied on extractive approaches .", "entities": []}
{"text": "Liang et al ( 2018 ) , Welbl et al ( 2017 ) , and Ha and Yaneva ( 2018 ) formulated choosing a distractor as a ranking problem from the given candidate set .", "entities": []}
{"text": "In the first two articles the candidate set constituted all distractors from the available MCQ dataset .", "entities": []}
{"text": "The authors then trained ML - based ranker ( s ) for choosing the best distractors .", "entities": []}
{"text": "In the last one , the candidate set was created using content engineers .", "entities": []}
{"text": "Distractors with a high similarity of their concept embeddings ( summed for multiple words ) and appearing in the same document as the key are ranked higher .", "entities": []}
{"text": "Stasaski and Hearst ( 2017 ) and Araki et al ( 2016 ) worked in the domain of biology .", "entities": []}
{"text": "Karamanis et al ( 2006 ) used thesaurus and tf - idf to identify key concepts in the given text and then select as distractors those having the same semantic type as the key .", "entities": []}
{"text": "The remaining four employed neural methods and are most relevant among the surveyed .", "entities": []}
{"text": "Zhou", "entities": []}
{"text": "et", "entities": []}
{"text": "Both articles used a beam search combined with filtering based on Jaccard coefficient at generation time .", "entities": []}
{"text": "Finally , Chung et al ( 2020 ) proposed a BERTbased method for English with answer - negative regularization , penalizing distractors for containing Then they rank every triple of distractors based on the entropy of a separately trained QA model .", "entities": []}
{"text": "Firstly , we did not include answer - negative regularization , since it is not always a good strategy .", "entities": []}
{"text": "For instance , given the stem \" When should you pay a fee if you apply for a visa ? \" and a key \" before you have submitted the application \" , the best distractor would be \" after you have submitted the application \" , which shares most of the words with the key .", "entities": []}
{"text": "Secondly , we generate distractors in arbitrary word order compared to left - to - right generation in ( Chung et al , 2020 ) .", "entities": []}
{"text": "Thirdly , at generation time , we use previously generated distractors as input for generating next ones , and always take tokens with a maximum probability .", "entities": []}
{"text": "This lowers the risk of generating ungrammatical distractors .", "entities": []}
{"text": "Finally , our training set is 100 times smaller compared to the training set used by Chung et al ( 2020 ) .", "entities": []}
{"text": "Around half of the generated distractors were found acceptable by the majority of teachers , and more than 50 % of MCQs had at least one plausible generated distractor , judging by the entropy of students ' responses .", "entities": []}
{"text": "System Demonstrations , pages 38 - 45 , Online .", "entities": []}
{"text": "Association for Computational Linguistics .", "entities": []}
{"text": "We have used publicly available texts from the websites of Swedish government agencies .", "entities": []}
{"text": "The exact list of URLs is provided in the GitHub repository associated with the paper .", "entities": []}
{"text": "The exact instructions given to students recruited to collect SweQUAD - MC dataset ( and their translation to English ) are presented in Figure 6 .", "entities": []}
{"text": "In addition to the given instructions , the students were also given the opportunity to slightly reformulate the distractors found in the text in order to align the syntactic structure with that of the key .", "entities": []}
{"text": "In addition to the metrics 1 - 10 presented in Section 6.1 , we have also looked at the following ones ( MCQ% means \" Percentage of MCQ \" and DIS means \" generated distractor ( s ) \" )", "entities": []}
{"text": "The rationale behind metric 11 was that capitalized answers are named entities and thus one would like distractors also to be named entities .", "entities": []}
{"text": "However , it does not always hold .", "entities": []}
{"text": "For instance , consider the stem \" Who gets an e - mail with a confirmation of a successful submission of the application for the work permit ? \" and the key \" you and your employer \" .", "entities": []}
{"text": "A distractor \" Migration Agency \" would suit the question perfectly , although capitalization is clearly different .", "entities": []}
{"text": "Metrics 12 - 17 were candidates to become overfitting indicators .", "entities": []}
{"text": "However , metric 2 was excluded , since AnyDisFromTrainDis is more informative , given phrases used as distractors in training data can be repeated in other texts .", "entities": []}
{"text": "Metrics 13 - 14 were excluded , since it 's unclear whether the higher or lower values are better .", "entities": []}
{"text": "For instance , if a text from the training data and the given text are thematically similar , would copying a distractor from training data be considered overfitting ?", "entities": []}
{"text": "Metrics 15 - 17 were rejected as too strict , leaving the possibility of actually missing overfitting if only 2 of 3 distractors would meet the criteria .", "entities": []}
{"text": "The quantitative performance metrics on the development set for the top - 3 models for each variant are presented in Table 5 .", "entities": []}
{"text": "We tried generating shortest distractors first ( SF ) , longest first ( LF ) or in a random order with a fixed seed of 42 ( RND ) .", "entities": []}
{"text": "The results of the experiment are presented in Table 6 .", "entities": []}
{"text": "Evidently , models with SF - generation consistently outperform ones with LF - generation .", "entities": []}
{"text": "SF - generation also performs on - par or better than RND - generation .", "entities": []}
{"text": "However , fixing a seed is not a generalizable solution , which is why we opted for SF - generation .", "entities": []}
{"text": "Evaluation from the student 's perspective has been conducted on the Prolific platform 7 .", "entities": []}
{"text": "We used Prolific 's pre - screening feature and required each subject to have Swedish as the first language and hold at least a high school diploma ( A - levels ) .", "entities": []}
{"text": "Given a text , your task is to create one or more multiple choice questions based on the text , i.e. : 1 . formulate a question with the correct answer in the text ; 2 . mark the correct answer in the text ; 3 . mark some wrong , but plausible options in the text .", "entities": []}
{"text": "When you have written your questions , marked the correct answer ( CA ) and the wrong alternatives in the text , click on \" Submit \" .", "entities": []}
{"text": "When you formulate the question , think about the following aspects .", "entities": []}
{"text": "The question must be independent , i.e. , one should not require additional information ( on top of the given text ) to be able to answer the question .", "entities": []}
{"text": "The question should be unambiguous and have only one possible interpretation .", "entities": []}
{"text": "One should not be able to answer your question without reading the text , which is why even wrong alternatives should be plausible .", "entities": []}
{"text": "Wrong options must be in the same grammatical form as the CA .", "entities": []}
{"text": "For instance , if the CA begins with a verb in Past Simple , all wrong options must begin with a verb in Past Simple .", "entities": []}
{"text": "Find as many questions as you can ( + the correct answer and wrong alternatives ) on each text and then get a new text when you ca n't find more .", "entities": []}
{"text": "Figure 6 : An English translation of the original instructions for SweQUAD - MC data collection ( the original instructions in Swedish can be found in the GitHub repository )", "entities": []}
{"text": "Thank you for participating in our study !", "entities": []}
{"text": "You will be presented with a number of multiple choice questions .", "entities": []}
{"text": "Your task is to answer as many of these questions correctly as possible .", "entities": []}
{"text": "If you do n't know which alternative is correct , choose the one that seems the most plausible .", "entities": []}
{"text": "You are allowed to use ONLY your own prior knowledge and common sense .", "entities": []}
{"text": "Please , do NOT consult any other external sources of information .", "entities": []}
{"text": "Figure 8 : An English translation of the original instructions given to subjects on the Prolific platform ( the original instructions in Swedish can be found in the GitHub repository ) is presented in Figure 7 .", "entities": []}
{"text": "The exact guidelines given to the subjects ( and their translation to English ) are presented in Figure 8 .", "entities": []}
{"text": "MCQs were presented in a random order , but the order of options for each MCQs was the same for each subject .", "entities": []}
{"text": "We used one sample t - test for conducting our analysis and thus the following assumptions were checked for .", "entities": []}
{"text": "1 .", "entities": []}
{"text": "The variable under study should be either an interval or ratio variable .", "entities": []}
{"text": "Our variable , the number of correctly answered MCQs , is clearly on a ratio scale .", "entities": []}
{"text": "The observations in the sample should be independent .", "entities": []}
{"text": "Subjects have performed the task independently of each other through a Prolific platform , hence the observations are independent .", "entities": []}
{"text": "3 .", "entities": []}
{"text": "The variable under study should be approximately normally distributed .", "entities": []}
{"text": "The distribution of the number of correctly answered MCQs is presented in Figure 7 ( the plot in the last row and the last column with the title \" num correct \" ) .", "entities": []}
{"text": "The distribution is indeed approximately normal .", "entities": []}
{"text": "4 .", "entities": []}
{"text": "The variable under study should have no extreme outliers .", "entities": []}
{"text": "The datapoints outside 1.5IQR are deemed mild outliers , whereas those outside 3IQR are considered extreme outliers .", "entities": []}
{"text": "Boxplots for our data with whiskers within both 1.5IQR and 3IQR are presented in Figure 9 .", "entities": []}
{"text": "Two datapoints can be considered mild outliers , but no extreme outliers are present , which means this assumption for the one sample t - test is not violated .", "entities": []}
{"text": "The exact guidelines given to the teachers and their translation to English , are presented in Figure 10 .", "entities": []}
{"text": "The total number of concordant and discordant pairs were summed for each pair of teachers for each MCQ .", "entities": []}
{"text": "To get a comprehensive overview of methods for generating distractors for MCQs , we employed a two - step process .", "entities": []}
{"text": "The second step was to select relevant references from the \" Related work \" sections of these articles .", "entities": []}
{"text": "This resulted into 15 additional articles .", "entities": []}
{"text": "Only 2 of these 28 papers worked with a language other than English ( Chinese and Basque ) .", "entities": []}
{"text": "A number of generated distractors along with the respective stems and keys from the dataset are presented in Figures 11 , 12 , 13 , 14 , 15 .", "entities": []}
{"text": "The questions are sampled based on the entropy of student 's an - Thank you for participating in our study !", "entities": []}
{"text": "You will be presented with a number of tests .", "entities": []}
{"text": "Your task is to judge which of the suggested distractors ( if any ) you would fit the purpose .", "entities": []}
{"text": "Select suitable distractors by simply ticking the respective checkboxes .", "entities": []}
{"text": "For the other distractors ( that you did n't select ) , please briefly state your reasons why these distractors were inappropriate in the respective text fields ( max 1 sentence ) .", "entities": []}
{"text": "Figure 10 : An English translation of the original instructions given to teachers ( the original instructions in Swedish can be found in the GitHub repository ) swers using the same 5 buckets as in sampling for teachers ' evaluation .", "entities": []}
{"text": "Hence , a red cross in the column \" F - DIS > 5 % \" entails that a given distractor is in fact an LF - DIS .", "entities": []}
{"text": "In this case , two of three distractors were accepted by the majority of teachers , although all of them were LF - DIS .", "entities": []}
{"text": "This is a good example of an MCQ with plausible distractors , but where the stem is too easy .", "entities": []}
{"text": "The MCQ in sample 2 presents an interesting case , when the distractor contains an obvious grammatical error ( comma before the first word in the distractor 3 ) .", "entities": []}
{"text": "While the distractor was rightfully rejected by the majority of teachers , it was still selected by more than 5 % of students .", "entities": []}
{"text": "The MCQ in sample 3 is a good example of longer distractors .", "entities": []}
{"text": "In this case , two distractors were accepted by teachers and two were selected by more than 5 % of students .", "entities": []}
{"text": "However , interestingly these sets are disjoint , meaning that all three distractors could potentially be useful .", "entities": []}
{"text": "Another more general observation , requiring future research , is that our model seems to struggle more when generating longer distractors in general , resulting in non - finished sentences or repetitions of words .", "entities": []}
{"text": "The MCQ in sample 4 is somewhat opposite to sample 3 , since one distractor that was accepted by the teachers turned out to be an LF - DIS .", "entities": []}
{"text": "This either means that the stem was too easy or that none of the distractors were potentially useful .", "entities": []}
{"text": "The MCQ in sample 5 is the one with a highest theoretically possible entropy between selecting the correct or a wrong option .", "entities": []}
{"text": "Note that it might still happen that some of the distractors is LF - DIS , since the entropy is calculated not between all four options , but only between the key and the distractors as a group .", "entities": []}
{"text": "Vad t\u00e4cker\u00f6ver h\u00e4lften av Sveriges yta ?", "entities": []}
{"text": "( What covers more than half of the surface of Sweden ? )", "entities": []}
{"text": "This work was supported by Vinnova ( Sweden 's Innovation Agency ) within project 2019 - 02997 .", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their comments , as well as Gabriel Skantze and Bram Willemsen for their helpful feedback prior to the submission of the paper .", "entities": []}
{"text": "Much Gracias : Semi - supervised Code - switch Detection for Spanish - English : How far can we get ?", "entities": []}
{"text": "Because of globalization , it is becoming more and more common to use multiple languages in a single utterance , also called codeswitching .", "entities": []}
{"text": "This results in special linguistic structures and , therefore , poses many challenges for Natural Language Processing .", "entities": []}
{"text": "In this paper , we explore semi - supervised approaches , that exploit out - of - domain monolingual training data .", "entities": []}
{"text": "1", "entities": []}
{"text": "Social platforms have been the cradle of the internet , driving vast amounts of communication among people from all over the world .", "entities": []}
{"text": "As a consequence , the way people communicate in written text has changed , as now it is common to use , for example , abbreviations of words , emoticons , references to other users and use multiple languages within the same utterance .", "entities": []}
{"text": "An annotated example sentence of this is the following tweet : Word El online exercise de hoy :", "entities": []}
{"text": "Label es en en es es other This phenomenon has caught particular interest in both sociolinguistics and Natural Language Processing ( NLP )", "entities": []}
{"text": "( Aguilar et al , 2020 ; Khanuja et al , 2020 ) .", "entities": []}
{"text": "Previous work has shown that high performances can be achieved for this task for many language pairs ( Molina et al , 2016 ; Banerjee et al , 2016 ) .", "entities": []}
{"text": "However , to the best of our knowledge , most previous work focused on supervised settings , restraining their usefulness to language pairs for which annotated datasets exist .", "entities": []}
{"text": "Recent efforts to unify existing datasets have collected annotation for 4 ( Aguilar et al , 2020 ) and 2 ( Khanuja et al , 2020 ) language pairs , which confirms that annotated data is not available for most language pairs .", "entities": []}
{"text": "In supervised settings , recent transformer models ( Vaswani et al , 2017 ; Devlin et al , 2019 ) have reached a new state - of - the - art ( Aguilar et al , 2020 ; Khanuja et al , 2020 ) , outperforming Bi - LSTMS and traditional machine learning methods used earlier ( Molina et al , 2016 ; Banerjee et al , 2016 ) .", "entities": []}
{"text": "We refer to this setup as semi - supervised , since we have no data annotated for the task at hand ( code - switch detection ) .", "entities": []}
{"text": "( RQ1 ) .", "entities": []}
{"text": "Since supervised methods have the advantage of learning from annotated data , the second research question is : How much can we reduce the gap in performance between the aforementioned semisupervised models and a supervised state - of - the - art model ?", "entities": []}
{"text": "( RQ2 ) .", "entities": []}
{"text": "Previous work in similar setups have automatically generated code - switched data from monolingual datasets ( Santy et al , 2021 ) .", "entities": []}
{"text": "We consider this approach to be orthogonal to ours , and Santy et al ( 2021 ) exploit mono - lingual in - domain data , syntactic parsers and parallel sentences .", "entities": []}
{"text": "In this section , we will first describe the manually annotated code - switched data that we use for evaluating our models , then we describe the monolingual data that we will use as \" training \" data .", "entities": []}
{"text": "It should be noted that this is not real training data , as it is not annotated for the task at hand ( thus the setting is semi - supervised ) .", "entities": []}
{"text": "( Aguilar et al , 2020 ) .", "entities": []}
{"text": "We chose this language pair because it has the challenge of increased similarity between the languages ( Tristram , 1999 ) .", "entities": []}
{"text": "We use the default development and test splits for our experiments .", "entities": []}
{"text": "In order to perform semi - supervised codeswitching detection , we use Wikipedia data , because it is available in many languages and easy to obtain .", "entities": []}
{"text": "We extracted dumps from September 1st 2020 with Wikiextractor 2 .", "entities": []}
{"text": "Without punctuation and numbers , the English dataset contains 420 K distinct words and the Spanish dataset contains 610 K distinct words .", "entities": []}
{"text": "It should be noted that there is a domain difference between the training and the dev / test data .", "entities": []}
{"text": "However , collecting monolingual data from Twitter 2 https://github.com/attardi/ wikiextractor is non - trivial .", "entities": []}
{"text": "3", "entities": []}
{"text": "Furthermore , it should be noted that the Wikipedia datasets are not 100 % monolingual , so there will be some Spanish data in the English dump and vice - versa .", "entities": []}
{"text": "Both of these artefacts might have a negative effect on performance .", "entities": []}
{"text": "Tokenization of the raw datasets is done using the English and Spanish SpaCy tokenization models 4 , as it matches the tokenization of the development and test sets .", "entities": []}
{"text": "Punctuation and non - word tokens ( the other class ) are identified with manually designed rules using regular expressions , and the python emoji package .", "entities": []}
{"text": "Tokens that are not identified as other , are labeled with the corresponding label based on the language of the wikipedia .", "entities": []}
{"text": "3 Methods", "entities": []}
{"text": "We first clean the mono - lingual Wikipedia data by removing XML / HTML tags from the articles and special tokens that belong to the other class .", "entities": []}
{"text": "We calculate the word probability based on the resulting data ( word frequency / total number of words ) using Laplace smoothing with a smoothing factor of 1 .", "entities": []}
{"text": "We also experiment with taking a larger context into account through bi - grams and tri - grams .", "entities": []}
{"text": "Here , we divide the frequency of the n - gram containing the word with the frequency of the leading ( n \u2212 1 ) gram .", "entities": []}
{"text": "The probability is computed this way for a given word in each language , and then the label with the highest probability is assigned to the word .", "entities": []}
{"text": "Laplace smoothing with a factor of 1 is used .", "entities": []}
{"text": "In our initial experiments , tri - grams showed very low performance , so we use bi - grams in the remainder of this paper .", "entities": []}
{"text": "For this model , we calculate the joint log probability of words based on the monolingual training data , and assign the most probable label .", "entities": []}
{"text": "We vary the n - gram size from 1 to 6 and use Laplace smoothing with a factor of 1 .", "entities": []}
{"text": "The problem of code - switching can be represented as a Hidden Markov Model ( HMM ) problem , since a sentence can be seen as a Markov chain with hidden states that are the two different languages .", "entities": []}
{"text": "We use the Viterbi decoding algorithm ( Forney , 1973 ) to find the most probable sequence of states given the observations - namely , to assign a language label ( state ) to each word ( observation ) .", "entities": []}
{"text": "The final hyperparameters are as follows : states : lang1 and lang2 , other tokens are identified based on heuristics ( see Section 2.3 ) ; initial probabilities : 0.6 for English and 0.4 for Spanish ; transition probabilities : 0.15 for transitioning to a different language and 0.85 for transitioning to the same language ; emission probabilities : these are estimated through a relative probability model , the probability of the word being emitted from English , for example , is : P ( w ) = P ( w | EN ) P ( w | EN )", "entities": []}
{"text": "+ P ( w | SP A ) , where P ( w | EN ) and P ( w | SP A ) are probabilities given by the dictionaries described in section 3.1 .", "entities": []}
{"text": "In our case , the documents are the words , the features are character n - grams ( with n 1 to 5 ) and the topics are English and Spanish .", "entities": []}
{"text": "Using TfidfVectorizer , we extract character n - gram features from each word , with n 1 to 5 .", "entities": []}
{"text": "We use the Scikit Learn implementation with all default parameters and select the first 100 , 000 words from each dataset .", "entities": []}
{"text": "Again , we use TfidfVectorizer to extract character n - gram features , with n 1 to 5 , and rely on the default Scikit Learn implementation .", "entities": []}
{"text": "We also experiment with ensembling the previous methods , where we use a simple majority voting .", "entities": []}
{"text": "We compare using all models , to using the best 3 and the best 5 models , as well as an oracle .", "entities": []}
{"text": "Results in Table 1 show that there is still a performance gap between the semi - supervised approaches and this state - of - the - art supervised model .", "entities": []}
{"text": "When comparing common confusions of our best semi - supervised model ( Viterbi ) to the output of MaChAmp , we found that there was more confusion in the Viterbi model about other , where 213 words were classified as lang1 and 60 as lang2 instead , compared to just 3 and 1 in MaChAmp .", "entities": []}
{"text": "Full confusion matrices can be found in the appendix .", "entities": []}
{"text": "The majority voting ensembling models do not lead to improved performance .", "entities": []}
{"text": "However , the oracle ensemble , which always picks the correct label if it is available from one of the models , shows that there is potential in improving the selection method for ensembling .", "entities": []}
{"text": "This might be due to a discrepancy between the label distribution of the two datasets and is a significant aspect to be investigated in future work .", "entities": []}
{"text": "This is also the case in our experiments , where the test data had a 54 % English and 46 % Spanish ratio .", "entities": []}
{"text": "For character n - grams , we observed that the more we increased the value of n , the better results we got , up until n = 6 .", "entities": []}
{"text": "This model achieves good results also because it addresses the problem of misspelled words .", "entities": []}
{"text": "This was then the preferred vectorizer in all models , as it helps decreasing the impact of very frequent character n - grams that are not expressing much value and gives more importance to less frequent character n - grams .", "entities": []}
{"text": "Improvements on the single models could be achieved by using bigger monolingual datasets of the same size or selecting a corpus that is more similar to the test set ( social media - like posts ) , which is not as easy to query as Wikipedia articles .", "entities": []}
{"text": "We ran the MaChAmp model in this environment and it completed in 53 , 990 seconds .", "entities": []}
{"text": "In comparison , the Viterbi training completed in 1 , 805 seconds , which is an improvement of almost 30 times faster than the MaChAmp model .", "entities": []}
{"text": "Furthermore , since the majority voting did not lead to improvements , we experimented with an Oracle model , which showed that by combining results form our models , the best score we could achieve is 98.47 % on validation data .", "entities": []}
{"text": "There is also a clear take away that , by using simpler , faster approaches like ours and when top performance is not crucial , one can avoid the extensive process of human - annotation and long training time that are needed by finetuning these large transformer models on supervised data .", "entities": []}
{"text": "It can be noted that the confusion matrix for MaChAmp model has more than the three labels we used , because it was trained on part of the original training set presented in Section 2.1 .", "entities": []}
{"text": "This set contained 8 classes , and , thus , occasionally , the model mistakenly predicted some of these classes .", "entities": []}
{"text": "It can be seen that there was more confusion in Viterbi model about other , where 213 words were classified as lang1 and 60 as lang2 instead , compared to just 3 and 1 in MaChAmp , which also had 7 other misclassifications .", "entities": []}
{"text": "To alleviate the sparsity problem and to leverage inherent decomposability of n - ary relations , we propose to learn relation representations of lower - arity facts that result from decomposing higher - arity facts .", "entities": []}
{"text": "The proposed method computes a score of a new nary fact by aggregating scores of its decomposed lower - arity facts .", "entities": []}
{"text": "While the vast majority of existing research focuses on extracting binary relations , there exists only few recent approaches to extract n - ary relations , that is , relations among n \u2265 2 entities ( Li et al , 2015 ; Ernst et al , 2018 ) .", "entities": []}
{"text": "As a motivating example , consider the following text from Wikipedia : \" Revis started off the 2009 season matched up against some of football 's best wide receivers .", "entities": []}
{"text": "In this example , two sentences collectively describes that Andre Johnson is a player of the football team the Texans during 2009 season , and thus we need cross - sentence information to correctly extract this ternary interaction among the three entities , i.e. Player ( Andre Johnson , Texans , 2009 season ) .", "entities": []}
{"text": "Previous methods ( Peng et al , 2017 ; Song et al , 2018 ) capture cross - sentence n - ary relation mentions by representing texts with a document graph which consists of both intra - and cross - sentence links between words .", "entities": []}
{"text": "However , these methods train the neural networks in a supervised manner using distant supervision ( Mintz et al , 2009 ) and , therefore , may suffer from the lack of sufficient positive labels when a well - populated knowledge base is not available .", "entities": []}
{"text": "In a universal schema approach , textual representations ( surface patterns ) of entities and their relations are encoded into the same vector space as the canonical knowledge base relations .", "entities": []}
{"text": "Thus , semantically similar surface patterns can share information of relation labels in a semisupervised manner .", "entities": []}
{"text": "This reduces the amount of required labeled training data .", "entities": []}
{"text": "Applying the universal schema approach to n - ary", "entities": []}
{"text": "(", "entities": []}
{"text": "1", "entities": []}
{"text": "This is be - cause the universal schema approach and its extensions ( Toutanova et al , 2015 ;", "entities": []}
{"text": "Verga et al , 2016Verga et al , , 2017 utilize co - occurring patterns of relation types between specific pair of entities .", "entities": []}
{"text": "Also , prior work has only addressed binary relations , and it is not trivial to define surface patterns among n > 2 entities and to encode these patterns into a vector representation .", "entities": []}
{"text": "To mitigate the aforementioned sparsity problem and utilize existing encoders for binary and unary surface patterns , we propose to train universal schema models on more dense lower - arity ( unary and binary ) facts instead of original sparse n - ary facts .", "entities": []}
{"text": "Our model learns representations of these lower - arity relations using the universal schema framework , and predicts new n - ary facts by aggregating scores of lower - arity facts .", "entities": []}
{"text": "3", "entities": []}
{"text": "The new datasets contain more entity tuples with known relational facts appeared in a knowledge base than the existing dataset ( Peng et al , 2017 ) , and , therefore , these datasets can be used to more effectively evaluate methods which predict relation labels for each individual entity tuple .", "entities": []}
{"text": "Let E be a set of entities , R KB be a set of relation types of an external knowledge base KB , and O KB = { r , ( e 1 , ... , e n ) : r ( e 1 , ... , e n ) KB , r 2 For example , the ternary relation AwardedFor ( director , movie , award ) can be decomposed into the binary relations DirectorOf ( director , movie ) and WonAward ( director , award ) .", "entities": []}
{"text": "Note that a similar idea is introduced in ( Ernst et al , 2018 ) as partial facts or partial patterns .", "entities": []}
{"text": "3 Our codes and datasets are available at https://github.com/aurtg/ nary - relation - extraction - decomposed .", "entities": []}
{"text": "R KB , e i E } be the set of facts in KB .", "entities": []}
{"text": "We collect a set of candidate entity tuples among which KB relation r R KB possibly holds .", "entities": []}
{"text": "4 Here , all entities in each candidate tuple ( e 1 , ... , e n ) are mentioned in the same text section T in a given set of documents .", "entities": []}
{"text": "We define a set of these entity mentions as O text = { T , ( e 1 , ... , e n ) :", "entities": []}
{"text": "e i E is mentioned in T } .", "entities": []}
{"text": "Here , text section T is a ( short ) span in a document which can describes relational facts among entities .", "entities": []}
{"text": "We use the term \" relation \" to refer to both relations r R KB and sections T .", "entities": []}
{"text": "O KB for relation r R KB given O", "entities": []}
{"text": "= O KB \u222a O text , where n \u2265 2 . 3 Proposed Method", "entities": []}
{"text": "To alleviate the sparsity problem of facts among n entities ( n > 2 ) and to utilize well - studied encoders for binary and unary surface patterns , we decompose a set of original n - ary facts , O , into a set of unary facts", "entities": []}
{"text": "O 1 and a set of binary facts O 2 ( Figure 1 ) .", "entities": []}
{"text": "the k - th argument of the original relation r.", "entities": []}
{"text": "If r is a KB relation , we define unary relation r ( k ) as a new canonicalized relation .", "entities": []}
{"text": "If r is section T , we define unary relation r ( k ) as a tuple r ( k )", "entities": []}
{"text": "= ( T , pos ( e k ) ) , where pos ( e k ) is a set of word position indices of entity e k in section T ( Figure 2 ) .", "entities": []}
{"text": "We denote a set of all decomposed unary facts by O 1 .", "entities": []}
{"text": "Intuitively , these unary relations represent semantic roles or types of corresponding arguments of the original relation r .", "entities": []}
{"text": "Binary Facts : Given an n - ary fact r , ( e 1 , ... , e", "entities": []}
{"text": "k ) O , we decompose it into a set of n ( n \u2212 1 ) binary facts { r ( k , l ) , ( e k , e l ) :", "entities": []}
{"text": "If r is a section T , we represent it by the shortest path between e k and e l on the document graph ( Quirk and Poon , 2017 ) of T ( Figure 2 ) , and denote it by path ( T ; e k , e l ) .", "entities": []}
{"text": "We denote the set of all decomposed binary facts by O 2 .", "entities": []}
{"text": "We learn a vector representation v ( r ) R dr for each unary or binary relation in O 1 or O 2 .", "entities": []}
{"text": "For r ( k ) or r ( k , l ) derived from a KB relation , we represent it by a trainable parameter vector .", "entities": []}
{"text": "On the other hand , for the one derived from a textual relation , we use the following encoders to compute its representations .", "entities": []}
{"text": "( Schuster and Paliwal , 1997 ) to compute a hidden representation h l R dr at each word position l.", "entities": []}
{"text": "Following recent works He et al , 2018 ; Lee et al , 2017 ) , we aggregate h l within a phrase of entity e k to compute v ( T ( k ) ) .", "entities": []}
{"text": "We use elementwise mean as aggregation function : v ( r ( k ) )", "entities": []}
{"text": "= mean ( { h l : l pos ( e k ) } ) .", "entities": []}
{"text": "( 1 ) Binary encoder : For a binary textual relation r ( k , l ) = path ( T ; e k , e l ) , we represent each token ( word or edge label ) in path ( T ; e k , e l ) by an embedding vector ( Toutanova et al , 2015 ; Verga et al , 2016 ) .", "entities": []}
{"text": "= max ( { h l : l = 1 , ... , L } ) .", "entities": []}
{"text": "( 2 )", "entities": []}
{"text": "We follow Verga et al ( 2017 ) to train relation representations ( 3.2 ) .", "entities": []}
{"text": "+", "entities": []}
{"text": "O i and those of K sampled negative facts r , p", "entities": []}
{"text": "\u2212 k /", "entities": []}
{"text": "O i .", "entities": []}
{"text": "We sample negative facts by randomly replacing entity tuple p + in the original fact by different entity tuples p \u2212 k .", "entities": []}
{"text": "L i = E r , p", "entities": []}
{"text": "+ O i r , p", "entities": []}
{"text": "\u2212 k /", "entities": []}
{"text": "O", "entities": []}
{"text": "] .", "entities": []}
{"text": "Entity tuple representations v ( p ; r ) are computed with a weighted average of the representations { v ( r ) : r V ( p ) } as shown in ( 4 ) and ( 5 ) where a ( r , r ; V ( p ) ) is the attention weight for each relation r V ( p ) .", "entities": []}
{"text": "5 v ( p ; r ) = r V ( p ) a ( r , r ; V ( p ) )", "entities": []}
{"text": "v ( r ) , a ( r , r ; V ( p ) )", "entities": []}
{"text": "= exp ( v ( r ) T v ( r ) )", "entities": []}
{"text": "r V ( p ) exp ( v ( r ) T v ( r ) ) .", "entities": []}
{"text": "( 4 ) V ( p ) = { r : r , e k O 1 } ( if p = e k ) { r : r , ( e k , e l ) O 2 } ( if p = ( e k , e l ) ) .", "entities": []}
{"text": "( 5 )", "entities": []}
{"text": "Note that L n directly contrasts n - ary scores associated with KB relations r R KB in a more supervised manner than both L 1 and L 2 . 6 ( 6 )", "entities": []}
{"text": "Since our proposed method and universal schemas baselines predict KB relations for each entity tuple instead of each surface pattern , the number of known facts of KB relations is crucial to reliably evaluate and compare these methods .", "entities": []}
{"text": "To create the Wiki - 90k and WF - 20k datasets , we used Wikidata and Freebase respectively as external knowledge bases .", "entities": []}
{"text": "Since these knowledge bases store only binary relational facts , we defined multiple ternary relations by combining a few binary relations .", "entities": []}
{"text": "7 , 8 For both datasets , we collected paragraphs from the English Wikipedia , and used Stanford CoreNLP ( Manning et al , 2014 ) to extract dependency and co - reference links .", "entities": []}
{"text": "We followed ( Peng et al , 2017 ) to extract co - occurring entity tuples and their surface patterns , that is , we selected tuples which occurred in a minimal span within at most M \u2264 3 consecutive sentences .", "entities": []}
{"text": "Entity tuples without a known KB relation are subsampled , since the number of such tuples are too large .", "entities": []}
{"text": "We randomly partitioned all entity tuples into train , development ( dev ) , and test sets .", "entities": []}
{"text": "( Song et al , 2018 ) :", "entities": []}
{"text": "The concatenated vector is then fed into a classifier to predict the relation label .", "entities": []}
{"text": "Since their method directly predicts a relation label for each surface pattern , it is more robust to the sparsity of surface patterns among a specific higher arity entity tuple .", "entities": []}
{"text": "However , due to their purely supervised training objective , its performance may degrade if the number of available training labels is small .", "entities": []}
{"text": "Universal schemas : We compared our method with semi - supervised methods based on universal schemas ( Toutanova et al , 2015 ; Verga et al , 2017 ) .", "entities": []}
{"text": "In our experiments , we used the same encoder as ( Song et al , 2018 ) to encode each surface pattern .", "entities": []}
{"text": "9 We tested two types of scoring functions , Model F and Model E , as in ( Toutanova et al , 2015 ) .", "entities": []}
{"text": "10 , 11", "entities": []}
{"text": "Unless otherwise noted , reported values are average values over six experiments , in which network parameters are randomly initialized .", "entities": []}
{"text": "All reported p - values are calculated based on Wilcoxon rank sum test ( Wilcoxon , 1945 ) with", "entities": []}
{"text": "Table 1 illustrates the performance of each method .", "entities": []}
{"text": "12", "entities": []}
{"text": "Interestingly , Model F performs well in Verga et al ( 2017 ) baseline , while it shows low performance in Toutanova et al ( 2015 ) baseline .", "entities": []}
{"text": "Ablation Study : Table 2 illustrates the performance of various settings of our proposed method .", "entities": []}
{"text": "In the result , U+B performs significantly better ( p < 0.005 ) than U and B , and this shows effectiveness of combining scores of both binary facts and unary facts .", "entities": []}
{"text": "On the other hand , there was no significant difference between U+B+N and N ( p > 0.9 ) .", "entities": []}
{"text": "Furthermore , we also investigated the influence of the training data size ( the number of positive labels ) of our proposed method and baseline methods .", "entities": []}
{"text": "13", "entities": []}
{"text": "ary facts into dense unary and binary facts .", "entities": []}
{"text": "Experiments on two datasets with multiple ternary relations show that our proposed method can statistically significantly improve over previous works , which suggests the effectiveness of using unary and binary interaction among entities in surface patterns .", "entities": []}
{"text": "However , as Fatemi et al ( 2019 ) suggests , there exists cases in which reconstructing n - ary facts from decomposed binary facts induces false positives .", "entities": []}
{"text": "Tackling this issue is one important future research direction .", "entities": []}
{"text": "We thank all the EMNLP reviewers and Daniel Andrade for their valuable comments and suggestions to improve the paper .", "entities": []}
{"text": "Parallel texts of Japanese and a non - pro - drop language have the potential of improving the performance of Japanese zero anaphora resolution ( ZAR ) because pronouns dropped in the former are usually mentioned explicitly in the latter .", "entities": []}
{"text": "In addition , we find that the incorporation of the masked language model training into MT leads to further gains .", "entities": []}
{"text": "This is , however , especially challenging for so - called prodrop languages like Japanese and Chinese because they usually omit pronouns that are inferable from context .", "entities": []}
{"text": "The task of identifying the referent of such a dropped element , as illustrated in Figure 1 ( a ) , is referred to as zero anaphora resolution ( ZAR ) .", "entities": []}
{"text": "A major barrier to improvement is the scarcity of training data .", "entities": []}
{"text": "The number of annotated sentences is the order of tens of thousands or less ( Kawahara et al , 2002 ;", "entities": []}
{"text": "Hangyo et al , 2012 ; Iida et al , 2017 ) , and the considerable linguistic expertise required for annotation makes drastic corpus expansion impractical .", "entities": []}
{"text": "Previous attempts to overcome this limitation exploit orders - of - magnitude larger parallel texts of Japanese and English , a non - pro - drop language ( Nakaiwa , 1999 ; Furukawa et al , 2017 ) .", "entities": []}
{"text": "The key idea is that Japanese zero pronouns can be recovered from parallel texts because they are usually mentioned explicitly in English , as in Figure 1 ( b ) .", "entities": []}
{"text": "If translation correspondences are identified and the anaphoric relation in English is identified , then we can identify the antecedent of the omitted argument in Japanese .", "entities": []}
{"text": "Their rule - based transfer from English to Japanese had met with limited success , however .", "entities": []}
{"text": "More importantly , the great linguistic differences between the two language often lead to parallel sentences without transparent syntactic correspondences ( Figure 1 ( c ) ) .", "entities": []}
{"text": "By generating English translations , a neural MT model should be able to implicitly recover omitted Japanese pronouns , thanks to its expressiveness and large training data .", "entities": []}
{"text": "We expect the knowledge gained during MT training to be transferred to ZAR .", "entities": []}
{"text": "Vu et al , 2020 ) :", "entities": []}
{"text": "1", "entities": []}
{"text": "A key challenge to this approach is a mismatch in model architectures .", "entities": []}
{"text": "The nominative argument of the underlined predicate is omitted .", "entities": []}
{"text": "The goal of the task is to detect the omission and to identify its antecedent \" son \" .", "entities": []}
{"text": "( b ) The corresponding English text .", "entities": []}
{"text": "The omitted argument in Japanese is present as a pronoun in English .", "entities": []}
{"text": "( c ) A Japanese - English pair ( Nabeshima and Brooks , 2020 , p. 74 ) whose correspondences are too obscure for rule - based transfer .", "entities": []}
{"text": "Because Japanese generally avoids having inanimate agents with animate patients , the English inanimate - subject sentence corresponds to two animate - subject clauses in Japanese , with two exophoric references to the reader ( i.e. , you ) .", "entities": []}
{"text": "it is non - trivial to combine the two distinct architectures , with the goal to help the former .", "entities": []}
{"text": "While this technique was previously used by Imamura and Sumita ( 2019 ) and Clinchant et al ( 2019 ) , they both aimed at improving MT performance .", "entities": []}
{"text": "We show that by ejecting the encoder part for use in fine - tuning ( Figure 2 ) , we can achieve performance improvements in ZAR .", "entities": []}
{"text": "ZAR has been extensively studied in major East Asian languages , Chinese and Korean as well as Japanese , which not only omit contextually inferable pronouns but also show no verbal agreement for person , number , or gender ( Park et al , 2015 ; Song et al , 2020 ; Kim et al , 2021 ) .", "entities": []}
{"text": "While supervised learning is the standard approach to ZAR ( Iida et al , 2016 ; Ouchi et al , 2017 ; Shibata and Kurohashi , 2018 ) , training data are so small that additional resources are clearly needed .", "entities": []}
{"text": "Early studies work on case frame construction from a large raw corpus ( Sasano et al , 2008 ; Sasano and Kurohashi , 2011 ; Yamashiro et al , 2018 ) , pseudo training data generation , and adversarial training ( Kurita et al , 2018 ) .", "entities": []}
{"text": "Konno et al , 2020 ) .", "entities": []}
{"text": "Their method is used as a state - of - the - art baseline in our experiments .", "entities": []}
{"text": "They found that performance gains were achieved by selecting target tokens by part of speech .", "entities": []}
{"text": "Konno et al ( 2021 ) introduce a more elaborate masking strategy as a ZAR - specific intermediate task They spot multiple occurrences of the same noun phrase , mask one of them , and force the model to identify the pseudo - antecedent .", "entities": []}
{"text": "Our use of parallel texts in ZAR is inspired by Nakaiwa ( 1999 ) and Furukawa et al ( 2017 ) , who identify a multi - hop link from a Japanese zero pronoun to its Japanese antecedent via English counterparts .", "entities": []}
{"text": "Their rule - based methods suffer from accumulated errors and syntactically non - transparent correspondences .", "entities": []}
{"text": "In addition , they do not handle inter - sentential anaphora , a non - negligible subtype of anaphora we cover in this paper .", "entities": []}
{"text": "While we exploit MT to improve the performance of ZAR , the exploitation in the reverse direction has been studied .", "entities": []}
{"text": "A line of research has been done on Chinese zero pronoun prediction ( ZPP ) with a primary aim of improving Chinese - English translation ( Wang et al , 2016 ( Wang et al , , 2018 ( Wang et al , , 2019b .", "entities": []}
{"text": "ZPP is different from ZAR in that it does not identify antecedents .", "entities": []}
{"text": "This is understandable given that classification of zero pronouns into overt ones suffices for MT .", "entities": []}
{"text": "Although Wang et al ( 2019b ) open question whether MT helps ZAR as well .", "entities": []}
{"text": "2", "entities": []}
{"text": "[ author ] [ NA ] !", "entities": []}
{"text": "[ CLS ] [ author ] [ NA ] !", "entities": []}
{"text": "Figure 3 : ZAR as argument selection .", "entities": []}
{"text": "model .", "entities": []}
{"text": "Imamura and Sumita ( 2019 ) manage to maintain the straightforward approach by adopting a twostage training procedure :", "entities": []}
{"text": "In the first stage , only the decoder is updated with the encoder frozen , while in the second stage , the entire model is updated .", "entities": []}
{"text": "Omission detection concerns whether a given predicate has an argument for a given case ( relation ) .", "entities": []}
{"text": "If not , the model must point to the special token [ NULL ] .", "entities": []}
{"text": "Otherwise the model must identify the antecedent of the zero pronoun by pointing either to a token in the given text or to a special token reserved for exophora .", "entities": []}
{"text": "Note that by getting the entire document as the input , the model can handle inter - sentential anaphora as well as intra - sentential anaphora .", "entities": []}
{"text": "Also note that in this formulation , ZAR is naturally subsumed into verbal predicate analysis ( VPA ) , which also covers instances where the predicate and the argument have a dependency relation and only the case marker is absent .", "entities": []}
{"text": "Formally , the probability of the token t j being the argument of the predicate t i for case c is : P ( t j | t i , c )", "entities": []}
{"text": "= exp ( s c ( t j , t i ) )", "entities": []}
{"text": "j exp ( s c ( t j , t i ) )", "entities": []}
{"text": "( 1 ) s c ( t j , t i )", "entities": []}
{"text": "= v tanh ( W c t j", "entities": []}
{"text": "+ U c t i )", "entities": []}
{"text": "( 2 ) where t i is the context - aware embedding of t", "entities": []}
{"text": "We output t j with the highest probability .", "entities": []}
{"text": "Our main proposal is to use MT as an intermediate task prior to fine - tuning on ZAR .", "entities": []}
{"text": "After the intermediate training on MT , we extract the encoder and move on to fine - tuning on ZAR and related tasks ( Figure 2 ) .", "entities": []}
{"text": "Specifically , we test the following two procedures for intermediate training : One - stage optimization", "entities": []}
{"text": "The entire model is updated throughout the training .", "entities": []}
{"text": "Two - stage optimization In the first stage", "entities": []}
{"text": ", the encoder is frozen and only the decoder is updated .", "entities": []}
{"text": "In the second stage , the entire model is updated ( Imamura and Sumita , 2019 ) .", "entities": []}
{"text": "Specifically , we mask some input tokens on the encoder Web News # of sentences 16 , 038 11 , 276 # of zeros 30 , 852 27 , 062 side and force the model to recover the original tokens , as depicted in the center of Figure 2 .", "entities": []}
{"text": "We choose 15 % of the tokens at random and 80 % of them are replaced with [ MASK ] , 10 % of them with a random token , and the rest are unchanged .", "entities": []}
{"text": "ZAR We used two corpora in our experiments : the Kyoto University Web Document Lead Corpus ( Hangyo et al , 2012 ) and the Kyoto University Text Corpus ( Kawahara et al , 2002 ) .", "entities": []}
{"text": "Based on their genres , we refer to them as the Web and News , respectively .", "entities": []}
{"text": "These corpora have been widely used in previous studies ( Shibata and Kurohashi , 2018 ;", "entities": []}
{"text": "Kurita et al , 2018 ; Ueda et al , 2020 ) .", "entities": []}
{"text": "We split the datasets into training , validation , and test sets following the published setting , where the ratio was around 0.75:0.1:0.15 .", "entities": []}
{"text": "Key statistics are shown in Table 1 .", "entities": []}
{"text": "MT We used a Japanese - English parallel corpus of newspaper articles distributed by the Yomiuri Shimbun .", "entities": []}
{"text": "3 It consisted of about 1.3 million sentence pairs 4 with sentence alignment scores .", "entities": []}
{"text": "Because the task of interest , ZAR , required inter - sentential reasoning , consecutive sentences were concatenated into chunks , with the maximum number of tokens equal to that of ZAR .", "entities": []}
{"text": "As a result , we obtained around 373 , 000 , 21 , 000 , and 21 , 000 chunks for the training , validation , and test data , respectively .", "entities": []}
{"text": "Japanese sentences were split into words using the morphological analyzer MeCab with the Juman dictionary ( Kudo et al , 2004 ) .", "entities": []}
{"text": "5 Both Japanese and English texts underwent subword tokenization .", "entities": []}
{"text": "We used separate vocabularies for Japanese and English , with the vocabulary sizes of around 32 , 000 and 16 , 000 , respectively .", "entities": []}
{"text": "6", "entities": []}
{"text": "It was trained on the full text of Japanese Wikipedia for approximately 1 million steps .", "entities": []}
{"text": "In two - stage optimization , the encoder was frozen during the first 15 epochs , then the entire model was updated for the remaining 35 epochs .", "entities": []}
{"text": "The details of hyper - parameters are given in Appendix A. ZAR For a fair comparison with Ueda et al ( 2020 ) , we used almost the same configuration as theirs .", "entities": []}
{"text": "We dealt with all subtypes of ZAR : intra - sentential anaphora , inter - sentential anaphora , and exophora .", "entities": []}
{"text": "For exophora , we targeted [ author ] , [ reader ] , and [ unspecified person ] .", "entities": []}
{"text": "We set the maximum sequence length to 128 .", "entities": []}
{"text": "7 All documents from the Web met this limitation .", "entities": []}
{"text": "In the News corpus , however , many documents exceeded the sequence length of 128 .", "entities": []}
{"text": "For such documents , we divided the document into multiple parts such that it had the longest preceding contexts .", "entities": []}
{"text": "The evaluation of ZAR was relaxed using a gold coreference chain .", "entities": []}
{"text": "The model was trained on the mixture of both corpora and evaluated on each corpus .", "entities": []}
{"text": "We used almost the same", "entities": []}
{"text": "Web News hyper - parameters as Ueda et al ( 2020 ) , which are included in Appendix B.", "entities": []}
{"text": "We decided to tune the training epochs for MT since we found that it slightly affected ZAR performance .", "entities": []}
{"text": "They were all trained on ZAR , and we chose the one with the highest score on the validation set .", "entities": []}
{"text": "We report the mean and the standard deviation of the 9 runs .", "entities": []}
{"text": "Table 2 summarizes the experimental results .", "entities": []}
{"text": "Tables 3 and 4 provide more detailed results .", "entities": []}
{"text": "The subsequent two blocks compare one - stage ( unmarked ) optimization with two - stage optimization .", "entities": []}
{"text": "MT yielded gains on all settings .", "entities": []}
{"text": "The gains were consistent across anaphora categories .", "entities": []}
{"text": "Two - stage optimization showed mixed results .", "entities": []}
{"text": "It worked for the Web but did not for the News .", "entities": []}
{"text": "The gains were larger with one - stage optimization than with two - stage optimization ( 1.4 vs. 0.3 on the Web ) .", "entities": []}
{"text": "The experimental results demonstrate that MT helps ZAR , but why does it work ?", "entities": []}
{"text": "To address this problem , Shimazu et al ( 2020 ) and Nagata and Morishita ( 2020 ) constructed Japanese - English parallel datasets that were designed to automatically evaluate MT models with regard to the translation of Japanese zero pronouns ( ZPT ) .", "entities": []}
{"text": "We used Shimazu et al 's dataset for its larger data size .", "entities": []}
{"text": "10 To facilitate automatic evaluation of ZPT , this dataset paired a correct English sentence with an incorrect one .", "entities": []}
{"text": "All we had to do was to calculate the ratio of instances for which the model assigned higher translation scores to the correct candidates .", "entities": []}
{"text": "The only difference between the two sentences involved the translation of a Japanese zero pronoun .", "entities": []}
{"text": "To choose the correct one , the MT model must sometimes refer to preceding sentences .", "entities": []}
{"text": "As in intermediate training , multiple source sentences were fed to the model to generate multiple target sentences .", "entities": []}
{"text": "We prepended as many preceding sentences as possible given the limit of 128 tokens .", "entities": []}
{"text": "In addition , this dataset recorded d , the sentencelevel distance between the zero pronoun in question and its antecedent .", "entities": []}
{"text": "We regarded the former as the instances of intra - sentential anaphora and the latter as the instances of inter - sentential anaphora .", "entities": []}
{"text": "As before , Web News intra - sentential anaphora 0.758 0.763 inter - sentential anaphora 0.871 0.879 5 shows the strong positive correlations between the two performance measures , especially the very strong correlation for inter - sentential anaphora .", "entities": []}
{"text": "These results were in line with our speculation that the performance gains in ZAR stemmed from the model 's increased ability to translate zero pronouns .", "entities": []}
{"text": "Pruksachatkun et al ( 2020 ) conjecture that it would mitigate catastrophic forgetting ( CF ) , but this is not the sole explanation .", "entities": []}
{"text": "In fact , Konno et al ( 2020 ) see token masking as a way to augment data .", "entities": []}
{"text": "We assume that this model was largely deprived of the power to mitigate CF while token masking still acted as a data augmenter .", "entities": []}
{"text": "Table 6 shows the results .", "entities": []}
{"text": "However , it did outperform + MT , and the gain was particularly large for the Web .", "entities": []}
{"text": "To gain further insights , we compared ZAR results with English translations automatically generated by the corresponding MT model .", "entities": []}
{"text": "Figure 4 gives two examples .", "entities": []}
{"text": "It is no great surprise that the translation quality was not satisfactory because we did not fully optimize the model for it .", "entities": []}
{"text": "In the exmple of Figure 4 ( a ) , MT seems to have helped ZAR .", "entities": []}
{"text": "The omitted nominative argument of \" \u3042\u308a \" ( is ) was correctly translated as \" the school \" , and the model successfully identified its antecedent \" \u5b66\u6821 \" ( school ) while the baseline failed .", "entities": []}
{"text": "Figure 4 ( b ) illustrates a limitation of the proposed approach .", "entities": []}
{"text": "The omitted nominative argument of the predicate , \" \u3067 \" ( be ) , points to \" \u5b9a\u5409 \" ( Sadakichi , the father of Jutaro ) .", "entities": []}
{"text": "Although the model correctly translated the zero pronoun as \" He \" , it failed in ZAR .", "entities": []}
{"text": "This is probably because not only \" \u5b9a\u5409 ( Sadakichi ) \"", "entities": []}
{"text": "but also \" \u9f8d\u99ac \" ( Ryoma ) and \" \u91cd\u592a\u90ce \" ( Jutaro ) can be referred to as \" He \" .", "entities": []}
{"text": "When disambiguation is not required to generate an overt pronoun , MT is not very helpful .", "entities": []}
{"text": "Because MT requires the encoder - decoder architecture , maintaining the model architecture between pretraining and intermediate training looked promising to us .", "entities": []}
{"text": "During fine - tuning , we added the ZAR argument selection layer on top of either the encoder or the decoder .", "entities": []}
{"text": "Unfortunately , gains from MT intermediate training were marginal for these models .", "entities": []}
{"text": "We gave up identifying the cause of poorer performance because it was extremely hard to apply comparable experimental conditions to large pretrained models .", "entities": []}
{"text": "Although previous studies reported negative results on the use of MT as an intermediate task , we demonstrated that it did work for Japanese ZAR .", "entities": []}
{"text": "Our analysis suggests that the intermediate training on MT simultaneously improved the model 's ability to translate Japanese zero pronouns and the ZAR performance .", "entities": []}
{"text": "Previous studies focusing on MT reported mixed results on this approach , but again , we demonstrated its considerable positive impact on ZAR .", "entities": []}
{"text": "The example in which MT apparently helped ZAR .", "entities": []}
{"text": "The nominative zero pronoun of \" \u3042\u308a \" ( is ) was correctly translated as \" the school \" .", "entities": []}
{"text": "The model also succeeded in identifying its antecedent \" \u5b66 \u6821 \" ( school ) .", "entities": []}
{"text": "( b ) The example in which MT was not helpful .", "entities": []}
{"text": "The model successfully translated the nominative zero pronoun of the underlined predicate , \" \u3067 \" ( be ) , as \" He \" .", "entities": []}
{"text": "It misidentified its antecedent , however .", "entities": []}
{"text": "We thank the Yomiuri Shimbun for providing Japanese - English parallel texts .", "entities": []}
{"text": "We are grateful for Nobuhiro Ueda ' help in setting up the baseline model .", "entities": []}
{"text": "We thank the anonymous reviewers for their insightful comments .", "entities": []}
{"text": "Although we followed Ueda et al ( 2020 ) with respect to hyper - parameter settings , there was one exception .", "entities": []}
{"text": "Verbal predicate analysis is conventionally divided into three types : overt , covert , and zero .", "entities": []}
{"text": "While Ueda et al ( 2020 ) excluded the easiest overt type from training , we targeted all the three types because we found slight performance improvements .", "entities": []}
{"text": "The overt type covers situations", "entities": []}
{"text": "Tables 9 and 10 show the performance on the validation sets .", "entities": []}
{"text": "Resolution and Zero Pronoun Translatoin", "entities": []}
{"text": "An event is defined as a real world occurrence that takes place in a specific time and space ; Atefeh and Khreich ( 2013 ) identifies these occurrences by the entities that took part on it as well as the activities done in it .", "entities": []}
{"text": "This project focuses on researching about the viability of modeling these events as ontologies using an automatic approach for entities and relationships extraction in order to obtain relevant information about the event in case .", "entities": []}
{"text": "Spanish data from Twitter was used as a study case and tested with the developed application .", "entities": []}
{"text": "According to Lobzhanidze et al ( 2013 ) , globalization and the increased use of social networks has made it possible for news and events related information to be propagated in a much faster manner to every part of the world .", "entities": []}
{"text": "It is in this context that event analysis is the most relevant since , as Valkanas and Gunopulos ( 2013 ) mention , now there is more data available to study and analyze than ever before .", "entities": []}
{"text": "An event is defined as a real world occurrence that takes place in a specific time and space ; Atefeh and Khreich ( 2013 ) identifies these occurrences by the entities that took part on it as well as the activities done in it .", "entities": []}
{"text": "Events will be the main study object in this paper and , more specifically , event data in Spanish obtained from Twitter will be used to test the different methods and techniques exposed on each Section .", "entities": []}
{"text": "In order to effectively analyze events there are two steps that need to be taken into consideration as mentioned in Kumbla ( 2016 ) : ( 1 ) event data acquisition , and ( 2 ) event data processing .", "entities": []}
{"text": "The first step is the one that benefits the most by social media streaming since more data is available , though one of the downsides to this is that the data is usually not ready to be used right away and most of the times a preprocessing step needs to happen .", "entities": []}
{"text": "This step is further explained on section Section 3 .", "entities": []}
{"text": "The second step will be the main focus on this paper since the biggest problem on event data analysis in Spanish is this one .", "entities": []}
{"text": "In particular , automatic approaches for entities and relationships extraction will be presented on Section 4 .", "entities": []}
{"text": "The remainder of this paper is organized as follows .", "entities": []}
{"text": "In Section 2 some relevant related work is exposed .", "entities": []}
{"text": "Later , in Section 3 the event acquisition process is further expanded upon .", "entities": []}
{"text": "Section 5 introduces a simple application developed in order to make use of the algorithms and techniques mentioned on the previous sections .", "entities": []}
{"text": "On section 6 we compare the results obtained with manually created ontologies and obtain precision and recall values for each case .", "entities": []}
{"text": "Finally , concluding remarks are provided in Section 7 .", "entities": []}
{"text": "The entities from the data are extracted based on rules related to the Arabic language .", "entities": []}
{"text": "In Ilknur et al ( 2011 ) a framework for learning relations between entities in Twitter is presented .", "entities": []}
{"text": "This framework allows for entities as well as entity types or topics to be detected , which results in a graph connecting semantically enriched resources to their respective entities .", "entities": []}
{"text": "Then relation discovery strategies are employed to detect pair of entities that have a certain type of relationship in a specific period of time .", "entities": []}
{"text": "This model also contains some key characteristics such as place , location , agents and products .", "entities": []}
{"text": "On the other hand , event - subevent relationships are used to build the related ontologies .", "entities": []}
{"text": "This model was developed for the Center for Digital Music and tested by structuring proceedings and concert descriptions .", "entities": []}
{"text": "This approach also uses a POS - tagging step in order to obtain the initial set of entities to process .", "entities": []}
{"text": "As it was mentioned before , nowadays there are numerous avenues for event data acquisition .", "entities": []}
{"text": "For this paper Twitter was chosen as the social network to use for retrieving data since this data is easily available and a good amount of it is related to events of different categories .", "entities": []}
{"text": "Twitter 's REST API was used in order to retrieve data related to these events : Each dataset had a file per day with all the tweets from the day and contained only the text that represents a tweet per line .", "entities": []}
{"text": "With the raw data ready to be used , the preprocessing step followed .", "entities": []}
{"text": "The sequence followed is exposed below :", "entities": []}
{"text": "1 .", "entities": []}
{"text": "Removing punctuation and unicode only characters except written accents .", "entities": []}
{"text": "Each tokenized tweet al o contains a reference to the original , unprocessed tweet , which will be used on Section 5 .", "entities": []}
{"text": "Machine learning 2 .", "entities": []}
{"text": "Statistical approach Statistical based algorithms are further discussed on Sections 4.3 and 4.4 .", "entities": []}
{"text": "Where Entity denotes a subject that interacts in the event , Temporal entity refers to the date when the particular activity takes place and object is the recipient of the activity .", "entities": []}
{"text": "This was one of the main points of interest and research on this paper , how to select the most representative entities for the event in order to not overwhelm people analyzing the results but also to not present too little or irrelevant information .", "entities": []}
{"text": "2 . UDPipe :", "entities": []}
{"text": "UDPipe allows to parse text in order to obtain the grammatical categories of the words in each sentence , as well as the syntactic dependencies or syntactic tree that envelops the whole sentence .", "entities": []}
{"text": "The entities are obtained from the grammatical category PROPN .", "entities": []}
{"text": "These two approaches were then implemented and tested with each dataset and a manual comparison was made between the entities that each approach captured .", "entities": []}
{"text": "Because of this , UDPipe was chosen as the main initial entity extraction tool moving forward .", "entities": []}
{"text": "After having a set of initial entities , further processing steps were taken to ensure a better result .", "entities": []}
{"text": "Entity clustering was done on two stages .", "entities": []}
{"text": "First , an algorithm for entity clustering was devised based on two metrics : 1 . Normalized frequency of two entities appearing in a single tweet : The frequency of appearance between two specific entities in tweets .", "entities": []}
{"text": "2 . Average Entity to entity distance in a tweet ( i.e. in the sentence \" Nadal venci\u00f3 a Federer \" , if both Nadal and Federer are identified as entities , they would have a distance of 3 for this tweet )", "entities": []}
{"text": "A threshold of 0.125 was set as the minimum normalized frequency for a pair of entities and a minimum average Entity to Entity distance of 1.65 .", "entities": []}
{"text": "These two values were set based on experimentation with the resulting clustered entities from each dataset .", "entities": []}
{"text": "After that , an approach based on Levenshtein distance ( minimum amount of additions , replacements or deletions needed to turn a word into another ) was employed , where two entities were clustered together if their distance was more than 0.9 times the length of the longest entity from the two .", "entities": []}
{"text": "An example of this distance can be seen on Figure 2 .", "entities": []}
{"text": "FCA is one of the approaches for entity extraction detailed on Cimiano ( 2006 ) .", "entities": []}
{"text": "It is the one that garners the most focus on this book as the main set - theoretical approach based on verb - subject components .", "entities": []}
{"text": "An example of how a formal context would look for a tourism domain knowledge can be seen on Table 1 . Cimiano ( 2006 ) bookable rentable rideable hotel X apartment X X bike X X X excursion X trip X In this paper we use the created formal contexts to discriminate between entities based on three metrics : Conditional ( n , v )", "entities": []}
{"text": "= P ( n , v )", "entities": []}
{"text": "= f ( n , v ) f ( v ) ( 1 )", "entities": []}
{"text": "P M I ( n , v )", "entities": []}
{"text": "= log 2 P ( n | v ) P ( n ) ( 2 )", "entities": []}
{"text": "Resnik ( n , v )", "entities": []}
{"text": "= SR ( v )", "entities": []}
{"text": "* P ( n | v )", "entities": []}
{"text": "( 3 )", "entities": []}
{"text": "Where : 1 .", "entities": []}
{"text": "f ( n , v )", "entities": []}
{"text": "= > Frequency of apparition of entity n with verb v 2 . f ( v )", "entities": []}
{"text": "= > Frequency of apparition of verb v with any entity", "entities": []}
{"text": "And : SR ( v ) = n P", "entities": []}
{"text": "( n | v )", "entities": []}
{"text": "* log 2 P ( n | v ) P ( n ) ( 4 ) A threshold of 0.1 as a minimum value is set for all of the three aforementioned metrics ( Conditional , PMI and Resnik weights ) , meaning that the ( entity , verb ) pairs that not surpass this threshold for any of the three metrics are pruned .", "entities": []}
{"text": "In this subsection UDPipe is also used in order to extract the syntactic dependencies , in particular , the focus is to obtain ' dobj ' and ' iobj ' objects , which refer to direct and indirect object respectively , and then obtain the root verb they stem from .", "entities": []}
{"text": "By doing this a verb can be linked to each object and furthermore , the entities related to verb , which were obtained from the Formal Context , can be linked to each object .", "entities": []}
{"text": "Doing this allows us to add activities for each entity , as well as create a relationship between two entities where one of them appears as an object in the action of another .", "entities": []}
{"text": "These ontologies were then presented to colleagues with more profound knowledge on each of the events for validation and were redone based on their feedback until they were accepted by them .", "entities": []}
{"text": "From these ontologies we obtained precision and recall values for both entities and relationships for each case .", "entities": []}
{"text": "These can be seen on Tables 2 , 3 and 4 : The main point of interest in these metrics lies on the precision , where the precision on the Australian Open case in quite higher than on the other two cases .", "entities": []}
{"text": "From further inspection on the corresponding data we could infer that this was the case because a big part of the tweets for the Australian Open where either formal tweets made by users representing news outlets or by the players themselves .", "entities": []}
{"text": "As for the other two cases , most of the tweets where a mix of news and discussion from common people about these events .", "entities": []}
{"text": "We conclude that , while the methods exposed on this paper work good enough on cases such as the Australian Open one , there is still work to be done when the general public is more engaged on the event such as the cases of the Puente Piedra toll and the March against the corruption .", "entities": []}
{"text": "The lack of labeled training data for new features is a common problem in rapidly changing real - world dialog systems .", "entities": []}
{"text": "We evaluate the quality of generated utterances using intrinsic evaluation metrics and by conducting downstream evaluation experiments with English as the source language and nine different target languages .", "entities": []}
{"text": "Our method shows promise across languages , even in a zero - shot setting where no seed data is available .", "entities": []}
{"text": "In recent years , approaches that train joint models for both tasks and that leverage powerful pre - trained neural models greatly improved the state - of - the - art performance on available benchmarks for IC and SL ( Louvan and Magnini , 2020 ;", "entities": []}
{"text": "Weld et al , 2021 ) .", "entities": []}
{"text": "A common challenge in real - world systems is the problem of feature bootstrapping : If a new feature should be supported , the label space needs to be extended with new intent or slot labels , and the model needs to be retrained to learn to classify corresponding utterances .", "entities": []}
{"text": "However , labeled examples for the new feature are typically limited to a small set of seed examples , as the collection of more annotations would make feature expansion costly and slow .", "entities": []}
{"text": "As a possible solution , previous work explored the automatic generation of paraphrases to augment the seed data ( Malandrakis et al , 2019 ; Cho et al , 2019 ; Jolly et al , 2020 ) .", "entities": []}
{"text": "In this work , we study feature bootstrapping in the case of a multilingual dialog system .", "entities": []}
{"text": "In such systems , the coverage of languages and the range of features is continuously expanded .", "entities": []}
{"text": "That can lead to differences in the supported intent and slot labels across languages , in particular if a new language is added later or if new features are not rolled out to all languages simultaneously .", "entities": []}
{"text": "As a consequence , labeled data for a feature can be available in one language , but limited or completely absent in another .", "entities": []}
{"text": "To address this setup , we follow the recent work of Jolly et al ( 2020 ) , which proposes to use an encoder - decoder model that maps from structured meaning representations to corresponding utterances .", "entities": []}
{"text": "Because such an input is language - agnostic , it is particularly well - suited for the multilingual setup .", "entities": []}
{"text": "We make the following extensions : First , we port their model to a transformer - based architecture and allow multilingual training by adding the desired target language as a new input to the conditional generation .", "entities": []}
{"text": "Second , we let the model generate slot labels along with tokens to alleviate the need for additional slot projection techniques .", "entities": []}
{"text": "And third , we introduce improved paraphrase decoding methods that leverage a model - based selec - tion strategy .", "entities": []}
{"text": "We evaluate our approach by simulating a crosslingual feature bootstrapping setting , either fewshot or zero - shot , on MultiATIS , a common IC / SL benchmark spanning nine languages .", "entities": []}
{"text": "We find that our method produces paraphrases of high novelty and diversity and using it for IC / SL training shows promising downstream classification performance .", "entities": []}
{"text": "However , our model uses an encoder - decoder approach which can handle the intent and language as categorical inputs in addition to the sequence input .", "entities": []}
{"text": "Our approach is different since it models the language and intent of the generation that can be controlled explicitly .", "entities": []}
{"text": "Also , our model is the first to enable zero - shot utterance generation .", "entities": []}
{"text": "We follow a similar approach but our model generates utterances from a sequence of slots rather than an utterance , which enables an explicitly controlled generation .", "entities": []}
{"text": "Also the number of seed utterances we use is merely 20 for the few shot setup unlike around 1 M seed para - carrier phrase pairs in Cho et al ( 2019 ) .", "entities": []}
{"text": "Our approach is focused towards generating utterances in the dialog domain that can generate utterances from a sequence of slots conditioned on both intent and language .", "entities": []}
{"text": "Our approach is different as our model can generate the slot annotations along with the the utterance , which are necessary for the slot labeling task .", "entities": []}
{"text": "Our model can be seen as an extension of the model by Jolly et al ( 2020 ) to a transformer based model , with the added functionality of controlling the language in which the utterance generation is needed , which in turn enables zero shot generation .", "entities": []}
{"text": "Xu et", "entities": []}
{"text": "al , 2020 ) .", "entities": []}
{"text": "If a feature is already being actively used , feedback signals from users , such as paraphrases or interruptions , can be used to obtain additional training data ( Muralidharan et al , 2019 ; .", "entities": []}
{"text": "We want to augment existing labeled utterances by generating additional novel utterances in a desired target language .", "entities": []}
{"text": "In our case , existing data consists of feature - unrelated data ( intents and slots already supported ) spanning all languages and featurerelated data , which is available in a source language but is small ( few - shot ) or not available ( zero shot ) in other languages .", "entities": []}
{"text": "For generation , we first extract the intent and slot types from the available data .", "entities": []}
{"text": "We then generate a new utterance by conditioning a multilingual language model on the intent , slot types and the target language .", "entities": []}
{"text": "We refer to utterances that have the same intent and slot types as paraphrases of each other since they convey the same meaning in the context of the SLU system .", "entities": []}
{"text": "The model architecture is outlined in Figure 1 .", "entities": []}
{"text": "The model uses self - attention based encoder and decoder similar to the transformer ( Vaswani et al , 2017 ) .", "entities": []}
{"text": "The encoder of the model receives as input the language embedding and the intent embedding , which are added to the slot embedding .", "entities": []}
{"text": "Unlike the transformer model ( Vaswani et al , 2017 ) , we do not use the positional embedding in the encoder .", "entities": []}
{"text": "This is because the order of the slot types in the input sequence does not matter and is thus made indistinguishable for the encoder .", "entities": []}
{"text": "Note that we already know the intent of the generated paraphrase since it is the same intent as specified while generating it .", "entities": []}
{"text": "The slot annotations , however , are not readily obtained from the input slot types .", "entities": []}
{"text": "We can make the slot annotations part of the output sequence by generating the slot label in BIO format in every alternate time step , which would be the slot label for the token generated in the previous time step .", "entities": []}
{"text": "This enables the model to generate the slot annotations along with the paraphrase .", "entities": []}
{"text": "An illustrative example is shown in Figure 1 .", "entities": []}
{"text": "Such a generation process is deterministic .", "entities": []}
{"text": "For our task of generating paraphrases , we are interested in generating diverse and novel utterances .", "entities": []}
{"text": "Non - deterministic sampling methods such as top - k sampling has been used in related work ( Fan et al , 2018 ;", "entities": []}
{"text": "Welleck et al , 2020 ; Jolly et al , 2020 ) to achieve this .", "entities": []}
{"text": "p ( x t = w | x < t )", "entities": []}
{"text": "= exp ( z w /\u03c4 )", "entities": []}
{"text": "w V exp ( z w /\u03c4 ) , ( 1 ) where V is the decoder 's vocabulary .", "entities": []}
{"text": "Setting \u03c4 > 1 encourages the resulting probability distribution to be less spiky , thereby encouraging diverse choices during sampling .", "entities": []}
{"text": "The top - k sampling restricts the size of the most likely candidate pool to k \u2264 | V | .", "entities": []}
{"text": "The generated paraphrases can be used to augment the existing training data .", "entities": []}
{"text": "In addition to deciding how many paraphrases to augment , it is also crucial to decide which paraphrases to use .", "entities": []}
{"text": "Preliminary experimental results showed that samping uniformly from all generated paraphrases does not lead to improvement over the baseline .", "entities": []}
{"text": "Upon manual examination we found that not all the paraphrases belong to the desired target intent .", "entities": []}
{"text": "We rank all the generated paraphrases based on these probabilities and select from the top of the pool for augmentation of the seed data .", "entities": []}
{"text": "We evaluate our approach by simulating few - shot and zero - shot feature bootstrapping scenarios .", "entities": []}
{"text": "We use the MultiATIS++ data ( Xu et al , 2020 ) , a parallel IC / SL corpus that was created by translating the original English dataset .", "entities": []}
{"text": "It covers a total of 9 languages : English , Hindi , Turkish , German , French , Portuguese , Spanish , Japanese and Chinese .", "entities": []}
{"text": "The languages encompass a diverse set of language families : Indo - European , Sino - Tibetan , Japonic and Altaic .", "entities": []}
{"text": "Choosing target intents To reduce the number of experiments , we only choose three different intents for simulating the feature bootstrapping scenario .", "entities": []}
{"text": "The MultiATIS++ dataset is highly imbalanced in terms of intent frequencies .", "entities": []}
{"text": "For instance , 74 % of the English training data has the intent atis_flight and as many as 9 intents have less than 20 training samples .", "entities": []}
{"text": "The trend is similar for the non - English languages .", "entities": []}
{"text": "For choosing target intents for simulating the zero shot and few shot training data , we therefore consider the following three target intents : ( a ) atis_airfare , which is highly frequent , ( b ) atis_airline , which has medium frequency , and ( c ) atis_city which is scarce .", "entities": []}
{"text": "Preprocessing We remove the samples in the MultiATIS++ data for which the number of tokens and the number of slot values do not match .", "entities": []}
{"text": "1", "entities": []}
{"text": "We also only consider the first intent for the samples that have multiple intent annotations .", "entities": []}
{"text": "We show the data sizes after preprocessing in Table 1 . Training setup To simulate the feature bootstrapping scenario , we consider only 20 samples ( few shot setup ) or no samples at all ( zero shot setup ) from the MultiATIS++ data for a specific target intent in a target language .", "entities": []}
{"text": "2 Language setup We use English as the source language and consider 8 target languages ( Hindi , Turkish , German , French , Portuguese , Spanish , Japanese , Chinese ) simultaneously .", "entities": []}
{"text": "This encourages the model parameters to be shared across all the 9 languages including the source language English .", "entities": []}
{"text": "The purpose of this setup is to enable us to study the knowledge transfer across multiple target languages in addition to that from the source language .", "entities": []}
{"text": "3", "entities": []}
{"text": "The number of encoder and decoder layers was set to 3 each .", "entities": []}
{"text": "The number of heads was set to 8 .", "entities": []}
{"text": "Generating paraphrases For generating paraphrases in the target intent in the target language , we used the slots appearing in the existing training data in the target intent .", "entities": []}
{"text": "We finally combined all generations and ranked the candidates using the baseline downstream system 's prediction probability .", "entities": []}
{"text": "The number of paraphrases that are selected is determined as in 3.3 , with 20 as the minimum .", "entities": []}
{"text": "The downstream model is trained using just the available seed examples for the target intent .", "entities": []}
{"text": "( b ) Oversampling :", "entities": []}
{"text": "We oversample the samples per intent uniformly at random to match the size of the augmented training data using the proposed method .", "entities": []}
{"text": "This is only applicable to the few shot setup since for the zero shot setup , there are no existing samples in the target intent in the target language to sample from .", "entities": []}
{"text": "Since the original formulation does not take into account the language of generation , we adapt the method for our case by defining the signature as the set { language , intent , slots } .", "entities": []}
{"text": "Finally we generated 100 carrier phrases for each carrier phrase input in the target intent in the target language .", "entities": []}
{"text": "Paraphrases were obtained by injecting the slot values to the generated carrier phrases .", "entities": []}
{"text": "The pool of all paraphrases was sorted using the baseline downstream system 's prediction probabilities .", "entities": []}
{"text": "( d )", "entities": []}
{"text": "For the few shot setup , we added all the translated utterances except the ones that correspond to those utterances we already picked as the few shot samples .", "entities": []}
{"text": "For the zero shot setup , we added all the translated utterances .", "entities": []}
{"text": "( Su et al , 2018 ) did not improve for 3 epochs .", "entities": []}
{"text": "We evaluate the quality of the generated paraphrases using the following metrics .", "entities": []}
{"text": "Let S be the set of input slot types and G be the set of generated slot types .", "entities": []}
{"text": "All retrieval score The all retrieval score r measures if all the input slots were retrieved in the generation .", "entities": []}
{"text": "r = 1 if | S \u2229 G", "entities": []}
{"text": "| =", "entities": []}
{"text": "otherwise ( 3 ) Partial match The partial match score r measures if at least one output slot matches an input slot .", "entities": []}
{"text": "r = 1 if | S \u2229 G", "entities": []}
{"text": "precision = | S \u2229", "entities": []}
{"text": "G | | G | , recall = | S \u2229", "entities": []}
{"text": "G | |", "entities": []}
{"text": "S | ( 5 ) Jaccard index Jaccard index measures the set similarity between S and G as their intersection size divided by the union size .", "entities": []}
{"text": "Novelty Let P be the set of paraphrases generated from a base utterance u. novelty = 1 | P", "entities": []}
{"text": "| u P 1 \u2212 BLEU4 ( u , u ) ( 6 )", "entities": []}
{"text": "Diversity The diversity is computed using the generated paraphrases P .", "entities": []}
{"text": "diversity = u P , u P , u = u 1 \u2212 BLEU4 ( u , u )", "entities": []}
{"text": "| P | \u00d7 ( | P | \u2212 1 ) ( 7 ) Language detection score We are interested in quantifying if a generated paraphrase is in the target language .", "entities": []}
{"text": "We use langdetect 5 to compute p ( lang = target lang ) .", "entities": []}
{"text": "Higher scores denote better language generation .", "entities": []}
{"text": "Each score shown is the average score of 10 runs .", "entities": []}
{"text": "5 Experimental results", "entities": []}
{"text": "For both the few shot and zero shot setups , the paraphrases used for intrinsic evaluation are generated in the target intent and the target language only .", "entities": []}
{"text": "Table 2 shows intrinsic evaluation results for different generation methods .", "entities": []}
{"text": "The highest scores for the above metrics are obtained for the greedy generation , which indicates that the generated slot types are most similar to the input slot types in that case .", "entities": []}
{"text": "However , it is the opposite for the novelty and diversity metrics where the scores are higher with larger top - k and temperatures .", "entities": []}
{"text": "For the zero shot setup , the overall trend is similar to the few shot setup .", "entities": []}
{"text": "The slot similarity based metrics are lower in general , which indicates that even as little as 20 samples in the few shot setup improve the generation of desired slots .", "entities": []}
{"text": "The novelty scores for the zero shot setup are 1 as we would expect .", "entities": []}
{"text": "The language detection score varies across languages , which may be due to the vocabulary overlap between languages , e.g. , San Francisco appears in both English and German utterances .", "entities": []}
{"text": "Interestingly we also observe code switching , i.e. mixedlanguage generations , while using our approach .", "entities": []}
{"text": "Since we are interested in measuring the variation in scores for the target intents , we only report the scores for the test samples in the target intents in Tables 4 and 5 .", "entities": []}
{"text": "We run each downstream training experiment 10 times and report the mean scores for each language and also the average across languages in the AVG column in Tables 4 and 5 .", "entities": []}
{"text": "We are also interested in tracking the scores for the test samples having intents other than the target intents since we need to ensure that the scores on the other intents does not go down .", "entities": []}
{"text": "6 In Tables 4 and 5 , our paraphrasing results outperform the baseline scores on average .", "entities": []}
{"text": "Both oversampling and MT approaches are competitive .", "entities": []}
{"text": "Oversampling performs the best for JA whereas MT performs the best for ES and HI .", "entities": []}
{"text": "However we note that the paraphrasing approach requires no dependencies on other models or other data , unlike the MT approach which requires a parallel corpus to train the MT model .", "entities": []}
{"text": "Paraphrases generated in different languages for a given input are shown in Table 6 .", "entities": []}
{"text": "The intent is airline and the slots are fromloc.city_name for columbus and toloc.city_name for minneapolis .", "entities": []}
{"text": "For this intent and the slots , the generated paraphrase in German ( translated to English ) is Show me all the airlines that fly from Toronto to Boston .", "entities": []}
{"text": "The desired intent , that is airline is realized in the gener - ated paraphrase .", "entities": []}
{"text": "Additionally , Toronto and Boston are the slot values respectively for the slot types fromloc.city_name and toloc.city_name .", "entities": []}
{"text": "For Spanish , the generated paraphrase ( translated to English ) is Which Airlines Fly from Atlanta to Philadelphia .", "entities": []}
{"text": "The airline intent is realized in the generated paraphrase and also Atlanta and Philadelphia are the slot values produced associated with the desired slot types .", "entities": []}
{"text": "As illustrated by the examples , the model is free to pick a specific slot value during generation , leading to variations across languages , but all are consistent with the slot type .", "entities": []}
{"text": "Our method is language agnostic and scalable , with no dependencies on pre - trained models or additional data .", "entities": []}
{"text": "We validate our method using experiments on the MultiATIS++ dataset containing utterances spanning 9 languages .", "entities": []}
{"text": "To the best of our knowledge , this is the first successful exploration of generating paraphrases for SLU in a cross - lingual setup .", "entities": []}
{"text": "We would like to thank our anonymous reviewers for their thoughtful comments and suggestions that improved the final version of this paper .", "entities": []}
{"text": "PBoS : Probabilistic Bag - of - Subwords for Generalizing Word Embedding", "entities": []}
{"text": "We rely solely on the spellings of words and propose a model , along with an efficient algorithm , that simultaneously models subword segmentation and computes subword - based compositional word embedding .", "entities": []}
{"text": "We call the model probabilistic bag - of - subwords ( PBoS ) , as it applies bag - of - subwords for all possible segmentations based on their likelihood .", "entities": []}
{"text": "Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge .", "entities": []}
{"text": "The motivation here is to extend the usefulness of pre - trained embeddings without expensive retraining over large text .", "entities": []}
{"text": "We here , however , focus more on the research question of how much one can achieve from just word compositions .", "entities": []}
{"text": "In addition , our proposed way of utilizing word composition information can be combined with the contextual embedding algorithms to further improve the performance of generalized embeddings .", "entities": []}
{"text": "The hidden assumption here is that words are made of meaningful parts ( cf . morphemes ) and that the meaning of a word is related to the meaning of their parts .", "entities": []}
{"text": "This way , humans are often able to guess the meaning of a word or term they have never seen before .", "entities": []}
{"text": "For example , \" postEMNLP \" probably means \" after EMNLP \" .", "entities": []}
{"text": "Stratos ( 2017 ) ; Pinter et al ( 2017 ) ; Kim et al ( 2018b ) model words at the character level .", "entities": []}
{"text": "However , they have been surpassed by later subword - level models , probably because of putting too much burden on the models to form and discover meaningful subwords from characters .", "entities": []}
{"text": "BoS composes a word embedding vector by taking the sum or average of the vectors of the subwords ( character n - grams ) that appear in the given word .", "entities": []}
{"text": "However , it ignores the importance of different subwords since all of them are given the same weight .", "entities": []}
{"text": "Intuitively , \" farm \" and \" land \" should be more relevant in composing representation for word \" farmland \" than some random subwords like \" armla \" .", "entities": []}
{"text": "Even more favorable would be a model 's ability to discover meaningful subword segmentations on its own .", "entities": []}
{"text": "Cotterell et al ( 2016 ) bases their model over morphemes but needs help from an external morphological analyzer such as Morfessor ( Virpioja et al , 2013 ) .", "entities": []}
{"text": "Sasaki et al ( 2019 ) use trainable self - attention to combine subword vectors .", "entities": []}
{"text": "While the attention implicitly facilitates interactions among subwords , there has been no explicit enforcement of mutual exclusiveness from subword segmentation , making it sometimes difficult to rule out less relevant subwords .", "entities": []}
{"text": "For example , \" her \" is itself a likely subword , but is unlikely to be relevant for \" higher \" as the remaining \" hig \" is unlikely .", "entities": []}
{"text": "We propose the probabilistic bag - of - subwords ( PBoS ) model for generalizing word embedding .", "entities": []}
{"text": "PBoS simultaneously models subword segmentation and composition of word representations out of subword representations .", "entities": []}
{"text": "The subword segmentation part is a probabilistic model capable of handling ambiguity of subword boundaries and ranking possible segmentations based on their overall likelihood .", "entities": []}
{"text": "For each segmentation , we compose a word vector as the sum of all subwords that appear in the segmentation .", "entities": []}
{"text": "The final embedding vector is the expectation of the word vectors from all possible segmentations .", "entities": []}
{"text": "An alternative view is that the model assigns word - specific weights to subwords based on how likely they appear as meaningful segments for the given word .", "entities": []}
{"text": "Coupled with an efficient algorithm , our model is able to compose better word embedding vectors with little computational overhead compared to BoS. Manual inspections show that PBoS is able to produce subword segmentations and subword weights that align with human intuition .", "entities": []}
{"text": "We summarize our contributions as follows : We propose PBoS , a subword - level word embedding model that is based on probabilistic segmentation of words into subwords , the first of its kind ( Section 2 ) .", "entities": []}
{"text": "We propose an efficient algorithm that leads to an efficient implementation 3 of PBoS with little overhead over previous much simpler BoS. ( Section 3 ) .", "entities": []}
{"text": "Manual inspection and affix prediction experiment show that PBoS is able to give reasonable subword segmentations and subword weights ( Section 4.1 and 4.2 ) .", "entities": []}
{"text": "show that word vectors generated by PBoS have better quality compared to previously proposed models across languages ( Section 4.3 and 4.4 ) .", "entities": []}
{"text": "Following the above intuition , in this section we describe the PBoS model in detail .", "entities": []}
{"text": "We first develop a model that segments a word into subword and associates each subword segmentation with a likelihood based on the meaningfulness of each subword segment .", "entities": []}
{"text": "We then apply BoS over each segmentation to compose a \" segmentation vector \" .", "entities": []}
{"text": "The final word embedding vector is then the probabilistic expectation of all the segmentation vectors .", "entities": []}
{"text": "The subword segmentation and likelihood association part require no explicit source of morphological knowledge and are tightly integrated with the word vector composition part , which in turn gives rise to an efficient algorithm that considers all possible segmentations simultaneously ( Section 3 ) .", "entities": []}
{"text": "= c i is the i - th letter .", "entities": []}
{"text": "Empirically , this is proportional to the unigram frequency of word w observed in large text in that language .", "entities": []}
{"text": "Note that we do not assume a vocabulary .", "entities": []}
{"text": "That is , we do not distinguish words from arbitrary strings made out of the alphabet .", "entities": []}
{"text": "The implicit assumption here is that a \" word \" in common sense is just a string associated with high probability .", "entities": []}
{"text": "In this sense , p w can also be seen as the likelihood of string w being a \" legit word \" .", "entities": []}
{"text": "This blurs the boundary between words and non - words , and automatically enables us to handle unseen words , alternative spellings , typos , and nonce words as normal cases .", "entities": []}
{"text": "w [ i : j ] = c i . . .", "entities": []}
{"text": "c j for some 1 \u2264 i \u2264 j \u2264 |", "entities": []}
{"text": "w | , i.e. s is a substring of w.", "entities": []}
{"text": "|", "entities": []}
{"text": "w | 1", "entities": []}
{"text": "( s =", "entities": []}
{"text": "w [ i : j ] )", "entities": []}
{"text": "Note that a subword s may occur more than once in the same word w.", "entities": []}
{"text": "For example , subword \" ana \" occurs twice in the word \" banana \" .", "entities": []}
{"text": "A subword segmentation g of word w of length", "entities": []}
{"text": "k", "entities": []}
{"text": "= | g | is a tuple ( s 1 , s 2 , . . .", "entities": []}
{"text": ", s k ) of subwords of w , so that w is the concatenation of s 1 , . .", "entities": []}
{"text": ".", "entities": []}
{"text": ", s k .", "entities": []}
{"text": "A subword transition graph for word w is a directed acyclic graph G", "entities": []}
{"text": "w = ( N w , E w ) .", "entities": []}
{"text": "Let l = | w | .", "entities": []}
{"text": ", l } correspond to the positions between w [ i ] and w [ i + 1 ] for all", "entities": []}
{"text": "i", "entities": []}
{"text": "i <", "entities": []}
{"text": "j \u2264 l } corresponds to subword w", "entities": []}
{"text": "[ i : j ] .", "entities": []}
{"text": "We use G w as a useful image for developing our model .", "entities": []}
{"text": "There are 2 | w | \u22121 different possible segmentations for word w.", "entities": []}
{"text": "Each edge ( i , j ) is associated with a weight p", "entities": []}
{"text": "w [ i : j ] - how likely w [ i : j ] itself is a meaningful subword .", "entities": []}
{"text": "p", "entities": []}
{"text": "\" i \" g p \" g \" h p \" h \" e p \" e \" r p \" r \" hi", "entities": []}
{"text": "p \" hi \" gher p \" gher \" gh p \" gh \" her p \" her \" high p \" high \" er p \" er \" Figure 1 : Diagram of probabilistic subwords transitions for word \" higher \" .", "entities": []}
{"text": "Some edges are omitted to reduce clutter .", "entities": []}
{"text": "Each edge is labeled by a subword s of the word , associated with ps .", "entities": []}
{"text": "p s .", "entities": []}
{"text": "( 2 ) Example .", "entities": []}
{"text": "Figure 1 illustrates G w for word w = \" higher \" of length 6 .", "entities": []}
{"text": "The likelihood p ( \" high \" , \" er \" ) | w of this particular segmentation is proportional to p \" high \" p \" er \" - the product of weights along the path .", "entities": []}
{"text": "The embedding vector w for word w is the expectation of all its segmentation - based word embedding : w =", "entities": []}
{"text": "g", "entities": []}
{"text": "Segw p g | w g ( 3 ) where g is the embedding for segmentation g. Given a subword segmentation g , we adopt the Bag - of - Subwords ( BoS ) model", "entities": []}
{"text": "Zhao et al , 2018 ) for composing word embedding from subwords .", "entities": []}
{"text": "Specifically , we apply BoS 4 over the subword segments in g :", "entities": []}
{"text": "g = s g s , ( 4 ) where s is the vector representation for subword s ,", "entities": []}
{"text": "as if the current segmentation g is the \" golden \" segmentation of the word .", "entities": []}
{"text": "In such case , we assume the meaning of the word is the combination of the meaning of all its subword segments .", "entities": []}
{"text": "Combining Eq .", "entities": []}
{"text": "W |", "entities": []}
{"text": "w W w \u2212 w * 2 2 .", "entities": []}
{"text": "( 6 ) 3 Efficient Algorithm PBoS simultaneously considers all possible subword segmentations and their contributions in composing word representations .", "entities": []}
{"text": "However , summing over embeddings of all possible segmentations can be awfully inefficient , as simply enumerating all possible segmentations of w takes number of steps exponential to the length of w ( Proposition 2 ) .", "entities": []}
{"text": "We therefore need an efficient way to compute Eq .", "entities": []}
{"text": "( 5 ) .", "entities": []}
{"text": "Exchanging the order of summations in Eq .", "entities": []}
{"text": "( 5 ) from segmentation first to subword first , we get w = s\u2286w a s | w s ( 7 ) where a s | w \u221d g Segw , g s p g | w ( 8 ) is the weight accumulated over subword s , summing over all segmentations of w that contain s. 5 Eq .", "entities": []}
{"text": "( 7 ) provides an alternative view of the word vector composed by our model : a weighted sum of all the word 's subword vectors .", "entities": []}
{"text": "Comparing to BoS , we assign different importance a s | w , instead of a uniform weight , to each subword .", "entities": []}
{"text": "a s | w can be viewed as the likelihood of subword s being a meaningful segment of the particular word w , considering both the likelihood of s itself being meaningful , and at the same time how likely the rest of the word can still be segmented into meaningful subwords .", "entities": []}
{"text": "Example .", "entities": []}
{"text": "Consider the contribution of subword s = \" gher \" in word w", "entities": []}
{"text": "= \" higher \" .", "entities": []}
{"text": "Possible contributions only come from segmentations that contain \" higher \" : g 1", "entities": []}
{"text": "= ( \" h \" , \" i \" , \" gher \" ) and g 2 = ( \" hi \" , \" gher \" ) .", "entities": []}
{"text": "Each segmentation g adds weight p g | w to a s | w .", "entities": []}
{"text": "In this case , a \" gher \" | w will be smaller than a \" er \" | w because both p g 1 | w and p g 2 | w would be rather small .", "entities": []}
{"text": "Now we can efficiently compute Eq .", "entities": []}
{"text": "( 7 ) if we can efficiently compute a s | w .", "entities": []}
{"text": "Here we present an algorithm that computes a s | w for all s \u2286 w in O", "entities": []}
{"text": "( | w | 2 ) time .", "entities": []}
{"text": "The specific structure of the subword transition graph means that edges only go from left to right .", "entities": []}
{"text": "Thus , we can split every path going through e into three parts : edges left to e , e itself and edges right to e. In terms of subwords , that is , for s =", "entities": []}
{"text": "w", "entities": []}
{"text": "[ i : j ] , l = | w | , each segmentation g that contains s can be divided into three parts : segmentation g w [ 1 : i\u22121 ] over w [ 1 : i \u2212 1 ] , =", "entities": []}
{"text": "p s b 1 , i\u22121 b j+1 , l , ( 10 )", "entities": []}
{"text": "where b i , j = g Seg w", "entities": []}
{"text": "[ i : j ] s g p s .", "entities": []}
{"text": "Now we can efficiently compute a s | w if we can efficiently compute b 1 , i\u22121 and b j+1 , l for all 1 \u2264 i ,", "entities": []}
{"text": "j \u2264 l. Fortunately , we can do so for b 1 , i using the following recursive relation b 1 , i = i\u22121 k=0", "entities": []}
{"text": "b 1 , k p w", "entities": []}
{"text": "[ k+1 : i ] ( 11 ) for i = 1 , . . .", "entities": []}
{"text": "Similar formulas hold for b j , l , j = 1 , . . .", "entities": []}
{"text": ", l with b l+1 , l", "entities": []}
{"text": "= 1 . Based on this , we devise Algorithm 1 for computing a s | w for all s \u2286 w.", "entities": []}
{"text": "Here we take the alternative view of our model as a weighted average of all possible subwords ( thus the normalization in Line 12 ) , and an extension to the unweighted averaging of subwords as used in Zhao et al ( 2018 ) .", "entities": []}
{"text": "Algorithm 1", "entities": []}
{"text": "Computing a s | w .", "entities": []}
{"text": "1 : Input : Word w , p s for all s \u2286 w. l", "entities": []}
{"text": "= | w | .", "entities": []}
{"text": "l do 4 : b 1 , i", "entities": []}
{"text": "i\u22121 k=0", "entities": []}
{"text": "p w [ k+1 : i ] b 1 , k 5 : b l\u2212i+1 , l l", "entities": []}
{"text": "l , j i . . .", "entities": []}
{"text": "l do 9 : \u00e3 p", "entities": []}
{"text": "w [ i : j ] b 1 , i\u22121 b j+1 , l 10 : \u00e3 w [ i : j ] |", "entities": []}
{"text": "w \u00e3 w [ i : j ] |", "entities": []}
{"text": "w + \u00e3 11 : end for 12 : a s | w \u00e3 s | w / s \u2286w\u00e3 s | w for all s \u2286 w 13 : return a | w Time complexity As we only access each subword once in each for - statement , the number of multiplications and additions involved is bounded by the number of subword locations of w. Each of Line 4 and Line 5 take i multiplications and i \u2212 1 additions respectively .", "entities": []}
{"text": "So Line 3 to Line 6 in total takes 2l 2 computations .", "entities": []}
{"text": "Line 8 to Line 11 takes 3l ( l+1 ) 2 computations .", "entities": []}
{"text": "Thus , the time complexity of Algorithm 1 is O ( l 2 ) .", "entities": []}
{"text": "Given a word of length 20 , O ( l 2 ) ( 20 2 = 400 ) is much better than enumerating all O ( 2 l ) ( 2 20 = 1 , 048 , 576 ) segmentations .", "entities": []}
{"text": "Using the setting in Section 4.3 , PBoS only takes 30 % more time ( 590 \u00b5s vs 454 \u00b5s ) in average than BoS ( by disabling a s | w computation ) to compose a 300 - dimensional word embedding vector .", "entities": []}
{"text": "We design experiments to answer two questions : Do the segmentation likelihood and subword weights computed by PBoS align with their meaningfulness ?", "entities": []}
{"text": "Are the word embedding vectors generated by PBoS of good quality ?", "entities": []}
{"text": "For the former , we inspect segmentation results and subword weights ( Section 4.1 ) , and see how good they are at predicting word affixes ( Section 4.2 ) .", "entities": []}
{"text": "Due to the page limit , we only report the most relevant settings and results in this section .", "entities": []}
{"text": "Other details , including hardware , running time and detailed list of hyperparameters , can be found in Appendix A.", "entities": []}
{"text": "In this subsection , we provide anecdotal evidence that PBoS is able to assign meaningful segmentation likelihood and subword weights .", "entities": []}
{"text": "Table 1 shows top subword segmentations and subsequent top subwords calculated by PBoS for some example word , ranked by their likelihood and weights respectively .", "entities": []}
{"text": "We use the same list for word probability p w throughout our experiments if not otherwise mentioned .", "entities": []}
{"text": "All other settings are the same as described for PBoS in Section 4.3 .", "entities": []}
{"text": "We can see the segmentation likelihood and subword weight favors the whole words as subword segments if the word appears in the word list , e.g. \" higher \" , \" farmland \" .", "entities": []}
{"text": "Second to the whole - word segmentation , or when the word is rare , e.g. \" penpineanpplepie \" , \" paradichlorobenzene \" , we see that PBoS gives higher likelihood to meaningful segmentations such as \" high / er \" , \" farm / land \" , \" pen / pineapple / pie \" and \" para / dichlorobenzene\"against other possible segmentations .", "entities": []}
{"text": "7 Subsequently , respective subword segments get higher weights among all possible subwords for the word , often by a good amount .", "entities": []}
{"text": "This behavior would help PBoS to focus on meaningful subwords when composing word embedding .", "entities": []}
{"text": "The fact that this can be achieved without any explicit source of morphological knowledge is itself interesting .", "entities": []}
{"text": "We quantitatively evaluate the quality of subword segmentations and subsequent subword weights by testing if our PBoS model is able to discover the most eminent word affixes .", "entities": []}
{"text": "Note this has nothing to do with embeddings , so no training is involved in this experiment .", "entities": []}
{"text": "The affix prediction task is to predict the most eminent affix for a given word .", "entities": []}
{"text": "For example , \" - able \" for \" replaceable \" and \" re - \" for \" rename \" .", "entities": []}
{"text": "Models We get affix prediction from our PBoS by taking the top - ranked subword that is one of the possible affixes .", "entities": []}
{"text": "To show our advantage , we Word w Top segmentation g ( and their p g | w )", "entities": []}
{"text": "Top subword s ( and their a s | w ) higher higher ( 0.924 ) , high / er ( 0.030 ) , highe / r ( 0.027 ) , h / igher ( 0.007 ) , hig / her ( 0.004 ) .", "entities": []}
{"text": "higher ( 0.852 ) , high ( 0.031 ) , er ( 0.029 ) , r ( 0.029 ) , highe ( 0.025 ) .", "entities": []}
{"text": "farmland farmland ( 0.971 ) , farmlan / d ( 0.010 ) , farm / land ( 0.006 ) , f / armland ( 0.005 ) . farmland ( 0.941 ) , d ( 0.010 ) , farmlan ( 0.009 ) , farm ( 0.008 ) , land ( 0.007 ) .", "entities": []}
{"text": "penpineapplepie pen / pineapple / pie ( 0.359 ) , pen / pineapple / pi / e ( 0.157 ) , pen / pineapple / p / ie ( 0.101 ) .", "entities": []}
{"text": "pineapple ( 0.238 ) , pen ( 0.186 ) , pie ( 0.131 ) , p ( 0.101 ) , e ( 0.099 ) .", "entities": []}
{"text": "paradichlorobenzene para / dichlorobenzene ( 0.611 ) , par / a / dichlorobenzene ( 0.110 ) , paradi / chlorobenzene ( 0.083 ) .", "entities": []}
{"text": "dichlorobenzene ( 0.344 ) , para ( 0.283 ) , a ( 0.061 ) , par ( 0.054 ) , ichlorobenzene ( 0.042 ) . compare it with a BoS - style baseline affix predictor .", "entities": []}
{"text": "Because BoS gives same weight to all subwords in a given word , we randomly choose one of the possible affixes that appear as subword of the word .", "entities": []}
{"text": "Benchmark We use the derivational morphology dataset 8 from Lazaridou et al ( 2013 ) .", "entities": []}
{"text": "The dataset contains 7449 English words in total along with their most eminent affixes .", "entities": []}
{"text": "Because no training is needed in this experiment , we use all the words for evaluation .", "entities": []}
{"text": "To make the task more challenging , we drop trivial instances where there is only one possible affix appears as a subword in the given word .", "entities": []}
{"text": "For example , \" rename \" is dropped because only prefix \" re - \" is present ; on the other hand , \" replaceable \" is kept because both \" re - \" and \" - able \" are present .", "entities": []}
{"text": "Besides excluding the trivial cases described above , we also exclude instances labeled with suffix \" - y \" , because it is always included by \" - ly \" and \" - ity \" .", "entities": []}
{"text": "Altogether , we acquire 3546 words with 17 possible affixes for this evaluation .", "entities": []}
{"text": "The task is given as pairs of words , along with their similarity scores labeled by language speakers .", "entities": []}
{"text": "The performance is then measured in Spearman 's correlation \u03c1 for all pairs .", "entities": []}
{"text": "Benchmarks We use WordSim353 ( WS ) from Finkelstein et al ( 2001 ) which mainly consists of common words .", "entities": []}
{"text": "Unlike some of previous models , we do not add special characters to indicate word boundaries and do not set any constraint on subword lengths .", "entities": []}
{"text": "For baselines , we compare against the bag - ofsubword model ( BoS ) from Zhao et al ( 2018 ) , and the best attention - based model ( KVQ - FH ) from Sasaki et al ( 2019 ) .", "entities": []}
{"text": "For BoS , we use our implementation by disabling subword weight computation .", "entities": []}
{"text": "For KVQ - FH , we use the implementation given in the paper .", "entities": []}
{"text": "All the hyperparameters are set the same as described in the original papers .", "entities": []}
{"text": "We choose to not include the character - RNN model ( MIMICK ) from Pinter et al ( 2017 ) , as it has been shown clearly outperformed by the two .", "entities": []}
{"text": "KVQ - FH , PBoS can often match and sometimes surpass it even though PBoS is a much simpler model with better explainability .", "entities": []}
{"text": "Compared to the scores by using just the target embeddings ( Table 3 , All pairs ) , PBoS is the only model that demonstrates improvement across all cases .", "entities": []}
{"text": "The only case where PBoS is not doing well is with Polyglot vectors and RW benchmark .", "entities": []}
{"text": "After many manual inspections , we conjecture that it may be related to the vector norm .", "entities": []}
{"text": "Sometimes the vector of a relevant subword can be of a small norm , prone to be overwhelmed by less relevant subword vectors .", "entities": []}
{"text": "To counter this , we tried to normalize subword vectors before summing them up into a word vector ( PBoS - n ) .", "entities": []}
{"text": "PBoS - n showed good improvement for the Polyglot RW case ( 25 to 32 ) , matching the performance of the other two .", "entities": []}
{"text": "However , this is largely because we do not constrain the length of subwords as in BoS or use hashing as in KVQ - FH .", "entities": []}
{"text": "We found that PBoS is insensitive to subword length constraints and decide to keep the setting simple .", "entities": []}
{"text": "Despite being an interesting direction , we decide to not involve hashing in this work to focus on the effect of our unique weighting scheme .", "entities": []}
{"text": "FaxtText Comparison Albeit targeted for a different task ( training word embedding ) which have access to contextual information , the popular fast - Text ) also uses a subwordlevel model .", "entities": []}
{"text": "To ensure a fair comparison , we restrict the vocabulary sizes and embedding dimensions to match those of Polyglot vectors .", "entities": []}
{"text": "Multilingual Results To evaluate and compare the effectiveness of PBoS across languages , we further train the models targeting multilingual Wikipedia2Vec vectors ( Yamada et al , 2020 ) and evaluate them on multilingual WordSim353 and SemLex999 from Leviant and Reichart ( 2015 ) which are available in English , German , Italian and Russian .", "entities": []}
{"text": "To better access the models ' ability to generalize , we only take the top 10k words from the target vectors for training , which yields decent OOV rates , ranging from 23 % to 84 % .", "entities": []}
{"text": "Detailed results can be found in Appendix Section A.3 .", "entities": []}
{"text": "In summary , we find 1 ) that PBoS surpasses KVQ - FH for English and German and is comparable to KVQ - FH for Italian ; 2 ) that PBoS and KVQ - FH surpasses BoS for English , German and Italian ; and 3 ) no definitive trend among the three models for Russian .", "entities": []}
{"text": "We further assess the quality of generated word embedding via the extrinsic task of POS tagging .", "entities": []}
{"text": "The task is to categorize each word in a given context into a particular part of speech , e.g. noun , verb , and adjective .", "entities": []}
{"text": "When predicting the tag for the i - th word w i in a sentence , the input to the classifier is the concatenation of the vectors w i\u22122 , w i\u22121 , w i , w i+1 , w i+2 for the word itself and the words in its context .", "entities": []}
{"text": "This setup allows a more direct evaluation of the quality of word vectors themselves , and thus gives better discriminative power .", "entities": []}
{"text": "Polyglot vectors contain 64 - dimensional vectors over 13 https://scikit - learn.org/0.19/ modules / generated / sklearn.linear_model .", "entities": []}
{"text": "Zhao et al ( 2018 ) , but found the numbers rather similar across embedding models .", "entities": []}
{"text": "One possible explanation is that LSTMs are so good at picking up contextual features that the impact of mild deviations of a single word vector is marginal .", "entities": []}
{"text": "an 100k vocabulary for each language and are used as target vectors for each of the subword - level embedding models in this experiment .", "entities": []}
{"text": "For PBoS , we use the Polyglot word counts for each language as the base for subword segmentation and subword weights calculation .", "entities": []}
{"text": "We use the default partition of training and testing set .", "entities": []}
{"text": "Statistics vary from language to language .", "entities": []}
{"text": "See Appendix A.4 for more details .", "entities": []}
{"text": "All the subword - level embedding models follow the same hyperparameters as in Section 4.3 .", "entities": []}
{"text": "For the one language ( Tamil ) where PBoS is not the most accurate , the difference to the best is small ( 0.003 ) .", "entities": []}
{"text": "KVQ - FH gives no significantly more accurate predictions than BoS despite it is more complex and is the only one tuned with hyperparameters .", "entities": []}
{"text": "al , 2019 ; Zhu et al , 2019 ) .", "entities": []}
{"text": "Among them are Charagram by Wieting et al ( 2016 ) which , albeit trained on specific downstream tasks , is similar to BoS followed by a non - linear activation , and the systematic evaluation by Zhu et al ( 2019 ) over various choices of word composition functions and subword segmentation methods .", "entities": []}
{"text": "However , all works above either pay little attention to the interaction among subwords inside a given word , or treat subword segmentation and composing word representation as separate problems .", "entities": []}
{"text": "Another interesting thread of works ( Oshikiri , 2017 ; Kim et al , 2018aKim et al , , 2019 attempted to model language solely at the subword level and learn subword embeddings directly from text , providing evidence to the power of subword - level models , especially as the notion of word is thought doubtful by some linguistics ( Haspelmath , 2011 ) .", "entities": []}
{"text": "Besides the recent interest in subwords , there have been long efforts of using morphology to improve word embedding ( Luong et al , 2013 ; Cotterell and Sch\u00fctze , 2015 ; Cui et al , 2015 ; Soricut and Och , 2015 ;", "entities": []}
{"text": "Bhatia et al , 2016 ; Cao and Rei , 2016 ; Xu et", "entities": []}
{"text": "al , 2018 ; \u00dcst\u00fcn et al , 2018 ; Edmiston and Stratos , 2018 ; Chaudhary et al , 2018 ; Park and Shin , 2018 ) .", "entities": []}
{"text": "However , most of them require an external oracle , such as Morfessor ( Creutz and Lagus , 2002 ; Virpioja et al , 2013 ) , for the morphological segmentations of input words , limiting their power to the quality and availability of such segmenters .", "entities": []}
{"text": "In the future , it would be interesting to see if PBoS can also help with the task of learning word embedding , and how hashing would impact the quality of composed embedding while facilitating a more compact model .", "entities": []}
{"text": "Here we list the details of our experiments that are omitted in the main paper due to space constraints .", "entities": []}
{"text": "We run all our experiments on a machine with an 8 - core Intel i7 - 6700 CPU @ 3.40GHz , 32 GB Memory , and GeForce GTX 970 GPU .", "entities": []}
{"text": "The meaning of hyperparameters shown in Table 6 , Table 7 and Table 8 as explained as follows .", "entities": []}
{"text": "min len :", "entities": []}
{"text": "The minimum length for a subword to be considered .", "entities": []}
{"text": "max len : The maximum length for a subword to be considered .", "entities": []}
{"text": "word boundary : Whether to add special characters to annotate word boundaries .", "entities": []}
{"text": "epochs : The number of training epochs .", "entities": []}
{"text": "This is especially the case when we evaluate the target vectors , where OOV rates can be significant .", "entities": []}
{"text": "Regarding the training epoch time , note that KVQ - FH uses GPU and is implemented using a deep learning library 17 with underlying optimized C code , whereas our PBoS is implemented using pure Python and uses only single thread CPU .", "entities": []}
{"text": "We omit the prediction time for KVQ - FH , as we found it hard to separate the actual inference time from time used for other processes such as batching and data transfer between CPU and GPU .", "entities": []}
{"text": "However , we believe the overall trend should be similar as for the training time .", "entities": []}
{"text": "One may notice that the prediction time for BoS in Table 9 is different from what was reported at the end of Section 3 .", "entities": []}
{"text": "This is largely because the BoS in Table 9 has a different ( smaller ) set of possible subwords to consider due to the subword length limits .", "entities": []}
{"text": "In Section 3 , to fairly access the impact of subword weights computation , we ensure that BoS and PBoS work with the same set of possible subwords ( that used by PBoS in Section 4.3 ) , and thus observe a slight longer prediction time for BoS.", "entities": []}
{"text": "We use Wikipedia2Vec", "entities": []}
{"text": "( Yamada et al , 2020 ) as target vectors , and keep the most frequent 10k words to get decent OOV rates .", "entities": []}
{"text": "For PBoS , we use the word frequencies from Polyglot for subword segmentation and subword weight calculation as the same for the multilingual POS tagging experiment ( Section 4.4 ) .", "entities": []}
{"text": "We evaluate all the models on multilingual Word - Sim353 ( mWS ) and SemLex999 ( mSL ) from Leviant and Reichart ( 2015 ) , which is available for English , German , Italian and Russian .", "entities": []}
{"text": "The dataset al o contains the relatedness ( rel ) and similarity ( sim ) benchmarks derived from mWS .", "entities": []}
{"text": "Table 7 and Table 8 show the hyperparameter values used in the POS tagging experiment ( Section 4.4 ) .", "entities": []}
{"text": "Following the observation in Sasaki et al ( 2019 ) , we tune the regularization parameter C for KVQ - FH for all values a \u00d7 10 b where a = 1 , . . .", "entities": []}
{"text": ".", "entities": []}
{"text": ", 4 .", "entities": []}
{"text": "The authors would like to thank anonymous reviewers of EMNLP for their comments .", "entities": []}
{"text": "ZJ would like to thank Xuezhou Zhang , Sidharth Mudgal , Matt Du and Harit Vishwakarma for their helpful discussions .", "entities": []}
{"text": "Ntrain is the number of training instances for the POS tagging model .", "entities": []}
{"text": "OOV % is the percentage of the words in the POS tagging testing set that is out of the vocabulary of the Polyglot vectors in that language .", "entities": []}
{"text": "Experimental results are included for convenience .", "entities": []}
{"text": "Answering complex questions that involve multiple entities and multiple relations using a standard knowledge base is an open and challenging task .", "entities": []}
{"text": "Most existing KBQA approaches focus on simpler questions and do not work very well on complex questions because they were not able to simultaneously represent the question and the corresponding complex query structure .", "entities": []}
{"text": "In this work , we encode such complex query structure into a uniform vector representation , and thus successfully capture the interactions between individual semantic components within a complex question .", "entities": []}
{"text": "This approach consistently outperforms existing methods on complex questions while staying competitive on simple questions .", "entities": []}
{"text": "One simple example is a question like this : \" What 's the capital of the United States ? \"", "entities": []}
{"text": "A common answer to such question is to identify the focus entity and the main relation predicate ( or a sequence ) in the question , and map the question to a triple fact query ( U S , capital , ? ) over KB .", "entities": []}
{"text": "The object answers are returned by executing the query .", "entities": []}
{"text": "The mapping above is typically learned from question - answer pairs through distant supervision .", "entities": []}
{"text": "While the above question can be answered by querying a single predicate or predicate sequence in the KB , many other more complex questions can not , e.g. the question in Figure 1 .", "entities": []}
{"text": "To answer the question \" What is the second longest river in United States \" , we need to infer several semantic clues : 1 ) the answer is contained by United States ; 2 ) the answer is a river ; 3 ) the answer ranks second by its length in descending order .", "entities": []}
{"text": "Thus , multiple predicates are required to constrain the answer set , and we call such questions \" complex questions \" throughout this paper .", "entities": []}
{"text": "For answering complex questions , it 's more important to understand the compositional semantic meanings of the question .", "entities": []}
{"text": "For example in Figure 1 , the query graph forms a tree shape .", "entities": []}
{"text": "The answer node A , serving as the root of the tree , is the variable vertex that represents the real answer entities .", "entities": []}
{"text": "The focus nodes ( US , river , 2nd ) are extracted from the mentions of the question , and they constrain the answer node via predicate sequences in the knowledge base .", "entities": []}
{"text": "Recently , neural network ( NN ) models have shown great promise in improving the performance of KBQA systems , and SP+NN techniques become the stateof - the - art on several KBQA datasets ( Qu et al , 2018 ; Bao et al , 2016 ) .", "entities": []}
{"text": "According to the discussion above , our work extends the current research in the SP+NN direction .", "entities": []}
{"text": "In order to define the similarity function between one question and a complex query graph , an intuitive solution is to split the query graph into multiple semantic components , as the predicate sequences separated by dashed boxes in Figure 1 .", "entities": []}
{"text": "Then previous methods can be applied for modeling the similarity between the question and each part of the graph .", "entities": []}
{"text": "However , such approach faces two limitations .", "entities": []}
{"text": "First , each semantic component is not directly comparable with the whole question , since it conveys only partial information of the question .", "entities": []}
{"text": "Second , and more importantly , the model encodes different components separately , without learning the representation of the whole graph , hence it 's not able to capture the compositional semantics in a global perspective .", "entities": []}
{"text": "Given candidate query graphs generated from one question , our model embeds the question surface and predicate sequences into a uniform vector space .", "entities": []}
{"text": "The main difference between our approach and previous methods is that we integrate hidden vectors of various semantic components and encode their interaction as the hidden semantics of the entire query graph .", "entities": []}
{"text": "The contribution of this paper is summarized below .", "entities": []}
{"text": "We propose a light - weighted and effective neural network model to solve complex KBQA task .", "entities": []}
{"text": "To the best of our knowledge , this is the first attempt to explicitly encode the complete semantics of a complex query graph ( Section 2.2 ) ;", "entities": []}
{"text": "We perform comprehensive experiments on multiple QA datasets , and our proposed method consistently outperforms previous approaches on complex questions , and produces competitive results on datasets made up of simple questions ( Section 3 ) .", "entities": []}
{"text": "In this section , we present our approach for solving complex KBQA .", "entities": []}
{"text": "First , we generate candidate query graphs by staged generation method ( Section 2.1 ) .", "entities": []}
{"text": "Second , we measure the semantic similarities between the question and each query graph using deep neural networks ( Section 2.2 ) .", "entities": []}
{"text": "We illustrate our staged candidate generation method in this section .", "entities": []}
{"text": "Compared to previous methods , such as Bao et al ( 2016 ) , we employ a more effective candidate generation strategy , which takes advantage of implicit type information in query graphs and time interval information in the KB .", "entities": []}
{"text": "In our work , we take 4 kinds of semantic constraints into account : entity , type , time and ordinal constraints .", "entities": []}
{"text": "Figure 2 shows a concrete example of our candidate generation .", "entities": []}
{"text": "For simplicity of discussion , we assume Freebase as the KB in this section .", "entities": []}
{"text": "Step 1 : Focus linking .", "entities": []}
{"text": "We extract possible ( mention , focus node ) pairs from the question .", "entities": []}
{"text": "Focus nodes are the starting points of various semantic constraints , refer to Figure 2 ( a ) .", "entities": []}
{"text": "For type linking , we brutally combine each type with all uni - , bi - and tri - gram mentions in the question , and pick top - 10 ( mention , type ) pairs with the highest word embedding similarities of each pair .", "entities": []}
{"text": "For time linking , we extract time mentions by simply matching year regex .", "entities": []}
{"text": "For ordinal linking , we leverage a predefined superlative word list 2 and recognize mentions by matching superlative words , or the \" ordinal number + superlative \" pattern .", "entities": []}
{"text": "The ordinal node is an integer representing the ordinal number in the mention .", "entities": []}
{"text": "Step 2 : Main path generation .", "entities": []}
{"text": "We build different main paths by connecting the answer node to different focus entities using 1 - hop or 2 - hopwith - mediator 3 predicate sequence .", "entities": []}
{"text": "Figure 2 ( b ) shows one of the main paths .", "entities": []}
{"text": "Further constraints are attached by connecting an anchor node x to an unused focus node through predicate sequences , where the anchor node x is a non - focus node in the main path ( A or v 1 in the example ) .", "entities": []}
{"text": "Step 3 : Attaching entity constraints .", "entities": []}
{"text": "We apply a depth - first search to search for combinations of multiple entity constraints to the main path through 1 - hop predicate .", "entities": []}
{"text": "Figure 2 ( c ) shows a valid entity constraint , ( v 1 , basic title , president ) .", "entities": []}
{"text": "The advantage of depth - first search is that we can involve unlimited number of entities in a query graph , which has a better coverage than template - based methods .", "entities": []}
{"text": "Step 4 : Type constraint generation .", "entities": []}
{"text": "Type constraints can only be applied at the answer node using IsA predicate .", "entities": []}
{"text": "3 Mediator is a kind of auxiliary nodes in Freebase maintaining N - ary facts . of the answer , derived from the outgoing predicates of the answer node .", "entities": []}
{"text": "For example in Figure 2 ( c ) , the domain type of the predicate government position is politician , which becomes the implicit type of the answer .", "entities": []}
{"text": "Thus we can filter type constraints which are irrelevant to the implicit types , preventing semantic drift and speeding up the generation process .", "entities": []}
{"text": "To judge whether two types in Freebase are relevant or not , we adopt the method in Luo et al ( 2015 ) to build a rich type hierarchy of Freebase .", "entities": []}
{"text": "Focus types are discarded , if they are not the super - or sub - types of any implicit types of the answer .", "entities": []}
{"text": "Step 5 : Time and ordinal constraint generation .", "entities": []}
{"text": "As shown in Figure 2 ( d ) , the time constraint is represented as a 2 - hop predicate sequence , where the second is a virtual predicate determined by the preposition before the focus time , indicating the time comparing operation , like \" before \" , \" after \" and \" in \" .", "entities": []}
{"text": "Similarly , the ordinal constraint also forms a 2 - hop predicate sequence , where the second predicate represents descending ( MaxAtN ) or ascending order ( MinAtN ) .", "entities": []}
{"text": "For the detail of time constraint , while existing approaches ( Yih et al , 2015 ; Bao et al , 2016 ) link the focus time with only single time predicate , our improvement is to leverage paired time predicates for representing a more accurate time constraint .", "entities": []}
{"text": "In Freebase , paired time predicates are used to represent facts within certain time intervals , like f rom and to 4 in Figure 2 ( d ) .", "entities": []}
{"text": "For time comparing operation \" in \" , we link the time focus to the starting time predicate , but use both predi - cates in SPARQL query , restricting that the focus time lies in the time interval of the paired predicates .", "entities": []}
{"text": "After finishing all these querying stages , we translate candidate graphs into SPARQL query , and produce their final output answers .", "entities": []}
{"text": "Finally , we discard query graphs with zero outputs , or using overlapped mentions .", "entities": []}
{"text": "The architecture of the proposed model is shown in Figure 3 .", "entities": []}
{"text": "We first replace all entity ( or time ) mentions used in the query graph by dummy tokens E ( or T m ) .", "entities": []}
{"text": "To encode the complex query structure , we split it into predicate sequences starting from answer to focus nodes , which we call semantic components .", "entities": []}
{"text": "The predicate sequence does n't include the information of focus nodes , except for type constraints , where we append the focus type to the IsA predicate , resulting in the predicate sequence like { IsA , river } .", "entities": []}
{"text": "To encode a semantic component p , we take the sequence of both predicate ids and predicate names into consideration .", "entities": []}
{"text": "As the example shown in Figure 3 , the i d sequence of the first semantic component is { contained by } , and the predicate word sequence is the concatenation of canonical names for each predicate , that is { \" contained \" , \" by \" } .", "entities": []}
{"text": "Given the word sequence { p ( w ) 1 , . . .", "entities": []}
{"text": ", p n } , we first use a word embedding matrix E w R |", "entities": []}
{"text": ".", "entities": []}
{"text": "Then we represent the word sequence using word averaging : p ( w )", "entities": []}
{"text": "= 1", "entities": []}
{"text": "n i p ( w ) i .", "entities": []}
{"text": "For the i d sequence { p ( i d ) 1 , . . .", "entities": []}
{"text": ", p ( i d ) m } , we simply take it as a whole unit , and directly translate it into vector representation using the embedding matrix E p R | Vp\u00d7d | at path level , where | V p | is the vocabulary size of predicate sequences .", "entities": []}
{"text": "There are two reasons for using such path embedding : 1 ) the length of i d sequence is not larger than two , based on our generation method ; 2 ) the number of distinct predicate sequences is roughly the same as the number of distinct predicates .", "entities": []}
{"text": "We get the fi - nal vector of the semantic component by elementwise addition : p = p ( w ) + p ( i d ) .", "entities": []}
{"text": "We encode the question in both global and local level , which captures the semantic information with respect to each component p.", "entities": []}
{"text": "The global information takes the token sequence as the input .", "entities": []}
{"text": "We use the same word embedding matrix E w to convert the token sequence into vectors { q ( w ) 1 , . . .", "entities": []}
{"text": ", q ( w ) n } .", "entities": []}
{"text": "[ \u2212 h ( w ) 1 ; \u2212 h ( w ) n ] .", "entities": []}
{"text": "Since the answer is denoted by the whword in the question , we extract the dependency path from the answer node to the focus mention in the question .", "entities": []}
{"text": "Similar with Xu et", "entities": []}
{"text": "al ( 2016 ) , we treat the path as the concatenation of words and dependency labels with directions .", "entities": []}
{"text": "For example , the dependency path between \" what \" and \" United States \" is { what , \u2212\u2212\u2212 nsubj , is , \u2212 \u2212 prep , in , \u2212 \u2212 pobj , E } .", "entities": []}
{"text": "Finally we combine global and local representation by element - wise addition , returning the representation of the question with respect to the semantic component , q p = q ( tok ) + q ( dep ) p .", "entities": []}
{"text": "Given the query graph with multiple semantic components , G = { p ( 1 ) , . . .", "entities": []}
{"text": ", p ( N ) } , now all its semantic components have been projected into a common vector space , representing hidden features in different aspects .", "entities": []}
{"text": "S sem ( q , G )", "entities": []}
{"text": "= cos ( max i p ( i ) , max i q ( i ) p ) .", "entities": []}
{"text": "( 1 )", "entities": []}
{"text": "The S - MART linker is a black box for our system , which is not extendable and tend to produce high precision but low recall linking results .", "entities": []}
{"text": "We first build a large lexicon by collecting all ( mention , entity ) pairs from article titles , anchor texts , redirects and disambiguation pages of Wikipedia .", "entities": []}
{"text": "Each pair is associated with statistical features , such as linking probability , letter - tri - gram jaccard similarity and popularity of the entity in Wikipedia .", "entities": []}
{"text": "Thus we learn a pseudo linking score for every pair in the lexicon , and for each question , we pick top - K highest pairs to enrich S - MART linking results , where K is a hyperparameter .", "entities": []}
{"text": "Table 1 lists the detail features .", "entities": []}
{"text": "( 2 ) For each question , we pick a candidate graph as positive data , if the F 1 score of its answer is larger than a threshold ( set to 0.1 in our work ) .", "entities": []}
{"text": "We randomly sample 20 negative graphs G \u2212 from the candidate set whose F 1 is lower than the corresponding G + .", "entities": []}
{"text": "Entity", "entities": []}
{"text": "In this section , we introduce the QA datasets and state - of - the - art systems that we compare .", "entities": []}
{"text": "We show the end - to - end results of the KBQA task , and perform detail analysis to investigate the importance of different modules used in our approach .", "entities": []}
{"text": "We use CompQ , WebQ and SimpQ as abbreviations of the above datasets , respectively .", "entities": []}
{"text": "CompQ contains 2 , 100 complex questions collected from Bing search query log , and the dataset is split into 1 , 300 training and 800 testing questions .", "entities": []}
{"text": "Each question is manually labeled with at least one answer entity in both datasets .", "entities": []}
{"text": "SimpQ consists of more than 100 K questions , and the gold answer of each question is a gold focus entity paired with a single predicate .", "entities": []}
{"text": "This dataset is designed mainly for answering simple questions , and we use it for complementary evaluation .", "entities": []}
{"text": "Knowledge bases : For experiments on both CompQ and WebQ , we follow the settings of Berant et al ( 2013 ) and Xu et al ( 2016 ) to use the full Freebase dump 5 as the knowledge base , which contains 46 M entities and 5 , 323 predicates .", "entities": []}
{"text": "We host the knowledge base with Virtuoso engine 6 .", "entities": []}
{"text": "For the experiments on SimpQ , the knowledge base we use is FB2 M , which is a subset of Freebase provided with the dataset , consisting 2 M entities and 10 M triple facts .", "entities": []}
{"text": "All the source codes , QA datasets , and detail results can be downloaded from http://202.120.38.146/CompQA/.", "entities": []}
{"text": "Now we perform KBQA experiments on WebQ and CompQ. We use the average F 1 score over all questions as our evaluation metric .", "entities": []}
{"text": "The official evaluation script 7 measures the correctness of output entities at string level .", "entities": []}
{"text": "While in CompQ , the annotated names of gold answer entities do n't match the case of their names in Freebase , thus we follow Bao et al ( 2016 ) to lowercase both annotated names and the output answer names before calculating the F 1 score .", "entities": []}
{"text": "We report the experimental results in Table 2 .", "entities": []}
{"text": "The result of Yih et al ( 2015 ) on CompQ is reported by Bao et al ( 2016 ) as their implemented result .", "entities": []}
{"text": "Our approach outperforms existing approaches on CompQ dataset , and ranks 2nd on WebQ among a long list of state - of - the - art works .", "entities": []}
{"text": "We point out that Xu et al ( 2016 ) uses Wikipedia texts as the external community knowledge for verifying candidate answers , and achieves a slightly higher F 1 score ( 53.3 ) than our model , but the performance decreases to 47.0 if this step is removed .", "entities": []}
{"text": "Besides , Yih et al ( 2015 ) and Bao et al ( 2016 ) used ClueWeb dataset for learning more accurate semantics , while based on the ablation test of Yih , the F 1 score of WebQ drops by 0.9 if ClueWeb information is removed .", "entities": []}
{"text": "Our results show that entity enrichment method improves the results on both datasets by a large margin ( 0.8 ) , which is a good help to our approach .", "entities": []}
{"text": "We argue that the enriched results are directly comparable with other approaches , as S - MART itself is learned from semi - structured information in Wikipedia , such as anchor texts , redirect links and disambiguation pages , the enrichment step does not bring extra knowledge into our system .", "entities": []}
{"text": "In addition , the improvements of the candidate generation step also show a positive effect .", "entities": []}
{"text": "If we remove our implicit type filtering in Step 4 and time interval constraints in Step 5 , the F 1 of CompQ slightly drops from 42.84 to 42.37 .", "entities": []}
{"text": "Al - though these improvements mainly concern timerelated questions ( around 25 % in CompQ ) , we believe these strategies can be useful tricks in the further researches .", "entities": []}
{"text": "As a complementary evaluation , we perform semantic matching experiments on SimpQ. Given the gold entity of each question , we recognize the entity mention in the question , replace it with E , then predict the correct predicate .", "entities": []}
{"text": "Table 3 shows the experimental results .", "entities": []}
{"text": "Yu et al ( 2017 ) proposed another approach using multi - layer BiL - STM with residual connections .", "entities": []}
{"text": "Our semantic matching model performs slightly below these two systems , since answering simple questions is not the main goal of this paper .", "entities": []}
{"text": "Comparing with these approaches , our semantic matching model is lightweighted , with a simpler structure and fewer parameters , thus is easier to tune and remains effective .", "entities": []}
{"text": "In this section , we explore the contributions of various components in our system .", "entities": []}
{"text": "Semantic component representation : We first evaluate the results on CompQ and WebQ under different path encoding methods .", "entities": []}
{"text": "Recap that the encoding result of a semantic component is the summation of its word and i d path representations ( Section 2.2.1 ) , thus we compare encoding methods by multiple combinations .", "entities": []}
{"text": "For encoding predicate i d sequence , we use average predicate embedding as the alternative of the current path - level embedding ( P athEmb ) .", "entities": []}
{"text": "The experimental results are shown in Table 4 .", "entities": []}
{"text": "The encoding method N one means that we do n't encode the i d or word sequence , and simply take the result of the other sequence as the representation of the whole component .", "entities": []}
{"text": "we observe that the top three combination settings , ignoring either word or i d sequence , perform worse than the bottom three settings .", "entities": []}
{"text": "The comparison demonstrates that predicate word and i d representation can be complementary to each other .", "entities": []}
{"text": "The performance gain is not that large , mainly because predicate i d features are largely covered by their word name features .", "entities": []}
{"text": "For the encoding of i d sequences , P athEmb works better than average embedding , consistently boosting F 1 by 0.65 on both datasets .", "entities": []}
{"text": "The former method treats the whole sequence as a single unit , which is more flexible and can potentially learn diverse representations of i d sequences that share the same predicates .", "entities": []}
{"text": "=", "entities": []}
{"text": "i cos ( p ( i ) ,", "entities": []}
{"text": "q ( i ) p ) .", "entities": []}
{"text": "For methods of question encoding , we setup ablations by turning off either sentential encoding or dependency encoding .", "entities": []}
{"text": "Table 5 shows the ablation results on CompQ and WebQ. When dependency path information is augmented with sentential information , the performance boosts by 0.42 on average .", "entities": []}
{"text": "Dependency paths focus on hidden features at syntactic and functional perspective , which is a good complementary to sentential encoding results .", "entities": []}
{"text": "However , performances drop by 2.17 if only dependency information is used , we find that under certain dependency structures , crucial words ( bolded ) are not in the path between the answer and the focus mention ( underlined ) , for example , \" who did draco malloy end up marrying \" and \" who did the philippines gain independence from \" .", "entities": []}
{"text": "While we observe about 5 % of such questions in WebQ , it 's hard to predict the correct query graph without crucial words .", "entities": []}
{"text": "The improvement on WebQ is smaller than on CompQ , largely due to the fact that 85 % questions in WebQ are simple questions ( Bao et al , 2016 ) .", "entities": []}
{"text": "As a result of combination , our approach significantly outperforms the vanilla SP+NN approach on CompQ by 1.28 , demonstrating the effectiveness of our approach .", "entities": []}
{"text": "Theoretically , the pooling outcome may lead to worse end - to - end result when there are too many semantic components in one graph , because the pooling layer takes too many vectors as input , different semantic features between similar query graphs become indistinguishable .", "entities": []}
{"text": "In our task , only 0.5 % of candidate graphs have more than 3 semantic components , so pooling is a reasonable way to aggregate semantic components in this scenario .", "entities": []}
{"text": "Two query graphs are likely to be the final answer : 1 ) ( ? , children , gimli person ) ; 2 ) ( ?", "entities": []}
{"text": ", f ictional children , gimli character )", "entities": []}
{"text": "( ? , appear in , hobbit ) .", "entities": []}
{"text": "If observing semantic components individually , the predicate children is most likely to be the correct one since \" 's father \" is highly related and with plenty of positive training data .", "entities": []}
{"text": "Both f ictional children and appear in get a much lower similarity compared with children , hence the baseline method prefer the first query graph .", "entities": []}
{"text": "In the meantime , our proposed method learns the hidden semantics of the second candidate by absorbing salient features from both predicates , and such compositional representation is closer to the semantics of the entire question than a simple \" children \" predicate .", "entities": []}
{"text": "That 's why our method manages to answer it correctly .", "entities": []}
{"text": "We randomly analyzed 100 questions from CompQ where no correct answers are returned .", "entities": []}
{"text": "We list the major causes of errors as follows : Main path error ( 10 % ) :", "entities": []}
{"text": "This type of error occurred when the model failed to understand the main semantics when facing some difficult questions ( e.g. \" What native american sports heroes earning two gold medals in the 1912 Olympics \" ) ; Constraint missing ( 42 % ) : These types of questions involve implicit constraints , for example , the question \" Who was US president when Traicho Kostov was teenager \" is difficult to answer because it implies an implicit time constraint \" when Traicho Kostov was teenager \" ;", "entities": []}
{"text": "This error occurs due to the highly ambiguity of mentions .", "entities": []}
{"text": "This error class contains questions with semantic ambiguity or not reasonable .", "entities": []}
{"text": "For example , the question \" Where is Byron Nelson 2012 \" is hard to understand , because \" Byron Nelson \" died in 2006 and maybe this question wants to ask where did he die .", "entities": []}
{"text": "There are various methods ( Yao and Van Durme , 2014 ; Bordes et al , 2015 ;", "entities": []}
{"text": "Dong et al , 2015 ; Xu et", "entities": []}
{"text": "al , 2016 ) to select candidate answers and to rank results .", "entities": []}
{"text": "In terms of logical representation of natural language questions , many methods have been tried , such as query graph ( Yih et al , 2014 ( Yih et al , , 2015 or RDF query language ( Unger et al , 2012 ; Cui et al , 2017 ; Hu et al , 2018 ) .", "entities": []}
{"text": "Recently , as the development of deep learning , NN - based approaches have been combined into the KBQA task ( Bordes et al , 2014 ) , showing promising result .", "entities": []}
{"text": "These approaches tries to use neural network models to encode both questions and answers ( or query structures ) into the vector space .", "entities": []}
{"text": "Subsequently , similarity functions are used to select the most appropriate query structure to generate the final answer .", "entities": []}
{"text": "For example , Bordes et al ( 2014 ) focuses on embedding the subgraph of the candidate answer ;", "entities": []}
{"text": "Yin et", "entities": []}
{"text": "al ( 2016 ) uses character - level CNN and word - level CNN to match different information ; Yu et al ( 2017 ) introduces the method of hierarchical residual RNN to compare questions and relation names ;", "entities": []}
{"text": "Qu et al ( 2018 ) proposes the AR - SMCNN model , which uses RNN to capture semantic - level correlation and employs CNN to extract literallevel words interaction .", "entities": []}
{"text": "Previous works such as Yih et al ( 2015 ) and Bao et al ( 2016 ) require a recognition of a main relation and regard other constraints as variables added to this main relation .", "entities": []}
{"text": "There are also some works ca n't be simply classified in to IR based methods or SP based methods .", "entities": []}
{"text": ", Abujabal et al ( 2017 ) , andCui et al ( 2017 ) try to interpret question intention by templates , which learned from KB or QA corpora .", "entities": []}
{"text": "Talmor and Berant ( 2018 ) attempts to answering complex questions by decomposing them into a sequence of simple questions .", "entities": []}
{"text": "To the best of our knowledge , this is the first work to handle complex KBQA task by explicitly encoding the complete semantics of a complex query graph using neural networks .", "entities": []}
{"text": "We stud - ied different methods to further improve the performance , mainly leveraging dependency parse and the ensemble method for linking enrichment .", "entities": []}
{"text": "Our model becomes the state - of - the - art on Com - plexQuestions dataset , and produces competitive results on other simple question based datasets .", "entities": []}
{"text": "Possible future work includes supporting more complex semantics like implicit time constraints .", "entities": []}
{"text": "Kenny Q. Zhu is the contact author and was supported by NSFC grants 91646205 and 61373031 .", "entities": []}
{"text": "Thanks to the anonymous reviewers for their valuable feedback .", "entities": []}
{"text": "Multi - Model and Crosslingual Dependency Analysis", "entities": []}
{"text": "We based our approach on an existing open source tool ( BistParser ) , which we modified in order to produce the required output .", "entities": []}
{"text": "Additionally we added a kind of pseudoprojectivisation .", "entities": []}
{"text": "This was needed since some of the task 's languages have a high percentage of non - projective dependency trees .", "entities": []}
{"text": "For the 4 surprise languages , the data provided seemed too little to train on .", "entities": []}
{"text": "Thus we decided to use the training data of typologically close languages instead .", "entities": []}
{"text": "Our system achieved a macro - averaged LAS of 68.61 % ( 10th in the overall ranking ) which improved to 69.38 % after bug fixes .", "entities": []}
{"text": "Having worked in the past on rule based dependency analysis , it became obvious that we need to adopt a more modern approach to dependency analysis .", "entities": []}
{"text": "All combinations of tools and treebanks had some advantages and some inconveniences .", "entities": []}
{"text": "For instance , the underlying linguistic models of the treebanks are not the same or some tools would not accept CONLLU input but only raw text and apply their own segmentation and POS tagging .", "entities": []}
{"text": "For the shared task we have trained models separately for each language .", "entities": []}
{"text": "So strictly speaking , this is not a multilingual but a monolingual multimodel approach .", "entities": []}
{"text": "Since the shared task requires that output dependency trees have exactly one root , we modified BistParser accordingly by deleting the additional ROOT node added to each sentence in the original version of this parser .", "entities": []}
{"text": "Currently BistParser uses forms and XPOS for both learning and predicting .", "entities": []}
{"text": "Some of the languages in the shared task have a large percentage of non - projective sentences .", "entities": []}
{"text": "We thus decided to implement a pseudoprojectivisation ( K\u00fcbler et al , 2009 , p. 37 ) of the input sentences before training or predicting .", "entities": []}
{"text": "The output sentences are than de - projectivised .", "entities": []}
{"text": "Sometimes of course , the de - projectivisation can fail , especially if there are other dependency relation errors .", "entities": []}
{"text": "Our tests showed , however , that the overall result for most languages is still better than without any pseudo - projectivisation .", "entities": []}
{"text": "Finally we implemented filters which ignore the special CONLLU lines for multi - word tokens ( 2 - 3 ... ) and elliptic insertions ( 4.1 ... ) and reinsert those lines after predicting .", "entities": []}
{"text": "For the same reason we modified Bist - Parser to read sentences one by one , to predict , and to output the result , instead of reading the entire test file at once 8 .", "entities": []}
{"text": "Since for some of the languages there were no development treebanks available , we split the training treebank in order to get a small development corpus ( 10 % of the training corpus is split to test during development ) .", "entities": []}
{"text": "This posed a certain problem for treebanks like Kazakh and Uyghur , which are hopelessly small ( 31 and 100 sentences respectively ) .", "entities": []}
{"text": "Eventhough both languages are geneti - cally and typologically very close to Turkish ( 3685 sentences ) , we finally trained on those small treebanks for time constraints ( with more time available we would have experimented with various other parameters and a cross - lingual approach ) .", "entities": []}
{"text": "In order to get the best results , we cleaned the text corpora ( e.g. deleting letter - digit combinations and separating punctuation symbols such as commas , question marks etc . by a white space from the preceding token ) .", "entities": []}
{"text": "For those languages which use an alphabet which has case distinction ( Latin , Cyrillic and Greek ) we put everything in lowercase .", "entities": []}
{"text": "For all other parameters of word2vec we used the default setting , apart from the lower frequency limit , which we increased to 15 words .", "entities": []}
{"text": "For the biggest text corpora like English ( 9 billion words ) , German ( 5 , 9 billion words ) , Indonesian ( 5 billion words ) French ( 4 , 8 billion words ) training for 500 dimensional word vectors took up to 6 hours ( English ) .", "entities": []}
{"text": "A subword model is described as a open model allowing each word to be represented not only by the word itself but also the subword components of the word in combination .", "entities": []}
{"text": "Subword components can be n - grams with varying values for n , stems , root words , prefixes , and suffixes or any other possible formalism .", "entities": []}
{"text": "For BistParser , the only other parameter we changed was the size of the first hidden layer ( default 100 ) which we set to 50 ( or lower , especially for languages whose treebanks are very small ) .", "entities": []}
{"text": "Every sentence of the training treebanks was pseudo - projectivised before training .", "entities": []}
{"text": "Using the weighted LAS , we then chose the best combination of parameters for each language .", "entities": []}
{"text": "Since the python version of CNN ( used by our adaptation of BistParser ) does not support GPU , training was slow 12 .", "entities": []}
{"text": "Thus we stopped training usually after 15 epochs unless the intermediary results were promising enough to continue .", "entities": []}
{"text": "Figure 1 shows the system architecture .", "entities": []}
{"text": "The upper part represents the data flow for the training , the lower part represents the predicting phase .", "entities": []}
{"text": "As said above , the version of the CNN library we used , does not run on GPU , so all training was single threaded .", "entities": []}
{"text": "depending on the size of the treebank .", "entities": []}
{"text": "This corresponds to 0.5 to 3 seconds per sentence during training .", "entities": []}
{"text": "Training for the surprise languages ( using treebanks of typologically close languages , cf . section , 4 ) , took significantly longer ( up to 90 hours for Czech ) .", "entities": []}
{"text": "So we expected a certain drop of LAS for the tests .", "entities": []}
{"text": "In order to be prepared , we tried to add erroneous lemmas and UPOS in the training data .", "entities": []}
{"text": "This , however , did not produce better results , so we abandoned the 12 The successor of CNN , Dynet , supports GPU , but since BistParser learns on a phrase by phrase base , no gain in time can be observed .", "entities": []}
{"text": "13 Intel Xeon CPU E5 - 1620 v4 at 3.50GHz and Intel Core i7 - 6900 K CPU at 3.20GHz respectively .", "entities": []}
{"text": "14 Apart from numerous punctuation symbols with wrong heads , we found several bad annotations for words as well in different languages .", "entities": []}
{"text": "UDpipe to have similar \" noise \" than the test treebanks .", "entities": []}
{"text": "The final results obtained with the development corpora ( or split from train corpora when there were no development corpora ) are shown in table 1 .", "entities": []}
{"text": "We did not ( yet ) use the morphological features ( column 6 ) .", "entities": []}
{"text": "First tests on French showed that a slight increase in LAS is possible , so we will work on this in the future .", "entities": []}
{"text": "The biggest challenge were the 4 surprise languages .", "entities": []}
{"text": "Having only between 20 and 109 sentences to train on ( even less if we wanted to split it into a train and development corpus ) did not help ( see table 2 for some details ) .", "entities": []}
{"text": "Since the word embedding files where also rather small we chose not to train on the languages themselves , but to keep all of the provided sentences for the development corpus .", "entities": []}
{"text": "So we first tried three similar approaches in order to be able to predict dependency relations for these languages :", "entities": []}
{"text": "The weighted LAS for the surprise languages is shown in table 3", "entities": []}
{"text": "Upper Sorbian is a slavonic language very close to Czech ( and slightly less close to Polish ) .", "entities": []}
{"text": "Northern Sami shares quite a lot of typological features with the Finnic branch of the Fenno - Ugric languages ( here Finnish and Estonian ) , and Kurmanji shares at least some typolological feature with Persian ( both are from the Iranian subgroup of the Indo - European language family .", "entities": []}
{"text": "However Buryat , a Mongolian language , is not typologically close to any of the shared task 's languages .", "entities": []}
{"text": "Even though Turkish seems close enough , to our surprise Hindi was finally the best guess .", "entities": []}
{"text": "As for the language mix , we replaced the forms of the closed word classes in the training corpora by the corresponding UPOS ( except nouns , verbs and adjectives ) and trained the modified treebanks ( cf .", "entities": []}
{"text": "tables 4 and 5 , best configuration in bold ) .", "entities": []}
{"text": "Buryat fa ( 100 As expected , Upper Sorbian and Norther Sami give quite acceptable results using models trained on Czech and Finnish respectively .", "entities": []}
{"text": "Due to the fact that the provided treebanks for Kazakh and Uyghur are both very small we tried to apply the same approach of using the training corpus of a typologically close language ( here Turkish ) .", "entities": []}
{"text": "However , the results were disappointing .", "entities": []}
{"text": "Thus , we continue to use the models trained on very small corpora for these two languages in the shared task .", "entities": []}
{"text": "The details show that our approach worked well for the bigger treebanks and the surprise languages ( where we ended up as 8th ) .", "entities": []}
{"text": "In general , the results per language are slightly lower than those we had during training on the development corpora ( cf . table 1 ) .", "entities": []}
{"text": "This is due to the fact we did our training on forms , lemmas , UPOS and XPOS of the training corpus , which are gold .", "entities": []}
{"text": "In the test data , lemmas , UPOS and XPOS ( if present ) , however , are predicted by UDpipe , and do contain some errors with respect to the gold standard .", "entities": []}
{"text": "After the end of the test phase , we discovered a bug in our chain , which concerned languages , which have only UPOS data .", "entities": []}
{"text": "In this case the UPOS information was totally discarded by error .", "entities": []}
{"text": "Thus all training and testing are done only on the 17 http://universaldependencies.org/ conll17 / results.html forms 18 .", "entities": []}
{"text": "During the tests the models trained on the basic gl , fr and sl treebanks were used instead .", "entities": []}
{"text": "After the test phase we corrected these errors .", "entities": []}
{"text": "Fortunately , their impact was not that hard .", "entities": []}
{"text": "All results are shown in table 6 .", "entities": []}
{"text": "The column on the right shows the difference between the results of the development corpora and test data .", "entities": []}
{"text": "For some languages , the test results are unexpectedly lower than the results on the development corpora .", "entities": []}
{"text": "The lower performance on languages like Chinese , Ukrainian , Vietnamese or Latin ( both ITTB and PROIEL ) seems to be caused by the nature of the test corpora themselves .", "entities": []}
{"text": "Systems of other participants seem to drop in performance as well ; for all these languages our system is still around the 10th position of the global ranking .", "entities": []}
{"text": "Perhaps a cause may be the fact that the XPOS we use ( predicted by UDpipe ) contain more errors than average for the Chinese , Ukrainian or Vietnamese treebanks than for languages where our test score is closer to the development score .", "entities": []}
{"text": "We would like to thank the developpers of Bist - Parser , Eliyahu Kiperwasser and Yoav Goldberg , and the developper of the CNN library ( Chris Dyer ) for making them available as open source on GitHub .", "entities": []}
{"text": "Finally we would like to thank our colleague Ghislain Putois for help on all aspects on neural networks .", "entities": []}
{"text": "However , substantial noise has been discovered in its state annotations .", "entities": []}
{"text": "Such noise brings about huge challenges for training DST models robustly .", "entities": []}
{"text": "Besides , it is costly to rectify all the problematic annotations .", "entities": []}
{"text": "ASSIST first generates pseudo labels for each sample in the training set by using an auxiliary model trained on a small clean dataset , then puts the generated pseudo labels and vanilla noisy labels together to train the primary model .", "entities": []}
{"text": "We show the validity of ASSIST theoretically .", "entities": []}
{"text": "It aims to keep track of users ' intentions at each turn of the conversation ( Mrk\u0161i\u0107 et al , 2017 ) .", "entities": []}
{"text": "The state information indicates the progress of the conversation and is leveraged to determine the next system action and generate the next system response ( Chen et al , 2017 ) .", "entities": []}
{"text": "As shown in Figure 1 , the dialogue state is typically represented as a set of ( slot , value ) pairs .", "entities": []}
{"text": "Hi , how may I help you ?", "entities": []}
{"text": "I need to book a room at autumn house .", "entities": []}
{"text": "Definitely , for how many people and how many nights ?", "entities": []}
{"text": "Just me , 3 nights .", "entities": []}
{"text": "Can you also give me information on the vue cinema ?", "entities": []}
{"text": "Sure .", "entities": []}
{"text": "It is in the city centre , and the phone number is 08451962320 .", "entities": []}
{"text": "Thanks for your help .", "entities": []}
{"text": "That 's all I need .", "entities": []}
{"text": "( hotel - name , autumn house ) ( hotel - name , autumn house ) ( hotel - book people , 1 ) ( hotel - book stay , 3 ) ( attraction - name , vue cinema )", "entities": []}
{"text": "( hotel - name , autumn house ) ( hotel - book people , 1 ) ( hotel - book stay , 3 ) ( attraction - name , vue cinema )", "entities": []}
{"text": "Dialogue State Figure 1 : An example dialogue spanning two domains .", "entities": []}
{"text": "On the left is the dialogue context with system responses shown in orange and user utterances in green .", "entities": []}
{"text": "The dialogue state at each turn is presented on the right .", "entities": []}
{"text": "Therefore , the problem of DST is defined as extracting the values for all slots from the dialogue context at each turn of the conversation .", "entities": []}
{"text": "Over the past few years , DST has made significant progress , attributed to a number of publicly available dialogue datasets , such as DSTC2 , FRAMES ( El Asri et", "entities": []}
{"text": "So far , lots of DST models have been built on top of it Wu et al , 2019 ; Ouyang et al , 2020 ;", "entities": []}
{"text": "Hu et al , 2020 ; Ye et al , 2021b ; Lin et al , 2021 ) .", "entities": []}
{"text": "These noisy labels may impede the training of robust DST models and lead to noticeable performance decrease ( Zhang et al , 2016 ) .", "entities": []}
{"text": "Even so , there are still plenty of noisy and inconsistent la - bels .", "entities": []}
{"text": "While the training set is still noisy , as it remains intact .", "entities": []}
{"text": "In reality , it is costly and laborious to refine existing large - scale noisy datasets or collect new ones with fully precise annotations ( Wei et al , 2020 ) , let al ne dialogue datasets with multiple domains and multiple turns .", "entities": []}
{"text": "In view of this , we argue that it is essential to devise particular learning algorithms to train DST models robustly from noisy labels .", "entities": []}
{"text": "However , as illustrated in Figure 1 , the dialogue state may contain multiple labels , which makes it unstraightforward to apply existing noisy label learning algorithms to the DST task .", "entities": []}
{"text": "ASSIST first trains an auxiliary model on a small clean dataset to generate pseudo labels for each sample in the noisy training set .", "entities": []}
{"text": "Then , it leverages both the generated pseudo labels and vanilla noisy labels to train the primary model .", "entities": []}
{"text": "Since the auxiliary model is trained on the clean dataset , it can be expected that the pseudo labels will help us train the primary model more robustly .", "entities": []}
{"text": "Note that ASSIST is based on the assumption that we have access to a small clean dataset .", "entities": []}
{"text": "This assumption is reasonable , as it is feasible to manually collect a small noise - free dataset or re - annotate a portion of a large noisy dataset .", "entities": []}
{"text": "In summary , our main contributions include : We propose a general framework ASSIST to train robust DST models from noisy labels .", "entities": []}
{"text": "To the best of our knowledge , we are the first to tackle the DST problem by taking into consideration the label noise .", "entities": []}
{"text": "We theoretically analyze why the pseudo labels are beneficial and show that a proper combination of the pseudo labels and vanilla noisy labels can approximate the unknown true labels more accurately .", "entities": []}
{"text": "We conduct extensive experiments on Multi - WOZ 2.0 & 2.4 .", "entities": []}
{"text": "The results demonstrate that ASSIST can improve the DST performance on both datasets by a large margin .", "entities": []}
{"text": "In this section , we first provide the conventional definition of DST and then extend the definition to the noisy label learning scenario .", "entities": []}
{"text": "Let X = { ( R 1 , U 1 ) , . . .", "entities": []}
{"text": ", ( R T , U T ) } denote a dialogue of T turns , where R t and U t represent the system response and user utterance at turn t , respectively .", "entities": []}
{"text": "The dialogue state at turn t is defined as B t = { ( s , v t )", "entities": []}
{"text": "| s S } , where S denotes the set of predefined slots and v t is the corresponding value of slot s.", "entities": []}
{"text": "Following previous work Hu et al , 2020 ; Ye et al , 2021b ) , a slot in this paper refers to the concatenation of the domain name and slot name so as to include the domain information .", "entities": []}
{"text": "For example , we use \" hotel - name \" to represent the slot \" name \" in the hotel domain .", "entities": []}
{"text": "In general , the issue of DST is defined as learning a dialogue state tracker F : X t B t that takes the dialogue context X t as input and predicts the dialogue state B t at each turn t as accurately as possible .", "entities": []}
{"text": "Here , X t represents the dialogue history up to turn t , i.e. , X t = { ( R 1 , U 1 ) , . . .", "entities": []}
{"text": ", ( R t , U t ) } .", "entities": []}
{"text": "Conventionally , all the state labels are assumed to be correct .", "entities": []}
{"text": "However , this assumption may not hold .", "entities": []}
{"text": "In practice , dialogue state annotations are errorprone ( Han et al , 2020b ) .", "entities": []}
{"text": "There are a couple of reasons .", "entities": []}
{"text": "First , the states are usually annotated by crowdworkers to improve the labelling efficiency .", "entities": []}
{"text": "Second , the dialogue may span multiple domains , which also increases the labelling difficulty .", "entities": []}
{"text": "Apparently , the noisy labels are harmful and likely to lead to sub - optimal performance .", "entities": []}
{"text": "Therefore , it is crucial to take them into consideration so as to train DST models more robustly .", "entities": []}
{"text": "LetB t = { ( s , \u1e7d t )", "entities": []}
{"text": "| s S } denote the noisy state annotations , where\u1e7d t is the noisy label of slot s at turn t.", "entities": []}
{"text": "We use B t = { ( s , v t )", "entities": []}
{"text": "| s S } to denote the noise - free state annotations .", "entities": []}
{"text": "Here , v t represents the true label of slot s at turn t , which is unknown .", "entities": []}
{"text": "In fact , existing DST approaches are only able to learn a sub - optimal dialogue state trackerF : X t B t rather than the optimal state tracker F : X t B t , as none of them have considered the influence of noisy labels .", "entities": []}
{"text": "In this work , we aim to learn a robust state tracker F * that can better approximate F from the noisy state annotationsB t .", "entities": []}
{"text": "We introduce a general framework ASSIST , aiming to train DST models robustly from noisy labels .", "entities": []}
{"text": "We assume that a small clean dataset is accessible .", "entities": []}
{"text": "Based on this dataset , ASSIST first trains an auxiliary model A.", "entities": []}
{"text": "Then , it leverages A to generate pseudo labels for each sample in the noisy training set .", "entities": []}
{"text": "The pseudo state annotations are represented asB t = { ( s , v t )", "entities": []}
{"text": "| s S } , wherev t denotes the pseudo label of slot s at turn t.", "entities": []}
{"text": "Afterwards , both the generated pseudo labels and vanilla", "entities": []}
{"text": "noisy labels are exploited to train the primary model", "entities": []}
{"text": "F * .", "entities": []}
{"text": "That is , we intend to learn F * :", "entities": []}
{"text": "X t C ( B t , B t ) , where C ( B t , B t ) is a combination ofB t andB t .", "entities": []}
{"text": "Essentially , any existing DST models can be employed as the auxiliary model .", "entities": []}
{"text": "However , these models may lead to overfitting due to the small size of the clean dataset .", "entities": []}
{"text": "To tackle this issue , we propose a new simple model as the auxiliary model 1 .", "entities": []}
{"text": "Similar to Ye et", "entities": []}
{"text": "Let Z t = R t U t be the concatenation of the system response and user utterance at turn t , where denotes the operator of sequence concatenation .", "entities": []}
{"text": "Then , the dialogue context X t can be represented as X t = Z 1 Z 2 Z t .", "entities": []}
{"text": "We also concatenate each slot - value pair and denote the representation of the dialogue state at turn t as B t = ( s , vt ) Bt ,", "entities": []}
{"text": "vt = none s v t , in which only non - none slots are included .", "entities": []}
{"text": "B t can serve as a compact representation of the dialogue history .", "entities": []}
{"text": "The complete input sequence to the encoder module is then denoted as : I t", "entities": []}
{"text": "= [ CLS ] X t\u22121 B t\u22121", "entities": []}
{"text": "[ SEP ] Z t [ SEP ] , 1 We adopt existing DST models as the primary model .", "entities": []}
{"text": "[ CLS ] \u22ef \u22ef", "entities": []}
{"text": "[ SEP ] [ SEP ]", "entities": []}
{"text": "[ CLS ] \u22ef", "entities": []}
{"text": "[ SEP ]", "entities": []}
{"text": "It | \u00d7d be the semantic matrix representation of I t .", "entities": []}
{"text": "Here , V s denotes the candidate value set of slot s.", "entities": []}
{"text": "Besides , we adopt the output vector corresponding to the special token [ CLS ] as an aggregated representation of slot s and value v , i.e. , h s", "entities": []}
{"text": "=", "entities": []}
{"text": "v [ SEP ] ) .", "entities": []}
{"text": "Specifically , the slot representation h s is regarded as the query vector , and the dialogue context representation H t is taken as both the key matrix and value matrix .", "entities": []}
{"text": "Let a s t R d denote a d - dimensional vector representation of the related information of slot s at turn t , we obtain : a s t = MultiHead ( h s , H t , H t ) .", "entities": []}
{"text": "a s t is expected to be close to the semantic vector representation of the true value of slot", "entities": []}
{"text": "The final slot - specific vector g s t R d is calculated as : g s t = LayerNorm ( Linear ( a s t ) ) .", "entities": []}
{"text": "The slot - value matching module is utilized to predict the value of each slot s.", "entities": []}
{"text": "It first calculates the distance between the slot - specific representation g s t and the semantic representation of each candidate value v V s , i.e. , h v .", "entities": []}
{"text": "Then , the candidate value with the smallest distance is selected as the prediction .", "entities": []}
{"text": "The 2 norm is adopted to compute the distance .", "entities": []}
{"text": "Denotingv t as the predicted value of slot s at turn t , we have :", "entities": []}
{"text": "v t =", "entities": []}
{"text": "argmin v", "entities": []}
{"text": "Vs g s t \u2212 h v 2 .", "entities": []}
{"text": "We leverage a small clean dataset to train the auxiliary model .", "entities": []}
{"text": "Since the true labels are available , the auxiliary model is directly trained to maximize the joint probability of all slot values .", "entities": []}
{"text": "The probability of the true value v t of slot s at turn t is defined as : p ( v t | X t , s ) = exp ( \u2212 g s t \u2212 h v t 2 )", "entities": []}
{"text": "v Vs exp ( \u2212 g s t \u2212 h v 2 ) , where h v t is the semantic representation of v t .", "entities": []}
{"text": "Maximizing the joint probability \u03a0 ( s , vt ) Bt p ( v t | X t , s ) is equivalent to minimizing the following objective :", "entities": []}
{"text": "L aux = ( s , vt )", "entities": []}
{"text": "Bt \u2212 log p ( v t | X t , s ) .", "entities": []}
{"text": "Our approach depends on the auxiliary model A to generate pseudo labelsB t = { ( s , v t )", "entities": []}
{"text": "| s S } for each sample in the noisy training set .", "entities": []}
{"text": "In this work , we treat each dialogue context X t rather than the entire dialogue as a training sample .", "entities": []}
{"text": "To reduce the influence of noisy labels , we combine the generated pseudo labels and vanilla noisy labels to train the primary model .", "entities": []}
{"text": "Letv t and\u1e7d t be the one - hot representation of the pseudo labelv t and vanilla noisy label\u1e7d t , respectively .", "entities": []}
{"text": "( s , v c t ) C ( Bt , Bt ) \u2212 log p ( v c t | X t , s ) =", "entities": []}
{"text": "By minimizing L pri , the primary model is trained to learn from the vanilla noisy labels and at the same time imitate the predictions of the auxiliary model .", "entities": []}
{"text": "Since the pseudo labels are generated by the auxiliary model that has been trained on a small clean dataset , it can be expected that the combined labels are able to serve as a better approximation to the unknown true labels .", "entities": []}
{"text": "Let v t denote the one - hot representation of the unknown true value v t of slot s at turn t.", "entities": []}
{"text": "Yv = 1 | D n | | S |", "entities": []}
{"text": "Xt Dn s S E Dc [ v t \u2212 v t 2 2 ] , where the expectation ranges over different choices of the clean dataset D c , and | | returns the cardinality of a set .", "entities": []}
{"text": "Next , we show that the approximation error of the combined labels can be smaller than that of both the vanilla noisy labels and the generated pseudo labels .", "entities": []}
{"text": "The details are presented in Theorem 1 . Theorem 1 .", "entities": []}
{"text": "v c = Y\u1e7dYv Y\u1e7d + Yv .", "entities": []}
{"text": "Proof .", "entities": []}
{"text": "Hence , we can potentially train the primary model more robustly .", "entities": []}
{"text": "The annotations of its validation set and test set have been manually rectified .", "entities": []}
{"text": "Since the hospital domain and police domain never occur in the test set , we use only the remaining five domains { attraction , hotel , restaurant , taxi , train } in our experiments .", "entities": []}
{"text": "These domains have 30 slots in total .", "entities": []}
{"text": "We preprocess the datasets following ( Ye et al , 2021b ) .", "entities": []}
{"text": "We use the validation set as the small clean dataset .", "entities": []}
{"text": "defined as the proportion of dialogue turns in which the values of all slots are correctly predicted .", "entities": []}
{"text": "It is the most important metric in the DST task .", "entities": []}
{"text": "A slot becomes active if its value is mentioned in current turn and is not inherited from previous turns .", "entities": []}
{"text": "To verify the effectiveness of the proposed framework , we apply the generated pseudo labels to three different primary models .", "entities": []}
{"text": "It treats the dialogue state as an explicit fixed - sized memory and selectively overwrites this memory at each turn .", "entities": []}
{"text": "It leverages a stacked slot self - attention mechanism to capture the slot dependencies automatically .", "entities": []}
{"text": "We also test using the proposed auxiliary model as the primary model .", "entities": []}
{"text": "For the sake of description , we refer to this model as AUX - DST .", "entities": []}
{"text": "The ground - truth one is used during training , and the predicted one is used during testing 3 .", "entities": []}
{"text": "We train the auxiliary model on the clean validation set and the primary model on the noisy training set .", "entities": []}
{"text": "When training the auxiliary model , the noisy training set is leveraged to choose the best model .", "entities": []}
{"text": "More training details can be found in Appendix B.", "entities": []}
{"text": "For comparison , we also include the results when only the vanilla labels or only the pseudo labels are used to train the primary models .", "entities": []}
{"text": "As can be seen , ASSIST consistently improves the performance of the three primary models on both datasets .", "entities": []}
{"text": "We also observe that all the primary models demonstrate relatively good performance when only the pseudo labels are used .", "entities": []}
{"text": "From these results , it can be con - cluded that the pseudo labels are beneficial and they can help us train DST models more robustly .", "entities": []}
{"text": "Although any existing DST models can be adopted as the auxiliary model , we chose to propose a new simple one to reduce overfitting .", "entities": []}
{"text": "The results indicate that the proposed auxiliary model is able to generate pseudo labels with higher quality .", "entities": []}
{"text": "Figure 4 shows the results of AUX - DST .", "entities": []}
{"text": "Considering that our proposed framework ASSIST relies on a small clean dataset to train the auxiliary model that is further leveraged to generate pseudo labels for the training set , it is valuable to explore the effects of the size of the clean dataset on the performance of the primary model .", "entities": []}
{"text": "For this purpose , we vary the number of dialogues in the clean dataset from 500 to 1000 4 to generate different pseudo labels .", "entities": []}
{"text": "We then combine these different pseudo labels with the vanilla labels to train the primary model AUX - DST .", "entities": []}
{"text": "The results on Multi - WOZ 2.4 are reported in Figure 5 .", "entities": []}
{"text": "For comparison , 4 There are 1000 dialogues in total in the validation set .", "entities": []}
{"text": "we also include the results when only the pseudo labels or only the vanilla labels are used to train the primary model .", "entities": []}
{"text": "As can be seen , the size of the clean dataset has a great impact on the performance of the primary model .", "entities": []}
{"text": "Apparently , fewer clean data will lead to worse performance .", "entities": []}
{"text": "Nevertheless , as long as the pseudo labels are combined with the vanilla labels , the primary model can consistently demonstrate the strongest performance .", "entities": []}
{"text": "The previous experiments have proven the effectiveness of the generated pseudo labels in training robust DST models .", "entities": []}
{"text": "In this part , we provide further analyses on the quality of the pseudo labels to gain more insights into why they can be beneficial .", "entities": []}
{"text": "We first investigate whether the pseudo labels are consistent with the true labels .", "entities": []}
{"text": "However , the true labels of the training set are unavailable .", "entities": []}
{"text": "As an alternative , we treat the vanilla noisy labels as true labels ( note that only a portion of the vanilla labels are noisy ) .", "entities": []}
{"text": "In this experiment , we also vary the number of clean dialogues to train the auxiliary model .", "entities": []}
{"text": "Figure 6 presents the results .", "entities": []}
{"text": "As shown in Figure 6 , the auxiliary model achieves higher performance when more clean dialogues are utilized to train it .", "entities": []}
{"text": "Given that the vanilla noisy labels are regarded as the true labels , we can conjecture that the true performance is actually higher .", "entities": []}
{"text": "This experiment shows that the pseudo labels are consistent with the unknown true labels to some extent and can serve as a good complement to the vanilla noisy labels .", "entities": []}
{"text": "Pseudo Labels [ sys ] : Sure , da vinci pizzeria is a cheap Italian restaurant in the area .", "entities": []}
{"text": "[ usr ] : Would you mind making a reservation for Thursday at 17:15 ?", "entities": []}
{"text": "( restaurant - name , da vinci pizzeria )", "entities": []}
{"text": "( restaurant - book day , thursday ) ( restaurant - book time , 17:15 ) ( restaurant - name , da vinci pizzeria )", "entities": []}
{"text": "[ sys ] : Do you have a preferred section of town ? [ usr ] : Not really , but I want free wifi and it should be 4 star .", "entities": []}
{"text": "( hotel - internet , free ) ( hotel - stars , 4 ) ( hotel - area , dontcare ) ( hotel - internet , free ) ( hotel - stars , 4 ) [ usr ] : I need to find out if there is a train going to stansted airport that leaves after 12:30 .", "entities": []}
{"text": "( train - arriveby , 13:03 ) ( train - destination , stansted airport )", "entities": []}
{"text": "( train - leaveat , 12:30 ) ( train - destination , stansted airport )", "entities": []}
{"text": "( attraction - area , west ) ( attraction - area , west ) ( hotel - area , west ) Table 2 : Four dialogue snippets with their vanilla labels and the generated pseudo labels .", "entities": []}
{"text": "To save space , we only present turn - active slots and their values .", "entities": []}
{"text": "To intuitively understand the quality of the pseudo labels , we show four dialogue snippets with their vanilla labels and the generated pseudo labels in Table 2 .", "entities": []}
{"text": "As can be seen , the vanilla labels of the first two dialogue snippets are incomplete , while all the missing information is presented in the pseudo labels .", "entities": []}
{"text": "For the third dialogue snippet , the vanilla labels contain an unmentioned slot - value pair \" ( trainarriveby , 13:03 ) \" .", "entities": []}
{"text": "This error has also been fixed in the pseudo labels .", "entities": []}
{"text": "For the last dialogue snippet , the vanilla labels are correct .", "entities": []}
{"text": "However , the pseudo labels introduce an overconfident prediction of the value of slot \" hotel - area \" .", "entities": []}
{"text": "This case study has verified again that the pseudo labels can be utilized to fix certain errors in the vanilla labels .", "entities": []}
{"text": "However , the pseudo labels may bring about some new errors .", "entities": []}
{"text": "Hence , we should combine the two types of labels so as to achieve the best performance .", "entities": []}
{"text": "Aiming to better validate the effectiveness of the proposed framework , we also report the results when the small clean dataset is directly combined with the large noisy training set to train the primary model .", "entities": []}
{"text": "We adopt AUX - DST as the primary model and show the results in Table 3 .", "entities": []}
{"text": "Since the clean dataset ( i.e. , the validation set in our experiments ) is combined with the training set , all the results in Table 3 are the best ones on the test set .", "entities": []}
{"text": "As can be observed , a simple combination of the noisy training set and clean dataset can lead to better results .", "entities": []}
{"text": "It is also observed that when both the clean dataset and the pseudo labels are utilized to train the model , even higher performance can be achieved .", "entities": []}
{"text": "These results indicate that our proposed framework can make better use of the small clean dataset to train the primary model .", "entities": []}
{"text": "We further investigate the error rate with respect to each slot .", "entities": []}
{"text": "We adopt AUX - DST as the primary model and use AUX - DST ( w/o p ) to denote the case when only the vanilla labels are employed to train the model .", "entities": []}
{"text": "Even though the error rate is reduced with the aid of the pseudo labels , it is still the highest one among all the slots .", "entities": []}
{"text": "This is because the labels of this slot are confusing .", "entities": []}
{"text": "It is also observed that the \" name \" - related slots have relatively high error rates .", "entities": []}
{"text": "However , when the pseudo labels are used , their error rates reduce remarkably .", "entities": []}
{"text": "Besides , we observe that the error rates of some slots are higher when the pseudo labels are leveraged .", "entities": []}
{"text": "In practice , the noise rate with respect to each slot in the vanilla labels may not be exactly the same .", "entities": []}
{"text": "This observation in - spires us that more advanced techniques should be developed to combine the pseudo labels and vanilla labels , which we leave as our future work .", "entities": []}
{"text": "In this section , we briefly review related work on DST and noisy label learning .", "entities": []}
{"text": "Ouyang et al , 2020 ; Hosseini - Asl et al , 2020 ; Hu et al , 2020 ; Ye et al , 2021b ; Lin et al , 2021 ; Liang et al , 2021 ) .", "entities": []}
{"text": "The open vocabulary - based models leverage either span prediction ( Heck et al , 2020 ; or sequence generation ( Wu et al , 2019 ; Hosseini - Asl et al , 2020 ) to extract slot values from the dialogue context directly .", "entities": []}
{"text": "Although these DST models have made a huge success , they can only achieve sub - optimal performance , due to the lack of handling noisy labels .", "entities": []}
{"text": "To the best of our knowledge , we are the first to take the noisy labels into consideration when tackling the DST problem .", "entities": []}
{"text": "Addressing noisy labels in supervised learning is a long - term studied problem ( Fr\u00e9nay and Verleysen , 2013 ; Song et al , 2020 ; Han et al , 2020a ) .", "entities": []}
{"text": "This issue becomes more prominent in the era of deep learning , as training deep models generally requires a lot of well - labelled data , but it is expensive and time - consuming to collect large - scale datasets with completely clean annotations .", "entities": []}
{"text": "This dilemma has sparked a surge of noisy label learning methods ( Hendrycks et al , 2018 ; Zhang and Sabuncu , 2018 ; Song et al , 2019 ; Wei et al , 2020 ) .", "entities": []}
{"text": "In this work , we have presented a general framework ASSIST , aiming to train DST models robustly from noisy labels .", "entities": []}
{"text": "ASSIST leverages an auxiliary model that is trained on a small clean dataset to generate pseudo labels for the large noisy training set .", "entities": []}
{"text": "The pseudo labels are combined with the vanilla labels to train the primary model .", "entities": []}
{"text": "Both theoretical analysis and empirical study have verified the validity of our proposed framework .", "entities": []}
{"text": "In the future , we intend to explore more advanced techniques to combine the pseudo labels and", "entities": []}
{"text": "vanilla noisy labels in a better way .", "entities": []}
{"text": "Considering that the pseudo labels are generated by the auxiliary model that is trained on an extra small clean dataset and this clean dataset is independent of the noisy training set , we can regard the pseudo labels and vanilla labels as independent of each other .", "entities": []}
{"text": "Consequently , we obtain : E", "entities": []}
{"text": "Dc [ ( \u1e7d t \u2212 v t )", "entities": []}
{"text": "T", "entities": []}
{"text": "( v t \u2212 v t ) ]", "entities": []}
{"text": "= [ E Dc [ \u1e7d t \u2212 v t ] ] T E Dc [ v t \u2212 v t ]", "entities": []}
{"text": "= [ E Dc [ \u1e7d t \u2212 v t ] ] T E Dc [", "entities": []}
{"text": "v t \u2212 E", "entities": []}
{"text": "Dc [ v t ] ]", "entities": []}
{"text": "Based on the formula above , we can now calculate the approximation error with respect to the combined label v c t of slot s as below : E", "entities": []}
{"text": "=", "entities": []}
{"text": "Then , we have : Y", "entities": []}
{"text": "v c = 1 | D n | | S |", "entities": []}
{"text": "| S |", "entities": []}
{"text": "Note that the proposed auxiliary model is also applied as one primary model in our experiments .", "entities": []}
{"text": "The warmup proportion is fixed at 0.1 .", "entities": []}
{"text": "The dropout ( Srivastava et al , 2014 ) probability and word dropout ( Bowman et al , 2016 ) probability are also fixed at 0.1 .", "entities": []}
{"text": "The best model is chosen according to the performance on the validation set .", "entities": []}
{"text": "Except for the size of the clean dataset , the distribution of the clean dataset may also affect the performance of the primary model , especially when the clean dataset has a significantly different distribution from the training set .", "entities": []}
{"text": "Thus , it is important to study the effects of the distribution of the clean dataset .", "entities": []}
{"text": "However , we are short of clean datasets with different distributions .", "entities": []}
{"text": "It is also challenging to model the distribution explicitly since the dialogue state may contain multiple labels .", "entities": []}
{"text": "To address this issue , we propose to remove all the dialogues that are related to a specific domain and use only the remaining ones as the clean dataset .", "entities": []}
{"text": "As thus , we can create multiple clean datasets with different distributions .", "entities": []}
{"text": "As can be observed , although different clean datasets indeed lead to different performance , compared to the situation where no clean data is used ( i.e. , only the vanilla labels are used to train the model ) , all these clean datasets still bring huge performance improvements .", "entities": []}
{"text": "Proof .", "entities": []}
{"text": "Our proof is based on the bias - variance decomposition theorem 5 .", "entities": []}
{"text": "For any sample X t in the noisy training set D n , the approximation error with respect to the pseudo labelv t of slot s is defined as ] , which , according to the biasvariance decomposition theorem , can be decomposed into a bias term and a variance term , i.e. , where In our approach , the auxiliary model is a BERTbased model , which has more than 110 M parameters .", "entities": []}
{"text": "Such a complex model is expected to be able to capture all the samples in the small clean dataset D c .", "entities": []}
{"text": "Therefore , we can reasonably assume that the bias term is close to zero .", "entities": []}
{"text": "Then , we have :", "entities": []}
{"text": "We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models .", "entities": []}
{"text": "The pre - training is done using self - supervised objectives on a large amount of monolingual data for many languages .", "entities": []}
{"text": "Overall , our models are ranked in the top four of all systems for the submitted language pairs , with first rank in Spanish Portuguese .", "entities": []}
{"text": "( Charoenpornsawat et al , 2002 ) , phrase - based statistical methods ( SMT ) ( Koehn et al , 2003 ) and neural methods ( NMT )", "entities": []}
{"text": "( Cho et al , 2014 ; Sutskever et al , 2014 ; Bahdanau et al , 2015 ; Vaswani et al , 2017 ) .", "entities": []}
{"text": "NMT has achieved high translation quality for several language pairs ( Bojar et al , 2018 ;", "entities": []}
{"text": "Barrault et al , 2019 ) , but this level of performance usually requires large amounts of aligned data in the order of millions of sentence pairs .", "entities": []}
{"text": "For low and medium resource languages , SMT performs better than NMT ( Koehn and Knowles , 2017 ; Sennrich and Zhang , 2019 ) .", "entities": []}
{"text": "SMT also shows better performance when there is a domain mismatch between the train and test datasets , which is typical of low and medium resource language pairs .", "entities": []}
{"text": "In these settings , NMT performance can be boosted by leveraging additional monolingual data to enforce various types of constraints or increasing the training data using back - translation .", "entities": []}
{"text": "These methods can be particularly helpful if the source and target languages in MT are closely related and share language structure and alphabet .", "entities": []}
{"text": "The first language pair is low resource and second is medium resource in terms of the parallel data available for the task .", "entities": []}
{"text": "Refer to Table 2 for the classification .", "entities": []}
{"text": "The rankings obtained by us in each of the language directions are listed in Table 1", "entities": []}
{"text": "The results and analysis are detailed in Section 5 .", "entities": []}
{"text": "We finally conclude in Section 6 .", "entities": []}
{"text": "SMT is tackled by building a phrase table from the aligned parallel data .", "entities": []}
{"text": "The target side translation is then generated by matching the most appropriate phrases in the source sentence conditioned on the target side language model along with a reordering model ( Koehn et al , 2003 ) .", "entities": []}
{"text": "But these models ' reliance on large aligned parallel data for the source and target languages makes them unsuitable for low / medium resource language pairs ( Koehn and Knowles , 2017 ) .", "entities": []}
{"text": "Some of the previous works in these settings to improve NMT performance are described below :", "entities": []}
{"text": "Instead of using only two languages ( source and target ) for training an NMT model , using multiple languages has been shown to help in low resource scenarios .", "entities": []}
{"text": "For example , it might be the case that a certain pair of languages have very little parallel data between them , but there exists a third language with abundant parallel data with the original two languages .", "entities": []}
{"text": "This third language acts as a pivot and helps in improving NMT between the two languages ( Aharoni et al , 2019 ; Gu et al , 2018 ; Liu et al , 2020 ; Zhang et al , 2020 ) .", "entities": []}
{"text": "Pseudo - parallel corpus for each direction is first obtained by generating the translations of the monolingual data for each language using the partially - trained MT models on the limited parallel data .", "entities": []}
{"text": "Using these pseudoparallel corpora , the partially - trained NMT models are then trained further for some number of steps .", "entities": []}
{"text": "In this way , millions of pseudo - parallel sentence pairs can be generated to improve NMT models because of the abundance of monolingual data .", "entities": []}
{"text": "Another version of using back - translation is the copying mechanism .", "entities": []}
{"text": "Currey et al ( 2017 ) proposes to copy the target side monolingual data on the source side to create additional data without modifying the training regimen for NMT .", "entities": []}
{"text": "This helps the model to generate fluent translations .", "entities": []}
{"text": "For NMT , the first step is the random initialization of model weights in both the encoder and decoder .", "entities": []}
{"text": "Instead of random initialization , NMT models can be initialized by pre - training parts of the model ( Conneau and Lample , 2019 ;", "entities": []}
{"text": "( Devlin et al , 2019 ) .", "entities": []}
{"text": "This helps the model to not only learn the alignment between source and target language spaces , but also syntax structure like dependency parse , part of speech , etc .", "entities": []}
{"text": "This helps in making the target side translations more fluent and conforming to the structure of the language .", "entities": []}
{"text": "We do not explore this direction in this paper .", "entities": []}
{"text": "We experimented with three different settings for hi \u2194 mr as listed below .", "entities": []}
{"text": "SMT", "entities": []}
{"text": "This phrase - based system leverages both monolingual and parallel data provided for the task .", "entities": []}
{"text": "We use Moses ( Koehn et al , 2007 ) for training the SMT systems .", "entities": []}
{"text": "It has been shown to give significant improvements over the random initialization for NMT and is the current state - of - the - art for many low resource language pairs .", "entities": []}
{"text": "We use the same vocabulary for Marathi and Portuguese also , even though they were not used during the pre - training phase .", "entities": []}
{"text": "For Marathi , we used the Nepali language token and for Portuguese , we used the Italian language token .", "entities": []}
{"text": "We could not use Spanish language token for Portuguese because we are doing translations to and from Spanish .", "entities": []}
{"text": "We use hi \u2194 mr and es \u2194 pt language pairs for our experiments .", "entities": []}
{"text": "Because of the constrained nature of the shared task , we only use the parallel data provided for this task .", "entities": []}
{"text": "We removed the empty instances for both language pairs ( < 2000 instances ) .", "entities": []}
{"text": "For es \u2194 pt , we do not use ' WikiTitles v2 ' part of the parallel data for training because of very short sentences in the dataset .", "entities": []}
{"text": "The cleaned parallel dataset statistics are provided in Table 2 .", "entities": []}
{"text": "Preprocessing We use sentence piece tokenization ( Kudo and Richardson , 2018 ) for generating the source and target sequences for the NMT architectures .", "entities": []}
{"text": "For the SMT model on hi \u2194 mr , we also use the monolingual data provided for this task .", "entities": []}
{"text": "We extract 5 Million monolingual sentences each for Hindi and Marathi after deduplication and use this set for training the language models .", "entities": []}
{"text": "We use Moses ( Koehn et al , 2007 ) for all tokenization / detokenization scripts .", "entities": []}
{"text": "Lample et al ( 2018 ) .", "entities": []}
{"text": "We used Moses ( Koehn et al , 2007 ) and Giza++ with standard settings to train the SMT model in both directions .", "entities": []}
{"text": "We follow the recommendations of Liu et al ( 2020 ) for the hyperparameter settings .", "entities": []}
{"text": "We stop the training after 25 K gradient updates for the model .", "entities": []}
{"text": "These updates take \u223c35 hours on 4 Nvidia V100 ( 32 GB ) GPUs .", "entities": []}
{"text": "These scores are calculated on the validation set to decide our primary and contrastive submissions .", "entities": []}
{"text": "al , 2010 ) .", "entities": []}
{"text": "Results Table 3 shows our results on the test set for our primary and contrastive submissions .", "entities": []}
{"text": "Table 4 lists our final results on this shared task .", "entities": []}
{"text": "This shows that some level of language independent multilingual embeddings are present in the pre - trained model weights which can be exploited for the transfer task .", "entities": []}
{"text": "We have shown that pre - trained models can help in low and medium resource NMT .", "entities": []}
{"text": "Our results demonstrate that pre - training can help even when the language used for fine - tuning is not present during pre - training .", "entities": []}
{"text": "One direction of future work is to add linguistic information during the pre - training phase to get more fluent translations .", "entities": []}
{"text": "When this information is not available directly ( especially for low resource languages ) , pre - training on a related high resource language with syntax information can help low resource languages also .", "entities": []}
{"text": "of India and IBM SUR awards", "entities": []}
{"text": ".", "entities": []}
{"text": "Any opinions , findings , conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies , either expressed or implied , of the funding agencies .", "entities": []}
{"text": "We thank the IIT Delhi HPC facility 3 for the computational resources .", "entities": []}
{"text": "We are also thankful to Ganesh Ramakrishnan and Pawan Goyal for initial discussions on the project .", "entities": []}
{"text": "Parag Singla is supported", "entities": []}
{"text": "Different components in these neural architectures are intended to tackle different challenges .", "entities": []}
{"text": "We observed that prediction errors reflect certain model - specific biases , which we further discuss in this paper .", "entities": []}
{"text": "Neural approaches to the latter have gained a lot of prominence especially owing to the recent spur in developing and publicly releasing large datasets on Machine Reading and Comprehension ( MRC ) .", "entities": []}
{"text": "These different sources implicitly affect the nature and properties of questions and answers in these datasets .", "entities": []}
{"text": "Based on the dataset , certain neural models capitalize on these biases while others are unable to .", "entities": []}
{"text": "The ability to generalize across different sources and domains is a desirable characteristic for any machine reading system .", "entities": []}
{"text": "For its collection , different sets of crowd - workers formulated questions and answers using passages obtained from \u223c500 Wikipedia articles .", "entities": []}
{"text": "The answer to each question is a span in the given passage , and many effective neural QA models have been developed for this dataset .", "entities": []}
{"text": "We focused on Bi - Directional Attention Flow ( BiDAF ) ( Seo et al , 2016 ) , Gated Self - Matching Networks ( R - Net )", "entities": []}
{"text": "( Wang et al , 2017 ) , Document Reader ( DrQA )", "entities": []}
{"text": "We mainly choose these models since they have comparable high performance on the evaluation metrics and it is easy to replicate their results due to availability of open source implementations .", "entities": []}
{"text": "The organization of the paper is as follows .", "entities": []}
{"text": "Section 2 gives a comprehensive overview of the models that are compared in further sections .", "entities": []}
{"text": "Section 3 describes the different experiments we conducted , and discusses our observations .", "entities": []}
{"text": "In Section 4 , we summarize our main conclusions from this work and describe our vision for the future .", "entities": []}
{"text": "We present a brief overview of the models which we considered for our analysis in this section .", "entities": []}
{"text": "Bi - Directional Attention Flow ( BiDAF ) :", "entities": []}
{"text": "This model , proposed by Seo et al ( 2016 ) , is a hierarchical multi - stage end - to - end neural network which takes inputs of different granularity ( character , word and phrase ) to obtain a query - aware context representation using memory - less contextto - query ( C2Q ) and query - to - context ( Q2C ) attention .", "entities": []}
{"text": "This representation can then be used for different final tasks .", "entities": []}
{"text": "Gated Self - Matching Networks ( R - Net ) :", "entities": []}
{"text": "This model , proposed by Wang et al ( 2017 ) , is a multilayer end - to - end neural network whose novelty lies in the use of a gated attention mechanism so as to give different levels of importance to different passage parts .", "entities": []}
{"text": "It also uses self - matching attention for the context to aggregate evidence from the entire passage to refine the query - aware context representation obtained .", "entities": []}
{"text": "The architecture contains character and word embedding layers , followed by question - passage encoding and matching layers , a passage self - matching layer and an output layer .", "entities": []}
{"text": "The implementation we used 3 had some minor changes for increased efficiency .", "entities": []}
{"text": "Its implementation 4 has paragraph and question encoding layers , and an output layer .", "entities": []}
{"text": "To perform a systematic comparison of errors across different models , we investigate the predictions based on the following criteria .", "entities": []}
{"text": "These results are summarized in Table 1 .", "entities": []}
{"text": "The DocQA model gives the best overall performance which aligns well with our expectation , owing to the usage of and improvements in the prior mechanisms introduced in BiDAF and R - Net .", "entities": []}
{"text": "To investigate trends at different granularities , we also measure sentence retrieval performance .", "entities": []}
{"text": "We analyze the impact of passage length on errors , since this can be an important factor in determining the difficulty of understanding the passage .", "entities": []}
{"text": "As seen in Figure 1 , DocQA performs the best on shorter passages , while R - Net and BiDAF are observed to be better for longer passages .", "entities": []}
{"text": "However , there are no systematic error patterns and overall error rates , surprisingly , are not much higher for longer passages .", "entities": []}
{"text": "This means that predictions on long passages are almost as good as on short ( presumably easier to understand ) passages .", "entities": []}
{"text": "We also do a similar error analysis for questions of different lengths .", "entities": []}
{"text": "Since there are very few questions which have length greater than 30 , the estimate for range 30 - 34 is not very reliable .", "entities": []}
{"text": "In Figure 2 , we observe that the error rate first decreases and then increases for BiDAF , DrQA and DocQA .", "entities": []}
{"text": "A plausible explanation for this is that shorter questions contain insufficient information in order to be able to select the correct answer span and can hence be confusing , but it also becomes difficult for end - to - end neural models to learn a good representation when the question becomes longer and syntactically more complicated .", "entities": []}
{"text": "However , R - Net has an irregular trend with respect to question length , which is difficult to explain .", "entities": []}
{"text": "For answers of varying lengths , the error rates are shown in Figure 3 .", "entities": []}
{"text": "Again , estimates for answers with length > 16 are not very reliable since data is sparse for high answer lengths .", "entities": []}
{"text": "Here , we observe an increasing trend initially and then a slight decrease ( bell shape ) .", "entities": []}
{"text": "This conforms to the hypothesis that shorter answers are easier to predict than longer answers , but only up to a certain answer length ( observed to be around 7 for most models ) .", "entities": []}
{"text": "The slightly better performance for very long answers is likely due to such answers having a higher chance of being ( almost ) entire sentences with simpler questions being asked about them .", "entities": []}
{"text": "In Table 2 , we analyze the number of erroneous predictions which overlap for different pairs of models , i.e. , which belong to the intersection of the sets of incorrect answers generated by models in each ( row , column ) pair .", "entities": []}
{"text": "Thus , the values in the table represent a symmetric matrix with diagonal elements indicating the number of errors which each model commits .", "entities": []}
{"text": "This analysis can be useful while determining suitable models for creating meta ensembles since a low incorrect answer overlap indicates that the combined predictive power One way in which this analysis can help in exploring ensemble - based methods is that instead of trying all possible combinations of models , we can adopt a greedy approach based on the incorrect answer overlap metric to decide which model to add to the ensemble ( and only if it leads to a statistically significant difference in this overlap ) .", "entities": []}
{"text": "The entire network can also be trained end - to - end .", "entities": []}
{"text": "This indicates that future work based on ensembling different neural models can give promising results and is worth exploring .", "entities": []}
{"text": "An example of a passage - question - answer that all of the models get wrong is : Passage : The University of Warsaw was established in 1816 , when the partitions of Poland separated Warsaw from the oldest and most influential Polish academic center , in Krakow .", "entities": []}
{"text": "Warsaw University of Technology is the second academic school of technology in the country , and one of the largest in East - Central Europe , employing 2 , 000 professors .", "entities": []}
{"text": "Other institutions for higher education include the Medical University of Warsaw , the largest medical school in Poland and one of the most prestigious , the National Defence University , highest military academic institution in Poland , the Fryderyk Chopin University of Music the oldest and largest music school in Poland , and one of the largest in Europe , the Warsaw School of Economics , the oldest and most renowned economic university in the country , and the Warsaw University of Life Sciences the largest agricultural university founded in 1818 .", "entities": []}
{"text": "Question : What is one of the largest music schools in Europe ?", "entities": []}
{"text": "Answer :", "entities": []}
{"text": "Fryderyk Chopin University of Music This passage - question - answer is difficult for automatic processing because there several entities of the same type ( school / university ) in the passage , and the question is a paraphrase of one segment of a very long , syntactically complicated sentence which contains the information required to be able to infer the correct answer .", "entities": []}
{"text": "This presents an interesting challenge , and such qualitative observations can be used to formulate a general technique for effectively testing machine reading systems .", "entities": []}
{"text": "Broadly , the errors observed were either because of incorrect answer span boundaries or inability to infer the meaning of the question / passage .", "entities": []}
{"text": "Examples of each error type are shown in Table 3 , and these are further described below .", "entities": []}
{"text": "Incorrect answer boundary ( longer ) :", "entities": []}
{"text": "This error category includes those cases where the predicted span is longer than the ground truth answer , but contains the answer .", "entities": []}
{"text": "Incorrect answer boundary ( shorter ) :", "entities": []}
{"text": "This error category includes those cases where the predicted span is shorter than the ground truth answer , and is a substring of the answer .", "entities": []}
{"text": "Soft Correct : This error category includes those cases where the prediction is actually correct , but due to inclusion / exclusion of certain question terms ( such as units ) along with the answer , it is deemed incorrect .", "entities": []}
{"text": "Multi - Sentence :", "entities": []}
{"text": "This error category includes those cases where inference is required to be performed across 2 or more sentences in the given passage to be able to arrive at the answer , which leads to an incorrect prediction based on only 1 passage sentence .", "entities": []}
{"text": "Paraphrase : This error category includes those cases where the question paraphrases certain parts of the sentence that it is asking about which makes lexical pattern matching difficult and leads to errors in prediction .", "entities": []}
{"text": "This error category includes those cases where the question is about an entity type which is present multiple times in the passage and the model returns a different entity than the ground truth entity but of the same type .", "entities": []}
{"text": "Requires World Knowledge : This error category includes questions which can not be answered using the given passage alone and require external knowledge to solve , leading to incorrect predictions .", "entities": []}
{"text": "Missing Inference : This category includes inference - related errors which do n't belong to any of the other categories mentioned above .", "entities": []}
{"text": "In this section , we record the main observations from our qualitative error analysis and analyze potential reasons for the error trends observed .", "entities": []}
{"text": "Figure 4 shows the different types of errors in predictions by various models .", "entities": []}
{"text": "We observe that BiDAF makes many boundarybased errors which indicates that a better output layer ( since this is responsible for span identification - although errors might have percolated from previous layers , most of these are cases where the model almost got the correct answer but not exactly ) or some post - processing of the answer might help improve performance .", "entities": []}
{"text": "Paraphrases also contribute to almost 15 % of errors observed which indicates that the question and the relevant parts of the context are not effectively matched in these cases .", "entities": []}
{"text": "We observe that R - Net makes fewer boundary errors , perhaps because self - attention enables it to accumulate evidence and return better answer spans , although this leads to more errors of the ' shorter ' answer type than ' longer ' .", "entities": []}
{"text": "Also , missing inference contributes to almost 20 % of the observed errors ( not including multiple sentences or paraphrases ) .", "entities": []}
{"text": "We observe that DocQA makes many boundary errors too , again making more mistakes by pre - dicting shorter answers than expected in most of the observed cases .", "entities": []}
{"text": "A better root cause analysis can be performed by visualizing outputs from different layers and evaluating these , and we leave this in - depth investigation to future work .", "entities": []}
{"text": "Although these state - of - the - art deep learning models for machine reading are supposed to have inference capabilities , our error analysis above points to their limitations .", "entities": []}
{"text": "These insights can be useful for developing benchmarks and datasets which enable realistic evaluation of systems which aim to ' solve ' the RC task .", "entities": []}
{"text": "In Wadhwa et al ( 2018 ) , we take a first step in this direction by proposing a method focused on questions involving referential inference , a setting to which these models fail to generalize well .", "entities": []}
{"text": "We observe interesting trends in the analysis , with some error patterns which are consistent across different models and some others which are specific to each model due to their different input features and architectures .", "entities": []}
{"text": "This is important to be able to interpret and gain an intuition for the effective functions that different components in a neural model architecture perform versus their intended functions , and also to understand model - specific biases .", "entities": []}
{"text": "Eventually , this can enable us to come up with new models including specific components which tackle these errors .", "entities": []}
{"text": "Alternatively , the overlap analysis demonstrates that learning ensembles of different neural models to combine their individual strengths and quirks might be an interesting direction to explore to achieve better performance .", "entities": []}
{"text": "We will continue to work on this since the ability of a model to generalize and to be able to learn from a particular domain and transfer some knowledge to a different domain is a very exciting research area .", "entities": []}
{"text": "We also believe that such analysis can help curate datasets which are better indicators of the actual natural language ' reading ' and ' comprehending ' capabilities of models rather than falling prey to shallow pattern matching .", "entities": []}
{"text": "One way to achieve this is by building new challenges that are specifically designed to put pressure on the identified weaknesses of neural models .", "entities": []}
{"text": "Thus , we can move towards the development of datasets and models which truly push the envelope of the challenging machine reading task .", "entities": []}
{"text": "We would like to thank Chaitanya Malaviya and Abhishek Chinni for their valuable feedback , and the Language Technologies Institute at CMU for the GPU resources used in this work .", "entities": []}
{"text": "We are also very grateful to the anonymous reviewers for their insightful comments and suggestions , which helped us polish the presentation of our work .", "entities": []}
{"text": "Our unsupervised approach is based on a simple but effective synthetic error generation method based on confusion sets from inverted spell - checkers .", "entities": []}
{"text": "In low - resource settings , we outperform the current state - ofthe - art results for German and Russian GEC tasks by a large margin without using any real error - annotated training data .", "entities": []}
{"text": "When combined with labelled data , our method can serve as an efficient pre - training technique .", "entities": []}
{"text": "Unfortunately , such resources are not easily available , particularly for languages other than English .", "entities": []}
{"text": "This has lead to an increased interest in unsupervised and low - resource GEC ( Rozovskaya et al , 2017 ; Bryant and Briscoe , 2018 ; Boyd , 2018 ; Rozovskaya and Roth , 2019 ) , which recently culminated in the low - resource track of the Building Educational Application ( BEA ) shared task .", "entities": []}
{"text": "1", "entities": []}
{"text": "A neural sequence - to - sequence model is trained on clean and synthetically noised sentences alone .", "entities": []}
{"text": "The noise is automatically created from confusion sets .", "entities": []}
{"text": "Additionally , if labelled data 1 https://www.cl.cam.ac.uk/research/nl/ bea2019st is available for fine - tuning ( Hinton and Salakhutdinov , 2006 ) , MAGEC can also serve as an efficient pre - training technique .", "entities": []}
{"text": "The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection ( Felice and Yuan , 2014 ) or back - translation models ( Rei et al , 2017 ; Kasewa et al , 2018 ; Htut and Tetreault , 2019 ) .", "entities": []}
{"text": "It also outperforms noising techniques that rely on random word replacements ( Xie et al , 2018 ; Zhao et al , 2019 ) .", "entities": []}
{"text": "Contrary to Ge et al ( 2018 ) or Lichtarge et", "entities": []}
{"text": "al ( 2018 ) , our approach can be easily used for effective pre - training of full encoder - decoder models as it is model - independent and only requires clean monolingual data and potentially an available spell - checker dictionary .", "entities": []}
{"text": "2", "entities": []}
{"text": "As an unsupervised approach , MAGEC is an alternative to recently proposed language model ( LM ) based approaches ( Bryant and Briscoe , 2018 ; Stahlberg et al , 2019 ) , but it does not require any amount of annotated sentences for tuning .", "entities": []}
{"text": "Our minimally - augmented GEC approach uses synthetic noise as its primary source of training data .", "entities": []}
{"text": "We generate erroneous sentences from monolingual texts via random word perturbations selected from automatically created confusion sets .", "entities": []}
{"text": "These are traditionally defined as sets of frequently confused words ( Rozovskaya and Roth , 2010 ) .", "entities": []}
{"text": "We experiment with three unsupervised methods for generating confusion sets : Edit distance Confusion sets consist of words with the shortest Levenshtein distance ( Levenshtein , 1966 ) to the selected confused word .", "entities": []}
{"text": "Spell - breaking Confusion sets are composed of suggestions from a spell - checker ; a suggestion list is extracted for the confused word regardless of its actual correctness .", "entities": []}
{"text": "These methods can be used to build confusion sets for any alphabetic language .", "entities": []}
{"text": "3 We find that confusion sets constructed via spell - breaking perform best ( Section 4 ) .", "entities": []}
{"text": "Most context - free spell - checkers combine a weighted edit distance and phonetic algorithms to order suggestions , which produces reliable confusion sets ( Table 1 ) .", "entities": []}
{"text": "We synthesize erroneous sentences as follows : given a confusion set", "entities": []}
{"text": "C", "entities": []}
{"text": "i = { c i 1 , c i 2 , c i 3 , ... } , and the vocabulary V , we sample word w j V from the input sentence with a probability approximated with a normal distribution N ( p WER , 0.2 ) , and perform one of the following operations : ( 1 ) substitution of w j with a random word c j k from its confusion set with probability p sub , ( 2 ) deletion of w j with p del , ( 3 ) insertion of a random word w k V at j + 1 with p ins , and ( 4 ) swapping w j and w j+1 with p swp .", "entities": []}
{"text": "When making a substitution , words within confusion sets are sampled uniformly .", "entities": []}
{"text": "To improve the model 's capability of correcting spelling errors , inspired by Lichtarge et al ( 2018 ) ; Xie et al ( 2018 ) , we randomly perturb 10 % of characters using the same edit operations as above .", "entities": []}
{"text": "Character - level noise is introduced on top of the synthetic errors generated via confusion sets .", "entities": []}
{"text": "A MAGEC model is trained solely on the synthetically noised data and then ensembled with a language model .", "entities": []}
{"text": "Being limited only by the amount of clean monolingual data , this large - scale unsupervised approach can perform better than training on small authentic error corpora .", "entities": []}
{"text": "If any small amount of error - annotated learner data is available , it can be used to fine - tune the pre - trained model and further boost its performance .", "entities": []}
{"text": "Pre - training of decoders of GEC models from language models has been introduced by Junczys - Dowmunt et al ( 2018b ) , we pretrain the full encoder - decoder models instead , as proposed by Grundkiewicz et al ( 2019 ) .", "entities": []}
{"text": "Data and evaluation Our approach requires a large amount of monolingual data that is used for generating synthetic training pairs .", "entities": []}
{"text": "We use the publicly available News crawl data 5 released for the WMT shared tasks ( Bojar et al , 2018 ) .", "entities": []}
{"text": "For English and German , we limit the size of the data to 100 million sentences ; for Russian , we use all the available 80.5 million sentences .", "entities": []}
{"text": "As primary development and test data , we use the following learner corpora ( Table 2 ) : English : the new W&I+LOCNESS corpus Granger , 1998 ) released for the BEA 2019 shared task and representing a diverse cross - section of English language ; German : the Falko - MERLIN GEC corpus ( Boyd , 2018 )", "entities": []}
{"text": "Russian : the recently introduced RULEC - GEC dataset (", "entities": []}
{"text": "Alsufieva et al , 2012 ; Rozovskaya and Roth , 2019 ) containing Russian texts from foreign and heritage speakers .", "entities": []}
{"text": "Unless explicitly stated , we do not use the training parts of those datasets .", "entities": []}
{"text": "For each language we follow the originally proposed preprocessing and evaluation settings .", "entities": []}
{"text": "English and German data are tokenized with Spacy 6 , while Russian is preprocessed with Mystem ( Segalovich , 2003 ) .", "entities": []}
{"text": "We additionally normalise punctuation in monolingual data using Moses scripts ( Koehn et al , 2007 ) .", "entities": []}
{"text": "English models are evaluated with ERRANT ( Bryant et al , 2017 ) using F 0.5 ; for German and Russian , the M2Scorer with the MaxMatch metric ( Dahlmeier and Ng , 2012 ) is used .", "entities": []}
{"text": "Synthetic data Confusion sets are created for each language for V = 96 , 000 most frequent lexical word forms from monolingual data .", "entities": []}
{"text": "We use the Levenshtein distance to generate edit - distance based confusion sets .", "entities": []}
{"text": "The maximum considered distance is 2 .", "entities": []}
{"text": "To generate spell - broken confusion sets we use Enchant 8 with Aspell dictionaries .", "entities": []}
{"text": "9 The size of confusion sets is limited to top 20 words .", "entities": []}
{"text": "When confusing a word , the probability p sub is set to 0.7 , other probabilities are set to 0.1 .", "entities": []}
{"text": "10 We use the training setting proposed by the authors 11 , but introduce stronger regularization : we increase dropout probabilities of source words to 0.3 , add dropout on transformer self - attention and filter layers of 0.1 , and use larger mini - batches with 2 , 500 sentences .", "entities": []}
{"text": "We do not pre - train the decoder parameters with a language model and train directly on the synthetic data .", "entities": []}
{"text": "In experiments with fine - tuning , the training hyperparameters remain unchanged .", "entities": []}
{"text": "All models are trained with Marian ( Junczys - Dowmunt et al , 2018a ) .", "entities": []}
{"text": "The training is continued for at most 5 epochs or until early - stopping is triggered after 5 stalled validation steps .", "entities": []}
{"text": "We found that using 10 , 000 synthetic sentences as validation sets , i.e. a fully unsupervised approach , is as effective as using the development parts of error corpora and does not decrease the final performance .", "entities": []}
{"text": "We observe further gains of +1.04 from keeping out - of - vocabulary spell - checker suggestions ( OOV ) and preserving consistent letter casing within confusion sets ( Case ) .", "entities": []}
{"text": "The word error rate of error corpora is an useful statistic that can be used to balance precision / recall ratios ( Rozovskaya and Roth , 2010 ; Junczys - Dowmunt et al , 2018b ; Hotate et al , 2019 ) .", "entities": []}
{"text": "Increasing WER in the synthetic data from 15 % to 25 % increases recall at the expense of precision , but no overall improvement is observed .", "entities": []}
{"text": "A noticeable recall gain that transfers to a higher F - score of 28.99 is achieved by increasing the importance of edited fragments with the edit - weighted MLE objective from Junczys - Dowmunt et al ( 2018b ) with \u039b = 2 .", "entities": []}
{"text": "We use this setting for the rest of our experiments .", "entities": []}
{"text": "All systems outperform the spell - checker baselines .", "entities": []}
{"text": "On German and Russian test sets , single MAGEC models without ensembling with a language model already achieve better performance than reported by Boyd ( 2018 ) Roth ( 2019 ) for their systems that use authentic error - annotated data for training ( Table 4b and 4c ) .", "entities": []}
{"text": "+7.0 and +11.4 F 0.5 .", "entities": []}
{"text": "Our English models do not compete with the top systems ( Grundkiewicz et al , 2019 ) from the BEA shared task trained on publicly available errorannotated corpora ( Table 4a ) .", "entities": []}
{"text": "It is difficult to compare with the top low - resource system from the shared task , because it uses additional parallel data from Wikipedia ( Grundkiewicz and Junczys - Dowmunt , 2014 ) , larger ensemble , and n - best list re - ranking with right - to - left models , which can be also implemented in this work .", "entities": []}
{"text": "The more authentic high - quality and in - domain training data is used , the greater the improvement , but even as few as~2 , 000 sentences are helpful ( Fig . 1 ) .", "entities": []}
{"text": "We found that fine - tuning on a 2:1 mixture of synthetic and oversampled authentic data prevents the model from over - fitting .", "entities": []}
{"text": "This is particularly visible for English which has the largest fine - tuning set ( 34 K sentences ) , and the difference of 5.2 F 0.5 between finetuning with and without synthetic data is largest .", "entities": []}
{"text": "The GEC task involves detection and correction of all types of error in written texts , including grammatical , lexical and orthographical errors .", "entities": []}
{"text": "Spelling and punctuation errors are among the most frequent error types and also the easiest to synthesize .", "entities": []}
{"text": "Our systems indeed perform best on misspellings and punctuation errors , but are capable of correcting various error types .", "entities": []}
{"text": "The disparity for Russian can be explained by the fact that it is a morphologically - rich language and we suffer from generally lower performance .", "entities": []}
{"text": "The method is model independent , requires easily available resources , and can be used for creating reliable baselines for supervised techniques or as an efficient pre - training method for neural GEC models with labelled data .", "entities": []}
{"text": "We have demonstrated the effectiveness of our method and outperformed state - of - the - art results for German and Russian benchmarks , trained with labelled data , by a large margin .", "entities": []}
{"text": "For future work , we plan to evaluate MAGEC on more languages and experiment with more diversified confusion sets created with additional unsupervised generation methods .", "entities": []}
{"text": "Feature attribution methods , proposed recently , help users interpret the predictions of complex models .", "entities": []}
{"text": "Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building .", "entities": []}
{"text": "To demonstrate the effectiveness our technique , we apply it to two tasks : ( 1 ) mitigating unintended bias in text classifiers by neutralizing identity terms ; ( 2 ) improving classifier performance in a scarce data setting by forcing the model to focus on toxic terms .", "entities": []}
{"text": "Our experiments show that i ) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task ; ii ) incorporating priors helps model performance in scarce data settings .", "entities": []}
{"text": "One of the recent challenges in machine learning ( ML ) is interpreting the predictions made by models , especially deep neural networks .", "entities": []}
{"text": "Understanding models is not only beneficial , but necessary for wide - spread adoption of more complex ( and potentially more accurate ) ML models .", "entities": []}
{"text": "From healthcare to financial domains , regulatory agencies mandate entities to provide explanations for their decisions ( Goodman and Flaxman , 2016 ) .", "entities": []}
{"text": "Hence , most machine learning progress made in those areas is hindered by a lack of model explainability - causing practitioners to resort to simpler , potentially low - performance models .", "entities": []}
{"text": "To supply for this demand , there has been many attempts for model interpretation in recent years for tree - based algorithms ( Lundberg et al , 2018 ) and deep learning algorithms ( Lundberg and Lee , 2017 ; Smilkov et al , 2017 ; Sundararajan et al , 2017 ; Bach et al , 2015 ; Dhurandhar et al , 2018 ) .", "entities": []}
{"text": "Baseline I am gay 0.915 I am straight 0.085 Our Method I am gay 0.141 I am straight 0.144", "entities": []}
{"text": "On the other hand , the amount of research focusing on explainable natural language processing ( NLP ) models ( Li et al , 2016 ; Murdoch et al , 2018 ; Lei et al , 2016 ) is modest as opposed to image explanation techniques .", "entities": []}
{"text": "Inherent problems in data emerge in a trained model in several ways .", "entities": []}
{"text": "Model explanations can show that the model is not inline with human judgment or domain expertise .", "entities": []}
{"text": "A canonical example is model unfairness , which stems from biases in the training data .", "entities": []}
{"text": "If employed , explanation techniques help divulge these issues , but fail to offer a remedy .", "entities": []}
{"text": "For instance , the sentence \" I am gay \" receives a high score on a toxicity model as seen in Table 1 .", "entities": []}
{"text": "The Integrated Gradients ( Sundararajan et al , 2017 ) explanation method attributes the majority of this decision to the word \" gay . \"", "entities": []}
{"text": "However , none of the explanations methods suggest next steps to fix the issue .", "entities": []}
{"text": "Instead , researchers try to reduce biases indirectly by mostly adding more data ( Dixon et al , 2018 ; , using unbiased word vectors ( Park et al , 2018 ) , or directly optimizing for a fairness proxy with adversarial training ( Madras et al , 2018 ; Zhang et al , 2018a ) .", "entities": []}
{"text": "These methods either offer to collect more data , which is costly in many cases , or make a tradeoff between original task performance and fairness .", "entities": []}
{"text": "In this paper , we attempt to enable injecting priors through model explanations to rectify issues in trained models .", "entities": []}
{"text": "Our main intuition is that undesirable correlations between toxicity labels and instances of identity terms cause the model to learn unfair biases which can be corrected by incorporating priors on these identity terms .", "entities": []}
{"text": "Moreover , our approach allows practitioners to impose priors in the other direction to tackle the problem of training a classifier when there is only a small amount of data .", "entities": []}
{"text": "As shown in our experiments , by setting a positive target attribution for known toxic words 1 , one can improve the performance of a toxicity classifier in a scarce data regime .", "entities": []}
{"text": "We validate our approach on the Wikipedia toxic comments dataset ( Wulczyn et al , 2017 ) .", "entities": []}
{"text": "Models trained with our technique also show lower attributions to identity terms on average .", "entities": []}
{"text": "Our technique produces much better word vectors as a by - product when compared to the baseline .", "entities": []}
{"text": "Lastly , by setting an attribution target of 1 on toxic words , a classifier trained with our objective function achieves better performance when only a subset of the data is present .", "entities": []}
{"text": "1 Full list of identity terms and toxic terms used as priors can be found in supplemental material .", "entities": []}
{"text": "Please note the toxic terms are not censored .", "entities": []}
{"text": "In this section , we give formal definitions of feature attribution and a primer on [ Path ] Integrated Gradients ( IG ) , which is the basis for our method .", "entities": []}
{"text": "Definition 2.1 .", "entities": []}
{"text": "An attribution of the prediction at input x is a vector a = ( a 1 , ... , a n ) and a i is defined as the attribution of x i .", "entities": []}
{"text": "Feature attribution methods have been studied to understand the contribution of each input feature to the output prediction score .", "entities": []}
{"text": "This contribution , then , can further be used to interpret model decisions .", "entities": []}
{"text": "Linear models are considered to be more desirable because of their implicit interpretability , where feature attribution is the product of the feature value and the coefficient .", "entities": []}
{"text": "To some , non - linear models such as gradient boosting trees and neural networks are less favorable due to the fact that they do not enjoy such transparent contribution of each feature and are harder to interpret ( Lou et al , 2012 ) .", "entities": []}
{"text": "One of these axioms is completeness , which postulates that the sum of attributions should be equal to the difference between uncertainty and model output .", "entities": []}
{"text": "Integrated Gradients ( Sundararajan et al , 2017 ) is a model attribution technique applicable to all models that have differentiable inputs w.r.t . outputs .", "entities": []}
{"text": "IG produces feature attributions relative to an uninformative baseline .", "entities": []}
{"text": "This baseline input is designed to produce a high - entropy prediction representing uncertainty .", "entities": []}
{"text": "IG , then , interpolates the baseline towards the actual input , with the prediction moving from uncertainty to certainty in the process .", "entities": []}
{"text": "Building on the notion that the gradient of a function , f , with respect to input can characterize sensitivity of f for each input dimension , IG simply aggregates the gradients of f with respect to the input along this path using a path integral .", "entities": []}
{"text": "The crux of using path integral rather than overall gradient at the input is that f 's gradients might have been saturated around the input and integrating over a path alleviates this phenomenon .", "entities": []}
{"text": "Even though there can be infinitely many paths from a baseline to input point , Integrated Gradients takes the straight path between the two .", "entities": []}
{"text": "We give the formal definition from the original paper in 2.2 . Definition 2.2 .", "entities": []}
{"text": "Given an input x and baseline x , the integrated gradient along the i th dimension is defined as follows .", "entities": []}
{"text": "IG i ( x , x ) : : = ( x i \u2212 x i ) \u00d7 1 \u03b1=0 \u2202f ( x + \u03b1\u00d7 ( x\u2212x ) )", "entities": []}
{"text": "\u2202x i d\u03b1 ( 1 )", "entities": []}
{"text": "where \u2202f ( x ) \u2202x i represents the gradient of f along the i th dimension at x. In the NLP setting , x is the concatenated embedding of the input sequence .", "entities": []}
{"text": "The attribution of each token is the sum of the attributions of its embedding .", "entities": []}
{"text": "There are other explainability methods that attribute a model 's decision to its features , but we chose IG in this framework due to several of its characteristics .", "entities": []}
{"text": "First , it is both theoretically justified ( Sundararajan et al , 2017 ) and proven to be effective in NLP - related tasks ( Mudrakarta et al , 2018 ) .", "entities": []}
{"text": "Second , the IG formula in 2.2 is differentiable everywhere with respect to model parameters .", "entities": []}
{"text": "Lastly , it is lightweight in terms of implementation and execution complexity .", "entities": []}
{"text": "Problems in data manifest themselves in a trained model 's performance on classification or fairness metrics .", "entities": []}
{"text": "Recently , attributions help uncover deficiencies causing models to perform poorly , but do not offer actionability .", "entities": []}
{"text": "To this end , we propose to add an extra term to the objective function to penalize the L 2 distance between model attributions on certain features and target attribution values .", "entities": []}
{"text": "This modification allows model practitioners to inject priors .", "entities": []}
{"text": "For example , consider a model that tends to predict every sentence containing \" gay \" as toxic in a comment moderation system .", "entities": []}
{"text": "Penalizing non - zero attributions on the tokens identifying protected groups would force the model to focus more on the context words rather than mere existence of certain tokens .", "entities": []}
{"text": "We give the formal definition of the new objective function that incorporates priors as the follows : Definition 3.1 .", "entities": []}
{"text": "Given a vector t of size n , where n is the length of the input sequence and t i is the attribution target value for the ith token in the input sequence .", "entities": []}
{"text": "=", "entities": []}
{"text": "n i ( a", "entities": []}
{"text": "i", "entities": []}
{"text": "\u2212 t i )", "entities": []}
{"text": "2 ( 2 ) where a i refers to attribution of the ith token as in Definition 2.1 .", "entities": []}
{"text": "For a multi - class problem , we train our model with the following joint objective , L joint = L ( y , p )", "entities": []}
{"text": "+", "entities": []}
{"text": "= C c \u2212y c log ( p c ) ( 4 ) where y is an indicator vector of the ground truth label and p c is the posterior probability of class c.", "entities": []}
{"text": "The joint objective function is differentiable w.r.t . model parameters when attribution is calculated through Equation 1 and can be trained with most off - the - shelf optimizers .", "entities": []}
{"text": "It only requires users to specify the target attribution value for tokens of interest in the corpus .", "entities": []}
{"text": "In the next section , we first show how we set the target attribution value for identity terms to remove unintended biases while retaining the same performance on the original task .", "entities": []}
{"text": "Then , using the same technique , we show how to set target attribution for toxic words to improve classifier performance in a scarce data setting .", "entities": []}
{"text": "Base", "entities": []}
{"text": "We incorporate human prior in model building on two applications .", "entities": []}
{"text": "For our experiments , we aim to mitigate the issue of neutral sentences with identity terms being classified as toxic for a given a set of identity terms .", "entities": []}
{"text": "A subset of the identity terms are listed in the first column of Table 2 .", "entities": []}
{"text": "Second , we force the model to focus on a list of human - selected toxic terms under scarce data scenario to increase model performance .", "entities": []}
{"text": "In the following section , we introduce the dataset we train and evaluate on along with a synthetic dataset to further validate our fairness improvements .", "entities": []}
{"text": "After that , we describe our experimental setup .", "entities": []}
{"text": "Lastly , we show the results demonstrating usefulness of our approach with data scarcity .", "entities": []}
{"text": "In this work , we use a dataset containing comments from Wikipedia Talk Pages ( Dixon et al , 2018 ) .", "entities": []}
{"text": "The ratio of positive ( toxic ) labels in the training set is 9.7 % .", "entities": []}
{"text": "The dataset was annotated by human raters , where toxicity was defined as a \" rude , disrespectful , or unreasonable comment that is likely to make you leave a discussion \" per Dixon et al ( 2018 ) .", "entities": []}
{"text": "Please refer to the corresponding paper for more details about collection methodology , biases present in the data , and toxicity distribution per comment length .", "entities": []}
{"text": "We also use a synthetically generated dataset to validate our approach on fairness as in Park et", "entities": []}
{"text": "al", "entities": []}
{"text": "For the text classifier , we built a convolutional neural network ( CNN ) classifier as in Kim ( 2014 ) .", "entities": []}
{"text": "Embeddings were randomly initialized and their size was set to 128 .", "entities": []}
{"text": "Shorter sequences are padded with < pad > token and longer sequences are truncated .", "entities": []}
{"text": "Tokens occurring 5 times or more are retained in the vocabulary .", "entities": []}
{"text": "The number of interpolating steps for IG is set to 50 ( as in the original paper ) for calculating Riemann approximation of the integral .", "entities": []}
{"text": "Since the output of the binary classification can be reduced to a single scalar output by taking the posterior of the positive ( toxic ) class , the prior is only added to the positive class in equation 3 .", "entities": []}
{"text": "We set t i = k , if x", "entities": []}
{"text": "i I", "entities": []}
{"text": "a i , otherwise , ( 5 ) where I is the set of selected terms and x i being the i th token in the sequence .", "entities": []}
{"text": "and I to the set of identity terms with the hope that these terms should be as neutral as possible when making predictions .", "entities": []}
{"text": "Hyperparamter \u03bb is searched in the range of ( 1 , 10 8 ) and increased from 1 by a scale of 10 on the dev set", "entities": []}
{"text": "and we pick the one with best F - 1 score .", "entities": []}
{"text": "\u03bb is set to 10 6 for the final model .", "entities": []}
{"text": "For data scarcity experiments , we set k to 1 and I to the set of toxic terms to force the model to make high attributions to these terms .", "entities": []}
{"text": "Hyperparameter \u03bb is set to 10 5 across all data size experiments by tuning on the dev set with model given 1 % of training data .", "entities": []}
{"text": "Each experiment was repeated for 5 runs with 10 epochs and the best model is selected according to the dev set .", "entities": []}
{"text": "There are two reasons that lead to this decision : ( i ) taking the gradient of the interpolate operation would break the axioms that IG guarantees ; ( ii ) the Hessian of the embedding matrix is slow to compute .", "entities": []}
{"text": "During training , the model parameters are updated with respect to both losses .", "entities": []}
{"text": "TOK Replace : Common technique for making models blind to identity terms ( Garg et al , 2018 ) .", "entities": []}
{"text": "All identity terms are replaced with a special < i d > token .", "entities": []}
{"text": "The aim of this experiment is to show that our method is also applicable for tweaking trained models , which could be useful if the original had been trained for a long time .", "entities": []}
{"text": "Results are shown in Table 4 .", "entities": []}
{"text": "Unlike previous approaches ( Park et al , 2018 ; Dixon et al , 2018 ; Madras et al , 2018 ) , our method does not degrade classifier performance ( it even improves ) in terms of all reported metrics .", "entities": []}
{"text": "We also look at samples containing identity terms .", "entities": []}
{"text": "Table 5 shows classifier performance metrics for such samples .", "entities": []}
{"text": "The importance weighting approach slightly outperforms the baseline classifier .", "entities": []}
{"text": "Replacing identity words with a special tokens , on the other hand , hurts the performance on the main task .", "entities": []}
{"text": "One of the reasons might be that replacing all identity terms with a token potentially removes other useful information model can rely on .", "entities": []}
{"text": "If we were to make an analogy between the token replacement method and hard ablation , then the same analogy can be made between our method and soft ablation .", "entities": []}
{"text": "Hence , the information pertaining to identity terms is not completely lost for our method , but come at a cost .", "entities": []}
{"text": "Results for fine - tuning experiments show the performance after 2 epochs .", "entities": []}
{"text": "It is seen that the model converges to similar performance with joint training after only 2 epochs , albeit being slightly poorer .", "entities": []}
{"text": "Now we run our experiments on the templatebased synthetic data .", "entities": []}
{"text": "As stated , this dataset is used to measure biases in the model since it is unbiased towards identities .", "entities": []}
{"text": "Park et al ( 2018 ) .", "entities": []}
{"text": "FPED sums absolute differences between overall false positive rate and false positive rates for each identity term .", "entities": []}
{"text": "FNED calculates the same for false negatives .", "entities": []}
{"text": "Results on this dataset are shown in Table 7 .", "entities": []}
{"text": "The fine - tuned model also outperforms the baseline for mitigating the bias .", "entities": []}
{"text": "The token replacement method comes out as a good baseline for mitigating the bias since it treats all identities the same .", "entities": []}
{"text": "The importance weighting approach fails to produce an unbiased model .", "entities": []}
{"text": "Models convert input tokens to embeddings before providing them to convolutional layers .", "entities": []}
{"text": "As embeddings make up the majority of the parameters of the network and can be exported for use in other tasks , we 're interested in how they change for the identity terms .", "entities": []}
{"text": "We show 10 nearest neighbors of the terms < i d > ( for the token replacement method ) , \" gay \" , and \" homosexual \" - top two identity terms with the most mean attribution difference ( our method vs. baseline ) , in Table 6 .", "entities": []}
{"text": "The word embedding of the term \" gay \" shifts from having swear words as its neighbors to having the < pad > token as the closest neighbor .", "entities": []}
{"text": "Although the term \" homosexual \" has lower mean attribution , its neighboring words are still mostly swear words in the baseline embedding space .", "entities": []}
{"text": "\" homosexual \" also moved to more neutral terms that should n't play a role in deciding if the comment is toxic or not .", "entities": []}
{"text": "The importance weighting technique penalizes the model on the sentence level instead of focusing on the token level .", "entities": []}
{"text": "Therefore , the word embedding of \" gay \" does n't seem to shift to neutral words .", "entities": []}
{"text": "The token replacement method , on the other hand , replaces the identity terms with a token that is surrounded with neutral words in the embedding space , so it results in greater improvement on the synthetic dataset .", "entities": []}
{"text": "However , since all identity terms are collapsed into one , it 's harder for the model to capture the context and as a result , classification performance on the original dataset drops .", "entities": []}
{"text": "We now demonstrate our approach on encouraging higher attributions on toxic words to increase model performance in scarce data regime .", "entities": []}
{"text": "We down - sample the dataset with different ratios to simulate a data scarcity scenario .", "entities": []}
{"text": "We also show that the attribution for these terms increases as training data increases for the baseline method .", "entities": []}
{"text": "We then show model performance on testing data for different data size ratios for the baseline and our method in Figure 1 .", "entities": []}
{"text": "Our method outperforms the baseline by a big margin in 1 % and 5 % ratio .", "entities": []}
{"text": "However , the impact of our approach diminishes after adding more data , since the model starts to learn to focus on toxic words itself for predicting toxicity without the need for prior injection .", "entities": []}
{"text": "We can also see that both the baseline and our method start to catch up with the rule based approach , where we give positive prediction if the toxic word is in the sentence , and eventually outperform it .", "entities": []}
{"text": "For explaining ML models , recent research attempts offer techniques ranging from building inherently interpretable models to building a proxy model for explaining a more complex model ( Ribeiro et al , 2016 ; Frosst and Hinton , 2017 ) to explaining inner mechanics of mostly uninterpretable neural networks ( Sundararajan et al , 2017 ; Bach et al , 2015 ) .", "entities": []}
{"text": "One family of interpretability methods uses sensitivity of the network with respect to data points ( Koh and Liang , 2017 ) or features ( Ribeiro et al , 2016 ) as a form of explanation .", "entities": []}
{"text": "These methods rely on small , local perturbations and check how a network 's response changes .", "entities": []}
{"text": "Explaining text models has another layer of complexity due to a lock of proper technique to generate counterfactuals in the form of small perturbations .", "entities": []}
{"text": "Hence , interpretability methods tailored for text are quite sparse ( Mudrakarta et al , 2018 ; Jia and Liang , 2017 ; Murdoch et al , 2018 ) .", "entities": []}
{"text": "On the other hand , there are many papers criticizing the aforementioned methods by questioning their faithfulness , correctness ( Adebayo et al , 2018 ; Kindermans et al , 2017 ) and usefulness .", "entities": []}
{"text": "Smilkov et al ( 2017 ) show that gradient based methods are susceptible to saturation and can be fooled by adversarial techniques .", "entities": []}
{"text": "Other sets of papers ( Miller , 2019 ; Gilpin et al , 2018 ) attack model explanation papers from a philosophical perspective .", "entities": []}
{"text": "However , the lack of actionability angle is often overlooked .", "entities": []}
{"text": "Lipton ( 2018 ) briefly questions the practical benefit of having model explanations from a practitioners perspective .", "entities": []}
{"text": "There are several works taking advantage of model explanations .", "entities": []}
{"text": "Namely , using model explanations to aid doctors in diagnosing retinopathy patients , and removing minimal features , called pathologies , from neural networks by tuning the model to have high entropy on pathologies ( Feng et al , 2018 ) .", "entities": []}
{"text": "The authors of Ross et al ( 2017 ) propose a similar idea to our approach in that they regularize input gradients to alter the decision boundary of the model to make it more consistent with domain knowledge .", "entities": []}
{"text": "However , the input gradients technique has been shown to be an inaccurate explanation technique ( Adebayo et al , 2018 ) .", "entities": []}
{"text": "Addressing and mitigating bias in NLP models are paramount tasks as the effects on these models adversely affect protected subpopulations ( Schmidt and Wiegand , 2017 ) .", "entities": []}
{"text": "One of the earliest works is Calders and Verwer ( 2010 ) .", "entities": []}
{"text": "Later , Bolukbasi et al ( 2016 ) proposed to unbias word vectors from gender stereotypes .", "entities": []}
{"text": "While their results seem to show promise for removing gender bias , their method does n't scale for other identity dimensions such as race and religion .", "entities": []}
{"text": "The authors of Dixon et al ( 2018 ) highlight the bias in toxic comment classifier models originating from the dataset .", "entities": []}
{"text": "They also supplement the training dataset from Wikipedia articles to shift positive class imbalance for sentences containing identity terms to dataset average .", "entities": []}
{"text": "Similarly , their approach alleviates the issue to a certain extent , but does not scale to similar problems as their augmentation technique is too data - specific .", "entities": []}
{"text": "Lastly , there are several works ( Davidson et al , 2017 ; Zhang et al , 2018b ) offering methodologies or datasets to evaluate models for unintended bias , but they fail to offer a general framework .", "entities": []}
{"text": "One of the main reasons our approach improves the model in the original task is that the model is now more robust thanks to the reinforcement provided to the model builder through attributions .", "entities": []}
{"text": "From a fairness angle , our technique shares similarities with adversarial training ( Zhang et al , 2018a ; Madras et al , 2018 ) in asking the model to optimize for an additional objective that transitively unbiases the classifier .", "entities": []}
{"text": "However , those approaches work to remove protected attributes from the representation layer , which is unstable .", "entities": []}
{"text": "Our approach , on the other hand , works with basic human - interpretable units of information - tokens .", "entities": []}
{"text": "Also , those approaches propose to sacrifice main task performance for fairness as well .", "entities": []}
{"text": "While our method enables model builders to inject priors to aid a model , it has several limitations .", "entities": []}
{"text": "In solving the fairness problem in question , it causes the classifier to not focus on the identity terms even for the cases where an identity term itself is being used as an insult .", "entities": []}
{"text": "Moreover , our approach requires prior terms to be manually provided , which bears resemblance to blacklist approaches and suffers from the same drawbacks .", "entities": []}
{"text": "Lastly , the evaluation methodology that we and previous papers ( Dixon et al , 2018 ; Park et al , 2018 ) rely on are based on a syntheticallygenerated dataset , which may contain biases of the individuals creating it .", "entities": []}
{"text": "In this paper , we proposed actionability on model explanations that enable ML practitioners to enforce priors on their model .", "entities": []}
{"text": "Our method incorporates Path Integrated Gradients attributions into the objective function with the aim of stopping the classifier from carrying along false positive bias from the data by punishing it when it focuses on identity words .", "entities": []}
{"text": "Applying model attribution as a fine - tuning step on a trained classifier makes it converge to a more debiased classifier in just a few epochs .", "entities": []}
{"text": "Additionally , we show that model can be also forced to focus on pre - determined tokens .", "entities": []}
{"text": "There are several avenues we can explore as future research .", "entities": []}
{"text": "Our technique can be applied to implement a more robust model by penalizing the attributions falling outside of tokens annotated to be relevant to the predicted class .", "entities": []}
{"text": "Another avenue is to incorporate different model attribution strategies such as DeepLRP ( Bach et al , 2015 ) into the objective function .", "entities": []}
{"text": "Finally , it would be worthwhile to invest in a technique to extract problematic terms from the model automatically rather than providing prescribed identity or toxic terms .", "entities": []}
{"text": "We thank Salem Haykal , Ankur Taly , Diego Garcia - Olano , Raz Mathias , and Mukund Sundararajan for their valuable feedback and insightful discussions .", "entities": []}
