{"text": "The 1 : BLEU scores of all systems .", "entities": [[3, 4, "MetricName", "BLEU"]]}
{"text": "Table 1 shows BLEU scores of all systems .", "entities": [[3, 4, "MetricName", "BLEU"]]}
{"text": "In addition , the system TBMT does not show consistent improvements over PBMT while both GBMT and GBMT ctx achieve better BLEU scores than TBMT on both ZH - EN ( +1.8 BLEU , in terms of The number of rules in GBMT ctx according to their type GBMT ctx ) and DE - EN ( +0.6 BLEU , in terms of GBMT ctx ) .", "entities": [[21, 22, "MetricName", "BLEU"], [32, 33, "MetricName", "BLEU"], [57, 58, "MetricName", "BLEU"]]}
{"text": "We also found that GBMT ctx is significantly better than GBMT on both ZH - EN ( +1.0 BLEU ) and , which indicates that explicitly modeling a segmentation using context is helpful .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "Table 3 shows BLEU scores of GBMT ctx when different types of rules are used .", "entities": [[3, 4, "MetricName", "BLEU"]]}
{"text": "The results show that : ASR improves the best current model for AS2 , i.e. , TANDA by \u223c3 % , corresponding to an error reduction of 10 % in Accuracy , on both Wik - iQA and TREC - QA .", "entities": [[30, 31, "MetricName", "Accuracy"], [38, 39, "DatasetName", "TREC"]]}
{"text": "We can train this model with log cross - entropy loss : L = \u2212 l { 0 , 1 } y l \u00d7 log ( \u0177 l ) on pairs of texts , where y l is the correct and incorrect answer label , \u0177 1 = p ( q , c ) , and\u0177 0", "entities": [[10, 11, "MetricName", "loss"], [17, 18, "DatasetName", "0"], [56, 57, "DatasetName", "0"]]}
{"text": "We use the final hidden vector E corresponding to the first input token [ CLS ] generated by the Transformer , and a classification layer with weights W R ( k+1 ) \u00d7 | E | , and train the model using a standard cross - entropy classification loss : y \u00d7 log ( sof tmax ( EW T ) ) , where y is a one - hot vector representing labels for the k + 1 candidates , i.e. , | y", "entities": [[19, 20, "MethodName", "Transformer"], [48, 49, "MetricName", "loss"], [54, 55, "DatasetName", "sof"]]}
{"text": "We use this matrix and a classification layer weights W R 2d , and compute a standard multi - class classification loss : L M ASR = y * log ( sof tmax ( V ( q , k+1 ) W T ) , ( 2 ) where y is a one - hot - vector , and", "entities": [[17, 21, "TaskName", "multi - class classification"], [21, 22, "MetricName", "loss"], [31, 32, "DatasetName", "sof"]]}
{"text": "This is also referred to Precision - at - 1 ( P@1 ) in the context of reranking , while standard Precision and Recall are not essential in our case as we assume the system does not abstain from providing answers .", "entities": [[5, 6, "MetricName", "Precision"], [11, 12, "MetricName", "P@1"], [21, 22, "MetricName", "Precision"], [23, 24, "MetricName", "Recall"]]}
{"text": "We also use Mean Average Precision ( MAP ) and Mean Reciprocal Recall ( MRR ) evaluated on the test set , using the entire set of candidates for each Table 4 : Results on WikiQA , TREC - QA and WQA , using RoBERTa base Transformer .", "entities": [[4, 6, "MetricName", "Average Precision"], [7, 8, "DatasetName", "MAP"], [12, 13, "MetricName", "Recall"], [14, 15, "MetricName", "MRR"], [35, 36, "DatasetName", "WikiQA"], [37, 38, "DatasetName", "TREC"], [44, 45, "MethodName", "RoBERTa"], [46, 47, "MethodName", "Transformer"]]}
{"text": "\u2020 is used to indicate that the difference in P@1 between ASR and the other marked systems is statistically significant at 95 % .", "entities": [[9, 10, "MetricName", "P@1"]]}
{"text": "Reranker training We adopt Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 2e - 5 for the transfer step on the ASNQ dataset ( Garg et al , 2020 ) , and a learning rate of 1e - 6 for the adapt step on the target dataset .", "entities": [[4, 5, "MethodName", "Adam"], [5, 6, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"], [27, 28, "DatasetName", "ASNQ"], [39, 41, "HyperparameterName", "learning rate"]]}
{"text": "We set the max number of epochs equal to 3 and 9 for the adapt and transfer steps , respectively .", "entities": [[4, 7, "HyperparameterName", "number of epochs"]]}
{"text": "KGAT and ASR training Again , we use the Adam optimizer with a learning rate of 2e - 6 for training the ASR model on the target dataset .", "entities": [[9, 10, "MethodName", "Adam"], [10, 11, "HyperparameterName", "optimizer"], [13, 15, "HyperparameterName", "learning rate"]]}
{"text": "We utilize 1 Tesla V100 GPU with 32 GB memory and a train batch size of eight .", "entities": [[13, 15, "HyperparameterName", "batch size"]]}
{"text": "We set the maximum sequence length for RoBERTa to 128 tokens and the number of epochs to 20 .", "entities": [[7, 8, "MethodName", "RoBERTa"], [13, 16, "HyperparameterName", "number of epochs"]]}
{"text": "Table 4 reports the P@1 , MAP and MRR of the rerankers , and different answer supporting models on WikiQA , TREC - QA and WQA datasets .", "entities": [[4, 5, "MetricName", "P@1"], [6, 7, "DatasetName", "MAP"], [8, 9, "MetricName", "MRR"], [19, 20, "DatasetName", "WikiQA"], [21, 22, "DatasetName", "TREC"]]}
{"text": "The table shows that : PR replicates the MAP and MRR of the stateof - the - art reranker by Garg et al ( 2020 ) on WikiQA .", "entities": [[8, 9, "DatasetName", "MAP"], [10, 11, "MetricName", "MRR"], [27, 28, "DatasetName", "WikiQA"]]}
{"text": "For example , it outperforms PR by almost 3 absolute percent points in P@1 on WikiQA , and by almost 6 points on TREC from 91.18 % to 97.06 % , which corresponds to an error reduction of 60 % .", "entities": [[13, 14, "MetricName", "P@1"], [15, 16, "DatasetName", "WikiQA"], [23, 24, "DatasetName", "TREC"]]}
{"text": "The P@1 also significantly improves by 2 % , i.e. , achieving 89.71 , which is impressively high .", "entities": [[1, 2, "MetricName", "P@1"]]}
{"text": "ACC is the overall accuracy while F1 refers to the category 0 .", "entities": [[0, 1, "MetricName", "ACC"], [3, 5, "MetricName", "overall accuracy"], [6, 7, "MetricName", "F1"], [11, 12, "DatasetName", "0"]]}
{"text": "7 shows that , given q and k = 3 candidates , PR chooses c 1 , a suitable but wrong answer .", "entities": [[7, 9, "HyperparameterName", "k ="]]}
{"text": "Our evaluation against a Japanese open - domain why - QA dataset , which was created using general web texts as a source of answer passages , revealed that the generator network significantly improved the accuracy of the top - ranked answer passages and that the combination significantly outperformed several strong baselines , including a combination of a generator network and a BERT model ( Devlin et al , 2019 ) .", "entities": [[35, 36, "MetricName", "accuracy"], [62, 63, "MethodName", "BERT"]]}
{"text": "Figure 2 illustrates the architecture shared by our fake - representation generator F and real - representation generator R , namely , Encoder ( t ; \u03b8 , q ) , where \u03b8 is a set of parameters , q is a why - question , and t is either an answer passage or a manually created compact - answer .", "entities": [[26, 27, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b8"]]}
{"text": "The pre - trained word embeddings used in Encoder ( t ; \u03b8 , q ) were obtained by concatenating two types of d - dimensional word embeddings ( d = 300 in this work ) : general word embeddings and causal word embeddings .", "entities": [[4, 6, "TaskName", "word embeddings"], [12, 13, "HyperparameterName", "\u03b8"], [26, 28, "TaskName", "word embeddings"], [38, 40, "TaskName", "word embeddings"], [42, 44, "TaskName", "word embeddings"]]}
{"text": "The learning rate was set to 0.001 , and the batch size for each iteration was set to 20 .", "entities": [[1, 3, "HyperparameterName", "learning rate"], [10, 12, "HyperparameterName", "batch size"]]}
{"text": "For the pre - training parameters , we followed the settings of BERT BASE in Devlin et al ( 2019 ) 3 except for the batch size of 50 .", "entities": [[12, 13, "MethodName", "BERT"], [13, 14, "MethodName", "BASE"], [25, 27, "HyperparameterName", "batch size"]]}
{"text": "We ran 3 epochs with the learning rate of 1e - 5 for finetuning the BERT - based models 4 .", "entities": [[6, 8, "HyperparameterName", "learning rate"], [15, 16, "MethodName", "BERT"]]}
{"text": "The transformer encoder processed the input representation to gen - 3 12 - layers , 768 hidden states , 12 heads and training for 1 - million steps with the warmup rate of 1 % using Adam optimizer with the learning rate of 1e - 4 . 4 We tested all the combinations of epochs { 1 , 2 , 3 , 4 , 5 } and learning rates of { 1e - 5 , 2e - 5 , 3e - 5 } and chose the one that maximized the performance on the development data in W hySet .", "entities": [[36, 37, "MethodName", "Adam"], [37, 38, "HyperparameterName", "optimizer"], [40, 42, "HyperparameterName", "learning rate"]]}
{"text": "5 We also evaluated a BERT - based model that did not use the attention feature embeddings , but its P@1 ( 41.4 ) was much lower than that of BERT ( 51.2 ) . P@1", "entities": [[5, 6, "MethodName", "BERT"], [20, 21, "MetricName", "P@1"], [30, 31, "MethodName", "BERT"], [35, 36, "MetricName", "P@1"]]}
{"text": "Table 3 shows the performances of all the methods in the Precision of the top answer ( P@1 ) and the Mean Average Precision ( MAP )", "entities": [[11, 12, "MetricName", "Precision"], [17, 18, "MetricName", "P@1"], [22, 24, "MetricName", "Average Precision"], [25, 26, "DatasetName", "MAP"]]}
{"text": "Compared with BASE and BASE+AddTr , neither of which used compactanswer representations or fake - representation generator F , Ours ( OP ) gave 3.4 % and 2.8 % improvement in P@1 , respectively .", "entities": [[2, 3, "MethodName", "BASE"], [31, 32, "MetricName", "P@1"]]}
{"text": "Although we sampled the random vectors from different distribution types with various ranges , we obtained at best similar performance to that of BASE : 51.6 in P@1 .", "entities": [[23, 24, "MethodName", "BASE"], [27, 28, "MetricName", "P@1"]]}
{"text": "All the methods were evaluated with EM and F1 scores , following Lin et al ( 2018 ) .", "entities": [[6, 7, "MetricName", "EM"], [8, 9, "MetricName", "F1"]]}
{"text": "Note that both Ours ( OP ) and Ours ( RV ) outperformed both previous methods , R 3 and OpenQA , except for the F1 score for the TriviaQA dataset .", "entities": [[25, 27, "MetricName", "F1 score"], [29, 30, "DatasetName", "TriviaQA"]]}
{"text": "The whole loss for the step - one generation is then L f irst = L avg + L disc ( 4 ) which is optimized by a minimax game with adversarial training ( Goodfellow et al , 2014 ) .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "For the other , we adopt the WAE model ( Gu et al , 2018 ) in which the prior p \u03d5 ( z | x , c ) is modeled by a mixture of Gaussian distributions GMM ( \u03c0 k , \u00b5 k , \u03c3 k 2 I ) K k=1 , where K is the number of Gaussian distributions and \u03c0 k is the mixture coefficient of the k - th component of the GMM module as computed : \u03c0 k = exp ( e k )", "entities": [[82, 84, "HyperparameterName", "k ="]]}
{"text": "( 10 ) where the probability is computed by a feedforward network f as the vanilla bag - of - word loss does ; { \u0233 } is the complementary response bag of y and its probability is computed as the average probability of responses in the bag ; and \u03bb is a scaling factor accounting for the difference in magnitude .", "entities": [[10, 12, "MethodName", "feedforward network"], [21, 22, "MetricName", "loss"]]}
{"text": "Totally , the whole loss for the step - two generation is then : L second = L cvae + L mbow ( 12 ) which can be optimized in an end - to - end way .", "entities": [[4, 5, "MetricName", "loss"], [18, 19, "MethodName", "cvae"]]}
{"text": "Method Multi - BLEU EMBEDDING Intra - Dist Inter - Dist BLEU - 1 BLEU - 2 G A E Dist - 1 Dist - 2 S2S+DB : the vanilla sequence - to - sequence model with the modified diversity - promoting beam search method ( Li et al , 2016b ) where a fixed diversity rate 0.5 is used . MMS : the modified multiple responding mechanisms enhanced dialogue model proposed by Zhou et al ( 2018a ) which introduces responding mechanism embeddings for diverse response generation .", "entities": [[3, 4, "MetricName", "BLEU"], [11, 12, "MetricName", "BLEU"], [14, 15, "MetricName", "BLEU"], [86, 88, "TaskName", "response generation"]]}
{"text": "BLEU : In dialogue generation , BLEU is widely used in previous studies ( Yao et al , 2017 ; Shang et al , 2018 ) .", "entities": [[0, 1, "MetricName", "BLEU"], [3, 5, "TaskName", "dialogue generation"], [6, 7, "MetricName", "BLEU"]]}
{"text": "All models are trained with the following hyperparameters : both encoder and decoder are set to one layer with GRU cells , where the hidden state size of GRU is 256 ; the utterance length is limited to 50 ; the vocabulary size is 50 , 000 and the word embedding dimension is 256 ; the word embeddings are shared by the encoder and decoder ; all trainable parameters are initialized from a uniform distribution [ - 0.08 , 0.08 ] ; we employ the Adam ( Kingma and Ba , 2014 ) for optimization with a mini - batch size 128 and initialized learning rate 0.001 ; the gradient clipping strategy is utilized to avoid gradient explosion , where the gradient clipping value is set to be 5 .", "entities": [[19, 20, "MethodName", "GRU"], [28, 29, "MethodName", "GRU"], [49, 52, "HyperparameterName", "word embedding dimension"], [56, 58, "TaskName", "word embeddings"], [85, 86, "MethodName", "Adam"], [97, 101, "HyperparameterName", "mini - batch size"], [104, 106, "HyperparameterName", "learning rate"], [109, 111, "MethodName", "gradient clipping"], [121, 123, "MethodName", "gradient clipping"]]}
{"text": "As to the discriminator , we set the initialized learning rate as 0.0002 and use 128 different kernels for each kernel size in { 2 , 3 , 4 } .", "entities": [[9, 11, "HyperparameterName", "learning rate"], [20, 22, "HyperparameterName", "kernel size"]]}
{"text": "The Seq2seq based models ( S2S , S2S - DB and MMS ) tend to generate fluent utterances and can share some overlapped words with the references , as the high BLEU - 2 scores show .", "entities": [[1, 2, "MethodName", "Seq2seq"], [31, 32, "MetricName", "BLEU"]]}
{"text": "Compared against these previous methods , our model can achieve the best or second best performances on different automatic evaluation metrics where the improvements are most consistent on BLEU - 1 and embedding - based metrics , which demonstrates the overall effectiveness of our proposed architecture .", "entities": [[28, 29, "MetricName", "BLEU"]]}
{"text": "We report a moderate gain of 0.5 BLEU over a single system on the Japanese - English ASPEC test set ( Nakazawa et al , 2016 ) by combining three BPE - based NMT models from using the sepbeam decoder .", "entities": [[7, 8, "MetricName", "BLEU"], [17, 18, "DatasetName", "ASPEC"], [30, 31, "MethodName", "BPE"]]}
{"text": "lmin max { L ( \u03ba ) , L ( 0 ) \u2212 \u03b4 } ; ( i ) denotes the ith element .", "entities": [[10, 11, "DatasetName", "0"], [13, 14, "HyperparameterName", "\u03b4"]]}
{"text": "We empirically set \u03ba 20 and \u03b4 3 .", "entities": [[6, 7, "HyperparameterName", "\u03b4"]]}
{"text": "Let x 0 be the sentence from the test set with label y 0 , then the smallest perturbation \u03b4 * under 0 norm distance is : 2 \u03b4 * : = argmin \u03b4", "entities": [[2, 3, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [19, 20, "HyperparameterName", "\u03b4"], [22, 23, "DatasetName", "0"], [28, 29, "HyperparameterName", "\u03b4"], [33, 34, "HyperparameterName", "\u03b4"]]}
{"text": "\u03b4 0 s.t . f", "entities": [[0, 1, "HyperparameterName", "\u03b4"], [1, 2, "DatasetName", "0"]]}
{"text": "( x 0 \u03b4 )", "entities": [[2, 3, "DatasetName", "0"], [3, 4, "HyperparameterName", "\u03b4"]]}
{"text": "In contrast , our second - order attacks fix \u03b4 = p and search for the vulnerable x 0 .", "entities": [[9, 10, "HyperparameterName", "\u03b4"], [18, 19, "DatasetName", "0"]]}
{"text": "( 3 ) by minimizing the crossentropy loss with regard to x", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "We set \u03b2 20 . 10 return None ;", "entities": [[2, 3, "HyperparameterName", "\u03b2"]]}
{"text": "Similar to the baseline performance reported in GLUE , our trained models have an evaluation accuracy of 81.4 % , 82.5 % , and 81.7 % , respectively .", "entities": [[7, 8, "DatasetName", "GLUE"], [15, 16, "MetricName", "accuracy"]]}
{"text": "To evaluate the impact of neighborhood size , we experiment with two configurations : ( 1 ) For the small neighborhood ( k = 2 ) , we use SO - Enum that finds the most similar vulnerable example .", "entities": [[22, 24, "HyperparameterName", "k ="]]}
{"text": "( 2 ) For the large neighborhood ( k = 6 ) , SO - Enum is not applicable and we use SO - Beam to find vulnerable examples .", "entities": [[8, 10, "HyperparameterName", "k ="]]}
{"text": "( k = 2 ) , and 1.9 for SO - Beam ( k = 6 ) .", "entities": [[1, 3, "HyperparameterName", "k ="], [13, 15, "HyperparameterName", "k ="]]}
{"text": "( 6 ( k = 3 ) in X filter .", "entities": [[3, 5, "HyperparameterName", "k ="]]}
{"text": "The model has 82.9 % accuracy similar to the baseline performance reported in GLUE .", "entities": [[5, 6, "MetricName", "accuracy"], [13, 14, "DatasetName", "GLUE"]]}
{"text": "We use the same setup as the base LSTM and attain 82.45 % accuracy .", "entities": [[8, 9, "MethodName", "LSTM"], [13, 14, "MetricName", "accuracy"]]}
{"text": "Here \" original \" is equivalent to k = 0 , \" perturbed \" is equivalent to k = 3 , p is in the form of ( male , female ) .", "entities": [[7, 9, "HyperparameterName", "k ="], [9, 10, "DatasetName", "0"], [17, 19, "HyperparameterName", "k ="]]}
{"text": "We evaluate model bias through the proposed B p , k for k = 0 , . . .", "entities": [[12, 14, "HyperparameterName", "k ="], [14, 15, "DatasetName", "0"]]}
{"text": "Here the bias for k = 0 is effectively measured on the original test set , and the bias for k \u2265 1 is measured on our constructed neighborhood .", "entities": [[4, 6, "HyperparameterName", "k ="], [6, 7, "DatasetName", "0"]]}
{"text": "We randomly sample a subset of constructed examples when k = 3 due to the exponential complexity .", "entities": [[9, 11, "HyperparameterName", "k ="]]}
{"text": "In Fig . 5 , the naive approach ( k = 0 ) observes very small biases on most tokens ( as constructed ) .", "entities": [[9, 11, "HyperparameterName", "k ="], [11, 12, "DatasetName", "0"]]}
{"text": "In contrast , when evaluated by our double perturbation framework ( k = 3 ) , we are able to observe noticeable bias , where most p has a positive bias on the base model .", "entities": [[11, 13, "HyperparameterName", "k ="]]}
{"text": "The plot for the average F soft ( x ; p ) ( i.e. , counterfactual token bias ) vs. neighborhood distance k. Results show that the counterfactual bias on p can be revealed when increasing k. negative when k = 0 , but becomes positive when k = 3 .", "entities": [[39, 41, "HyperparameterName", "k ="], [41, 42, "DatasetName", "0"], [47, 49, "HyperparameterName", "k ="]]}
{"text": "In contrast , our method is able to construct 141 , 780 similar natural sentences when k = 3 and shifts the distribution to the right ( positive ) .", "entities": [[16, 18, "HyperparameterName", "k ="]]}
{"text": "As shown in the right - most panel , the bias is small when k = 1 , and becomes more significant as k increases ( larger neighborhood ) .", "entities": [[14, 16, "HyperparameterName", "k ="]]}
{"text": "In Fig . 8 , we measure the bias on X test and observe positive bias on most tokens for both k = 0 and k = 3 , which indicates that the model \" tends \" to make more positive predictions for examples containing certain female pronouns than male pro - nouns .", "entities": [[21, 23, "HyperparameterName", "k ="], [23, 24, "DatasetName", "0"], [25, 27, "HyperparameterName", "k ="]]}
{"text": "Distance k = 1 97 % Negative ( 97 % Negative )", "entities": [[1, 3, "HyperparameterName", "k ="]]}
{"text": "Distance k = 2 88 % Negative ( 87 % Negative )", "entities": [[1, 3, "HyperparameterName", "k ="]]}
{"text": "Distance k = 3 52 % Positive ( 64 % Positive )", "entities": [[1, 3, "HyperparameterName", "k ="]]}
{"text": "c j + t j=0 ( m j t k = j+1 f k ) s j ( 10 ) =", "entities": [[9, 11, "HyperparameterName", "k ="]]}
{"text": "We adopt stochastic gradient descent ( SGD ) to optimize our model with batch size 100 , L2 regularization 10 \u22128 , initial learning rate lr 0.2 and the learning rate is decayed 4 with respect to the number of epoch .", "entities": [[2, 5, "MethodName", "stochastic gradient descent"], [6, 7, "MethodName", "SGD"], [13, 15, "HyperparameterName", "batch size"], [17, 19, "HyperparameterName", "L2 regularization"], [23, 25, "HyperparameterName", "learning rate"], [29, 31, "HyperparameterName", "learning rate"]]}
{"text": "For models with dependency trees , we take the models BiLSTM - GCN - CRF and dependency - 4 We set the decay as 0.1 and the learning rate for each epoch equals to lr/ ( 1 + decay *", "entities": [[10, 11, "MethodName", "BiLSTM"], [12, 13, "MethodName", "GCN"], [14, 15, "MethodName", "CRF"], [27, 29, "HyperparameterName", "learning rate"]]}
{"text": "We adopt stochastic gradient descent ( SGD ) to optimize our model with batch size 100 , L2 regularization 10 \u22128 , learning rate 0.2 and the learning rate is decayed with respect to the number of epoch 9 .", "entities": [[2, 5, "MethodName", "stochastic gradient descent"], [6, 7, "MethodName", "SGD"], [13, 15, "HyperparameterName", "batch size"], [17, 19, "HyperparameterName", "L2 regularization"], [22, 24, "HyperparameterName", "learning rate"], [27, 29, "HyperparameterName", "learning rate"]]}
{"text": "9 We set the decay as 0.1 and the learning rate for each epoch equals to learning_rate/ ( 1 + decay *", "entities": [[9, 11, "HyperparameterName", "learning rate"]]}
{"text": "The total number of parameters is 11M. Table 10 shows the performance of our model on the dev sets of OntoNotes 5.0 English and Chinese , SemEval", "entities": [[2, 5, "HyperparameterName", "number of parameters"], [20, 22, "DatasetName", "OntoNotes 5.0"]]}
{"text": "For the GRU model , the hidden size is set at 150 , so that both models have comparable number of parameters .", "entities": [[2, 3, "MethodName", "GRU"], [19, 22, "HyperparameterName", "number of parameters"]]}
{"text": "We initialise the learning rate as 0.001 with a decay rate of 0.986 every 10 steps .", "entities": [[3, 5, "HyperparameterName", "learning rate"], [9, 11, "HyperparameterName", "decay rate"]]}
{"text": "Table 5 gives the average F1 scores and the average F1 scores weighted with the frequency of CBT labels for all models under the oversampling ratio 1:1 .", "entities": [[4, 6, "MetricName", "average F1"], [9, 11, "MetricName", "average F1"], [17, 18, "DatasetName", "CBT"]]}
{"text": "Table 6 shows the F1 - measure of the compared models that detect thinking errors , emotions and situations under the 1 : 1 oversampling ratio .", "entities": [[4, 5, "MetricName", "F1"]]}
{"text": "Although SVM - BOW is comparable to 100 dimensional GRU - Skip - thought in terms on average F1 , in all other cases CNN - GloVe and GRU - Skipthought overshadow SVM - BOW .", "entities": [[1, 2, "MethodName", "SVM"], [9, 10, "MethodName", "GRU"], [17, 19, "MetricName", "average F1"], [26, 27, "MethodName", "GloVe"], [28, 29, "MethodName", "GRU"], [32, 33, "MethodName", "SVM"]]}
{"text": ", K = { k 1 , ... , k t } , of t demographic identity word vectors from a particular protected group ( i.e. national origin , religion , etc . ) , we define a set , P , containing the predicted negative sentiment probability via minimizer , f * , normalized to be one probability mass .", "entities": [[1, 3, "HyperparameterName", "K ="]]}
{"text": "We report four ranking metrics following Welleck et al ( 2019 ) : Hits@1 , Entail@1 , Neutral@1 and Contradict@1 .", "entities": [[13, 14, "MetricName", "Hits@1"]]}
{"text": "The perplexity scores of contradictory words ( 106.7 ) were considerably lower than those of the counterparts in GT utterances ( 280.1 ) .", "entities": [[1, 2, "MetricName", "perplexity"]]}
{"text": "Table 2 shows an example of such dialogue instance with perplexity per word .", "entities": [[10, 11, "MetricName", "perplexity"]]}
{"text": "( 1 ) where \u03b2 on S t 0 is the listener rationality coefficient that controls the amount of information from the current timestep compared to the cumulative prior p t ( i ) .", "entities": [[4, 5, "HyperparameterName", "\u03b2"], [8, 9, "DatasetName", "0"]]}
{"text": "\u03b1 \u00d7 S t 0", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [4, 5, "DatasetName", "0"]]}
{"text": "( u t | i , h , u < t ) , ( 2 ) where \u03b1 is the speaker rationality coefficient that determines how much the likelihood is considered .", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "We set k = 2048 .", "entities": [[2, 4, "HyperparameterName", "k ="], [4, 5, "DatasetName", "2048"]]}
{"text": "+ \u03b1 , 0 ) , ( 5 ) where \u03b1 is a positive margin , which we set as 0.2 .", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [3, 4, "DatasetName", "0"], [10, 11, "HyperparameterName", "\u03b1"]]}
{"text": "Hits@1 is the accuracy of choosing the ground - truth next - utterance among 20 candidates as the models rank the candidates by perplexity .", "entities": [[0, 1, "MetricName", "Hits@1"], [3, 4, "MetricName", "accuracy"], [23, 24, "MetricName", "perplexity"]]}
{"text": "Our self - conscious agent S 1 significantly reduces Contradict@1 scores and increases the Entail@1 along with the Hits@1 accuracy of the literal agents S 0 .", "entities": [[4, 5, "DatasetName", "agent"], [18, 19, "MetricName", "Hits@1"], [19, 20, "MetricName", "accuracy"], [25, 26, "DatasetName", "0"]]}
{"text": "Thus , Entail@1 is a lenient version of Hits@1 ( Welleck et al , 2019 ) .", "entities": [[8, 9, "MetricName", "Hits@1"]]}
{"text": "Our model S 1 outperforms all other generative dialogue agents in terms of consistency related metrics , i.e. Hits@1 and C score .", "entities": [[18, 19, "MetricName", "Hits@1"]]}
{"text": "We add \u03b2 in L t 0 to control the amount of information incorporated to the world prior p t ( i ) .", "entities": [[2, 3, "HyperparameterName", "\u03b2"], [6, 7, "DatasetName", "0"]]}
{"text": "Figure 5 depicts that when \u03b2 is large , the Hits@1 scores ( i.e. the GT accuracy ) drop .", "entities": [[5, 6, "HyperparameterName", "\u03b2"], [10, 11, "MetricName", "Hits@1"], [16, 17, "MetricName", "accuracy"]]}
{"text": "With a big \u03b2 , the information S t 0", "entities": [[3, 4, "HyperparameterName", "\u03b2"], [9, 10, "DatasetName", "0"]]}
{"text": "Therefore , setting of \u03b2 \u2264 1 is advantageous for the self - conscious agent to incrementally decode .", "entities": [[4, 5, "HyperparameterName", "\u03b2"], [14, 15, "DatasetName", "agent"]]}
{"text": "Figure 6 shows an example of how generated responses vary according to the intensity of speaker rationality \u03b1 .", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "For both methods , we use top - k ranked personas as distractors and set k = 4 for all the methods .", "entities": [[15, 17, "HyperparameterName", "k ="]]}
{"text": "We use Adam optimizer ( Kingma and Ba , 2015 ) with learning rate 2e - 5 and finetune BERT - Uncased - Base up to 3 epochs .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [12, 14, "HyperparameterName", "learning rate"], [19, 20, "MethodName", "BERT"]]}
{"text": "We set \u03b1 = 8 , \u03b2 = 0.5 , and | I | = 5 .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [6, 7, "HyperparameterName", "\u03b2"]]}
{"text": "In the case of using the Distractor Memory ( DM ) , first we initialize BERT - Uncased - Base with pretrained weights and finetune it up to 3 epochs with Adam optimizer with learning rate 2e - 5 .", "entities": [[15, 16, "MethodName", "BERT"], [31, 32, "MethodName", "Adam"], [32, 33, "HyperparameterName", "optimizer"], [34, 36, "HyperparameterName", "learning rate"]]}
{"text": "For Dialogue NLI evaluation , we set the speaker rationality \u03b1 = 8.0 , the listener rationality \u03b2", "entities": [[10, 11, "HyperparameterName", "\u03b1"], [17, 18, "HyperparameterName", "\u03b2"]]}
{"text": "In PersonaChat evaluation , we set \u03b1 = 2.0 , \u03b2 = 0.3 for ControlSeq2Seq", "entities": [[6, 7, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b2"]]}
{"text": "( See et al , 2019 ) , \u03b1 = 2 , \u03b2 = 0.9 for TransferTransfo ( Wolf et al , 2019b ) , and \u03b1 = 2.0 , \u03b2 = 0.5 for Blender 90 M .", "entities": [[8, 9, "HyperparameterName", "\u03b1"], [12, 13, "HyperparameterName", "\u03b2"], [26, 27, "HyperparameterName", "\u03b1"], [30, 31, "HyperparameterName", "\u03b2"], [34, 35, "MethodName", "Blender"]]}
{"text": "We experiment \u03b1 = { 1.0 , 2.0 , 4.0 , 8.0 , 16.0 } , \u03b2 = { 0.3 , 0.5 , 0.9 , 1.0 , 2.0 , 4.0 } , and | I | = { 2 , 3 , 5 } .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [16, 17, "HyperparameterName", "\u03b2"]]}
{"text": "We set the number of nearest neighbor k = 2048 . Inference .", "entities": [[7, 9, "HyperparameterName", "k ="], [9, 10, "DatasetName", "2048"]]}
{"text": "Since the development and evaluation data splits are balanced , random baseline label accuracy ignoring the requirement for evidence is 33.33 % .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "2 Beyond the FEVER score , it computes precision , recall , F 1 , and label accuracy to provide diagnostic information .", "entities": [[3, 4, "DatasetName", "FEVER"], [17, 18, "MetricName", "accuracy"]]}
{"text": "The team which scored highest on evidence precision and evidence F1 was Papelo ( precision = 92.18 % and F 1 = 64.85 % ) who report using a combination of TF - IDF for document retrieval and string matching using named entities and capitalized expressions .", "entities": [[10, 11, "MetricName", "F1"]]}
{"text": "In preliminary evaluation , the system achieved 57.36 % FEVER score , 61.08 % label accuracy , and 64.85 % evidence F1 on the FEVER shared task test set .", "entities": [[9, 10, "DatasetName", "FEVER"], [15, 16, "MetricName", "accuracy"], [21, 22, "MetricName", "F1"], [24, 25, "DatasetName", "FEVER"]]}
{"text": "Our approach achieved a 42.77 % evidence F1 - score , a 51.36 % label accuracy and a 38.33 % FEVER score .", "entities": [[7, 10, "MetricName", "F1 - score"], [15, 16, "MetricName", "accuracy"], [20, 21, "DatasetName", "FEVER"]]}
{"text": "Our submission achieved a score of 0.3695 on the Evidence F1 metric for retrieving relevant evidential sentences ( 10 th out of 24 ) and a score of 0.2376 on the FEVER metric ( just below the baseline system ) .", "entities": [[10, 11, "MetricName", "F1"], [31, 32, "DatasetName", "FEVER"]]}
{"text": "We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6 % to 0.5 % .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "With no access to parameters of the chatbot , the attacker model can infer speakers ' personas with 37.59 % accuracy over 4 , 332 personas .", "entities": [[7, 8, "TaskName", "chatbot"], [20, 21, "MetricName", "accuracy"]]}
{"text": "We combine proposed KL divergence loss ( KL loss ) with mutual information loss ( MI loss ) ( Song et al , 2019 ) as additional defense objectives to train the GPT - 2 and decrease the attacker 's persona inference accuracy to 0.53 % .", "entities": [[5, 6, "MetricName", "loss"], [8, 9, "MetricName", "loss"], [13, 14, "MetricName", "loss"], [16, 17, "MetricName", "loss"], [32, 33, "MethodName", "GPT"], [42, 43, "MetricName", "accuracy"]]}
{"text": "= CE ( A ( f ( u kj ) ) , s kj ) , ( 2 ) where CE refers to cross - entropy loss between persona label s kj and A ( f ( u kj ) ) .", "entities": [[26, 27, "MetricName", "loss"]]}
{"text": "Following the intuition that the adversary can not obtain better results than a random guess , in Section 4.1 , we propose KL loss that aims to flatten the persona predictor 's estimated distribution .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "Based on minimizing the mutual information between hidden states f ( u ) of chatbots and private persona attributes s , we propose MI loss in Section 4.2 .", "entities": [[24, 25, "MetricName", "loss"]]}
{"text": "| p ( s ) ) , ( 7 ) where p ( s ) can be any distribution for s , q ( x ) refers to probability distribution of model f parameterized by \u03b8 f and f ( u ) is assumed to be sampled from the conditional distribution q ( f ( u )", "entities": [[35, 36, "HyperparameterName", "\u03b8"]]}
{"text": "( 8 ) Therefore , our objective in Equation 6 can be formulated as an adversarial training objective : min \u03b8 f max \u03a8 E q ( f ( u ) )", "entities": [[20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "( 9 ) log p ( s ) is independent of f ( u ) , and we may leave this term out in Equation 9 : min \u03b8 f max \u03a8 E q ( f ( u ) )", "entities": [[28, 29, "HyperparameterName", "\u03b8"]]}
{"text": "( 10 ) Then , Equation 10 illustrates an adversarial game between an adversary p \u03a8 who manages to infer s from f ( u ) and a defender who modifies \u03b8 f to protect s from persona inference attack .", "entities": [[31, 32, "HyperparameterName", "\u03b8"], [38, 40, "TaskName", "inference attack"]]}
{"text": "( 11 ) We can rewrite Equation 11 into two losses : L mi1 ( u kj , s kj ; \u03b8 Ap )", "entities": [[21, 22, "HyperparameterName", "\u03b8"]]}
{"text": "Then our MI loss can be formulated as : Lmi = \u03bb0Lmi1 + Lmi2 , ( 12 ) where \u03bb 0 controls the ratio between two the fake attacker", "entities": [[3, 4, "MetricName", "loss"], [20, 21, "DatasetName", "0"]]}
{"text": "In our experiment , we use a 2 - layer neural network with cross - entropy loss as the attacker model .", "entities": [[16, 17, "MetricName", "loss"]]}
{"text": "For utility , we apply BERTScore , Distinct ( Li et al , 2016 ) , BLEU", "entities": [[16, 17, "MetricName", "BLEU"]]}
{"text": "( Papineni et al , 2002 ) and perplexity ( PPL ) as evaluation metrics .", "entities": [[8, 9, "MetricName", "perplexity"]]}
{"text": "From the table , the test persona inference accuracy on the LM achieves 37.59 % while guessing on the label with most occurrences merely has 0.72 % accuracy .", "entities": [[8, 9, "MetricName", "accuracy"], [27, 28, "MetricName", "accuracy"]]}
{"text": "To avoid the persona overlearning issue , we use additional defense objectives illustrated in Section 4 . LM+KL+MI utilizes language modeling , KL loss and MI loss in Equation 13 to train the GPT - 2 .", "entities": [[23, 24, "MetricName", "loss"], [26, 27, "MetricName", "loss"], [33, 34, "MethodName", "GPT"]]}
{"text": "As demonstrated in Table 2 , the attacker performance on LM+KL+MI significantly reduces the attacking accuracy from 37.59 % to 0.53 % and F1 - score drops from 0.37 to nearly 0 .", "entities": [[15, 16, "MetricName", "accuracy"], [23, 26, "MetricName", "F1 - score"], [31, 32, "DatasetName", "0"]]}
{"text": "The accuracy of Best Guess reveals that the most frequent label in the test set has a ratio of 0.72 % .", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "The result is shown in Table 2 . LM+KL indicates the GPT - 2 is trained with language modeling and KL loss .", "entities": [[11, 12, "MethodName", "GPT"], [21, 22, "MetricName", "loss"]]}
{"text": "The LM+MI shares similar test accuracy and F1 - score with LM+KL+MI , but nearly all predictions are made on a single persona label with a ratio of 99.84 % .", "entities": [[5, 6, "MetricName", "accuracy"], [7, 10, "MetricName", "F1 - score"]]}
{"text": "After KL loss is applied on LM+KL+MI , the Max - Ratio drops to 81.87 % .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "These results verify that KL loss introduces flatter estimation and MI loss is more effective against persona overlearning , which conforms to our intuition of their objectives in Section 4 .", "entities": [[5, 6, "MetricName", "loss"], [11, 12, "MetricName", "loss"]]}
{"text": "BLEU - 1 , BLEU - 2 and BLEU - 4 are applied to evaluate generation similarity with ground truth .", "entities": [[0, 1, "MetricName", "BLEU"], [4, 5, "MetricName", "BLEU"], [8, 9, "MetricName", "BLEU"]]}
{"text": "The result indicates that adding KL loss will increase the perplexity greatly from 14.8 to 28.9 .", "entities": [[6, 7, "MetricName", "loss"], [10, 11, "MetricName", "perplexity"]]}
{"text": "After combining KL loss with MI loss , its perplexity decreases to 19.674 .", "entities": [[3, 4, "MetricName", "loss"], [6, 7, "MetricName", "loss"], [9, 10, "MetricName", "perplexity"]]}
{"text": "A plausible explanation is that KL loss confuses the persona predictor and indirectly increases the uncertainty of the GPT - 2 .", "entities": [[6, 7, "MetricName", "loss"], [18, 19, "MethodName", "GPT"]]}
{"text": "All GPT - 2 models have relatively low BLEU scores due to the one - to - many mapping between contexts and responses .", "entities": [[1, 2, "MethodName", "GPT"], [8, 9, "MetricName", "BLEU"]]}
{"text": "Under imbalanced data distribution , the attack on the defensed LM has Acc 0.47 % , F1 1.90e - 3 and Max - Ratio 94.06 % .", "entities": [[12, 13, "MetricName", "Acc"], [16, 17, "MetricName", "F1"]]}
{"text": "The learning rate is 3e - 5 with linear warm - up and decay .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "The code of PAM has been modified to integrate Wscore and Uscore calculation , though the actual alignment ( Schmid , 1994 ) is used , with a reported tagging accuracy exceeding 96 % .", "entities": [[30, 31, "MetricName", "accuracy"]]}
{"text": "In Tables 3 and 4 , the first 3 rows correspond to single NMT models generated when Marian - NMT is trained to optimise ( i ) BLEU , ( ii ) entropy and ( iii ) the word - wise normalised cross - entropy ( this is denoted as \" ce - mean \" ) .", "entities": [[27, 28, "MetricName", "BLEU"]]}
{"text": "The final three rows of Tables 3 and 4 report the accuracy of translations obtained by NMT ensembles .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "Based on Table 3 , for testset1 the best BLEU scores are achieved by the Marian - NMT ensemble in comparison to single - NMT models .", "entities": [[9, 10, "MetricName", "BLEU"]]}
{"text": "For instance , for BLEU , the score is only 19.0 to 20.0 for single NMT models , but rises to more than 28.0 for the ensembles , which equates to more than eight BLEU percentage points of improvement .", "entities": [[4, 5, "MetricName", "BLEU"], [34, 35, "MetricName", "BLEU"]]}
{"text": "For testset1 , the scores of the single NMT systems and the NMT - ensembles are relatively close , differing by less than 2 BLEU points .", "entities": [[24, 25, "MetricName", "BLEU"]]}
{"text": "According to the Wilcoxon test , these differences are statistically significant , at a 0.05 level , only for the s2s ( BLEU score ) and the transformer model ( both BLEU and NIST scores ) .", "entities": [[22, 24, "MetricName", "BLEU score"], [31, 32, "MetricName", "BLEU"]]}
{"text": "All three ensembles ( i.e. PAM - Wscore , PAM - Uscore and Marian - ensemble ) have statistically superior scores to Marian ( optimised with ce - mean ) for both BLEU and NIST , at a significance level of 0.01 .", "entities": [[32, 33, "MetricName", "BLEU"]]}
{"text": "Results are shown in Table 5 , where the accuracy of each transformer NMT is expressed as a fraction of the Marian - ensemble score .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "The best single transformer model achieves for testset1 88.7 % of the baseline BLEU score and 93.1 % of the NIST score .", "entities": [[13, 15, "MetricName", "BLEU score"]]}
{"text": "Using the Wscore ensembling method , this rises to 90.7 % for BLEU and 95.2 % for NIST , showing a gain of 2 % .", "entities": [[12, 13, "MetricName", "BLEU"]]}
{"text": "Turning to dataset2 , the single transformer scores just 70.5 % in comparison to the baseline BLEU score and 73.4 % of the NIST score ( therefore it is 27 % to 30 % lower ) .", "entities": [[16, 18, "MetricName", "BLEU score"]]}
{"text": "The Wscore ensemble improves relative scores , reaching 92.7 % and 94.5 % of the baseline scores for BLEU and NIST respectively .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "For instance , when transformer NMT models are tasked to translate testset1 , the BLEU - optimised NMT generates 26 ungrammatical words , the entropyoptimised NMT generates 24 ungrammatical words and the cross - entropy optimised model produces 23 ungrammatical words .", "entities": [[14, 15, "MetricName", "BLEU"]]}
{"text": "We found that the BLEU scores between the machine - translated and post - edited sentences were 63.30 for KorNLI and 73.26 for KorSTS , and for approximately half the time ( 47 % for KorNLI and 53 % for KorSTS ) , the translators did not have to change the machinetranslated sentence at all .", "entities": [[4, 5, "MetricName", "BLEU"], [19, 20, "DatasetName", "KorNLI"], [23, 24, "DatasetName", "KorSTS"], [35, 36, "DatasetName", "KorNLI"], [40, 41, "DatasetName", "KorSTS"]]}
{"text": "In Table 5 , we report the test set scores for crossencoding models fine - tuned on KorNLI ( accuracy ) and KorSTS ( Spearman correlation ) .", "entities": [[17, 18, "DatasetName", "KorNLI"], [19, 20, "MetricName", "accuracy"], [22, 23, "DatasetName", "KorSTS"], [24, 26, "MetricName", "Spearman correlation"]]}
{"text": "In Table 6 , we present the KorSTS test set scores ( 100 \u00d7 Spearman correlation ) for the biencoding models .", "entities": [[7, 8, "DatasetName", "KorSTS"], [14, 16, "MetricName", "Spearman correlation"]]}
{"text": "Fortunately , it was also pointed out in ( Conneau et al , 2018 ) that annotators could recover the NLI labels at a similar accuracy in translated pairs ( 83 % in French ) as in original pairs ( 85 % in English ) .", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "However , our left - to - right approach is simpler than the original top - down stack - pointer parser ( not requiring a stack ) and reduces transition sequence length in half , from 2n \u2212 1 actions to n. This results in a quadratic non - projective parser that runs twice as fast as the original while achieving the best accuracy to date on the English PTB dataset ( 96.04 % UAS , 94.43 % LAS ) among fully - supervised singlemodel dependency parsers , and improves over the former top - down transition system in the majority of languages tested .", "entities": [[63, 64, "MetricName", "accuracy"], [69, 70, "DatasetName", "PTB"]]}
{"text": "In the past two years , this kind of dependency parsers have been ahead in terms of accuracy thanks to the graph - based neural architecture developed by Dozat and Manning ( 2016 ) , which not only achieved state - of - the - art accuracies on the Stanford Dependencies conversion of the English Penn Treebank ( hereinafter , PTB - SD ) , but also obtained the best results in the majority of languages in the CoNLL 2017 Shared Task ( Dozat et al , 2017 ) .", "entities": [[17, 18, "MetricName", "accuracy"], [55, 57, "DatasetName", "Penn Treebank"], [60, 61, "DatasetName", "PTB"]]}
{"text": "Like ( Ma et al , 2018 ) , we report the average accuracy over 5 repetitions .", "entities": [[12, 14, "MetricName", "average accuracy"]]}
{"text": "The end result is that two independent attention layers attend to the ten snippets and ten emotional representations independently , and we call the resulting system Emotional Attention ( see Figure 3 ) .", "entities": [[7, 9, "HyperparameterName", "attention layers"]]}
{"text": "All four achieve greater F1 Macro scores than the baseline BERT model without pre - processing ( see Figure 2 ) .", "entities": [[4, 6, "MetricName", "F1 Macro"], [10, 11, "MethodName", "BERT"]]}
{"text": "This required us to set a minimum cosine similarity threshold \u03b8 to determine if two words are connected , which we treat as a hyperparameter ( with values [ .6 , .7 , .8 , .9 ] ) .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "Compared to a baseline with unannotated training , this architecture increased the BLEU score of German to English film subtitle translation outputs by 1.61 points using named entity tags ; however , the BLEU score decreased by 0.38 points using part - of - speech tags .", "entities": [[12, 14, "MetricName", "BLEU score"], [33, 35, "MetricName", "BLEU score"], [40, 43, "DatasetName", "part - of"]]}
{"text": "Automatic neural NER systems have achieved accuracy exceeding 92 % F 1 scores in many languages and domains ( Wang et al , 2019 ; Akbik et al , 2018 ) .", "entities": [[2, 3, "TaskName", "NER"], [6, 7, "MetricName", "accuracy"]]}
{"text": "POS taggers have also achieved very high accuracy exceeding 98 % on public treebank datasets ( Akbik et al , 2018 ) .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Compared to an untagged baseline system on word - tokenized data , our tagged translation system improved the BLEU score by 1.61 points on German to English parallel film subtitles data tagged with publicly available pre - trained named entity recognition systems , while part - of - speech tagging decreased the score by 0.38 BLEU points .", "entities": [[18, 20, "MetricName", "BLEU score"], [38, 41, "TaskName", "named entity recognition"], [44, 50, "TaskName", "part - of - speech tagging"], [55, 56, "MetricName", "BLEU"]]}
{"text": "P ( token k | prefix ; src ) = softmax k ( T d i ) ( 4 ) P ( tag k | prefix ; src ) = softmax k ( \u03c4 d i ) ( 5 ) We model these probabilities independently ( 6 ) for the same data sparsity and model size reasons as the embeddings , and we can compute each pair probability and loss accordingly ( 7 ) .", "entities": [[10, 11, "MethodName", "softmax"], [30, 31, "MethodName", "softmax"], [69, 70, "MetricName", "loss"]]}
{"text": "L = \u2212 log P ( token | prefix ; src ) \u2212 log P ( tag | prefix ; src ) ( 7 ) This combined loss encourages the shared decoder state d i to model the correct tag identity so that it can be used by the token prediction layer to improve translation .", "entities": [[27, 28, "MetricName", "loss"]]}
{"text": "Training was done for 40 epochs at half precision with the optimizer known as Adam ( Kingma and Ba , 2015 ) with \u03b2 = ( 0.9 , 0.98 ) and an inverse square root learning schedule with maximum learning rate 5 \u00d7 10 \u22124 after 500 updates and decay 1 \u00d7 10 \u22124 .", "entities": [[11, 12, "HyperparameterName", "optimizer"], [14, 15, "MethodName", "Adam"], [23, 24, "HyperparameterName", "\u03b2"], [39, 41, "HyperparameterName", "learning rate"]]}
{"text": "Parameter updates occurred after every 8 , 192 token - tag pairs at most ( rounding off to complete sentences ) , with 30 % dropout and label smoothing of 0.1 on the training loss .", "entities": [[27, 29, "MethodName", "label smoothing"], [34, 35, "MetricName", "loss"]]}
{"text": "At inference time , a beam of 5 candidates was maintained , and the models were evaluated with their BLEU score on the token sequence only ( tagging accuracy was not evaluated due to the difficulty of establishing alignment ) .", "entities": [[19, 21, "MetricName", "BLEU score"], [28, 29, "MetricName", "accuracy"]]}
{"text": "BLEU scores from untagged and tagged translation experiments show an improvement from the use of NER tags ( Table 1 ) .", "entities": [[0, 1, "MetricName", "BLEU"], [15, 16, "TaskName", "NER"]]}
{"text": "Adding NER tags , the 3 baseline 4 enhanced baseline / ablation study 5 ablation study BLEU score on sentences containing some named entities improved by a larger margin , 3.07 points , presumably due to the tags ' assistance with translating those named entities .", "entities": [[1, 2, "TaskName", "NER"], [16, 18, "MetricName", "BLEU score"]]}
{"text": "We also note an improvement in the BLEU score on sentences containing no named entities , which increased by 1.14 points .", "entities": [[7, 9, "MetricName", "BLEU score"]]}
{"text": "These improvements averaged out to a net gain of 1.61 BLEU points on the entire test split .", "entities": [[10, 11, "MetricName", "BLEU"]]}
{"text": "We also evaluated a model trained with POS tags , but found a decrease in BLEU score ( Table 2 ) .", "entities": [[15, 17, "MetricName", "BLEU score"]]}
{"text": "Translation scores with POS tags decreased by 0.38 BLEU points .", "entities": [[0, 1, "TaskName", "Translation"], [8, 9, "MetricName", "BLEU"]]}
{"text": "Adding NER tags improved the results , adding 0.22 points to the BLEU score , with the improvement again coming largely from the target side tagging , and again showing a larger improvement on sentences with named entities than on those without ( Table 3 ) .", "entities": [[1, 2, "TaskName", "NER"], [12, 14, "MetricName", "BLEU score"]]}
{"text": "However , the reduced magnitude of these deltas to the range of 0.1 - 0.4 BLEU points suggests these are not significant changes to the translation performance , in the subword tokenization case .", "entities": [[15, 16, "MetricName", "BLEU"]]}
{"text": "Due to the conditional independence assumption , the cross - entropy loss ( 7 ) conveniently decomposes into separate terms for tokens and tags ( 8 ) , allowing us to measure the relative information content of each channel ( Table 5 ) .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "While adding tag information naturally increases the overall cross - entropy , as there are more possibilities to account for and to be predicted , restricting our attention only to the token loss shows that the token - level cross - entropy is consistently reduced from 2.000 ( base - 2 ) to 1.985 with NER tags or 1.972 for POS tags .", "entities": [[32, 33, "MetricName", "loss"], [55, 56, "TaskName", "NER"]]}
{"text": "However , on word tokenized data , the 1.61 point increase in BLEU score using named entity tags demonstrates that the proposed architecture is useful for improving translation outputs with automatic named entity recognition , while the 0.38 point decrease using part - of - speech tags indicates more difficulty in utilizing that tag information .", "entities": [[12, 14, "MetricName", "BLEU score"], [31, 34, "TaskName", "named entity recognition"], [41, 44, "DatasetName", "part - of"]]}
{"text": "We selected Multinomial Naive Bayes as the best model because it produced the highest AUC , Kappa and weighted average F - Score ( see Table 2 for a summary of results ) .", "entities": [[14, 15, "MetricName", "AUC"], [22, 23, "MetricName", "Score"]]}
{"text": "For disambiguation , while early work on dependency parsing focused on global linear models , e.g. , structured perceptron ( Collins , 2002 ) , recent work shows that deep learning techniques , e.g. , LSTM ( Hochreiter and Schmidhuber , 1997 ) , is able to significantly advance the state - of - the - art of the parsing accuracy .", "entities": [[7, 9, "TaskName", "dependency parsing"], [35, 36, "MethodName", "LSTM"], [60, 61, "MetricName", "accuracy"]]}
{"text": "Even a data - driven parsing system achieves a high in - domain accuracy , it usually performs rather poorly on the out - of - domain data ( Oepen et al , 2015 ) .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "We define the loss term as : max ( 0 , \u2206 ( G * , \u011c ) \u2212 SCORE ( G * )", "entities": [[3, 4, "MetricName", "loss"], [9, 10, "DatasetName", "0"]]}
{"text": "In particular , if a loss of a bilexical relation between two tokens is less than 0.05 , we would exclude the loss .", "entities": [[5, 6, "MetricName", "loss"], [22, 23, "MetricName", "loss"]]}
{"text": "The batch size is 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "Table 1 lists the parsing accuracy of our system as well as the best published results in the literature for comparison .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "Our parser significantly improves state - ofthe - art accuracy on three out of total four data sets from SemEval 2015 for English / Chinese parsing and the CCGBank data for Chinese parsing .", "entities": [[9, 10, "MetricName", "accuracy"], [28, 29, "DatasetName", "CCGBank"]]}
{"text": "+ ( 1 \u2212 q ) ln ( 1 \u2212 p \u03b8 ( d l ) ) ]", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": ", ( 5 ) in which \u03b8 indicates all the parameters in the RSN .", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "Thus , we use the conditional entropy loss ( Grandvalet and Bengio , 2005 ) , which reaches the maximum when p = 0.5 , to penalize close - boundary distribution of data points :", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "+ ( 1 \u2212 p \u03b8 ( du ) )", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "ln ( 1 \u2212 p \u03b8 ( du ) ) ] .", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "| | p \u03b8 ( d l , t1 , t2 ) ) ] , ( 7 ) in which D KL indicates the Kullback - Leibler divergence , p \u03b8 ( d l , t 1 , t 2 ) indicates a new distance estimation with perturbations t 1 and t 2 on both input instances respectively .", "entities": [[3, 4, "HyperparameterName", "\u03b8"], [30, 31, "HyperparameterName", "\u03b8"]]}
{"text": "Specifically , t 1 and t 2 are worst - case perturbations that maximize the KL divergence between p \u03b8 ( d l ) and p \u03b8 ( d l , t 1 , t 2 ) with a limited length .", "entities": [[19, 20, "HyperparameterName", "\u03b8"], [26, 27, "HyperparameterName", "\u03b8"]]}
{"text": "( Lu + \u03bbvLvu ) , ( 10 ) which treats auto - labeled data as labeled data but removes the virtual adversarial loss on the autolabeled data .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "We also impose L2 regularization on the convolutional layer and the FC layer , with parameters of 0.0002 and 0.001 respectively .", "entities": [[3, 5, "HyperparameterName", "L2 regularization"]]}
{"text": "For optimization , we use Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0001 , which is selected from { 0.1 , 0.01 , 0.001 , 0.0001 , 0.00001 } .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [16, 18, "HyperparameterName", "learning rate"]]}
{"text": "The batch size is 100 selected from { 25 , 50 , 100 } .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "Table 1 shows the experimental results , from which we can observe that : ( 1 ) RSN models outperform all baseline models on precision , recall , and F1 - score , among which Weakly - supervised RSN ( SN - L+CV ) achieves state - of - the - art performances .", "entities": [[29, 32, "MetricName", "F1 - score"]]}
{"text": "Here , m = ( m \u03b1 , m \u03c9 ) is a mention that denotes the start and end locations for the entity within the input tokens x , which term is commonly used for defining entities ( F\u00e9vry et al , 2020 ) .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "F F ( \u0124 l ) = \u03c3 ( \u0124 l W 1 ) W 2 , H l = LN ( \u0124 l + F F ( \u0124 l ) ) , where LN is a layer normalization ( Ba et al , 2016 ) , \u03c3 is an activation function ( Hendrycks and Gimpel , 2016 ) , W 2 R d \u00d7d and W 1 R d\u00d7d are weight matrices , and d is an intermediate hidden size .", "entities": [[37, 39, "MethodName", "layer normalization"], [50, 52, "HyperparameterName", "activation function"]]}
{"text": "\u03c3 ( \u0124 l W 1 ) W 2 , H l = \u0393 LN ( \u0124 l + F F ( \u0124 l ) )", "entities": [[13, 14, "HyperparameterName", "\u0393"]]}
{"text": "= [ \u03b3 1 , . . .", "entities": [[2, 3, "HyperparameterName", "\u03b3"]]}
{"text": "\u03b2 1 , . . .", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "For instance , in Figure 3 , \u03b3 and \u03b2 for token ' New ' are conditioned on the corresponding entity New_York .", "entities": [[7, 8, "HyperparameterName", "\u03b3"], [9, 10, "HyperparameterName", "\u03b2"]]}
{"text": "However , if tokens are not part of any entity ( e.g. , ' is ' ) , \u03b3 and \u03b2 for such tokens are fixed to 1 and 0 , respectively .", "entities": [[18, 19, "HyperparameterName", "\u03b3"], [20, 21, "HyperparameterName", "\u03b2"], [29, 30, "DatasetName", "0"]]}
{"text": "The easiest way is to retrieve the entity embedding of e , associated to the typical to - ken , from the entity memory E , and then use the retrieved entity embedding as the input to obtain \u03b3 and \u03b2 for every entity ( See Figure 3 ) .", "entities": [[38, 39, "HyperparameterName", "\u03b3"], [40, 41, "HyperparameterName", "\u03b2"]]}
{"text": "Formally , for each entity e E and its mention ( m \u03b1 , m \u03c9 ) M , v = EntEmbed ( e ) ( 2 ) \u03b3 j = 1 + h 1 ( v ) , \u03b2 j = h 2 ( v ) , \u03b3 j = 1 + h 3 ( v ) , \u03b2 j = h 4 ( v ) , m \u03b1 \u2264 j \u2264 m \u03c9 , where v is the retrieved entity embedding from the entity memory , h 1 , h 2 , h 3 , and h 4 are mutually independent Multi - Layer Perceptrons ( MLPs ) which return a zero vector 0 if e = e .", "entities": [[12, 13, "HyperparameterName", "\u03b1"], [28, 29, "HyperparameterName", "\u03b3"], [39, 40, "HyperparameterName", "\u03b2"], [48, 49, "HyperparameterName", "\u03b3"], [59, 60, "HyperparameterName", "\u03b2"], [69, 70, "HyperparameterName", "\u03b1"], [115, 116, "DatasetName", "0"]]}
{"text": "\u2212m \u03b1 +1", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "2 . Fine - Tuning + more params : A baseline with one more transformer layer at the end of the means and standard deviations of performances over five different runs with Exact Match / F1 score as a metric .", "entities": [[7, 8, "MetricName", "params"], [32, 34, "MetricName", "Exact Match"], [35, 37, "MetricName", "F1 score"]]}
{"text": "However , the experimental results in Table 1 and 2 show that increasing the parameters for PLM during fine - tuning ( + more params ) yields marginal performance improvements over naive fine - tuning .", "entities": [[24, 25, "MetricName", "params"]]}
{"text": "We first analyze the effect of feature modulation parameters ( i.e. , gamma and beta ) in transformers by ablating a subset of them in Table 3 , in which we observe that using both gamma and beta after both layer normalization on a transformer layer obtains the best performance .", "entities": [[12, 13, "HyperparameterName", "gamma"], [14, 15, "HyperparameterName", "beta"], [35, 36, "HyperparameterName", "gamma"], [37, 38, "HyperparameterName", "beta"], [40, 42, "MethodName", "layer normalization"]]}
{"text": "On the other hand , DAPT , while not suffering from performance loss , requires huge computational costs as it trains on 112 times larger data for further pre - training ( See Appendix B.3 for detailed explanations on training data ) .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "In terms of the functions h 1 , h 2 , h 3 , and h 4 in the KFM of Equation 2 , we use two linear layers with the ReLU ( Nair and Hinton , 2010 ) activation function , where the dimension is set to 768 .", "entities": [[31, 32, "MethodName", "ReLU"], [39, 41, "HyperparameterName", "activation function"]]}
{"text": "Specifically , we stack two GNN layers with the RELU activation function and also use the dropout with a probability of 0.1 .", "entities": [[9, 10, "MethodName", "RELU"], [10, 12, "HyperparameterName", "activation function"]]}
{"text": "For all experiments on extractive QA tasks , we fine - tune the Pre - trained Language Model ( PLM ) for 2 epochs with the weight decay of 0.01 , learning rate of 3e - 5 , maximum sequence length of 384 , batch size of 12 , linear learning rate decay of 0.06 warmup rate , and half precision ( Micikevicius et al , 2018 ) .", "entities": [[26, 28, "MethodName", "weight decay"], [31, 33, "HyperparameterName", "learning rate"], [44, 46, "HyperparameterName", "batch size"], [50, 52, "HyperparameterName", "learning rate"]]}
{"text": "For all experiments on NER tasks , we finetune the PLM for 20 epochs , where the learning rate is set to 5e - 5 , maximum sequence length is set to 128 , and batch size is set to 32 .", "entities": [[4, 5, "TaskName", "NER"], [17, 19, "HyperparameterName", "learning rate"], [35, 37, "HyperparameterName", "batch size"]]}
{"text": "We use AdamW ( Loshchilov and Hutter , 2019 ) as an optimizer using BERT - base as the PLM .", "entities": [[2, 3, "MethodName", "AdamW"], [12, 13, "HyperparameterName", "optimizer"], [14, 15, "MethodName", "BERT"]]}
{"text": "For the generative QA task in Table 7 , we finetune the T5 - small ( Raffel et al , 2020 ) for 4 epochs with the learning rate of 1e - 4 , maximum sequence length of 512 , and batch size of 64 .", "entities": [[12, 13, "MethodName", "T5"], [27, 29, "HyperparameterName", "learning rate"], [41, 43, "HyperparameterName", "batch size"]]}
{"text": "We also use the Adafactor ( Shazeer and Stern , 2018 ) optimizer .", "entities": [[4, 5, "MethodName", "Adafactor"], [12, 13, "HyperparameterName", "optimizer"]]}
{"text": "Instead of training with the same optimizer as in BERT for QA and NER , we instead use the independent AdamW optimizer with the learning rate of 1e - 4 and weight decay of 0.01 to train the KALA module with T5 .", "entities": [[6, 7, "HyperparameterName", "optimizer"], [9, 10, "MethodName", "BERT"], [13, 14, "TaskName", "NER"], [20, 21, "MethodName", "AdamW"], [21, 22, "HyperparameterName", "optimizer"], [24, 26, "HyperparameterName", "learning rate"], [31, 33, "MethodName", "weight decay"], [41, 42, "MethodName", "T5"]]}
{"text": "We use the weight decay of 0.01 , learning rate of 5e - 5 , maximum sequence length of 384 , batch size of 12 , and linear learning rate decay of 0.06 warmup rate , with a half - precision .", "entities": [[3, 5, "MethodName", "weight decay"], [8, 10, "HyperparameterName", "learning rate"], [21, 23, "HyperparameterName", "batch size"], [28, 30, "HyperparameterName", "learning rate"]]}
{"text": "In particular , the learning rate is set to 5e - 5 , batch size is set to 32 , and the maximum sequence length is set to 128 .", "entities": [[4, 6, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "batch size"]]}
{"text": "We also use AdamW ( Loshchilov and Hutter , 2019 ) as the optimizer for all experiments .", "entities": [[3, 4, "MethodName", "AdamW"], [13, 14, "HyperparameterName", "optimizer"]]}
{"text": "In the case of T5 - small for generative QA in Table 7 , we further pre - train the PLM for 4 epochs with the learning rate of 0.001 , batch size of 64 , maximum sequence length of 384 , and Adafac - tor ( Shazeer and Stern , 2018 ) optimizer .", "entities": [[4, 5, "MethodName", "T5"], [26, 28, "HyperparameterName", "learning rate"], [31, 33, "HyperparameterName", "batch size"], [53, 54, "HyperparameterName", "optimizer"]]}
{"text": "Regarding the setting of TAPT ( + RecAdam ) on all tasks , we follow the best setting in the original paper - sigmoid as an annealing function with annealing parameters : k = 0.5 , t 0 = 250 , and the pretraining coefficient of 5000 .", "entities": [[32, 34, "HyperparameterName", "k ="], [37, 38, "DatasetName", "0"]]}
{"text": "For both News and Medical domains , we further pre - train the BERT - base model for 50 epochs with the batch size of 64 , to match the similar computational cost used in Gururangan et al ( 2020 ) .", "entities": [[13, 14, "MethodName", "BERT"], [22, 24, "HyperparameterName", "batch size"]]}
{"text": "\u03b1 + 1 m", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "= m \u03b1 h l\u22121 i , ( 4 )", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "We also give the supervised retrieval loss ( ELLoss in F\u00e9vry et al ( 2020 ) ) , when training the EaE model .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "To see how much amount of value on gamma and beta is used to shift and scale the intermediate hidden representations in transformer layers , we visualize the modulation values , namely gamma and beta , in Figure 11 .", "entities": [[8, 9, "HyperparameterName", "gamma"], [10, 11, "HyperparameterName", "beta"], [32, 33, "HyperparameterName", "gamma"], [34, 35, "HyperparameterName", "beta"]]}
{"text": "We first observe that , as shown in Figure 11 , the distribution of values of gamma and beta approximately follow the Gaussian dis - tribution , with zero mean for beta and one mean for gamma .", "entities": [[16, 17, "HyperparameterName", "gamma"], [18, 19, "HyperparameterName", "beta"], [31, 32, "HyperparameterName", "beta"], [36, 37, "HyperparameterName", "gamma"]]}
{"text": "As shown in Table 6 , we confirm that the computational cost of our KALA with the full memory is similar to the cost of the more params baseline that uses one additional transformer layer on top of BERT - base .", "entities": [[27, 28, "MetricName", "params"], [38, 39, "MethodName", "BERT"]]}
{"text": "The weighted average F1 - score of Kannada , Malayalam , and Tamil language are 0.69 , 0.92 , and 0.76 respectively , ranked 6th , 6th , and 3rd .", "entities": [[3, 6, "MetricName", "F1 - score"]]}
{"text": "We set the learning rate as 2e - 5 , the maximum sequence length is 256 , and the gradient steps are set to 4 .", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}
{"text": "The batch size is set to 32 , as shown in table 2 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "After the evaluation by the organizer , we obtained the weighted average F1 - score in the three languages , as shown in table 3 .", "entities": [[12, 15, "MetricName", "F1 - score"]]}
{"text": "Our team 's F1 - score is 0.69 , ranked 6th place for the Kannada language .", "entities": [[3, 6, "MetricName", "F1 - score"]]}
{"text": "For the Malayalam language , our team 's F1 - score is 0.92 ranked 6th place , and for the Tamil language , our team 's F1 - score is 0.76 ranked 3rd place .", "entities": [[8, 11, "MetricName", "F1 - score"], [26, 29, "MetricName", "F1 - score"]]}
{"text": "In addition to the train and development sets , the data includes multiple test sets , some of which are adversarial : \u03b1 1 represents a standard test set that is both topically and lexically similar to the training data ; \u03b1 2 hypotheses are designed to be lexically adversarial 4 ; and \u03b1 3 tables are drawn from topics unavailable in the training set .", "entities": [[22, 23, "HyperparameterName", "\u03b1"], [41, 42, "HyperparameterName", "\u03b1"], [53, 54, "HyperparameterName", "\u03b1"]]}
{"text": "Since | T R | | T | , the extraction benefits larger tables ( especially in \u03b1 3 set ) which exceed the model 's token limit .", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "We set \u03b4 to 0.5 for all experiments .", "entities": [[2, 3, "HyperparameterName", "\u03b4"]]}
{"text": "The alignment based approach using SimAlign underperforms , especially on the \u03b1 1 and \u03b1 2 test sets .", "entities": [[11, 12, "HyperparameterName", "\u03b1"], [14, 15, "HyperparameterName", "\u03b1"]]}
{"text": "However , its performance on the \u03b1 3 data , with out of domain and longer tables , is competitive to other methods .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "( \u03c4 = 1 ) ) by 4.3 % ( \u03b1 1 ) , 2.5 % ( \u03b1 2 ) and 5.4 % ( \u03b1 3 ) absolute score .", "entities": [[10, 11, "HyperparameterName", "\u03b1"], [17, 18, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b1"]]}
{"text": "Since the table domains and the NLI reasoning involved for \u03b1 1 and \u03b1 2 are sim - ilar , so is their evidence extraction performance .", "entities": [[10, 11, "HyperparameterName", "\u03b1"], [13, 14, "HyperparameterName", "\u03b1"]]}
{"text": "However , the performance of \u03b1 3 , which contains out - of - domain and longer tables ( an average of thirteen rows , versus nine rows in \u03b1 1 and \u03b1 2 ) is relatively worse .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [29, 30, "HyperparameterName", "\u03b1"], [32, 33, "HyperparameterName", "\u03b1"]]}
{"text": "The unsupervised approaches are still 12.69 % ( \u03b1 1 ) , 13.49 % ( \u03b1 2 ) , and 19.81 % ( \u03b1 3 ) behind the human performance , highlighting the challenges of the task .", "entities": [[8, 9, "HyperparameterName", "\u03b1"], [15, 16, "HyperparameterName", "\u03b1"], [23, 24, "HyperparameterName", "\u03b1"]]}
{"text": "The best supervised model with Hard Negative ( 3\u00d7 ) sampling improves evidence extraction performance by 8.7 % ( \u03b1 1 ) , 10.8 % ( \u03b1 2 ) , and 4.2 % ( \u03b1 3 ) absolute score over the best unsupervised model , namely SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( \u03c4 = 1 ) ) .", "entities": [[19, 20, "HyperparameterName", "\u03b1"], [26, 27, "HyperparameterName", "\u03b1"], [34, 35, "HyperparameterName", "\u03b1"], [46, 47, "MethodName", "SimCSE"]]}
{"text": "The human oracle outperforms the best supervised model by 4.13 % ( \u03b1 1 ) and 2.65 % ( \u03b1 2 ) absolute scores - a smaller gap than the best unsupervised approach .", "entities": [[12, 13, "HyperparameterName", "\u03b1"], [19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "We also observe that the supervision does not benefit the \u03b1 3 set much , where the performance gap to humans is still about 15.95 % ( only 3.80 % improvement over unsupervised approach ) .", "entities": [[10, 11, "HyperparameterName", "\u03b1"]]}
{"text": "We suspect this is because of the distributional changes in \u03b1 3 set noted earlier .", "entities": [[10, 11, "HyperparameterName", "\u03b1"]]}
{"text": "Compared to the baseline DRR , our unsupervised DRR ( Re - Rank + Top - 2 ( \u03c4 = 1 ) ) performs similarly for \u03b1 2 , worse by 1.12 % on \u03b1 1 , and outperforms by 0.95 % on \u03b1 3 .", "entities": [[26, 27, "HyperparameterName", "\u03b1"], [34, 35, "HyperparameterName", "\u03b1"], [43, 44, "HyperparameterName", "\u03b1"]]}
{"text": "Using evidence extraction with the best supervised model , Hard Negative ( 3\u00d7 ) , trained on human - extracted ( Oracle ) rows results in 2.68 % ( \u03b1 1 ) , 3.93 % ( \u03b1 2 ) , and 4.04 % ( \u03b1 3 ) improvements against DRR .", "entities": [[29, 30, "HyperparameterName", "\u03b1"], [36, 37, "HyperparameterName", "\u03b1"], [44, 45, "HyperparameterName", "\u03b1"]]}
{"text": "The human oracle based evidence extraction leads to largest performance improvements of 3.05 % ( \u03b1 1 ) , 4.39 % ( \u03b1 2 ) , and 6.67 % ( \u03b1 3 ) over DRR .", "entities": [[15, 16, "HyperparameterName", "\u03b1"], [22, 23, "HyperparameterName", "\u03b1"], [30, 31, "HyperparameterName", "\u03b1"]]}
{"text": "On the \u03b1 3 set , Type - I and Type - II errors are substantially higher than \u03b1 1 and \u03b1 2 .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [18, 19, "HyperparameterName", "\u03b1"], [21, 22, "HyperparameterName", "\u03b1"]]}
{"text": "This highlights the fact that on the \u03b1 3 set , the model disagrees with with humans the most .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "Furthermore , the ratio of Type - II over Type - I errors is much higher for \u03b1 3 .", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "This indicates that the super - vised extraction model marks many irrelevant rows as evidence ( Type - II error ) for \u03b1 3 set .", "entities": [[22, 23, "HyperparameterName", "\u03b1"]]}
{"text": "The out - ofdomain origin of \u03b1 3 tables , as well as their larger size , might be one explanation for this poor performance .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "As evident from our experiments , relevant row supervision improves the evidence extraction , especially on \u03b1 1 and \u03b1 2 sets compared to unsupervised extraction .", "entities": [[16, 17, "HyperparameterName", "\u03b1"], [19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "As evident from 5.2 , the evidence extraction performance of outof - domain tables in \u03b1 3 needs further improvements , setting up a domain adaptation research question as future work .", "entities": [[15, 16, "HyperparameterName", "\u03b1"], [24, 26, "TaskName", "domain adaptation"]]}
{"text": "These settings can focus on various aspects of reasoning , such as perturbed premises for evidence selection , zeroshot transferability ( \u03b1 3 ) , counterfactual premises ( Jain et al , 2021 ) , and contrasting hypotheses \u03b1 2 .", "entities": [[21, 22, "HyperparameterName", "\u03b1"], [38, 39, "HyperparameterName", "\u03b1"]]}
{"text": "Figure 3 , 4 , and 5 compare the average F1 - score over three runs on the three test sets \u03b1 1 , \u03b1 2 and \u03b1 3 respectively .", "entities": [[10, 13, "MetricName", "F1 - score"], [21, 22, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b1"], [27, 28, "HyperparameterName", "\u03b1"]]}
{"text": "However , we also find that 20 % supervision is adequate for reasonably good evidence extraction with only < 5 % F1 - score gap with full supervision .", "entities": [[21, 24, "MetricName", "F1 - score"]]}
{"text": "For example , the dogwhistle \" remigration \" had the lowest F1 score for both the dimensionality reduced sentence representations ( 0.72 ) and the original sentence representations ( 0.85 ) , as well as the lowest IAA overall ( 0.74/0.55 ) , as can be seen in table 2 .", "entities": [[4, 5, "DatasetName", "dogwhistle"], [11, 13, "MetricName", "F1 score"]]}
{"text": "For example , \" enrich \" in table 4 reports the best defined clusters overall ( measured by a low DB score and high CH score ) , while only having a marginally greater F1 score ( 0.98/0.98 ) on the SVM task than \" suburban gang \" ( 0.98/0.97 ) .", "entities": [[34, 36, "MetricName", "F1 score"], [41, 42, "MethodName", "SVM"]]}
{"text": "Dice loss is based on the S\u00f8rensen - Dice coefficient ( Sorensen , 1948 ) or Tversky index ( Tversky , 1977 ) , which attaches similar importance to false positives and false negatives , and is more immune to the data - imbalance issue .", "entities": [[0, 2, "MethodName", "Dice loss"], [8, 9, "MetricName", "Dice"]]}
{"text": "To handle the first issue , we propose to replace CE or MLE with losses based on the S\u00f8rensen - Dice coefficient ( Sorensen , 1948 ) or Tversky index ( Tversky , 1977 ) .", "entities": [[20, 21, "MetricName", "Dice"]]}
{"text": "Pang et al ( 2019 ) proposed a novel method called IoU - balanced sampling and designed a ranking model to replace the conventional classification task with an average - precision loss to alleviate the class imbalance issue .", "entities": [[11, 15, "MethodName", "IoU - balanced sampling"], [31, 32, "MetricName", "loss"]]}
{"text": "Shen et al ( 2018 ) investigated the influence of Dice - based loss for multi - class organ segmentation using a dataset of abdominal CT volumes .", "entities": [[10, 11, "MetricName", "Dice"], [13, 14, "MetricName", "loss"]]}
{"text": "The vanilla cross entropy ( CE ) loss is given by : CE = \u2212 1 N i j { 0 , 1 } y", "entities": [[7, 8, "MetricName", "loss"], [20, 21, "DatasetName", "0"]]}
{"text": "\u03b1 i j { 0 , 1 } y", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [4, 5, "DatasetName", "0"]]}
{"text": "where \u03b1 i [ 0 , 1 ] may be set by the inverse class frequency or treated as a hyperparameter to set by cross validation .", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [4, 5, "DatasetName", "0"]]}
{"text": "Empirically , these two methods are not widely used due to the trickiness of selecting \u03b1 especially for multi - class classification tasks and that inappropriate selection can easily bias towards rare classes ( Valverde et al , 2017 ) .", "entities": [[15, 16, "HyperparameterName", "\u03b1"], [18, 22, "TaskName", "multi - class classification"]]}
{"text": "S\u00f8rensen - Dice coefficient ( Sorensen , 1948 ; Dice , 1945 ) , dice coefficient ( DSC ) for short , is an F1oriented statistic used to gauge the similarity of two sets .", "entities": [[2, 3, "MetricName", "Dice"], [9, 10, "MetricName", "Dice"]]}
{"text": "+ \u03b3 p 2 i1 + y 2 i1 + \u03b3 TL 1 \u2212 p i1 y i1", "entities": [[1, 2, "HyperparameterName", "\u03b3"], [10, 11, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b3 DSC 1 \u2212 2 ( 1\u2212p i1 )", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "i j { 0 , 1 } ( 1 \u2212 p ij ) \u03b3 log p ij DSC", "entities": [[3, 4, "DatasetName", "0"], [13, 14, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b3 ( 6 )", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b3 p 2 i1 + y 2 i1 + \u03b3 ( 7 )", "entities": [[1, 2, "HyperparameterName", "\u03b3"], [10, 11, "HyperparameterName", "\u03b3"]]}
{"text": "y 2 i1 + \u03b3 ( 8 )", "entities": [[4, 5, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b2 | B\\A | ( 9 )", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b2 = 0.5 .", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "1 N i 1 \u2212 pi1yi1 + \u03b3 pi1yi1", "entities": [[7, 8, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b1 pi1yi0 + \u03b2 pi0yi1 + \u03b3 ( 10 )", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [4, 5, "HyperparameterName", "\u03b2"], [7, 8, "HyperparameterName", "\u03b3"]]}
{"text": "The computation of F 1 score is actually as follows : The derivative of DSC approaches zero right after p exceeds 0.5 , and for the other losses , the derivatives reach 0 only if the probability is exactly 1 , which means they will push p to 1 as much as possible . F1 ( x i )", "entities": [[32, 33, "DatasetName", "0"], [54, 55, "MetricName", "F1"]]}
{"text": "+ \u03b3 ( 1 \u2212 p i1 )", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b3 ( 12 ) One can think ( 1 \u2212 p i1 ) as a weight associated with each example , which changes as training proceeds .", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "For SQuADv1.1 , our proposed method outperforms XLNet by +1.25 in terms of F1 score and +0.84 in terms of EM .", "entities": [[7, 8, "MethodName", "XLNet"], [13, 15, "MetricName", "F1 score"], [20, 21, "MetricName", "EM"]]}
{"text": "For SQuAD v2.0 , the proposed method achieves 87.65 on EM and 89.51 on F1 .", "entities": [[1, 2, "DatasetName", "SQuAD"], [10, 11, "MetricName", "EM"], [14, 15, "MetricName", "F1"]]}
{"text": "On QuoRef , the proposed method surpasses XLNet by +1.46 on EM and +1.41 on F1 .", "entities": [[1, 2, "DatasetName", "QuoRef"], [7, 8, "MethodName", "XLNet"], [11, 12, "MetricName", "EM"], [15, 16, "MetricName", "F1"]]}
{"text": "Specially , for + positive , DSC achieves minor improvements ( +0.05 F1 ) over DL .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "The highest F1 on Chinese OntoNotes4.0 is 84.67 when \u03b1 is set to 0.6 while for QuoRef , the highest F1 is 68.44 when \u03b1 is set to 0.4 .", "entities": [[2, 3, "MetricName", "F1"], [9, 10, "HyperparameterName", "\u03b1"], [16, 17, "DatasetName", "QuoRef"], [20, 21, "MetricName", "F1"], [24, 25, "HyperparameterName", "\u03b1"]]}
{"text": "= 1 \u2212 \u03b1 and thus we only list \u03b1 here .", "entities": [[3, 4, "HyperparameterName", "\u03b1"], [9, 10, "HyperparameterName", "\u03b1"]]}
{"text": "The linguistic and clinical content features improve predictive accuracy above the baselines , yielding a higher F1 score than the strongest baseline ( .67 compared to .59 ) .", "entities": [[8, 9, "MetricName", "accuracy"], [16, 18, "MetricName", "F1 score"]]}
{"text": "The retention of 56 features indicates that individual linguistic predictors provide a weak classification signal but , taken together , they complement each other in a way that ultimately provides a higher accuracy .", "entities": [[32, 33, "MetricName", "accuracy"]]}
{"text": "We define the similarity score between two sentences in [ 0 , 1 ] as follows : ssim ( s 1 , s 2 ) : = t s 1 msim ( t , s 2 ) 2 | s 1 | + t s 2 msim ( t , s 1 ) 2 | s 2 | To predict the semantic similarity score in [ 0 , 5 ] , we multiply ssim by 5 , however , this does not change our evaluation results because the Pearson correlation is scale invariant :", "entities": [[10, 11, "DatasetName", "0"], [17, 18, "TaskName", "ssim"], [61, 63, "TaskName", "semantic similarity"], [66, 67, "DatasetName", "0"], [73, 74, "TaskName", "ssim"], [88, 90, "MetricName", "Pearson correlation"]]}
{"text": "We trained the neural net until the error rate between two iterations did not change more than \u03b5 = 10 \u22125 .", "entities": [[17, 18, "HyperparameterName", "\u03b5"]]}
{"text": "f 1 is the common ROUGE ( Lin , 2004 ) metric used in automatic text summarization and f 2 is a modified version of the BLEU ( Papineni et", "entities": [[15, 17, "TaskName", "text summarization"], [26, 27, "MetricName", "BLEU"]]}
{"text": "For both documents s 1 and s 2 , we compute the topic distributions \u03b8 1 and \u03b8 2 and use the Hellinger distance to measure the similarity between the documents .", "entities": [[14, 15, "HyperparameterName", "\u03b8"], [17, 18, "HyperparameterName", "\u03b8"]]}
{"text": "This can be formally written as f 4 ( s 1 , s 2 ) = 1 \u2212 1 \u221a 2 k i=1 \u03b8 1", "entities": [[23, 24, "HyperparameterName", "\u03b8"]]}
{"text": "i \u2212 \u03b8 2 i 2 where k represents the number of learned LDA topics .", "entities": [[2, 3, "HyperparameterName", "\u03b8"], [13, 14, "MethodName", "LDA"]]}
{"text": "In the prediction phase of the al - gorithm , we select k = 100 nearest neighbors from the data sets provided from 2012 to 2015 .", "entities": [[12, 14, "HyperparameterName", "k ="]]}
{"text": "\u03b8 . DU AL ( q i , s j ) = 1 |", "entities": [[0, 1, "HyperparameterName", "\u03b8"]]}
{"text": "We choose the threshold value ( \u03b8 ) which gives the best accuracy on manually labelled balanced validation dataset consisting of 380 question and specification pairs .", "entities": [[6, 7, "HyperparameterName", "\u03b8"], [12, 13, "MetricName", "accuracy"], [17, 19, "DatasetName", "validation dataset"]]}
{"text": "In addition , Shen et al ( 2019 ) adopted a mixture of experts to diversify machine translation , where a minimum - loss predictor is assigned to each source input .", "entities": [[16, 18, "TaskName", "machine translation"], [23, 24, "MetricName", "loss"]]}
{"text": "z | x , G x ) , and train the model with the EM algorithm ( Dempster et al , 1977 ) .", "entities": [[14, 15, "MetricName", "EM"]]}
{"text": "Training proceeds via hard - EM can be written as : E - step : estimate the responsibilities of each expert r z 1 [ z = arg max z p ( y ,", "entities": [[5, 6, "MetricName", "EM"]]}
{"text": "z | x , G x ) ] using the current parameters \u03b8 ; M - step : update the parameters with gradients of the chosen expert ( r z = 1 ) from E - step .", "entities": [[12, 13, "HyperparameterName", "\u03b8"]]}
{"text": "Independently parameterizing each expert may exacerbate overfitting since the number of parameters increases linearly with the number of experts ( Shen et al , 2019 ) .", "entities": [[9, 12, "HyperparameterName", "number of parameters"]]}
{"text": "For model training , we used Adam with batch size of 60 , learning rate of 3e - 5 , L2 weight decay of 0.01 , learning rate warm up over the first 10 , 000 steps , and linear decay of learning rate .", "entities": [[6, 7, "MethodName", "Adam"], [8, 10, "HyperparameterName", "batch size"], [13, 15, "HyperparameterName", "learning rate"], [21, 23, "MethodName", "weight decay"], [26, 28, "HyperparameterName", "learning rate"], [42, 44, "HyperparameterName", "learning rate"]]}
{"text": "All Transformer - based methods were trained with 30 epochs , taken about 4 - 5 hours on the ComVE dataset and 7 - 9 hours on the \u03b1 - NLG dataset .", "entities": [[1, 2, "MethodName", "Transformer"], [28, 29, "HyperparameterName", "\u03b1"]]}
{"text": "The quality is measured by standard N - gram based metrics , including the BLEU score ( Papineni et al , 2002 ) and the ROUGE score ( Lin , 2004 ) .", "entities": [[14, 16, "MetricName", "BLEU score"]]}
{"text": "This measures the highest accuracy comparing the best hypothesis among the top - K with the target ( Vijayakumar et al , 2018 ) .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "MoKGE can further boost diversity by at least 1.57 % and 1.83 % on Self - BLEU - 3 and Self - BLEU - 4 , compared with the vanilla MoE methods .", "entities": [[16, 17, "MetricName", "BLEU"], [22, 23, "MetricName", "BLEU"]]}
{"text": "Specifically , on the ComVE dataset , MoKGE achieved the best performance on BLEU - 4 and ROUGE - L , and on the \u03b1 - NLG dataset , the perfor - mance gap between MoKGE and the best baseline method was always less than 0.5 % on BLEU - 4 . Ablation study .", "entities": [[13, 14, "MetricName", "BLEU"], [17, 20, "MetricName", "ROUGE - L"], [24, 25, "HyperparameterName", "\u03b1"], [48, 49, "MetricName", "BLEU"]]}
{"text": "Besides , integrating commonsense knowledge graph into the MoEbased generation model brought both quality and diversity improvement on the ComVE , but might sacrifice a little quality ( less than 0.5 % on BLEU - 4 ) on the \u03b1 - NLG dataset .", "entities": [[33, 34, "MetricName", "BLEU"], [39, 40, "HyperparameterName", "\u03b1"]]}
{"text": "As described in Tevet and Berant ( 2021 ) , existing automatic diversity metrics ( e.g. Self - BLEU ) perform worse than humans on the task of estimating content diversity , indicating a low correlation between metrics and human judgments .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "At training time , we use the same procedure , but with teacher forcing , allowing us to use the correct distractor tokens as targets for the cross - entropy loss ( see example training datapoints for one MCQ in Table 2 ) .", "entities": [[30, 31, "MetricName", "loss"]]}
{"text": "al , 2002 ) , ROUGE ( Lin , 2004 ) , METEOR ( Denkowski and Lavie , 2014 ) , CIDEr ( Vedantam et al , 2015 ) , became popular in NLG in recent years .", "entities": [[12, 13, "DatasetName", "METEOR"], [21, 22, "MetricName", "CIDEr"]]}
{"text": "Given that the purpose is to show that the data supports H 0 , we have set both the probability \u03b1 of type I errors and the probability \u03b2 of type II errors to be 0.05 .", "entities": [[12, 13, "DatasetName", "0"], [20, 21, "HyperparameterName", "\u03b1"], [28, 29, "HyperparameterName", "\u03b2"]]}
{"text": "( Faul et al , 2009 ) to calculate the required sample size for finding a medium effect size ( 0.5 ) and the given \u03b1 and \u03b2 , which turned out to be 54 subjects .", "entities": [[25, 26, "HyperparameterName", "\u03b1"], [27, 28, "HyperparameterName", "\u03b2"]]}
{"text": "The inter - annotator agreement ( IAA ) was estimated using Goodman - Kruskal 's \u03b3 ( Goodman and Kruskal , 1979 ) , specifically its multirater version \u03b3 N proposed by Kalpakchi and Boye ( 2021 ) .", "entities": [[15, 16, "HyperparameterName", "\u03b3"], [28, 29, "HyperparameterName", "\u03b3"]]}
{"text": "On the scale proposed by Rosenthal ( 1996 ) , we have found a very large agreement ( \u03b3 N = 0.85 , see Appendix D.2.2 for more details on IAA calculations ) .", "entities": [[18, 19, "HyperparameterName", "\u03b3"]]}
{"text": "Qiu et al ( 2020 ) trained a sequence - to - sequence ( seq2seq ) model with a number of attention layers .", "entities": [[14, 15, "MethodName", "seq2seq"], [21, 23, "HyperparameterName", "attention layers"]]}
{"text": "al ( 2020 ) also employed a seq2seq model , but with a hierarchical attention to capture the interaction between a text and a question , as well as semantic similarity loss .", "entities": [[7, 8, "MethodName", "seq2seq"], [29, 31, "TaskName", "semantic similarity"], [31, 32, "MetricName", "loss"]]}
{"text": "To evaluate the inter - annotator agreement ( IAA ) between the teachers , we have reformulated the problem into a ranking problem , where all accepted distractors were given the rank of 1 and those rejected - the rank of 2 . IAA was then estimated using Goodman - Kruskal 's \u03b3 ( Goodman and Kruskal , 1979 ) , specifically its multirater version \u03b3 N proposed by Kalpakchi and Boye ( 2021 ) .", "entities": [[52, 53, "HyperparameterName", "\u03b3"], [65, 66, "HyperparameterName", "\u03b3"]]}
{"text": "The resulting \u03b3 N equals to 0.85 , indicating a very large agreement on the scale proposed by Rosenthal ( 1996 ) .", "entities": [[2, 3, "HyperparameterName", "\u03b3"]]}
{"text": "Recall that distractors are said to be low frequency ( LF - DIS ) if they were chosen by less than 5 % of students .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "The Viterbi model was the best semi - supervised model , scoring a weighted F1 score of 92.23 % , whereas a fully supervised state - of - the - art BERT - based model scored 98.43 % .", "entities": [[14, 16, "MetricName", "F1 score"], [31, 32, "MethodName", "BERT"]]}
{"text": "Yirmibe\u015foglu and Eryigit ( 2018 ) tackled this task in a semi - supervised setup as well , where they used character n - gram language models trained on monolingual data to predict perplexity on the target word for classification .", "entities": [[33, 34, "MetricName", "perplexity"]]}
{"text": "They show that this obtains a micro - average F1 score of 92.9 % , compared to 95.6 % with a supervised CRF - model .", "entities": [[8, 10, "MetricName", "average F1"], [22, 23, "MethodName", "CRF"]]}
{"text": "We used eginhard 's implementation 5 of the Viterbi algorithm and modified the starting and transition probabilities to the values specified below , which were found to be optimal using grid search on the development set using the range of initial probabilities for English from 0.1 to 0.9 with step size 0.1 , transition probabilities for English from 0.05 to 0.95 with step size 0.05 .", "entities": [[49, 51, "HyperparameterName", "step size"], [62, 64, "HyperparameterName", "step size"]]}
{"text": "To evaluate the performance of our models , we use weighted F1 score 7 .", "entities": [[11, 13, "MetricName", "F1 score"]]}
{"text": "When inspecting the performances of the models per class ( see also Table 2 in the appendix ) , we found that , for the development dataset , all models have a better F1 score for English than for Spanish and , for the test dataset , the other way around .", "entities": [[33, 35, "MetricName", "F1 score"]]}
{"text": "The higher order n - grams performed better with around 12 % difference in validation weighted F1 score , as we can capture groups of letters that are representative for a language , e.g. ' tion ' in English and ' cion ' in Spanish .", "entities": [[16, 18, "MetricName", "F1 score"]]}
{"text": "For word n - grams , using tri - grams resulted in worse predictions than using bi - grams with around 11 % difference in validation weighted F1 score .", "entities": [[27, 29, "MetricName", "F1 score"]]}
{"text": "The fact that the oracle model has a 3 % higher weighted F1 score than the best model ( in validation data ) , suggests that there is room for improvement for the ensemble model with other methods than majority voting .", "entities": [[12, 14, "MetricName", "F1 score"]]}
{"text": "The overall performance of the models can also be slightly improved by a more complex method for the other class ( the existing rule - based method scored an F1 of 96.76 , see Table 2 in the appendix ) .", "entities": [[29, 30, "MetricName", "F1"]]}
{"text": "We found that most of the models achieved promising results , however , the Viterbi model performed the best with a weighted F1 score of 95.76 % on validation data and 92.23 % on test data ( RQ1 ) .", "entities": [[22, 24, "MetricName", "F1 score"]]}
{"text": "Even though the results were good , our models still underperformed compared to the supervised MaChAmp model , that scored 99.24 % weighted F1 score on validation data and 98.43 % on test data ( RQ2 ) .", "entities": [[23, 25, "MetricName", "F1 score"]]}
{"text": "Since most n - ary relations can be decomposed into a set of k - ary relations ( k = 1 , 2 ) which are implied by the n - ary relation , 2 we can easily acquire lower - arity facts by decomposing n - ary facts .", "entities": [[18, 20, "HyperparameterName", "k ="]]}
{"text": "Unary Facts : Given an n - ary fact r , ( e 1 , ... , e k ) O , we decompose it into a set of n unary facts { r ( k ) , e k : k = 1 , ... , n } , where r ( k ) is a tentative unary relation w.r.t .", "entities": [[41, 43, "HyperparameterName", "k ="]]}
{"text": "k , l = 1 , ... , n , k = l } , where r ( k , l ) is a tentative binary relation between the k - th and l - th argument of the original relation r. If r is a KB relation , we define binary relation r ( k , l ) as a new canonicalized relation .", "entities": [[10, 12, "HyperparameterName", "k ="]]}
{"text": "We define a score \u03b8 r , p for each lower - arity fact r , p O 1 \u222a O 2 , and minimize the following loss ( 3 ) for each arity i = 1 , 2 . Here , placeholder p refers to either an entity ( if r , p O 1 ) or an entity tuple ( if r , p O 2 ) , and we simply refer to both as entity tuple .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [27, 28, "MetricName", "loss"]]}
{"text": "( 3 ) The score of fact r , p is defined as \u03b8 r , p = v ( r ) T v ( p ; r ) .", "entities": [[13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "To predict n - ary facts of KB relation r R KB , we compute its score \u03b8 r , ( e 1 , ... , en ) by aggregating lower - arity scores as in ( 6 ) , where w ( ) r is a positive scalar weight defined for each KB relation which sum to one : k w ( k ) r +", "entities": [[17, 18, "HyperparameterName", "\u03b8"]]}
{"text": "k = l w ( k , l ) r = 1 .", "entities": [[0, 2, "HyperparameterName", "k ="]]}
{"text": "We can set al weights w ( k ) r and w ( k , l ) r to 1 / n 2 , or train these weights to give higher scores to positive n - ary facts by minimizing additional loss function L n .", "entities": [[41, 42, "MetricName", "loss"]]}
{"text": "L n = E r , p + O KB r , p \u2212 / O KB [ max ( 0 , 1 \u2212 \u03b8 r , p + + \u03b8 r , p \u2212 ) ] ( 7 )", "entities": [[20, 21, "DatasetName", "0"], [24, 25, "HyperparameterName", "\u03b8"], [30, 31, "HyperparameterName", "\u03b8"]]}
{"text": "The overall loss function is now L = L 1 + L 2 + \u03b1L n .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "By changing \u03b1 , we can balance the semisupervised effect of lower - arity universal schemas ( L 1 , L 2 ) and that of the supervision with n - ary relation labels ( L n ) .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "We compared the methods in the held - out evaluation as in ( Mintz et al , 2009 ) and report ( weighted ) mean average precision ( MAP ) .", "entities": [[25, 27, "MetricName", "average precision"], [28, 29, "DatasetName", "MAP"]]}
{"text": "U , B , and N stand for using the loss functions L 1 , L 2 , and \u03b1L n respectively .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "Here , \u03b1 = stands for optimizing L n instead of L 1 + L 2 + \u03b1L n .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "As shown in Figure 3 , \u03b1 = 1 achieved higher performance than \u03b1 = , showing that introducing lower - arity semi - supervised loss ( L 1 + L 2 ) improves the performance for dataset with few positive labels .", "entities": [[6, 7, "HyperparameterName", "\u03b1"], [13, 14, "HyperparameterName", "\u03b1"], [25, 26, "MetricName", "loss"]]}
{"text": "On the other hand , the lower performance of \u03b1 = 0 compared to \u03b1 = 0.1 , 1 suggests that information of higher - arity facts introduced from L n is benefitial for n - ary relation extraction .", "entities": [[9, 10, "HyperparameterName", "\u03b1"], [11, 12, "DatasetName", "0"], [14, 15, "HyperparameterName", "\u03b1"], [37, 39, "TaskName", "relation extraction"]]}
{"text": "We proposed a new method for cross - sentence nary relation extraction that decomposes sparse n - 12 For the proposed method , we set \u03b1 = 10 . 13", "entities": [[10, 12, "TaskName", "relation extraction"], [25, 26, "HyperparameterName", "\u03b1"]]}
{"text": "In this experiment , we conducted four experiments per each setting and set K = 10 .", "entities": [[13, 15, "HyperparameterName", "K ="]]}
{"text": "We adopted Adam ( Kingma and Ba , 2017 ) as the optimizer .", "entities": [[2, 3, "MethodName", "Adam"], [12, 13, "HyperparameterName", "optimizer"]]}
{"text": "We set the total number of epochs to 50 .", "entities": [[4, 7, "HyperparameterName", "number of epochs"]]}
{"text": "We set a mini - batch size to about 500 .", "entities": [[3, 7, "HyperparameterName", "mini - batch size"]]}
{"text": "We collected checkpoints at the interval of 5 epochs out of 45 epochs , in addition to the one with the lowest validation loss .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "For each checkpoint we collected during intermediate training , we ( 1 ) measured the ZPT accuracy and ( 2 ) finetuned it to obtain the F1 score for ZAR .", "entities": [[16, 17, "MetricName", "accuracy"], [26, 28, "MetricName", "F1 score"]]}
{"text": "Web News F1 F1 + MT 70.5 - 57.7 - + MT w/ masking 71.1 0.6 57.8 0.1 + MT w/ MLM", "entities": [[2, 3, "MetricName", "F1"], [3, 4, "MetricName", "F1"], [21, 22, "DatasetName", "MLM"]]}
{"text": "71.9 1.4 58.3 0.6 To dig into this question , we conducted an ablation study by introducing a model with token masking but without the corresponding loss function ( denoted as + MT w/ masking ) .", "entities": [[26, 27, "MetricName", "loss"]]}
{"text": "Jolly et al ( 2020 ) showed that an interpretationto - text model can be used with shuffling - based sampling techniques to generate diverse and novel paraphrases from small amounts of seed data , that improve accuracy when augmenting to the existing training data .", "entities": [[37, 38, "MetricName", "accuracy"]]}
{"text": "The model was trained using Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 5e - 4 and a gradient clipping of 1 .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [16, 18, "HyperparameterName", "learning rate"], [24, 26, "MethodName", "gradient clipping"]]}
{"text": "The training was stopped when the development loss did not improve for 5 epochs .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "We used greedy decoding and top - k sampling with k = 3 , 5 , 10 and \u03c4 = 1.0 , 2.0 .", "entities": [[10, 12, "HyperparameterName", "k ="]]}
{"text": "We set the model 's hidden dimension to 128 , used the 100 - dimensional GloVe embeddings ( Pennington et al , 2014 ) pretrained on Wikipedia , and trained the model without freezing embeddings using early stopping with a patience of 5 epochs by monitoring the development loss .", "entities": [[15, 17, "MethodName", "GloVe embeddings"], [36, 38, "MethodName", "early stopping"], [48, 49, "MetricName", "loss"]]}
{"text": "otherwise ( 2 ) Exact match The exact match score r measures if all the input slots and output slots exactly match ( Malandrakis et al , 2019 ) .", "entities": [[4, 6, "MetricName", "Exact match"], [7, 9, "MetricName", "exact match"]]}
{"text": "otherwise ( 4 ) F1 slot score The F1 slot score F 1 measures the set similarity between S and G using precision and recall which are defined for sets as follows .", "entities": [[4, 5, "MetricName", "F1"], [8, 9, "MetricName", "F1"]]}
{"text": "Table 5 : Downstream slot labeling F1 scores ( % ) .", "entities": [[6, 7, "MetricName", "F1"]]}
{"text": "In the few shot setup , our paraphrasing approach outperforms the CVAE seq2seq approach in 6 ( DE , ES , FR , HI , JA , ZH ) out of 8 languages in intent classification and overall obtains an improvement of 1.9 % intent classification accuracy across all target languages .", "entities": [[11, 12, "MethodName", "CVAE"], [12, 13, "MethodName", "seq2seq"], [34, 36, "TaskName", "intent classification"], [44, 46, "TaskName", "intent classification"], [46, 47, "MetricName", "accuracy"]]}
{"text": "In terms of slot F1 scores , we see mixed results with no clear best method ( baseline , oversampling and CVAE all result in 87.6 % F1 score ) .", "entities": [[4, 5, "MetricName", "F1"], [21, 22, "MethodName", "CVAE"], [27, 29, "MetricName", "F1 score"]]}
{"text": "Notably , the MT approach results in the lowest overall slot F1 score of just 84.8 % on average .", "entities": [[11, 13, "MetricName", "F1 score"]]}
{"text": "In terms of slot F1 scores , our paraphrasing approach and the baseline approach both result in almost similar overall scores ( 85.5 % and 85.4 % ) , both higher than the MT approach .", "entities": [[4, 5, "MetricName", "F1"]]}
{"text": "POS tagging experiment over 23 languages shows that PBoS improves accuracy compared in all but one language to the previous best models , often by a big margin .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "A word w of length l = | w | is a string made of l letters in \u0393 , i.e. w = c 1 c 2 . . .", "entities": [[18, 19, "HyperparameterName", "\u0393"]]}
{"text": "( 3 ) and ( 4 ) , we can compose vector representation for any word w \u0393 + as w = g", "entities": [[17, 18, "HyperparameterName", "\u0393"]]}
{"text": "Segw p g | w s g s. ( 5 ) Given a set of target pre - trained word vectors w * defined for words within a finite vocabulary W , our model can be trained by minimizing the mean square loss : minimize S 1 |", "entities": [[42, 43, "MetricName", "loss"]]}
{"text": "Results Affix prediction results in terms of macro precision , recall , and F1 score are shown in Table 2 .", "entities": [[13, 15, "MetricName", "F1 score"]]}
{"text": "We can see a definite advantage of PBoS at predicting most word affixes , where all the metrics boost about 0.4 and F1 almost doubles compared to BoS , providing evidence that PBoS is able to assign meaningful subword weights .", "entities": [[22, 23, "MetricName", "F1"]]}
{"text": "PBoS is trained 50 epochs using vanilla SGD with initial learning rate 1 and inverse square root decay .", "entities": [[7, 8, "MethodName", "SGD"], [10, 12, "HyperparameterName", "learning rate"]]}
{"text": "Results Table 5 shows the POS tagging accuracy over the 23 languages that appear in both Polyglot and UD .", "entities": [[7, 8, "MetricName", "accuracy"], [18, 19, "DatasetName", "UD"]]}
{"text": "Even with that , PBoS is able to achieve the best POS tagging accuracy in all but one language regardless of morphological types , OOV rates , and the number of training instances ( Appendix Table 12 ) .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "Particularly , PBoS improvement accuracy by greater than 0.1 for 9 languages .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "We use the POS tagging accuracy for English as criterion , and choose C = 70 . Table 12 lists some statistics of the datasets used in the POS tagging experiment .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "During training step , we adopt hinge loss to maximize the margin between positive graphs G + and negative graphs G \u2212 : loss = max { 0 , \u03bb \u2212 S ( q , G + ) + S ( q , G \u2212 ) } .", "entities": [[7, 8, "MetricName", "loss"], [23, 24, "MetricName", "loss"], [27, 28, "DatasetName", "0"]]}
{"text": "We tune the margin \u03bb in { 0.1 , 0.2 , 0.5 } , the ensemble threshold K in { 1 , 2 , 3 , 5 , 10 , + INF } , and the batch size B in { 16 , 32 , 64 } .", "entities": [[36, 38, "HyperparameterName", "batch size"]]}
{"text": "We set \u03bb = 0.5 , B = 32 , K = 3 for WebQ and K = 5 for CompQ , as reaching the highest average F 1 on the validation set of each dataset .", "entities": [[10, 12, "HyperparameterName", "K ="], [16, 18, "HyperparameterName", "K ="]]}
{"text": "For our work in our lab ( Orange - Deski\u00f1 ) we needed a robust dependency analysis for written French with the highest Labeled Attachment Score ( LAS ) 1 possible , using a wide range of dependency relations .", "entities": [[25, 26, "MetricName", "Score"]]}
{"text": "The \" mix \" is then trained with a hidden layer size of either 100 or 50 , but without word embeddings .", "entities": [[9, 12, "HyperparameterName", "hidden layer size"], [20, 22, "TaskName", "word embeddings"]]}
{"text": "Using the \" mix \" with 23 languages ( 3 ) resulted in the best weighted LAS , 35.2 % ( 35.3 % if using a hidden layer size of 50 ) .", "entities": [[26, 29, "HyperparameterName", "hidden layer size"]]}
{"text": "Our final macro - averaged LAS F1 score on the CoNLL 2017 UD Shared Task test data ( Nivre et al , 2017a ) was 68.61 % , ( 10th out of 33 ) 17 .", "entities": [[6, 8, "MetricName", "F1 score"], [12, 13, "DatasetName", "UD"]]}
{"text": "Experimental results also demonstrate that AS - SIST improves the joint goal accuracy of DST by up to 28.16 % on MultiWOZ 2.0 and 8.41 % on MultiWOZ 2.4 , compared to using only the vanilla noisy labels .", "entities": [[12, 13, "MetricName", "accuracy"], [21, 23, "DatasetName", "MultiWOZ 2.0"], [27, 29, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "Due to limited knowledge , crowdworkers can not annotate all the states with 100 % accuracy , which naturally incurs noisy labels ( Han et al , 2020a ) .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "Then , we can define the combined label as : v c t = \u03b1v t + ( 1 \u2212 \u03b1 ) \u1e7d t , where \u03b1 ( 0 \u2264 \u03b1 \u2264 1 ) L pri =", "entities": [[20, 21, "HyperparameterName", "\u03b1"], [26, 27, "HyperparameterName", "\u03b1"], [28, 29, "DatasetName", "0"], [30, 31, "HyperparameterName", "\u03b1"]]}
{"text": "Bt \u2212 log p ( v t | X t , s ) + ( 1 \u2212 \u03b1 ) ( s , \u1e7dt )", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "Bt \u2212 log p ( \u1e7d t | X t , s ) = \u03b1L pseudo + ( 1 \u2212 \u03b1 ) L vanilla , where L pseudo and L vanilla correspond to the training objective of using only the pseudo labels and using only the vanilla noisy labels , respectively .", "entities": [[20, 21, "HyperparameterName", "\u03b1"]]}
{"text": "The proof is presented in Appendix A. Theorem 1 indicates that if \u03b1 is set properly , the combined labels can approximate the unknown true labels more accurately .", "entities": [[12, 13, "HyperparameterName", "\u03b1"]]}
{"text": "The joint goal accuracy is 2 Despite this change , we still call the dataset MultiWOZ 2.0 in this paper for ease of exposition .", "entities": [[3, 4, "MetricName", "accuracy"], [15, 17, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "For all primary models , the parameter \u03b1 is set to 0.6 on MutliWOZ 2.0 and 0.4 on MultiWOZ 2.4 .", "entities": [[7, 8, "HyperparameterName", "\u03b1"], [18, 20, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "More concretely , compared to the results obtained using only the vanilla labels , AS - SIST improves the joint goal accuracy of SOM - DST , STAR , and AUX - DST on MultiWOZ 2.0 by 25.69 % , 25.82 % , and 28.16 % absolute gains , respectively .", "entities": [[21, 22, "MetricName", "accuracy"], [23, 24, "MethodName", "SOM"], [27, 28, "DatasetName", "STAR"], [34, 36, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "On MultiWOZ 2.4 , ASSIST also leads to 8.41 % , 5.79 % , and 7.77 % absolute joint goal accuracy gains .", "entities": [[1, 3, "DatasetName", "MultiWOZ 2.4"], [20, 21, "MetricName", "accuracy"]]}
{"text": "Another observation from Table 1 is that SOM - DST tends to show comparable or even higher joint turn accuracy compared to STAR and AUX - DST , although its performance is worse in terms of joint goal accuracy and slot accuracy .", "entities": [[7, 8, "MethodName", "SOM"], [19, 20, "MetricName", "accuracy"], [22, 23, "DatasetName", "STAR"], [38, 39, "MetricName", "accuracy"], [41, 42, "MetricName", "accuracy"]]}
{"text": "Here , we study the effects of \u03b1 by varying its value in the range of 0 to 1 with a step size of 0.1 .", "entities": [[7, 8, "HyperparameterName", "\u03b1"], [16, 17, "DatasetName", "0"], [21, 23, "HyperparameterName", "step size"]]}
{"text": "The best performance is achieved when \u03b1 is set to 0.6 on MultiWOZ 2.0 and 0.4 on MultiWOZ 2.4 .", "entities": [[6, 7, "HyperparameterName", "\u03b1"], [12, 14, "DatasetName", "MultiWOZ 2.0"], [17, 19, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "It is also noted that the performance difference between MultiWOZ 2.0 and MultiWOZ 2.4 dwindles away as \u03b1 increases .", "entities": [[9, 11, "DatasetName", "MultiWOZ 2.0"], [12, 14, "DatasetName", "MultiWOZ 2.4"], [17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "If the entire validation set is used , it achieves around 50 % joint goal accuracy and around 75 % joint turn accuracy .", "entities": [[15, 16, "MetricName", "accuracy"], [22, 23, "MetricName", "accuracy"]]}
{"text": "Dc [ v c t \u2212 v t 2 2 ] = E Dc [ \u03b1v t + ( 1 \u2212 \u03b1 ) \u1e7d t \u2212 v t 2 2 ]", "entities": [[21, 22, "HyperparameterName", "\u03b1"]]}
{"text": "E Dc [ \u03b1 ( v t \u2212 v t ) + ( 1 \u2212 \u03b1 ) ( \u1e7d t \u2212 v t ) 2 2 ] = \u03b1 2 E Dc [ v t \u2212 v t 2 2 ] + ( 1 \u2212 \u03b1 ) 2 E Dc [ \u1e7d t \u2212 v t 2 2 ] , where the last equality holds because of E Dc [ ( \u1e7d t \u2212 v t ) T ( v t \u2212 v t ) ]", "entities": [[3, 4, "HyperparameterName", "\u03b1"], [15, 16, "HyperparameterName", "\u03b1"], [28, 29, "HyperparameterName", "\u03b1"], [45, 46, "HyperparameterName", "\u03b1"]]}
{"text": "Xt Dn s S E Dc [ v c t \u2212 v t 2 2 ] = \u03b1 2 | D n |", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "Xt Dn s S E Dc [ v t \u2212 v t 2 2 ] + ( 1 \u2212 \u03b1 ) 2 | D n | | S |", "entities": [[19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "Xt Dn s S E Dc [ \u1e7d t \u2212 v t 2 2 ] = \u03b1 2 Yv + ( 1 \u2212 \u03b1 ) 2 Y\u1e7d .", "entities": [[16, 17, "HyperparameterName", "\u03b1"], [23, 24, "HyperparameterName", "\u03b1"]]}
{"text": "In both cases , AdamW ( Kingma and Ba , 2014 ) is adopted as the optimizer , and a linear schedule with warmup is created to adjust the learning rate dynamically .", "entities": [[4, 5, "MethodName", "AdamW"], [16, 17, "HyperparameterName", "optimizer"], [29, 31, "HyperparameterName", "learning rate"]]}
{"text": "The peak learning rate is set to 2.5e - 5 .", "entities": [[2, 4, "HyperparameterName", "learning rate"]]}
{"text": "When taken as the auxiliary model , the model is trained for at most 30 epochs with a batch size of 8 .", "entities": [[18, 20, "HyperparameterName", "batch size"]]}
{"text": "When taken as the primary model , the batch size and training epochs are set to 8 and 12 , respectively .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}
{"text": "The number of parameters in the model are approximately 248 Million and it takes \u223c26 hours on 4 Nvidia V100 ( 32 GB ) GPUs .", "entities": [[1, 4, "HyperparameterName", "number of parameters"]]}
{"text": "We use case - insensitive BLEU scores ( Papineni et al , 2002 ) calculated using sacreBLEU 2 ( Post , 2018 ) .", "entities": [[5, 6, "MetricName", "BLEU"], [16, 17, "MetricName", "sacreBLEU"]]}
{"text": "For evaluating performance on the test set , the organizers use BLEU , TER ( Snover et al , 2006 ) , and RIBES ( Isozaki et", "entities": [[11, 12, "MetricName", "BLEU"]]}
{"text": "The context given for each question - answer pair is split into sentences using the NLTK sentence tokenizer 7 , and the sentence - level accuracy of each of the models is computed ( Table 1 ) .", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "Also , all 5 models combined have an error overlap of 13.68 % , i.e. , if we had a mechanism to perfectly choose between these models , we would get an Exact Match score of 86.32 % .", "entities": [[32, 34, "MetricName", "Exact Match"]]}
{"text": "For qualitative error analysis , we sample 100 incorrect predictions ( based on EM ) from each model and try to find common error categories .", "entities": [[13, 14, "MetricName", "EM"]]}
{"text": "Synthetic errors are introduced into monolingual texts to mimic word error rate ( WER ) of about 15 % , i.e. p WER = 0.15 , which resembles error frequency in common ESL error corpora .", "entities": [[9, 15, "MetricName", "word error rate ( WER )"]]}
{"text": "We first compare the GEC systems with simple baselines using a greedy and context spell - checking ( Table 4 ) ; the latter selects the best correction suggestion based on the sentence perplexity from a Transformer language model .", "entities": [[33, 34, "MetricName", "perplexity"], [36, 37, "MethodName", "Transformer"]]}
{"text": "Our approach adds an L 2 distance loss between feature attributions and task - specific prior values to the objective .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "The core idea is to add L 2 distance between Path Integrated Gradients attributions for pre - selected tokens and a target attribution value in the objective function as a loss term .", "entities": [[30, 31, "MetricName", "loss"]]}
{"text": "For model fairness , we impose the loss on keywords identifying protected groups with target attribution of 0 , so the trained model is penalized for attributing model decisions to those keywords .", "entities": [[7, 8, "MetricName", "loss"], [17, 18, "DatasetName", "0"]]}
{"text": "\u03bb C c L prior ( a c , t c ) ( 3 ) where a c and t c are the attribution and attribution target for class c , \u03bb is the hyperparameter that controls the stength of the prior loss and L is the crossentropy loss defined as follows : L ( y , p )", "entities": [[42, 43, "MetricName", "loss"], [48, 49, "MetricName", "loss"]]}
{"text": "Then , we compare our method to a classifier trained without the prior loss and 2 other baselines .", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "Number of samples are 95 , 692 / 32 , 128 / 31 , 866 in the train / dev / test sets respectively .", "entities": [[0, 3, "HyperparameterName", "Number of samples"]]}
{"text": "We set dropout as 0.2 and used Adam ( Kingma and Ba , 2015 ) as our optimizer with initial learning rate set to 0.001 .", "entities": [[7, 8, "MethodName", "Adam"], [17, 18, "HyperparameterName", "optimizer"], [20, 22, "HyperparameterName", "learning rate"]]}
{"text": "We did n't perform extensive network architecture search to improve the performance as it is a reasonably strong classifier with the initial performance of 95.5 % accuracy .", "entities": [[26, 27, "MetricName", "accuracy"]]}
{"text": "Training takes 1 minute for a model with cross - entropy loss and 30 minutes for a model with joint loss on an NVidia V100 GPU .", "entities": [[11, 12, "MetricName", "loss"], [20, 21, "MetricName", "loss"]]}
{"text": "However , reducing the step size in IG for calculating Riemann approximation of the integral to 10 steps reduces the training time to 6 minutes .", "entities": [[4, 6, "HyperparameterName", "step size"]]}
{"text": "We compare our work to 3 models with the same CNN architecture , but different training settings : Baseline : A baseline classifier trained with cross - entropy loss .", "entities": [[28, 29, "MetricName", "loss"]]}
{"text": "Importance : Classifier trained with crossentropy loss , but the loss for samples containing identity words are weighted in the range ( 1 , 10 8 ) , where the actual coefficient is determined to be 10 on the dev set based on F - 1 score .", "entities": [[6, 7, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}
{"text": "We first verify that the prior loss term does not adversely affect overall classifier performance on the main task using general performance metrics such as accuracy and F - 1 .", "entities": [[6, 7, "MetricName", "loss"], [25, 26, "MetricName", "accuracy"]]}
{"text": "We use AUC along with False Positive Equality Difference ( FPED ) and False Negative Equality Difference ( FNED ) , which measure a proxy of Equality of Odds ( Hardt et al , 2016 ) , as in Dixon et al ( 2018 ) ;", "entities": [[2, 3, "MetricName", "AUC"]]}
{"text": "To directly validate the effectiveness of prior loss on attributions , we first show that the attribution of the toxic words have higher values for our method across different data ratios compared to the baseline in Table 8 .", "entities": [[7, 8, "MetricName", "loss"]]}
